{"type": "initialize_logs", "logs": [{"messages": [{"role": "user", "content": "Hello"}, {"role": "assistant", "content": "Hello"}], "input_metadata": {"row_id": "1"}, "rollout_status": {"status": "finished"}, "created_at": "2025-08-08T09:52:30.335005"}, {"messages": [{"role": "user", "content": "Hello 0"}], "input_metadata": {"row_id": "row_0"}, "rollout_status": {"status": "finished"}, "created_at": "2025-08-08T09:47:43.816761"}, {"messages": [{"role": "user", "content": "Hello 1"}], "input_metadata": {"row_id": "row_1"}, "rollout_status": {"status": "finished"}, "created_at": "2025-08-08T09:41:55.015216"}, {"messages": [{"role": "user", "content": "Hello 2"}], "input_metadata": {"row_id": "row_2"}, "rollout_status": {"status": "finished"}, "created_at": "2025-08-08T09:41:55.015767"}, {"messages": [{"role": "system", "content": "You are a helpful assistant that can answer questions about Fireworks.\nALWAYS provide code or commands to execute to answer the question."}, {"role": "user", "content": "Can you teach me about how to manage deployments on Fireworks"}], "input_metadata": {"row_id": "describe-use-entire-person-education", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct"}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "created_at": "2025-08-08T10:00:08.522553", "eval_metadata": {"name": "test_pytest_mcp_url", "description": "Run math evaluation on sample dataset using pytest interface.", "version": "v0.2.6.dev12+g2dc40d5.dirty", "status": "stopped", "num_runs": 1, "aggregation_method": "mean"}, "pid": 67635}, {"messages": [{"role": "system", "content": "You are a helpful assistant that can answer questions about Fireworks.\nALWAYS provide code or commands to execute to answer the question."}, {"role": "user", "content": "Can you teach me about how to manage deployments on Fireworks"}], "input_metadata": {"row_id": "meet-solid-big-picture-car", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct"}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "created_at": "2025-08-08T10:01:12.825337", "eval_metadata": {"name": "test_pytest_mcp_url", "description": "Run math evaluation on sample dataset using pytest interface.", "version": "v0.2.6.dev12+g2dc40d5.dirty", "status": "stopped", "num_runs": 1, "aggregation_method": "mean"}, "pid": 68229}, {"messages": [{"role": "system", "content": "You are a helpful assistant that can answer questions about Fireworks.\nALWAYS provide code or commands to execute to answer the question."}, {"role": "user", "content": "Can you teach me about how to manage deployments on Fireworks"}, {"role": "assistant", "content": "I'll help you learn about managing deployments on Fireworks AI. Let me search for comprehensive information about deployment management.", "tool_calls": [{"id": "functions.search:0", "function": {"arguments": "{\"query\": \"deployment management Fireworks AI\"}", "name": "search"}, "type": "function"}]}, {"role": "tool", "content": [{"text": "Title: Fireworks AI Developer Platform\nLink: https://docs.fireworks.ai/getting-started/introduction\nContent: Fireworks AI is the best platform for building AI product experiences with open source AI models. You can run and customize AI models with just a few lines of code!\n\nFireworks AI is the best platform for building AI product experiences with open source AI models. You can run and customize AI models with just a few lines of code!\n\nStart building with open source AI models\n\n", "type": "text"}, {"text": "Title: How do I close my Fireworks.ai account?\nLink: https://docs.fireworks.ai/faq-new/account-access/how-do-i-close-my-fireworksai-account\nContent: To close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed\n\nTo close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed\n\nHow do I close my Fireworks.ai account?\nTo close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed\n\n", "type": "text"}, {"text": "Title: Do you host your deployments in the EU or Asia?\nLink: https://docs.fireworks.ai/faq-new/support-general/do-you-host-your-deployments-in-the-eu-or-asia\nContent: We are currently deployed in multiple U.S.-based locations. However, we\u2019re open to hearing more about your specific requirements. You can: Join our Discord community Write to us at inquiries@fireworks.ai If you're an Enterprise customer, please contact your dedicated customer support representative to ensure a timely response.\n\nWe are currently deployed in multiple U.S.-based locations. However, we\u2019re open to hearing more about your specific requirements. You can: Join our Discord community Write to us at inquiries@fireworks.ai If you're an Enterprise customer, please contact your dedicated customer support representative to ensure a timely response.\n\nDo you host your deployments in the EU or Asia?\nWe are currently deployed in multiple U.S.-based locations. However, we\u2019re open to hearing more about your specific requirements. You can: Join our Discord community Write to us at inquiries@fireworks.ai If you're an Enterprise customer, please contact your dedicated customer support representative to ensure a timely response.\n\n", "type": "text"}, {"text": "Title: Introducing the Fireworks Build SDK\nLink: https://docs.fireworks.ai/tools-sdks/python-client/sdk-introduction\nContent: The Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.\n\nThe Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.\n\nIntroducing the Fireworks Build SDK\nThe Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.\n\n", "type": "text"}, {"text": "Title: On-demand deployments\nLink: https://docs.fireworks.ai/guides/ondemand-deployments\nContent: If you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.\n\nIf you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.\n\nOn-demand deployments\nIf you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.\n\n", "type": "text"}, {"text": "Title: I have multiple Fireworks accounts. When I try to login with Google on Fireworks' web UI, I'm getting signed into the wrong account. How do I fix this?\nLink: https://docs.fireworks.ai/faq-new/account-access/i-have-multiple-fireworks-accounts-when-i-try-to-login-with-google-on-fireworks\nContent: If you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.\n\nIf you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.\n\nI have multiple Fireworks accounts. When I try to login with Google on Fireworks' web UI, I'm getting signed into the wrong account. How do I fix this?\nIf you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.\n\n", "type": "text"}, {"text": "Title: Introduction\nLink: https://docs.fireworks.ai/api-reference/introduction\nContent: Fireworks AI REST API enables you to interact with various language, image and embedding models using an API Key. It also lets you automate management of models, deployments, datasets, and more.\n\nFireworks AI REST API enables you to interact with various language, image and embedding models using an API Key. It also lets you automate management of models, deployments, datasets, and more.\n\nIntroduction\nFireworks AI REST API enables you to interact with various language, image and embedding models using an API Key. It also lets you automate management of models, deployments, datasets, and more.\n\n", "type": "text"}, {"text": "Title: Create a deployment\nLink: https://docs.fireworks.ai/tools-sdks/firectl/commands/create-deployment\nContent: Create a Deployment on Fireworks AI platform\n\nCreate a Deployment on Fireworks AI platform\n\nCreates a new deployment.\n\n", "type": "text"}, {"text": "Title: How much does Fireworks cost?\nLink: https://docs.fireworks.ai/faq-new/billing-pricing/how-much-does-fireworks-cost\nContent: Fireworks AI operates on a pay-as-you-go model for all non-Enterprise usage, and new users automatically receive free credits. You pay based on: Per token for serverless inference Per GPU usage time for on-demand deployments Per token of training data for fine-tuning For customers needing enterprise-grade security and reliability , please reach out to us at inquiries@fireworks.ai to discuss options. Find out more about our current pricing on our Pricing page.\n\nFireworks AI operates on a pay-as-you-go model for all non-Enterprise usage, and new users automatically receive free credits. You pay based on: Per token for serverless inference Per GPU usage time for on-demand deployments Per token of training data for fine-tuning For customers needing enterprise-grade security and reliability , please reach out to us at inquiries@fireworks.ai to discuss options. Find out more about our current pricing on our Pricing page.\n\nHow much does Fireworks cost?\nFireworks AI operates on a pay-as-you-go model for all non-Enterprise usage, and new users automatically receive free credits. You pay based on: Per token for serverless inference Per GPU usage time for on-demand deployments Per token of training data for fine-tuning For customers needing enterprise-grade security and reliability , please reach out to us at inquiries@fireworks.ai to discuss options. Find out more about our current pricing on our Pricing page.\n\n", "type": "text"}, {"text": "Title: Python (Fireworks)\nLink: https://docs.fireworks.ai/getting-started/quickstart\nContent: This is the recommended way to get started : This  Python (Fireworks)  tab uses our  Fireworks Build\nSDK  for the best performance and developer experience.\nSee our  Client-side performance\noptimization  guide for\nmore details. Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. pip install --upgrade fireworks-ai The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either  ~/.bash_profile  for Bash or  ~/.zshrc  for  Zsh .\nYou can do this by running the command: vim ~/.bash_profile vim ~/.zshrc Add a new line to the file with the following: export FIREWORKS_API_KEY=\"<API_KEY>\" After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. source ~/.bash_profile source ~/.zshrc You can verify that the variable has been set correctly by running  echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. setx FIREWORKS_API_KEY \"<API_KEY>\" To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: echo %FIREWORKS_API_KEY% You can quickly instantiate the  LLM  class and call the Fireworks API. The Build SDK handles deployment management automatically. from fireworks import LLM\n\n# Basic usage - SDK automatically selects optimal deployment type\nllm = LLM(model=\"llama4-maverick-instruct-basic\", deployment_type=\"auto\")\n\nresponse = llm.chat.completions.create(\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}]\n)\n\nprint(response.choices[0].message.content) You can also pass the API key directly to the LLM constructor:  LLM(model=\"llama4-maverick-instruct-basic\", deployment_type=\"auto\", api_key=\"<FIREWORKS_API_KEY>\")\n\nThis is the recommended way to get started : This  Python (Fireworks)  tab uses our  Fireworks Build\nSDK  for the best performance and developer experience.\nSee our  Client-side performance\noptimization  guide for\nmore details. Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. pip install --upgrade fireworks-ai The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either  ~/.bash_profile  for Bash or  ~/.zshrc  for  Zsh .\nYou can do this by running the command: vim ~/.bash_profile vim ~/.zshrc Add a new line to the file with the following: export FIREWORKS_API_KEY=\"<API_KEY>\" After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. source ~/.bash_profile source ~/.zshrc You can verify that the variable has been set correctly by running  echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. setx FIREWORKS_API_KEY \"<API_KEY>\" To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: echo %FIREWORKS_API_KEY% You can quickly instantiate the  LLM  class and call the Fireworks API. The Build SDK handles deployment management automatically. from fireworks import LLM\n\n# Basic usage - SDK automatically selects optimal deployment type\nllm = LLM(model=\"llama4-maverick-instruct-basic\", deployment_type=\"auto\")\n\nresponse = llm.chat.completions.create(\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}]\n)\n\nprint(response.choices[0].message.content) You can also pass the API key directly to the LLM constructor:  LLM(model=\"llama4-maverick-instruct-basic\", deployment_type=\"auto\", api_key=\"<FIREWORKS_API_KEY>\")\n\nSet up your developer environment & call the Fireworks API\nThis is the recommended way to get started : This  tab uses our Fireworks Build\nSDK for the best performance and developer experience.\nSee our Client-side performance\noptimization guide for\nmore details. Install SDK Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Configure API Key Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either ~/.bash_profile for Bash or ~/.zshrc for Zsh .\nYou can do this by running the command: Add a new line to the file with the following: After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. You can verify that the variable has been set correctly by running echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: Sending the first API Request You can quickly instantiate the LLM class and call the Fireworks API. The Build SDK handles deployment management automatically. You can also pass the API key directly to the LLM constructor: LLM(model=\"llama4-maverick-instruct-basic\", deployment_type=\"auto\", api_key=\"<FIREWORKS_API_KEY>\") Install SDK Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. Fireworks AI platform offers drop-in replacement with OpenAI Python Client. Configure API Key Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either ~/.bash_profile for Bash or ~/.zshrc for Zsh .\nYou can do this by running the command: Add a new line to the file with the following: After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. You can verify that the variable has been set correctly by running echo $OPENAI_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. setx OPENAI_API_BASE \"https://api.fireworks.ai/inference/v1\"\nsetx OPENAI_API_KEY \"<API_KEY>\"\n\n", "type": "text"}], "tool_call_id": "functions.search:0"}, {"role": "assistant", "content": "Let me search for more specific information about deployment management tools and commands.", "tool_calls": [{"id": "functions.search:1", "function": {"arguments": "{\"query\": \"firectl deployment management commands CLI\"}", "name": "search"}, "type": "function"}]}, {"role": "tool", "content": [{"text": "Title: Why am I getting \"invalid id\" errors when using firectl commands like create deployment or list deployments?\nLink: https://docs.fireworks.ai/faq-new/fine-tuning/why-am-i-getting-invalid-id-errors-when-using-firectl-commands-like-create-deplo\nContent: This error typically occurs when your account ID is not properly configured.\n\nThis error typically occurs when your account ID is not properly configured.\n\nWhy am I getting \"invalid id\" errors when using firectl commands like create deployment or list deployments?\nThis error typically occurs when your account ID is not properly configured.\n\n", "type": "text"}, {"text": "Title: Installing the CLI\nLink: https://docs.fireworks.ai/guides/ondemand-deployments\nContent: To create and manage on-demand deployments, use our CLI tool firectl . Install the tool by following the installation guide . Verify that your installation is working and you are logged into the correct account by running: firectl whoami\n\nTo create and manage on-demand deployments, use our CLI tool firectl . Install the tool by following the installation guide . Verify that your installation is working and you are logged into the correct account by running: firectl whoami\n\nTo create and manage on-demand deployments, use our CLI tool firectl . Install the tool by following the installation guide . Verify that your installation is working and you are logged into the correct account by running:\n\n", "type": "text"}, {"text": "Title: Cli overview\nLink: https://docs.fireworks.ai/evaluators/cli_reference/cli_overview\nContent: Command Line Interface Reference\nThe Reward Kit provides a command-line interface (CLI) for common operations like previewing evaluations, deploying reward functions, and running agent evaluations.\n\nCommand Line Interface Reference\nThe Reward Kit provides a command-line interface (CLI) for common operations like previewing evaluations, deploying reward functions, and running agent evaluations.\n\nCommand Overview\nThe Reward Kit CLI supports the following main commands: run : Run a local evaluation pipeline using a Hydra configuration. preview : Preview evaluation results or re-evaluate generated outputs. deploy : Deploy a reward function as an evaluator. agent-eval : Run agent evaluations on task bundles. list : List existing evaluators (coming soon). delete : Delete an evaluator (coming soon).\n\n", "type": "text"}, {"text": "Title: Using the CLI (reward-kit deploy)\nLink: https://docs.fireworks.ai/evaluators/developer_guide/evaluation_workflows\nContent: The reward-kit deploy command is suitable for deploying reward functions defined in script files. The --metrics-folders argument should point to the directory containing your reward function script (e.g., a main.py with the @reward_function decorator). For more details on reward-kit deploy , see the CLI Reference.\n\nThe reward-kit deploy command is suitable for deploying reward functions defined in script files. The --metrics-folders argument should point to the directory containing your reward function script (e.g., a main.py with the @reward_function decorator). For more details on reward-kit deploy , see the CLI Reference.\n\nThe reward-kit deploy command is suitable for deploying reward functions defined in script files. The --metrics-folders argument should point to the directory containing your reward function script (e.g., a main.py with the @reward_function decorator). # Deploy with the CLI\nreward-kit deploy \\\n  --id helpfulness-evaluator \\\n  --metrics-folders \"helpfulness=./path/to/your/metric_script_directory\" \\\n  --display-name \"Helpfulness Evaluator\" \\\n  --description \"Evaluates the helpfulness of responses\" \\\n  --force\n\n", "type": "text"}, {"text": "Title: Does Fireworks support custom base models?\nLink: https://docs.fireworks.ai/faq-new/models-inference/does-fireworks-support-custom-base-models\nContent: Yes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.\n\nYes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.\n\nDoes Fireworks support custom base models?\nYes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.\n\n", "type": "text"}, {"text": "Title: \ud83d\ude80 Easier & faster LoRA fine-tune deployments on Fireworks\nLink: https://docs.fireworks.ai/updates/changelog\nContent: You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment \"accounts/fireworks/models/<MODEL_ID of lora model>\"\n\nYou can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment \"accounts/fireworks/models/<MODEL_ID of lora model>\"\n\nYou can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: Previously, this involved two distinct steps, and the resulting deployment was slower than the base model: Create a deployment using firectl create deployment \"accounts/fireworks/models/<MODEL_ID of base model>\" --enable-addons Then deploy the addon to the deployment: firectl load-lora <MODEL_ID> --deployment <DEPLOYMENT_ID> For more information, see our deployment documentation. This change is for dedicated deployments with a single LoRA. You can still deploy multiple LoRAs on a deployment or deploy LoRA(s) on some Serverless models as described in the documentation.\n\n", "type": "text"}, {"text": "Title: Quick deployment\nLink: https://docs.fireworks.ai/fine-tuning/single-lora\nContent: Deploy your LoRA fine-tuned model with one simple command: firectl create deployment \"accounts/fireworks/models/<MODEL_ID of lora model>\"\n\nDeploy your LoRA fine-tuned model with one simple command: firectl create deployment \"accounts/fireworks/models/<MODEL_ID of lora model>\"\n\nAlternative deployment method\nThis two-step method is the standard approach for multi-LoRA deployments where multiple LoRA models share the same base model. While it can also be used for single LoRA deployments, it provides slower performance compared to live merge and is not recommended for single LoRA use cases. You can also deploy single LoRA models using a two-step process: Create base model deployment Deploy the base model with addons enabled: firectl create deployment \"accounts/fireworks/models/<MODEL_ID of base model>\" --enable-addons\n\n", "type": "text"}, {"text": "Title: 1. Setup\nLink: https://docs.fireworks.ai/tools-sdks/python-client/the-tutorial\nContent: To get started with the Fireworks AI Python SDK, you need to install the firectl CLI tool and create an API key. Install our CLI tool firectl to interact with the Fireworks AI platform. Sign in to Fireworks by running the following command: firectl signin\n\nTo get started with the Fireworks AI Python SDK, you need to install the firectl CLI tool and create an API key. Install our CLI tool firectl to interact with the Fireworks AI platform. Sign in to Fireworks by running the following command: firectl signin\n\nTo get started with the Fireworks AI Python SDK, you need to install the firectl CLI tool and create an API key. Install our CLI tool firectl to interact with the Fireworks AI platform. Sign in to Fireworks by running the following command: A browser window will open to the Fireworks AI login page. Once you login, your machine will be authenticated. Create an API key by running the following command: $ firectl create api-key --key-name \"quick-start\"\nKey Id: key_42vAYeb7rwt9zzg1\nDisplay Name: quick-start\nKey: fw_3ZLd....\nSecure: true\nBe sure to save this key. It will not be shown again.\n\n", "type": "text"}, {"text": "Title: Deploying a fine-tuned model using an on-demand deployment\nLink: https://docs.fireworks.ai/fine-tuning/fine-tuning-models\nContent: Use the following command to deploy your fine-tuned model using an on-demand deployment: firectl create deployment <FINE_TUNED_MODEL_ID>\n\nUse the following command to deploy your fine-tuned model using an on-demand deployment: firectl create deployment <FINE_TUNED_MODEL_ID>\n\nDeploying a fine-tuned model serverlessly\nNot all base models support serverless addons. Please check this list for the full list of serverless models that support LoRA add-ons. If your base model supports serverless addons, you can run the following command to load the fine-tuned model into a serverless deployment: firectl load-lora <FINE_TUNED_MODEL_ID>\n\n", "type": "text"}, {"text": "Title: Create a deployment\nLink: https://docs.fireworks.ai/tools-sdks/firectl/commands/create-deployment\nContent: Creates a new deployment. firectl create deployment [flags]\n\nCreates a new deployment. firectl create deployment [flags]\n\nExample\nfirectl create deployment falcon-7b\n\n", "type": "text"}], "tool_call_id": "functions.search:1"}], "input_metadata": {"row_id": "leave-ask-logical-face-way", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct"}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "created_at": "2025-08-08T11:06:51.152819", "eval_metadata": {"name": "test_pytest_mcp_url", "description": "Run math evaluation on sample dataset using pytest interface.", "version": "v0.2.6.dev15+g6486dbd", "status": "stopped", "num_runs": 1, "aggregation_method": "mean"}, "pid": 83882}, {"messages": [{"role": "system", "content": "You are a helpful assistant that can answer questions about Fireworks.\nALWAYS provide code or commands to execute to answer the question."}, {"role": "user", "content": "Can you teach me about how to manage deployments on Fireworks"}], "input_metadata": {"row_id": "investigate-recent-dangerous-hope-magazine", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct"}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "created_at": "2025-08-08T11:10:27.844496", "eval_metadata": {"name": "test_pytest_mcp_url", "description": "Run math evaluation on sample dataset using pytest interface.", "version": "v0.2.6.dev15+g6486dbd", "status": "stopped", "num_runs": 1, "aggregation_method": "mean"}, "pid": 84847}, {"messages": [{"role": "system", "content": "You are a helpful assistant that can answer questions about Fireworks.\nALWAYS provide code or commands to execute to answer the question."}, {"role": "user", "content": "Can you teach me about how to manage deployments on Fireworks"}], "input_metadata": {"row_id": "seem-environmental-media-television-concert", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct"}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "created_at": "2025-08-08T11:12:00.215991", "eval_metadata": {"name": "test_pytest_mcp_url", "description": "Run math evaluation on sample dataset using pytest interface.", "version": "v0.2.6.dev15+g6486dbd.dirty", "status": "stopped", "num_runs": 1, "aggregation_method": "mean"}, "pid": 85545}, {"messages": [{"role": "system", "content": "You are a helpful assistant that can answer questions about Fireworks.\nALWAYS provide code or commands to execute to answer the question."}, {"role": "user", "content": "Can you teach me about how to manage deployments on Fireworks"}], "input_metadata": {"row_id": "treat-suppose-salty-week-plan", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct"}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "created_at": "2025-08-08T11:12:34.382113", "eval_metadata": {"name": "test_pytest_mcp_url", "description": "Run math evaluation on sample dataset using pytest interface.", "version": "v0.2.6.dev15+g6486dbd.dirty", "status": "stopped", "num_runs": 1, "aggregation_method": "mean"}, "pid": 85754}, {"messages": [{"role": "system", "content": "You are a helpful assistant that can answer questions about Fireworks.\nALWAYS provide code or commands to execute to answer the question."}, {"role": "user", "content": "Can you teach me about how to manage deployments on Fireworks"}], "input_metadata": {"row_id": "contain-recent-bad-plan-change", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct"}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "created_at": "2025-08-08T11:12:46.750720", "eval_metadata": {"name": "test_pytest_mcp_url", "description": "Run math evaluation on sample dataset using pytest interface.", "version": "v0.2.6.dev15+g6486dbd.dirty", "status": "stopped", "num_runs": 1, "aggregation_method": "mean"}, "pid": 85851}, {"messages": [{"role": "system", "content": "You are a helpful assistant that can answer questions about Fireworks.\nALWAYS provide code or commands to execute to answer the question."}, {"role": "user", "content": "Can you teach me about how to manage deployments on Fireworks"}, {"role": "assistant", "content": "I'll help you learn about managing deployments on Fireworks! Let me search for comprehensive information about deployment management.", "tool_calls": [{"id": "functions.search:0", "function": {"arguments": "{\"query\": \"deployments management create update delete scale\"}", "name": "search"}, "type": "function"}]}, {"role": "tool", "content": [{"text": "Title: Delete Deployment\nLink: https://docs.fireworks.ai/api-reference/delete-deployment\nContent: \n\n\n\nREST Endpoint DELETE /v1/accounts/{account_id}/deployments/{deployment_id}\n\nServer(s): https://api.fireworks.ai\n\n\nSummary:  \n\nDescription: N/A\n\nSecurity methods: \nheader: \n\tAuthorization: bearer auth\n\t\n\nRequest params: \npath params:\naccount_id:\n\tstring\n\tDescription: The Account Id\n\tExample: \"<string>\"\ndeployment_id:\n\tstring\n\tDescription: The Deployment Id\n\tExample: \"<string>\"\n\nquery params:\nhard:\n\tboolean\n\tDescription: If true, this will perform a hard deletion.\n\tExample: true\nignoreChecks:\n\tboolean\n\tDescription: If true, this will ignore checks and force the deletion of a deployment that is currently\ndeployed and is in use.\n\tExample: true\n\n\nRequest body: N/A\n\n\nResponses: \n200: object\n\tDescription: N/A\n\tExample: {}\n\n", "type": "text"}, {"text": "Title: How does billing and scaling work for on-demand GPU deployments?\nLink: https://docs.fireworks.ai/faq-new/deployment-infrastructure/how-does-billing-and-scaling-work-for-on-demand-gpu-deployments\nContent: On-demand GPU deployments have unique billing and scaling characteristics compared to serverless deployments: Billing: Charges start when the server begins accepting requests Billed by GPU-second for each active instance Costs accumulate even if there are no active API calls Scaling options: Supports autoscaling from 0 to multiple GPUs Each additional GPU adds to the billing rate Can handle unlimited requests within the GPU\u2019s capacity Management requirements: Not fully serverless; requires some manual management Manually delete deployments when no longer needed Or configure autoscaling to scale down to 0 during inactive periods Cost control tips: Regularly monitor active deployments Delete unused deployments to avoid unnecessary costs Consider serverless options for intermittent usage Use autoscaling to 0 to optimize costs during low-demand times\n\nOn-demand GPU deployments have unique billing and scaling characteristics compared to serverless deployments: Billing: Charges start when the server begins accepting requests Billed by GPU-second for each active instance Costs accumulate even if there are no active API calls Scaling options: Supports autoscaling from 0 to multiple GPUs Each additional GPU adds to the billing rate Can handle unlimited requests within the GPU\u2019s capacity Management requirements: Not fully serverless; requires some manual management Manually delete deployments when no longer needed Or configure autoscaling to scale down to 0 during inactive periods Cost control tips: Regularly monitor active deployments Delete unused deployments to avoid unnecessary costs Consider serverless options for intermittent usage Use autoscaling to 0 to optimize costs during low-demand times\n\nHow does billing and scaling work for on-demand GPU deployments?\nOn-demand GPU deployments have unique billing and scaling characteristics compared to serverless deployments: Billing: Charges start when the server begins accepting requests Billed by GPU-second for each active instance Costs accumulate even if there are no active API calls Scaling options: Supports autoscaling from 0 to multiple GPUs Each additional GPU adds to the billing rate Can handle unlimited requests within the GPU\u2019s capacity Management requirements: Not fully serverless; requires some manual management Manually delete deployments when no longer needed Or configure autoscaling to scale down to 0 during inactive periods Cost control tips: Regularly monitor active deployments Delete unused deployments to avoid unnecessary costs Consider serverless options for intermittent usage Use autoscaling to 0 to optimize costs during low-demand times\n\n", "type": "text"}, {"text": "Title: Managing default deployments\nLink: https://docs.fireworks.ai/guides/ondemand-deployments\nContent: Since a model may be deployed to multiple deployments, querying by model name will route to the \"default\" deployed model. You can see which deployed model entry is marked with Default: true using: This will show the Deployed Model Refs section with the Default: true entry. To update the default deployed model, note the Name of the deployed model reference above. Then run: To delete a default deployment you must delete all other deployments for the same model first, or designate a different deployed model as the default as described above. This is to ensure that querying by model name will always route to an unambiguous default deployment as long as deployments for the model exist.\n\nSince a model may be deployed to multiple deployments, querying by model name will route to the \"default\" deployed model. You can see which deployed model entry is marked with Default: true using: This will show the Deployed Model Refs section with the Default: true entry. To update the default deployed model, note the Name of the deployed model reference above. Then run: To delete a default deployment you must delete all other deployments for the same model first, or designate a different deployed model as the default as described above. This is to ensure that querying by model name will always route to an unambiguous default deployment as long as deployments for the model exist.\n\nSince a model may be deployed to multiple deployments, querying by model name will route to the \"default\" deployed model. You can see which deployed model entry is marked with Default: true using: This will show the Deployed Model Refs section with the Default: true entry. To update the default deployed model, note the Name of the deployed model reference above. Then run: firectl update deployed-model <DEPLOYED_MODEL_ID_2> --default\n\n", "type": "text"}, {"text": "Title: Create a deployment\nLink: https://docs.fireworks.ai/tools-sdks/firectl/commands/create-deployment\nContent: Creates a new deployment.\n\nCreates a new deployment.\n\nCreates a new deployment. firectl create deployment [flags]\n\n", "type": "text"}, {"text": "Title: Delete a deployment\nLink: https://docs.fireworks.ai/tools-sdks/firectl/commands/delete-resources\nContent: Deletes an model deployment.\n\nDeletes an model deployment.\n\nDeletes an model deployment. firectl delete deployment [flags]\n\n", "type": "text"}, {"text": "Title: Update Deployment\nLink: https://docs.fireworks.ai/api-reference/update-deployment\nContent: \n\n\n\nREST Endpoint PATCH /v1/accounts/{account_id}/deployments/{deployment_id}\n\nServer(s): https://api.fireworks.ai\n\n\nSummary:  \n\nDescription: N/A\n\nSecurity methods: \nheader: \n\tAuthorization: bearer auth\n\t\n\nRequest params: \npath params:\naccount_id:\n\tstring\n\tDescription: The Account Id\n\tExample: \"<string>\"\ndeployment_id:\n\tstring\n\tDescription: The Deployment Id\n\tExample: \"<string>\"\n\n\nRequest body: object\n\tDescription: N/A\n\tExample: {\"displayName\":\"<string>\",\"description\":\"<string>\",\"expireTime\":\"2023-11-07T05:31:56Z\",\"minReplicaCount\":123,\"maxReplicaCount\":123,\"autoscalingPolicy\":{\"scaleUpWindow\":\"<string>\",\"scaleDownWindow\":\"<string>\",\"scaleToZeroWindow\":\"<string>\",\"loadTargets\":{}},\"baseModel\":\"<string>\",\"acceleratorCount\":123,\"acceleratorType\":\"ACCELERATOR_TYPE_UNSPECIFIED\",\"precision\":\"PRECISION_UNSPECIFIED\",\"enableAddons\":true,\"draftTokenCount\":123,\"draftModel\":\"<string>\",\"ngramSpeculationLength\":123,\"deploymentTemplate\":\"<string>\",\"autoTune\":{\"longPrompt\":true},\"placement\":{\"region\":\"REGION_UNSPECIFIED\",\"multiRegion\":\"MULTI_REGION_UNSPECIFIED\",\"regions\":[\"REGION_UNSPECIFIED\"]},\"disableDeploymentSizeValidation\":true}\n\n\nResponses: \n200: object\n\tDescription: N/A\n\tExample: {\"name\":\"<string>\",\"displayName\":\"<string>\",\"description\":\"<string>\",\"createTime\":\"2023-11-07T05:31:56Z\",\"expireTime\":\"2023-11-07T05:31:56Z\",\"purgeTime\":\"2023-11-07T05:31:56Z\",\"deleteTime\":\"2023-11-07T05:31:56Z\",\"state\":\"STATE_UNSPECIFIED\",\"status\":{\"code\":\"OK\",\"message\":\"<string>\"},\"minReplicaCount\":123,\"maxReplicaCount\":123,\"replicaCount\":123,\"autoscalingPolicy\":{\"scaleUpWindow\":\"<string>\",\"scaleDownWindow\":\"<string>\",\"scaleToZeroWindow\":\"<string>\",\"loadTargets\":{}},\"baseModel\":\"<string>\",\"acceleratorCount\":123,\"acceleratorType\":\"ACCELERATOR_TYPE_UNSPECIFIED\",\"precision\":\"PRECISION_UNSPECIFIED\",\"cluster\":\"<string>\",\"enableAddons\":true,\"draftTokenCount\":123,\"draftModel\":\"<string>\",\"ngramSpeculationLength\":123,\"numPeftDeviceCached\":123,\"deploymentTemplate\":\"<string>\",\"autoTune\":{\"longPrompt\":true},\"placement\":{\"region\":\"REGION_UNSPECIFIED\",\"multiRegion\":\"MULTI_REGION_UNSPECIFIED\",\"regions\":[\"REGION_UNSPECIFIED\"]},\"region\":\"REGION_UNSPECIFIED\",\"updateTime\":\"2023-11-07T05:31:56Z\",\"disableDeploymentSizeValidation\":true}\n\n", "type": "text"}, {"text": "Title: What should I expect for deployment and scaling performance?\nLink: https://docs.fireworks.ai/faq-new/deployment-infrastructure/what-should-i-expect-for-deployment-and-scaling-performance\nContent: Initial deployment : Should complete within minutes Scaling from zero : You may experience brief availability delays while the system scales up Troubleshooting : If deployment takes over 1 hour, this typically indicates a crash and should be investigated Best practice : Monitor deployment status and contact support if deployment times are unusually long\n\nInitial deployment : Should complete within minutes Scaling from zero : You may experience brief availability delays while the system scales up Troubleshooting : If deployment takes over 1 hour, this typically indicates a crash and should be investigated Best practice : Monitor deployment status and contact support if deployment times are unusually long\n\nWhat should I expect for deployment and scaling performance?\nInitial deployment : Should complete within minutes Scaling from zero : You may experience brief availability delays while the system scales up Troubleshooting : If deployment takes over 1 hour, this typically indicates a crash and should be investigated Best practice : Monitor deployment status and contact support if deployment times are unusually long\n\n", "type": "text"}, {"text": "Title: delete_deployment(ignore_checks: bool = False, wait: bool = True)\nLink: https://docs.fireworks.ai/tools-sdks/python-client/sdk-reference\nContent: Deletes the deployment associated with this LLM instance if one exists. Arguments: ignore_checks bool, optional - Whether to ignore safety checks. Defaults to False. wait bool, optional - Whether to wait for deletion to complete. Defaults to True. llm.delete_deployment(ignore_checks=True)\n\nDeletes the deployment associated with this LLM instance if one exists. Arguments: ignore_checks bool, optional - Whether to ignore safety checks. Defaults to False. wait bool, optional - Whether to wait for deletion to complete. Defaults to True.\n\nscale_to_zero()\nSends a request to scale the deployment to 0 replicas but does not wait for it to complete. Returns: The deployment object, or None if no deployment exists deployment = llm.scale_to_zero()\n\n", "type": "text"}, {"text": "Title: How does billing work for on-demand deployments?\nLink: https://docs.fireworks.ai/faq-new/deployment-infrastructure/how-does-billing-work-for-on-demand-deployments\nContent: On-demand deployments come with automatic cost optimization features: Default autoscaling : Automatically scales to 0 replicas when not in use Pay for what you use : Charged only for GPU time when replicas are active Flexible configuration : Customize autoscaling behavior to match your needs Best practices for cost management: Leverage default autoscaling : The system automatically scales down deployments when not in use Customize carefully : While you can modify autoscaling behavior using our configuration options , note that preventing scale-to-zero will result in continuous GPU charges Consider your use case : For intermittent or low-frequency usage, serverless deployments might be more cost-effective For detailed configuration options, see our deployment guide.\n\nOn-demand deployments come with automatic cost optimization features: Default autoscaling : Automatically scales to 0 replicas when not in use Pay for what you use : Charged only for GPU time when replicas are active Flexible configuration : Customize autoscaling behavior to match your needs Best practices for cost management: Leverage default autoscaling : The system automatically scales down deployments when not in use Customize carefully : While you can modify autoscaling behavior using our configuration options , note that preventing scale-to-zero will result in continuous GPU charges Consider your use case : For intermittent or low-frequency usage, serverless deployments might be more cost-effective For detailed configuration options, see our deployment guide.\n\nHow does billing work for on-demand deployments?\nOn-demand deployments come with automatic cost optimization features: Default autoscaling : Automatically scales to 0 replicas when not in use Pay for what you use : Charged only for GPU time when replicas are active Flexible configuration : Customize autoscaling behavior to match your needs Best practices for cost management: Leverage default autoscaling : The system automatically scales down deployments when not in use Customize carefully : While you can modify autoscaling behavior using our configuration options , note that preventing scale-to-zero will result in continuous GPU charges Consider your use case : For intermittent or low-frequency usage, serverless deployments might be more cost-effective For detailed configuration options, see our deployment guide.\n\n", "type": "text"}, {"text": "Title: Create Deployment\nLink: https://docs.fireworks.ai/api-reference/create-deployment\nContent: \n\n\n\nREST Endpoint POST /v1/accounts/{account_id}/deployments\n\nServer(s): https://api.fireworks.ai\n\n\nSummary:  \n\nDescription: N/A\n\nSecurity methods: \nheader: \n\tAuthorization: bearer auth\n\t\n\nRequest params: \npath params:\naccount_id:\n\tstring\n\tDescription: The Account Id\n\tExample: \"<string>\"\n\nquery params:\ndisableAutoDeploy:\n\tboolean\n\tDescription: By default, a deployment created with a currently undeployed base model\nwill be deployed to this deployment. If true, this auto-deploy function\nis disabled.\n\tExample: true\ndisableSpeculativeDecoding:\n\tboolean\n\tDescription: By default, a deployment will use the speculative decoding settings from\nthe base model. If true, this will disable speculative decoding.\n\tExample: true\ndeploymentId:\n\tstring\n\tDescription: The ID of the deployment. If not specified, a random ID will be generated.\n\tExample: \"<string>\"\n\n\nRequest body: object\n\tDescription: N/A\n\tExample: {\"displayName\":\"<string>\",\"description\":\"<string>\",\"expireTime\":\"2023-11-07T05:31:56Z\",\"minReplicaCount\":123,\"maxReplicaCount\":123,\"autoscalingPolicy\":{\"scaleUpWindow\":\"<string>\",\"scaleDownWindow\":\"<string>\",\"scaleToZeroWindow\":\"<string>\",\"loadTargets\":{}},\"baseModel\":\"<string>\",\"acceleratorCount\":123,\"acceleratorType\":\"ACCELERATOR_TYPE_UNSPECIFIED\",\"precision\":\"PRECISION_UNSPECIFIED\",\"enableAddons\":true,\"draftTokenCount\":123,\"draftModel\":\"<string>\",\"ngramSpeculationLength\":123,\"deploymentTemplate\":\"<string>\",\"autoTune\":{\"longPrompt\":true},\"placement\":{\"region\":\"REGION_UNSPECIFIED\",\"multiRegion\":\"MULTI_REGION_UNSPECIFIED\",\"regions\":[\"REGION_UNSPECIFIED\"]},\"disableDeploymentSizeValidation\":true}\n\n\nResponses: \n200: object\n\tDescription: N/A\n\tExample: {\"name\":\"<string>\",\"displayName\":\"<string>\",\"description\":\"<string>\",\"createTime\":\"2023-11-07T05:31:56Z\",\"expireTime\":\"2023-11-07T05:31:56Z\",\"purgeTime\":\"2023-11-07T05:31:56Z\",\"deleteTime\":\"2023-11-07T05:31:56Z\",\"state\":\"STATE_UNSPECIFIED\",\"status\":{\"code\":\"OK\",\"message\":\"<string>\"},\"minReplicaCount\":123,\"maxReplicaCount\":123,\"replicaCount\":123,\"autoscalingPolicy\":{\"scaleUpWindow\":\"<string>\",\"scaleDownWindow\":\"<string>\",\"scaleToZeroWindow\":\"<string>\",\"loadTargets\":{}},\"baseModel\":\"<string>\",\"acceleratorCount\":123,\"acceleratorType\":\"ACCELERATOR_TYPE_UNSPECIFIED\",\"precision\":\"PRECISION_UNSPECIFIED\",\"cluster\":\"<string>\",\"enableAddons\":true,\"draftTokenCount\":123,\"draftModel\":\"<string>\",\"ngramSpeculationLength\":123,\"numPeftDeviceCached\":123,\"deploymentTemplate\":\"<string>\",\"autoTune\":{\"longPrompt\":true},\"placement\":{\"region\":\"REGION_UNSPECIFIED\",\"multiRegion\":\"MULTI_REGION_UNSPECIFIED\",\"regions\":[\"REGION_UNSPECIFIED\"]},\"region\":\"REGION_UNSPECIFIED\",\"updateTime\":\"2023-11-07T05:31:56Z\",\"disableDeploymentSizeValidation\":true}\n\n", "type": "text"}], "tool_call_id": "functions.search:0"}, {"role": "assistant", "content": "Let me search for more specific information about deployment creation and management workflows.", "tool_calls": [{"id": "functions.search:1", "function": {"arguments": "{\"query\": \"firectl deployment management commands create update list\"}", "name": "search"}, "type": "function"}]}, {"role": "tool", "content": [{"text": "Title: Why am I getting \"invalid id\" errors when using firectl commands like create deployment or list deployments?\nLink: https://docs.fireworks.ai/faq-new/fine-tuning/why-am-i-getting-invalid-id-errors-when-using-firectl-commands-like-create-deplo\nContent: This error typically occurs when your account ID is not properly configured.\n\nThis error typically occurs when your account ID is not properly configured.\n\nWhy am I getting \"invalid id\" errors when using firectl commands like create deployment or list deployments?\nThis error typically occurs when your account ID is not properly configured.\n\n", "type": "text"}, {"text": "Title: Create a deployment\nLink: https://docs.fireworks.ai/tools-sdks/firectl/commands/create-deployment\nContent: Creates a new deployment. firectl create deployment [flags]\n\nCreates a new deployment. firectl create deployment [flags]\n\nCreates a new deployment.\n\n", "type": "text"}, {"text": "Title: Creating an on-demand deployment\nLink: https://docs.fireworks.ai/guides/ondemand-deployments\nContent: Choose a model Check out our model library to see a list of models available for on-demand deployment. You can also upload and use your own custom base model. Create a deployment To create a new deployment of a model from the model library, use: Or, for a custom base model you have uploaded to your account: This command will complete when the deployment is READY . To let it run asynchronously, remove the --wait flag. If your account has purchased reserved capacity and this deployment meets the reservation criteria, it will be counted against that reservation and not billed as an on-demand deployment. Verify the deployment is running You can verify the deployment is active by running: The state field should show READY. The deployment ID is the last part of the deployment name: accounts/<ACCOUNT_ID>/deployments/<DEPLOYMENT_ID>. You can also list all deployments in your account at any time by running: firectl list deployments\n\nChoose a model Check out our model library to see a list of models available for on-demand deployment. You can also upload and use your own custom base model. Create a deployment To create a new deployment of a model from the model library, use: Or, for a custom base model you have uploaded to your account: This command will complete when the deployment is READY . To let it run asynchronously, remove the --wait flag. If your account has purchased reserved capacity and this deployment meets the reservation criteria, it will be counted against that reservation and not billed as an on-demand deployment. Verify the deployment is running You can verify the deployment is active by running: The state field should show READY. The deployment ID is the last part of the deployment name: accounts/<ACCOUNT_ID>/deployments/<DEPLOYMENT_ID>. You can also list all deployments in your account at any time by running: firectl list deployments\n\nPublishing a deployed model\nBy default, models can only be queried by the account that owns them. To make a deployment public so anyone with a valid Fireworks API key can query it, update the deployed model with the --public flag. Find the Deployed Model ID Every model running on a deployment receives a unique deployed model ID. You can find this ID if you know the model name and deployment name using the following command: firectl list deployed-models --filter 'model=\"<MODEL_NAME>\" AND deployment=\"<DEPLOYMENT_NAME>\"'\n\n", "type": "text"}, {"text": "Title: \ud83d\ude80 Easier & faster LoRA fine-tune deployments on Fireworks\nLink: https://docs.fireworks.ai/updates/changelog\nContent: You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment \"accounts/fireworks/models/<MODEL_ID of lora model>\"\n\nYou can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment \"accounts/fireworks/models/<MODEL_ID of lora model>\"\n\nYou can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: Previously, this involved two distinct steps, and the resulting deployment was slower than the base model: Create a deployment using firectl create deployment \"accounts/fireworks/models/<MODEL_ID of base model>\" --enable-addons Then deploy the addon to the deployment: firectl load-lora <MODEL_ID> --deployment <DEPLOYMENT_ID> For more information, see our deployment documentation. This change is for dedicated deployments with a single LoRA. You can still deploy multiple LoRAs on a deployment or deploy LoRA(s) on some Serverless models as described in the documentation.\n\n", "type": "text"}, {"text": "Title: Quick deployment\nLink: https://docs.fireworks.ai/fine-tuning/single-lora\nContent: Deploy your LoRA fine-tuned model with one simple command: firectl create deployment \"accounts/fireworks/models/<MODEL_ID of lora model>\"\n\nDeploy your LoRA fine-tuned model with one simple command: firectl create deployment \"accounts/fireworks/models/<MODEL_ID of lora model>\"\n\nAlternative deployment method\nThis two-step method is the standard approach for multi-LoRA deployments where multiple LoRA models share the same base model. While it can also be used for single LoRA deployments, it provides slower performance compared to live merge and is not recommended for single LoRA use cases. You can also deploy single LoRA models using a two-step process: Create base model deployment Deploy the base model with addons enabled: firectl create deployment \"accounts/fireworks/models/<MODEL_ID of base model>\" --enable-addons\n\n", "type": "text"}, {"text": "Title: Deploying a fine-tuned model using an on-demand deployment\nLink: https://docs.fireworks.ai/fine-tuning/fine-tuning-models\nContent: Use the following command to deploy your fine-tuned model using an on-demand deployment: firectl create deployment <FINE_TUNED_MODEL_ID>\n\nUse the following command to deploy your fine-tuned model using an on-demand deployment: firectl create deployment <FINE_TUNED_MODEL_ID>\n\nfirectl\nfirectl create dataset <DATASET_ID> /path/to/jsonl/file\n\n", "type": "text"}, {"text": "Title: Does Fireworks support custom base models?\nLink: https://docs.fireworks.ai/faq-new/models-inference/does-fireworks-support-custom-base-models\nContent: Yes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.\n\nYes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.\n\nDoes Fireworks support custom base models?\nYes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.\n\n", "type": "text"}, {"text": "Title: firectl\nLink: https://docs.fireworks.ai/fine-tuning/fine-tuning-vlm\nContent: # Create a deployment for your fine-tuned VLM\nfirectl create deployment my-custom-vlm\n\n# Check deployment status\nfirectl get deployment accounts/your-account/deployment/deployment-id\n\n# Create a deployment for your fine-tuned VLM\nfirectl create deployment my-custom-vlm\n\n# Check deployment status\nfirectl get deployment accounts/your-account/deployment/deployment-id\n\nfirectl create sftj \\\n  --base-model accounts/fireworks/models/qwen2p5-vl-32b-instruct \\\n  --dataset my-vlm-dataset \\\n  --output-model my-custom-vlm \\\n  --epochs 3 For additional parameters like learning rates, evaluation datasets, and batch sizes, see  Additional SFT job settings .\n\n", "type": "text"}, {"text": "Title: Uploading the model locally (firectl)\nLink: https://docs.fireworks.ai/models/uploading-custom-models\nContent: To upload a custom base model, run the following command. firectl create model <MODEL_ID> /path/to/files/\n\nTo upload a custom base model, run the following command. firectl create model <MODEL_ID> /path/to/files/\n\nPublishing\nBy default, all models you create are only visible to and deployable by users within your account. To publish a model so anyone with a Fireworks account can deploy it, you can create it with the --public flag. This will allow it to show up in public model lists. To unpublish the model, just run firectl update model <MODEL_ID> --public=false\n\n", "type": "text"}, {"text": "Title: firectl\nLink: https://docs.fireworks.ai/models/quantization\nContent: firectl create deployment <MODEL> --accelerator-type NVIDIA_H100_80GB --precision FP8\n\nfirectl create deployment <MODEL> --accelerator-type NVIDIA_H100_80GB --precision FP8\n\nCreating an FP8 deployment\nBy default, creating a dedicated deployment will use the FP16 checkpoint. To see what precisions are available for a\nmodel, run:  get model <MODEL_ID>\n\n", "type": "text"}], "tool_call_id": "functions.search:1"}, {"role": "assistant", "content": "Based on my research, here's a comprehensive guide to managing deployments on Fireworks:\n\n## Deployment Management on Fireworks\n\n### 1. Creating Deployments\n\n**Basic deployment creation:**\n```bash\n# Create a deployment from the model library\nfirectl create deployment accounts/fireworks/models/<MODEL_NAME>\n\n# Create a deployment for a custom model\nfirectl create deployment <YOUR_MODEL_ID>\n\n# Create with specific configuration\nfirectl create deployment <MODEL_ID> --wait\n```\n\n**Advanced deployment options:**\n```bash\n# Deploy with specific hardware requirements\nfirectl create deployment <MODEL> --accelerator-type NVIDIA_H100_80GB --precision FP8\n\n# Deploy LoRA fine-tuned models (single command)\nfirectl create deployment \"accounts/fireworks/models/<LORA_MODEL_ID>\"\n\n# Deploy with addons enabled (for multi-LoRA)\nfirectl create deployment <BASE_MODEL> --enable-addons\n```\n\n### 2. Monitoring Deployments\n\n**Check deployment status:**\n```bash\n# List all deployments\nfirectl list deployments\n\n# Get specific deployment details\nfirectl get deployment accounts/<ACCOUNT_ID>/deployments/<DEPLOYMENT_ID>\n\n# Check if deployment is ready\n# The state field should show \"READY\"\n```\n\n**Verify deployed models:**\n```bash\n# List deployed models\nfirectl list deployed-models\n\n# Find specific deployed model\nfirectl list deployed-models --filter 'model=\"<MODEL_NAME>\" AND deployment=\"<DEPLOYMENT_NAME>\"'\n```\n\n### 3. Managing Default Deployments\n\n**Check default deployment:**\n```bash\n# See which deployment is marked as default\nfirectl list deployed-models\n# Look for \"Default: true\" in the output\n```\n\n**Update default deployment:**\n```bash\n# Update which deployment is the default\nfirectl update deployed-model <DEPLOYED_MODEL_ID> --default\n```\n\n**Important note:** To delete a default deployment, you must either:\n- Delete all other deployments for the same model first, OR\n- Designate a different deployed model as the default\n\n### 4. Scaling and Cost Management\n\n**Autoscaling configuration:**\n- **Default behavior:** Automatically scales to 0 replicas when not in use\n- **Billing:** Pay only for GPU time when replicas are active\n- **Manual scaling:** Use autoscaling policies to customize behavior\n\n**Cost optimization commands:**\n```bash\n# Scale deployment to zero (cost saving)\nfirectl update deployment <DEPLOYMENT_ID> --min-replica-count 0\n\n# Or using the Python SDK\nfrom fireworks.client import Fireworks\nllm = Fireworks(model=\"<MODEL_ID>\")\nllm.scale_to_zero()\n```\n\n### 5. Updating Deployments\n\n**Update deployment configuration:**\n```bash\n# Update display name, description, or scaling policies\nfirectl update deployment <DEPLOYMENT_ID> \\\n  --display-name \"New Name\" \\\n  --min-replica-count 1 \\\n  --max-replica-count 5\n```\n\n**Available update options:**\n- Display name and description\n- Replica counts (min/max)\n- Autoscaling policies\n- Expiration time\n- Hardware configuration\n\n### 6. Deleting Deployments\n\n**Basic deletion:**\n```bash\n# Delete a deployment\nfirectl delete deployment <DEPLOYMENT_ID>\n\n# Force deletion (ignoring safety checks)\nfirectl delete deployment <DEPLOYMENT_ID> --ignore-checks\n\n# Using Python SDK\nllm.delete_deployment(ignore_checks=True)\n```\n\n**API deletion:**\n```bash\n# Via REST API\ncurl -X DELETE \\\n  https://api.fireworks.ai/v1/accounts/{account_id}/deployments/{deployment_id} \\\n  -H \"Authorization: Bearer YOUR_API_KEY\"\n```\n\n### 7. Deployment Performance Expectations\n\n**Timeline expectations:**\n- **Initial deployment:** Should complete within minutes\n- **Scaling from zero:** Brief availability delays while scaling up\n- **Troubleshooting:** If deployment takes >1 hour, likely indicates a crash\n\n### 8. Best Practices\n\n**Cost management:**\n1. **Monitor regularly:** Check active deployments frequently\n2. **Delete unused deployments:** Avoid unnecessary GPU charges\n3. **Use autoscaling:** Leverage scale-to-zero for intermittent usage\n4. **Consider serverless:** For low-frequency usage patterns\n\n**Configuration tips:**\n- Use `--wait` flag for synchronous deployment creation\n- Configure appropriate min/max replica counts for your use case\n- Set expiration times for temporary deployments\n- Monitor deployment status during initial creation\n\n### 9. Common Workflows\n\n**Complete deployment lifecycle:**\n```bash\n# 1. Create deployment\nfirectl create deployment <MODEL_ID> --wait\n\n# 2. Verify it's ready\nfirectl get deployment <DEPLOYMENT_ID>\n\n# 3. Use the deployment\n# Make API calls to the deployment\n\n# 4. Monitor usage\nfirectl list deployments\n\n# 5. Scale down when not needed\nfirectl update deployment <DEPLOYMENT_ID> --min-replica-count 0\n\n# 6. Delete when completely done\nfirectl delete deployment <DEPLOYMENT_ID>\n```\n\nThis comprehensive guide covers everything from basic deployment creation to advanced cost optimization strategies. The key is understanding the balance between performance needs and cost efficiency through proper scaling configuration.", "tool_calls": []}], "input_metadata": {"row_id": "move-little-tidy-pain-work", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct"}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "evaluation_result": {"score": 1.0, "is_score_valid": true, "reason": "At least one tool call was made", "metrics": {}}, "created_at": "2025-08-08T11:15:45.037115", "eval_metadata": {"name": "test_pytest_mcp_url", "description": "Run math evaluation on sample dataset using pytest interface.", "version": "v0.2.6.dev15+g6486dbd.dirty", "status": "finished", "num_runs": 1, "aggregation_method": "mean"}, "pid": 86994}, {"messages": [{"role": "system", "content": "You are a helpful assistant that can answer questions about Fireworks.\nALWAYS provide code or commands to execute to answer the question."}, {"role": "user", "content": "Can you teach me about how to manage deployments on Fireworks"}, {"role": "assistant", "content": "I'll help you learn about managing deployments on Fireworks AI. Let me search for comprehensive documentation about deployment management.", "tool_calls": [{"id": "functions.search:0", "function": {"arguments": "{\"query\": \"deployment management Fireworks AI\"}", "name": "search"}, "type": "function"}]}, {"role": "tool", "content": [{"text": "Title: Fireworks AI Developer Platform\nLink: https://docs.fireworks.ai/getting-started/introduction\nContent: Fireworks AI is the best platform for building AI product experiences with open source AI models. You can run and customize AI models with just a few lines of code!\n\nFireworks AI is the best platform for building AI product experiences with open source AI models. You can run and customize AI models with just a few lines of code!\n\nStart building with open source AI models\n\n", "type": "text"}, {"text": "Title: How do I close my Fireworks.ai account?\nLink: https://docs.fireworks.ai/faq-new/account-access/how-do-i-close-my-fireworksai-account\nContent: To close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed\n\nTo close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed\n\nHow do I close my Fireworks.ai account?\nTo close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed\n\n", "type": "text"}, {"text": "Title: Do you host your deployments in the EU or Asia?\nLink: https://docs.fireworks.ai/faq-new/support-general/do-you-host-your-deployments-in-the-eu-or-asia\nContent: We are currently deployed in multiple U.S.-based locations. However, we\u2019re open to hearing more about your specific requirements. You can: Join our Discord community Write to us at inquiries@fireworks.ai If you're an Enterprise customer, please contact your dedicated customer support representative to ensure a timely response.\n\nWe are currently deployed in multiple U.S.-based locations. However, we\u2019re open to hearing more about your specific requirements. You can: Join our Discord community Write to us at inquiries@fireworks.ai If you're an Enterprise customer, please contact your dedicated customer support representative to ensure a timely response.\n\nDo you host your deployments in the EU or Asia?\nWe are currently deployed in multiple U.S.-based locations. However, we\u2019re open to hearing more about your specific requirements. You can: Join our Discord community Write to us at inquiries@fireworks.ai If you're an Enterprise customer, please contact your dedicated customer support representative to ensure a timely response.\n\n", "type": "text"}, {"text": "Title: Introducing the Fireworks Build SDK\nLink: https://docs.fireworks.ai/tools-sdks/python-client/sdk-introduction\nContent: The Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.\n\nThe Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.\n\nIntroducing the Fireworks Build SDK\nThe Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.\n\n", "type": "text"}, {"text": "Title: On-demand deployments\nLink: https://docs.fireworks.ai/guides/ondemand-deployments\nContent: If you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.\n\nIf you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.\n\nOn-demand deployments\nIf you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.\n\n", "type": "text"}, {"text": "Title: I have multiple Fireworks accounts. When I try to login with Google on Fireworks' web UI, I'm getting signed into the wrong account. How do I fix this?\nLink: https://docs.fireworks.ai/faq-new/account-access/i-have-multiple-fireworks-accounts-when-i-try-to-login-with-google-on-fireworks\nContent: If you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.\n\nIf you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.\n\nI have multiple Fireworks accounts. When I try to login with Google on Fireworks' web UI, I'm getting signed into the wrong account. How do I fix this?\nIf you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.\n\n", "type": "text"}, {"text": "Title: Introduction\nLink: https://docs.fireworks.ai/api-reference/introduction\nContent: Fireworks AI REST API enables you to interact with various language, image and embedding models using an API Key. It also lets you automate management of models, deployments, datasets, and more.\n\nFireworks AI REST API enables you to interact with various language, image and embedding models using an API Key. It also lets you automate management of models, deployments, datasets, and more.\n\nIntroduction\nFireworks AI REST API enables you to interact with various language, image and embedding models using an API Key. It also lets you automate management of models, deployments, datasets, and more.\n\n", "type": "text"}, {"text": "Title: Create a deployment\nLink: https://docs.fireworks.ai/tools-sdks/firectl/commands/create-deployment\nContent: Create a Deployment on Fireworks AI platform\n\nCreate a Deployment on Fireworks AI platform\n\nCreates a new deployment.\n\n", "type": "text"}, {"text": "Title: How much does Fireworks cost?\nLink: https://docs.fireworks.ai/faq-new/billing-pricing/how-much-does-fireworks-cost\nContent: Fireworks AI operates on a pay-as-you-go model for all non-Enterprise usage, and new users automatically receive free credits. You pay based on: Per token for serverless inference Per GPU usage time for on-demand deployments Per token of training data for fine-tuning For customers needing enterprise-grade security and reliability , please reach out to us at inquiries@fireworks.ai to discuss options. Find out more about our current pricing on our Pricing page.\n\nFireworks AI operates on a pay-as-you-go model for all non-Enterprise usage, and new users automatically receive free credits. You pay based on: Per token for serverless inference Per GPU usage time for on-demand deployments Per token of training data for fine-tuning For customers needing enterprise-grade security and reliability , please reach out to us at inquiries@fireworks.ai to discuss options. Find out more about our current pricing on our Pricing page.\n\nHow much does Fireworks cost?\nFireworks AI operates on a pay-as-you-go model for all non-Enterprise usage, and new users automatically receive free credits. You pay based on: Per token for serverless inference Per GPU usage time for on-demand deployments Per token of training data for fine-tuning For customers needing enterprise-grade security and reliability , please reach out to us at inquiries@fireworks.ai to discuss options. Find out more about our current pricing on our Pricing page.\n\n", "type": "text"}, {"text": "Title: Python (Fireworks)\nLink: https://docs.fireworks.ai/getting-started/quickstart\nContent: This is the recommended way to get started : This  Python (Fireworks)  tab uses our  Fireworks Build\nSDK  for the best performance and developer experience.\nSee our  Client-side performance\noptimization  guide for\nmore details. Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. pip install --upgrade fireworks-ai The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either  ~/.bash_profile  for Bash or  ~/.zshrc  for  Zsh .\nYou can do this by running the command: vim ~/.bash_profile vim ~/.zshrc Add a new line to the file with the following: export FIREWORKS_API_KEY=\"<API_KEY>\" After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. source ~/.bash_profile source ~/.zshrc You can verify that the variable has been set correctly by running  echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. setx FIREWORKS_API_KEY \"<API_KEY>\" To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: echo %FIREWORKS_API_KEY% You can quickly instantiate the  LLM  class and call the Fireworks API. The Build SDK handles deployment management automatically. from fireworks import LLM\n\n# Basic usage - SDK automatically selects optimal deployment type\nllm = LLM(model=\"llama4-maverick-instruct-basic\", deployment_type=\"auto\")\n\nresponse = llm.chat.completions.create(\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}]\n)\n\nprint(response.choices[0].message.content) You can also pass the API key directly to the LLM constructor:  LLM(model=\"llama4-maverick-instruct-basic\", deployment_type=\"auto\", api_key=\"<FIREWORKS_API_KEY>\")\n\nThis is the recommended way to get started : This  Python (Fireworks)  tab uses our  Fireworks Build\nSDK  for the best performance and developer experience.\nSee our  Client-side performance\noptimization  guide for\nmore details. Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. pip install --upgrade fireworks-ai The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either  ~/.bash_profile  for Bash or  ~/.zshrc  for  Zsh .\nYou can do this by running the command: vim ~/.bash_profile vim ~/.zshrc Add a new line to the file with the following: export FIREWORKS_API_KEY=\"<API_KEY>\" After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. source ~/.bash_profile source ~/.zshrc You can verify that the variable has been set correctly by running  echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. setx FIREWORKS_API_KEY \"<API_KEY>\" To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: echo %FIREWORKS_API_KEY% You can quickly instantiate the  LLM  class and call the Fireworks API. The Build SDK handles deployment management automatically. from fireworks import LLM\n\n# Basic usage - SDK automatically selects optimal deployment type\nllm = LLM(model=\"llama4-maverick-instruct-basic\", deployment_type=\"auto\")\n\nresponse = llm.chat.completions.create(\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}]\n)\n\nprint(response.choices[0].message.content) You can also pass the API key directly to the LLM constructor:  LLM(model=\"llama4-maverick-instruct-basic\", deployment_type=\"auto\", api_key=\"<FIREWORKS_API_KEY>\")\n\nSet up your developer environment & call the Fireworks API\nThis is the recommended way to get started : This  tab uses our Fireworks Build\nSDK for the best performance and developer experience.\nSee our Client-side performance\noptimization guide for\nmore details. Install SDK Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Configure API Key Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either ~/.bash_profile for Bash or ~/.zshrc for Zsh .\nYou can do this by running the command: Add a new line to the file with the following: After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. You can verify that the variable has been set correctly by running echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: Sending the first API Request You can quickly instantiate the LLM class and call the Fireworks API. The Build SDK handles deployment management automatically. You can also pass the API key directly to the LLM constructor: LLM(model=\"llama4-maverick-instruct-basic\", deployment_type=\"auto\", api_key=\"<FIREWORKS_API_KEY>\") Install SDK Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. Fireworks AI platform offers drop-in replacement with OpenAI Python Client. Configure API Key Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either ~/.bash_profile for Bash or ~/.zshrc for Zsh .\nYou can do this by running the command: Add a new line to the file with the following: After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. You can verify that the variable has been set correctly by running echo $OPENAI_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. setx OPENAI_API_BASE \"https://api.fireworks.ai/inference/v1\"\nsetx OPENAI_API_KEY \"<API_KEY>\"\n\n", "type": "text"}], "tool_call_id": "functions.search:0"}, {"role": "assistant", "content": "Let me search for more specific information about deployment management commands and workflows.", "tool_calls": [{"id": "functions.search:1", "function": {"arguments": "{\"query\": \"firectl deployment commands create manage update delete\"}", "name": "search"}, "type": "function"}]}, {"role": "tool", "content": [{"text": "Title: Why am I getting \"invalid id\" errors when using firectl commands like create deployment or list deployments?\nLink: https://docs.fireworks.ai/faq-new/fine-tuning/why-am-i-getting-invalid-id-errors-when-using-firectl-commands-like-create-deplo\nContent: This error typically occurs when your account ID is not properly configured.\n\nThis error typically occurs when your account ID is not properly configured.\n\nWhy am I getting \"invalid id\" errors when using firectl commands like create deployment or list deployments?\nThis error typically occurs when your account ID is not properly configured.\n\n", "type": "text"}, {"text": "Title: Delete a deployment\nLink: https://docs.fireworks.ai/tools-sdks/firectl/commands/delete-resources\nContent: Deletes an model deployment. firectl delete deployment [flags]\n\nDeletes an model deployment. firectl delete deployment [flags]\n\nExample\nfirectl delete deployment my-deployment\n\n", "type": "text"}, {"text": "Title: Create a deployment\nLink: https://docs.fireworks.ai/tools-sdks/firectl/commands/create-deployment\nContent: Creates a new deployment. firectl create deployment [flags]\n\nCreates a new deployment. firectl create deployment [flags]\n\nCreate a Deployment on Fireworks AI platform\n\n", "type": "text"}, {"text": "Title: Managing default deployments\nLink: https://docs.fireworks.ai/guides/ondemand-deployments\nContent: Since a model may be deployed to multiple deployments, querying by model name will route to the \"default\" deployed model. You can see which deployed model entry is marked with Default: true using: This will show the Deployed Model Refs section with the Default: true entry. To update the default deployed model, note the Name of the deployed model reference above. Then run: firectl update deployed-model <DEPLOYED_MODEL_ID_2> --default\n\nSince a model may be deployed to multiple deployments, querying by model name will route to the \"default\" deployed model. You can see which deployed model entry is marked with Default: true using: This will show the Deployed Model Refs section with the Default: true entry. To update the default deployed model, note the Name of the deployed model reference above. Then run: firectl update deployed-model <DEPLOYED_MODEL_ID_2> --default\n\nSince a model may be deployed to multiple deployments, querying by model name will route to the \"default\" deployed model. You can see which deployed model entry is marked with Default: true using: This will show the Deployed Model Refs section with the Default: true entry. To update the default deployed model, note the Name of the deployed model reference above. Then run: To delete a default deployment you must delete all other deployments for the same model first, or designate a different deployed model as the default as described above. This is to ensure that querying by model name will always route to an unambiguous default deployment as long as deployments for the model exist.\n\n", "type": "text"}, {"text": "Title: \ud83d\ude80 Easier & faster LoRA fine-tune deployments on Fireworks\nLink: https://docs.fireworks.ai/updates/changelog\nContent: You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment \"accounts/fireworks/models/<MODEL_ID of lora model>\"\n\nYou can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment \"accounts/fireworks/models/<MODEL_ID of lora model>\"\n\nYou can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: Previously, this involved two distinct steps, and the resulting deployment was slower than the base model: Create a deployment using firectl create deployment \"accounts/fireworks/models/<MODEL_ID of base model>\" --enable-addons Then deploy the addon to the deployment: firectl load-lora <MODEL_ID> --deployment <DEPLOYMENT_ID> For more information, see our deployment documentation. This change is for dedicated deployments with a single LoRA. You can still deploy multiple LoRAs on a deployment or deploy LoRA(s) on some Serverless models as described in the documentation.\n\n", "type": "text"}, {"text": "Title: Deployment Command\nLink: https://docs.fireworks.ai/evaluators/examples/gcp_cloud_run_deployment_example\nContent: It's recommended to run the deployment command from the directory containing the reward function script ( dummy_rewards.py ) and your rewardkit.yaml (if used), for example, from examples/gcp_cloud_run_deployment_example/. Ensure your virtual environment is active: Run the deployment command: Command Explanation: --id my-dummy-gcp-evaluator : A unique ID for your evaluator on the Fireworks AI platform. --target gcp-cloud-run : Specifies deployment to GCP Cloud Run. --function-ref dummy_rewards.hello_world_reward : The Python import path to your reward function. If dummy_rewards.py is in the current directory, this reference works. --gcp-auth-mode api-key : Configures the Cloud Run service with API key authentication. reward-kit will generate a key, store it in GCP Secret Manager, and configure the service. The key is also saved to your local rewardkit.yaml under evaluator_endpoint_keys . This is the default if not specified. --verbose : Shows detailed output, including gcloud commands being executed. --force : (Optional) If an evaluator with the same --id already exists, this flag will delete the existing one before creating the new one.\n\nIt's recommended to run the deployment command from the directory containing the reward function script ( dummy_rewards.py ) and your rewardkit.yaml (if used), for example, from examples/gcp_cloud_run_deployment_example/. Ensure your virtual environment is active: Run the deployment command: Command Explanation: --id my-dummy-gcp-evaluator : A unique ID for your evaluator on the Fireworks AI platform. --target gcp-cloud-run : Specifies deployment to GCP Cloud Run. --function-ref dummy_rewards.hello_world_reward : The Python import path to your reward function. If dummy_rewards.py is in the current directory, this reference works. --gcp-auth-mode api-key : Configures the Cloud Run service with API key authentication. reward-kit will generate a key, store it in GCP Secret Manager, and configure the service. The key is also saved to your local rewardkit.yaml under evaluator_endpoint_keys . This is the default if not specified. --verbose : Shows detailed output, including gcloud commands being executed. --force : (Optional) If an evaluator with the same --id already exists, this flag will delete the existing one before creating the new one.\n\nIt's recommended to run the deployment command from the directory containing the reward function script ( dummy_rewards.py ) and your rewardkit.yaml (if used), for example, from examples/gcp_cloud_run_deployment_example/. Ensure your virtual environment is active: source .venv/bin/activate\n\n", "type": "text"}, {"text": "Title: Quick deployment\nLink: https://docs.fireworks.ai/fine-tuning/single-lora\nContent: Deploy your LoRA fine-tuned model with one simple command: firectl create deployment \"accounts/fireworks/models/<MODEL_ID of lora model>\"\n\nDeploy your LoRA fine-tuned model with one simple command: firectl create deployment \"accounts/fireworks/models/<MODEL_ID of lora model>\"\n\nAlternative deployment method\nThis two-step method is the standard approach for multi-LoRA deployments where multiple LoRA models share the same base model. While it can also be used for single LoRA deployments, it provides slower performance compared to live merge and is not recommended for single LoRA use cases. You can also deploy single LoRA models using a two-step process: Create base model deployment Deploy the base model with addons enabled: firectl create deployment \"accounts/fireworks/models/<MODEL_ID of base model>\" --enable-addons\n\n", "type": "text"}, {"text": "Title: How do I close my Fireworks.ai account?\nLink: https://docs.fireworks.ai/faq-new/account-access/how-do-i-close-my-fireworksai-account\nContent: To close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed\n\nTo close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed\n\nHow do I close my Fireworks.ai account?\nTo close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed\n\n", "type": "text"}, {"text": "Title: Delete Deployment\nLink: https://docs.fireworks.ai/api-reference/delete-deployment\nContent: \n\n\n\nREST Endpoint DELETE /v1/accounts/{account_id}/deployments/{deployment_id}\n\nServer(s): https://api.fireworks.ai\n\n\nSummary:  \n\nDescription: N/A\n\nSecurity methods: \nheader: \n\tAuthorization: bearer auth\n\t\n\nRequest params: \npath params:\naccount_id:\n\tstring\n\tDescription: The Account Id\n\tExample: \"<string>\"\ndeployment_id:\n\tstring\n\tDescription: The Deployment Id\n\tExample: \"<string>\"\n\nquery params:\nhard:\n\tboolean\n\tDescription: If true, this will perform a hard deletion.\n\tExample: true\nignoreChecks:\n\tboolean\n\tDescription: If true, this will ignore checks and force the deletion of a deployment that is currently\ndeployed and is in use.\n\tExample: true\n\n\nRequest body: N/A\n\n\nResponses: \n200: object\n\tDescription: N/A\n\tExample: {}\n\n", "type": "text"}, {"text": "Title: Deploying a fine-tuned model using an on-demand deployment\nLink: https://docs.fireworks.ai/fine-tuning/fine-tuning-models\nContent: Use the following command to deploy your fine-tuned model using an on-demand deployment: firectl create deployment <FINE_TUNED_MODEL_ID>\n\nUse the following command to deploy your fine-tuned model using an on-demand deployment: firectl create deployment <FINE_TUNED_MODEL_ID>\n\nfirectl\nfirectl create dataset <DATASET_ID> /path/to/jsonl/file\n\n", "type": "text"}], "tool_call_id": "functions.search:1"}, {"role": "assistant", "content": "Let me search for more information about listing and managing existing deployments.", "tool_calls": [{"id": "functions.search:0", "function": {"arguments": "{\"query\": \"list deployments firectl commands deployment management\"}", "name": "search"}, "type": "function"}]}, {"role": "tool", "content": [{"text": "Title: Why am I getting \"invalid id\" errors when using firectl commands like create deployment or list deployments?\nLink: https://docs.fireworks.ai/faq-new/fine-tuning/why-am-i-getting-invalid-id-errors-when-using-firectl-commands-like-create-deplo\nContent: This error typically occurs when your account ID is not properly configured.\n\nThis error typically occurs when your account ID is not properly configured.\n\nWhy am I getting \"invalid id\" errors when using firectl commands like create deployment or list deployments?\nThis error typically occurs when your account ID is not properly configured.\n\n", "type": "text"}, {"text": "Title: List deployments\nLink: https://docs.fireworks.ai/tools-sdks/firectl/commands/list-resources\nContent: Prints all deployments in the account. firectl list deployments [flags]\n\nPrints all deployments in the account. firectl list deployments [flags]\n\nList deployed models\nPrints all deployed models in an account. firectl list deployed-models [flags]\n\n", "type": "text"}, {"text": "Title: \ud83d\ude80 Easier & faster LoRA fine-tune deployments on Fireworks\nLink: https://docs.fireworks.ai/updates/changelog\nContent: You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment \"accounts/fireworks/models/<MODEL_ID of lora model>\"\n\nYou can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment \"accounts/fireworks/models/<MODEL_ID of lora model>\"\n\nYou can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: Previously, this involved two distinct steps, and the resulting deployment was slower than the base model: Create a deployment using firectl create deployment \"accounts/fireworks/models/<MODEL_ID of base model>\" --enable-addons Then deploy the addon to the deployment: firectl load-lora <MODEL_ID> --deployment <DEPLOYMENT_ID> For more information, see our deployment documentation. This change is for dedicated deployments with a single LoRA. You can still deploy multiple LoRAs on a deployment or deploy LoRA(s) on some Serverless models as described in the documentation.\n\n", "type": "text"}, {"text": "Title: Managing default deployments\nLink: https://docs.fireworks.ai/guides/ondemand-deployments\nContent: Since a model may be deployed to multiple deployments, querying by model name will route to the \"default\" deployed model. You can see which deployed model entry is marked with Default: true using: firectl get model <MODEL_ID>\n\nSince a model may be deployed to multiple deployments, querying by model name will route to the \"default\" deployed model. You can see which deployed model entry is marked with Default: true using: firectl get model <MODEL_ID>\n\nCreating an on-demand deployment\nChoose a model Check out our model library to see a list of models available for on-demand deployment. You can also upload and use your own custom base model. Create a deployment To create a new deployment of a model from the model library, use: Or, for a custom base model you have uploaded to your account: This command will complete when the deployment is READY . To let it run asynchronously, remove the --wait flag. If your account has purchased reserved capacity and this deployment meets the reservation criteria, it will be counted against that reservation and not billed as an on-demand deployment. Verify the deployment is running You can verify the deployment is active by running: The state field should show READY. The deployment ID is the last part of the deployment name: accounts/<ACCOUNT_ID>/deployments/<DEPLOYMENT_ID>. You can also list all deployments in your account at any time by running: firectl list deployments\n\n", "type": "text"}, {"text": "Title: Does Fireworks support custom base models?\nLink: https://docs.fireworks.ai/faq-new/models-inference/does-fireworks-support-custom-base-models\nContent: Yes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.\n\nYes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.\n\nDoes Fireworks support custom base models?\nYes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.\n\n", "type": "text"}, {"text": "Title: Quick deployment\nLink: https://docs.fireworks.ai/fine-tuning/single-lora\nContent: Deploy your LoRA fine-tuned model with one simple command: firectl create deployment \"accounts/fireworks/models/<MODEL_ID of lora model>\"\n\nDeploy your LoRA fine-tuned model with one simple command: firectl create deployment \"accounts/fireworks/models/<MODEL_ID of lora model>\"\n\nAlternative deployment method\nThis two-step method is the standard approach for multi-LoRA deployments where multiple LoRA models share the same base model. While it can also be used for single LoRA deployments, it provides slower performance compared to live merge and is not recommended for single LoRA use cases. You can also deploy single LoRA models using a two-step process: Create base model deployment Deploy the base model with addons enabled: firectl create deployment \"accounts/fireworks/models/<MODEL_ID of base model>\" --enable-addons\n\n", "type": "text"}, {"text": "Title: List Deployments\nLink: https://docs.fireworks.ai/api-reference/list-deployments\nContent: \n\n\n\nREST Endpoint GET /v1/accounts/{account_id}/deployments\n\nServer(s): https://api.fireworks.ai\n\n\nSummary:  \n\nDescription: N/A\n\nSecurity methods: \nheader: \n\tAuthorization: bearer auth\n\t\n\nRequest params: \npath params:\naccount_id:\n\tstring\n\tDescription: The Account Id\n\tExample: \"<string>\"\n\nquery params:\npageSize:\n\tinteger\n\tDescription: The maximum number of deployments to return. The maximum page_size is 200,\nvalues above 200 will be coerced to 200.\nIf unspecified, the default is 50.\n\tExample: 123\npageToken:\n\tstring\n\tDescription: A page token, received from a previous ListDeployments call. Provide this\nto retrieve the subsequent page. When paginating, all other parameters\nprovided to ListDeployments must match the call that provided the page\ntoken.\n\tExample: \"<string>\"\nfilter:\n\tstring\n\tDescription: Only deployment satisfying the provided filter (if specified) will be\nreturned. See https://google.aip.dev/160 for the filter grammar.\n\tExample: \"<string>\"\norderBy:\n\tstring\n\tDescription: A comma-separated list of fields to order by. e.g. \"foo,bar\"\nThe default sort order is ascending. To specify a descending order for a\nfield, append a \" desc\" suffix. e.g. \"foo desc,bar\"\nSubfields are specified with a \".\" character. e.g. \"foo.bar\"\nIf not specified, the default order is by \"create_time\".\n\tExample: \"<string>\"\nshowDeleted:\n\tboolean\n\tDescription: If set, DELETED deployments will be included.\n\tExample: true\nreadMask:\n\tstring\n\tDescription: The fields to be returned in the response. If empty or \"*\", all fields will be returned.\n\tExample: \"<string>\"\n\n\nRequest body: N/A\n\n\nResponses: \n200: object\n\tDescription: N/A\n\tExample: {\"deployments\":[{\"name\":\"<string>\",\"displayName\":\"<string>\",\"description\":\"<string>\",\"createTime\":\"2023-11-07T05:31:56Z\",\"expireTime\":\"2023-11-07T05:31:56Z\",\"purgeTime\":\"2023-11-07T05:31:56Z\",\"deleteTime\":\"2023-11-07T05:31:56Z\",\"state\":\"STATE_UNSPECIFIED\",\"status\":{\"code\":\"OK\",\"message\":\"<string>\"},\"minReplicaCount\":123,\"maxReplicaCount\":123,\"replicaCount\":123,\"autoscalingPolicy\":{\"scaleUpWindow\":\"<string>\",\"scaleDownWindow\":\"<string>\",\"scaleToZeroWindow\":\"<string>\",\"loadTargets\":{}},\"baseModel\":\"<string>\",\"acceleratorCount\":123,\"acceleratorType\":\"ACCELERATOR_TYPE_UNSPECIFIED\",\"precision\":\"PRECISION_UNSPECIFIED\",\"cluster\":\"<string>\",\"enableAddons\":true,\"draftTokenCount\":123,\"draftModel\":\"<string>\",\"ngramSpeculationLength\":123,\"numPeftDeviceCached\":123,\"deploymentTemplate\":\"<string>\",\"autoTune\":{\"longPrompt\":true},\"placement\":{\"region\":\"REGION_UNSPECIFIED\",\"multiRegion\":\"MULTI_REGION_UNSPECIFIED\",\"regions\":[\"REGION_UNSPECIFIED\"]},\"region\":\"REGION_UNSPECIFIED\",\"updateTime\":\"2023-11-07T05:31:56Z\",\"disableDeploymentSizeValidation\":true}],\"nextPageToken\":\"<string>\",\"totalSize\":123}\n\n", "type": "text"}, {"text": "Title: Deploy Command\nLink: https://docs.fireworks.ai/evaluators/cli_reference/cli_overview\nContent: The deploy command deploys a reward function as an evaluator on the Fireworks platform.\n\nThe deploy command deploys a reward function as an evaluator on the Fireworks platform.\n\nCommand Overview\nThe Reward Kit CLI supports the following main commands: run : Run a local evaluation pipeline using a Hydra configuration. preview : Preview evaluation results or re-evaluate generated outputs. deploy : Deploy a reward function as an evaluator. agent-eval : Run agent evaluations on task bundles. list : List existing evaluators (coming soon). delete : Delete an evaluator (coming soon).\n\n", "type": "text"}, {"text": "Title: Deploying a fine-tuned model using an on-demand deployment\nLink: https://docs.fireworks.ai/fine-tuning/fine-tuning-models\nContent: Use the following command to deploy your fine-tuned model using an on-demand deployment: firectl create deployment <FINE_TUNED_MODEL_ID>\n\nUse the following command to deploy your fine-tuned model using an on-demand deployment: firectl create deployment <FINE_TUNED_MODEL_ID>\n\nDeploying a fine-tuned model serverlessly\nNot all base models support serverless addons. Please check this list for the full list of serverless models that support LoRA add-ons. If your base model supports serverless addons, you can run the following command to load the fine-tuned model into a serverless deployment: firectl load-lora <FINE_TUNED_MODEL_ID>\n\n", "type": "text"}, {"text": "Title: Create a deployment\nLink: https://docs.fireworks.ai/tools-sdks/firectl/commands/create-deployment\nContent: Creates a new deployment. firectl create deployment [flags]\n\nCreates a new deployment. firectl create deployment [flags]\n\nCreate a Deployment on Fireworks AI platform\n\n", "type": "text"}], "tool_call_id": "functions.search:0"}, {"role": "assistant", "content": "Based on my research, here's a comprehensive guide to managing deployments on Fireworks AI:\n\n## Overview of Deployment Types\n\nFireworks AI offers several deployment options:\n\n1. **Serverless Models**: Pay-per-token, shared infrastructure\n2. **On-demand Deployments**: Dedicated GPU instances billed by GPU-second\n3. **Reserved Capacity**: Pre-purchased dedicated capacity for high-volume usage\n\n## Key Benefits of On-demand Deployments\n- Predictable performance unaffected by other users\n- No hard rate limits\n- Cost-effective for high utilization\n- Access to larger model selection\n- Support for custom base models\n\n## Managing Deployments with firectl\n\n### Installation and Setup\nFirst, install the Fireworks CLI tools:\n\n```bash\npip install fireworks-ai\n```\n\nSet up your API key:\n```bash\nexport FIREWORKS_API_KEY=\"your-api-key-here\"\n```\n\n### Basic Deployment Commands\n\n#### 1. List All Deployments\n```bash\nfirectl list deployments\n```\n\n#### 2. Create a New Deployment\n```bash\n# Basic deployment\nfirectl create deployment <MODEL_ID>\n\n# With custom configuration\nfirectl create deployment <MODEL_ID> --min-replica-count 1 --max-replica-count 3\n\n# Deploy a LoRA fine-tuned model\nfirectl create deployment \"accounts/fireworks/models/<LORA_MODEL_ID>\"\n\n# Deploy with addons enabled (for multi-LoRA)\nfirectl create deployment <MODEL_ID> --enable-addons\n```\n\n#### 3. Check Deployment Status\n```bash\nfirectl get deployment <DEPLOYMENT_ID>\n```\n\n#### 4. Delete a Deployment\n```bash\n# Soft delete\nfirectl delete deployment <DEPLOYMENT_ID>\n\n# Force delete (ignores usage checks)\nfirectl delete deployment <DEPLOYMENT_ID> --ignore-checks\n```\n\n### Managing Default Deployments\nWhen you have multiple deployments of the same model:\n\n```bash\n# Check which deployment is default\nfirectl get model <MODEL_ID>\n\n# Update default deployment\nfirectl update deployed-model <DEPLOYED_MODEL_ID> --default\n```\n\n## Using the Fireworks Build SDK (Python)\n\nThe Fireworks Build SDK provides a higher-level interface:\n\n```python\nfrom fireworks import LLM\n\n# Auto-managed deployment\nllm = LLM(model=\"llama4-maverick-instruct-basic\", deployment_type=\"auto\")\n\n# Explicit on-demand deployment\nllm = LLM(model=\"your-model-id\", deployment_type=\"on-demand\")\n\n# Make API calls\nresponse = llm.chat.completions.create(\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n```\n\n## REST API Management\n\n### List Deployments via REST API\n```bash\ncurl -X GET \\\n  \"https://api.fireworks.ai/v1/accounts/{account_id}/deployments\" \\\n  -H \"Authorization: Bearer $FIREWORKS_API_KEY\"\n```\n\n### Delete Deployment via REST API\n```bash\ncurl -X DELETE \\\n  \"https://api.fireworks.ai/v1/accounts/{account_id}/deployments/{deployment_id}\" \\\n  -H \"Authorization: Bearer $FIREWORKS_API_KEY\"\n```\n\n## Deployment Workflow Examples\n\n### 1. Deploy a Fine-tuned Model\n```bash\n# Single command deployment\nfirectl create deployment \"accounts/fireworks/models/my-lora-model\"\n\n# Wait for deployment to be ready\nfirectl get deployment <deployment-id>\n```\n\n### 2. Multi-LoRA Deployment (Advanced)\n```bash\n# Step 1: Create base deployment with addons\nfirectl create deployment \"accounts/fireworks/models/base-model\" --enable-addons\n\n# Step 2: Load LoRA models\nfirectl load-lora <LORA_MODEL_ID_1> --deployment <DEPLOYMENT_ID>\nfirectl load-lora <LORA_MODEL_ID_2> --deployment <DEPLOYMENT_ID>\n```\n\n### 3. Custom Model Deployment\n```bash\n# First upload your custom model\n# Then deploy it\nfirectl create deployment \"accounts/{account_id}/models/custom-model-id\"\n```\n\n## Best Practices\n\n1. **Monitor Costs**: On-demand deployments are billed by GPU-second\n2. **Use Reserved Capacity**: For consistent high-volume usage\n3. **Set Appropriate Scaling**: Configure min/max replica counts based on expected load\n4. **Clean Up**: Delete unused deployments to avoid charges\n5. **Check Status**: Always verify deployment state before using\n\n## Troubleshooting\n\n- **\"Invalid ID\" errors**: Ensure your account ID is properly configured\n- **Deployment stuck**: Check resource quotas and account limits\n- **Performance issues**: Verify deployment is in READY state and check replica counts\n\nWould you like me to dive deeper into any specific aspect of deployment management, such as scaling configurations, cost optimization, or advanced deployment patterns?", "tool_calls": []}], "input_metadata": {"row_id": "call-stand-low-night-conference", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct"}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "evaluation_result": {"score": 1.0, "is_score_valid": true, "reason": "At least one tool call was made", "metrics": {}}, "created_at": "2025-08-08T11:19:37.022552", "eval_metadata": {"name": "test_pytest_mcp_url", "description": "Run math evaluation on sample dataset using pytest interface.", "version": "v0.2.6.dev15+g6486dbd.dirty", "status": "finished", "num_runs": 1, "aggregation_method": "mean"}, "pid": 89723}, {"messages": [{"role": "system", "content": "You are a helpful assistant that can answer questions about Fireworks.\nALWAYS provide code or commands to execute to answer the question."}, {"role": "user", "content": "Can you teach me about how to manage deployments on Fireworks"}, {"role": "assistant", "content": "I'll help you learn about managing deployments on Fireworks! Let me search for comprehensive information about deployment management.", "tool_calls": [{"id": "functions.search:0", "function": {"arguments": "{\"query\": \"deployment management Fireworks AI\"}", "name": "search"}, "type": "function"}]}, {"role": "tool", "content": [{"text": "Title: Fireworks AI Developer Platform\nLink: https://docs.fireworks.ai/getting-started/introduction\nContent: Fireworks AI is the best platform for building AI product experiences with open source AI models. You can run and customize AI models with just a few lines of code!\n\nFireworks AI is the best platform for building AI product experiences with open source AI models. You can run and customize AI models with just a few lines of code!\n\nStart building with open source AI models\n\n", "type": "text"}, {"text": "Title: How do I close my Fireworks.ai account?\nLink: https://docs.fireworks.ai/faq-new/account-access/how-do-i-close-my-fireworksai-account\nContent: To close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed\n\nTo close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed\n\nHow do I close my Fireworks.ai account?\nTo close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed\n\n", "type": "text"}, {"text": "Title: Do you host your deployments in the EU or Asia?\nLink: https://docs.fireworks.ai/faq-new/support-general/do-you-host-your-deployments-in-the-eu-or-asia\nContent: We are currently deployed in multiple U.S.-based locations. However, we\u2019re open to hearing more about your specific requirements. You can: Join our Discord community Write to us at inquiries@fireworks.ai If you're an Enterprise customer, please contact your dedicated customer support representative to ensure a timely response.\n\nWe are currently deployed in multiple U.S.-based locations. However, we\u2019re open to hearing more about your specific requirements. You can: Join our Discord community Write to us at inquiries@fireworks.ai If you're an Enterprise customer, please contact your dedicated customer support representative to ensure a timely response.\n\nDo you host your deployments in the EU or Asia?\nWe are currently deployed in multiple U.S.-based locations. However, we\u2019re open to hearing more about your specific requirements. You can: Join our Discord community Write to us at inquiries@fireworks.ai If you're an Enterprise customer, please contact your dedicated customer support representative to ensure a timely response.\n\n", "type": "text"}, {"text": "Title: Introducing the Fireworks Build SDK\nLink: https://docs.fireworks.ai/tools-sdks/python-client/sdk-introduction\nContent: The Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.\n\nThe Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.\n\nIntroducing the Fireworks Build SDK\nThe Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.\n\n", "type": "text"}, {"text": "Title: On-demand deployments\nLink: https://docs.fireworks.ai/guides/ondemand-deployments\nContent: If you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.\n\nIf you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.\n\nOn-demand deployments\nIf you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.\n\n", "type": "text"}, {"text": "Title: I have multiple Fireworks accounts. When I try to login with Google on Fireworks' web UI, I'm getting signed into the wrong account. How do I fix this?\nLink: https://docs.fireworks.ai/faq-new/account-access/i-have-multiple-fireworks-accounts-when-i-try-to-login-with-google-on-fireworks\nContent: If you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.\n\nIf you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.\n\nI have multiple Fireworks accounts. When I try to login with Google on Fireworks' web UI, I'm getting signed into the wrong account. How do I fix this?\nIf you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.\n\n", "type": "text"}, {"text": "Title: Introduction\nLink: https://docs.fireworks.ai/api-reference/introduction\nContent: Fireworks AI REST API enables you to interact with various language, image and embedding models using an API Key. It also lets you automate management of models, deployments, datasets, and more.\n\nFireworks AI REST API enables you to interact with various language, image and embedding models using an API Key. It also lets you automate management of models, deployments, datasets, and more.\n\nIntroduction\nFireworks AI REST API enables you to interact with various language, image and embedding models using an API Key. It also lets you automate management of models, deployments, datasets, and more.\n\n", "type": "text"}, {"text": "Title: Create a deployment\nLink: https://docs.fireworks.ai/tools-sdks/firectl/commands/create-deployment\nContent: Create a Deployment on Fireworks AI platform\n\nCreate a Deployment on Fireworks AI platform\n\nCreates a new deployment.\n\n", "type": "text"}, {"text": "Title: How much does Fireworks cost?\nLink: https://docs.fireworks.ai/faq-new/billing-pricing/how-much-does-fireworks-cost\nContent: Fireworks AI operates on a pay-as-you-go model for all non-Enterprise usage, and new users automatically receive free credits. You pay based on: Per token for serverless inference Per GPU usage time for on-demand deployments Per token of training data for fine-tuning For customers needing enterprise-grade security and reliability , please reach out to us at inquiries@fireworks.ai to discuss options. Find out more about our current pricing on our Pricing page.\n\nFireworks AI operates on a pay-as-you-go model for all non-Enterprise usage, and new users automatically receive free credits. You pay based on: Per token for serverless inference Per GPU usage time for on-demand deployments Per token of training data for fine-tuning For customers needing enterprise-grade security and reliability , please reach out to us at inquiries@fireworks.ai to discuss options. Find out more about our current pricing on our Pricing page.\n\nHow much does Fireworks cost?\nFireworks AI operates on a pay-as-you-go model for all non-Enterprise usage, and new users automatically receive free credits. You pay based on: Per token for serverless inference Per GPU usage time for on-demand deployments Per token of training data for fine-tuning For customers needing enterprise-grade security and reliability , please reach out to us at inquiries@fireworks.ai to discuss options. Find out more about our current pricing on our Pricing page.\n\n", "type": "text"}, {"text": "Title: Python (Fireworks)\nLink: https://docs.fireworks.ai/getting-started/quickstart\nContent: This is the recommended way to get started : This  Python (Fireworks)  tab uses our  Fireworks Build\nSDK  for the best performance and developer experience.\nSee our  Client-side performance\noptimization  guide for\nmore details. Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. pip install --upgrade fireworks-ai The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either  ~/.bash_profile  for Bash or  ~/.zshrc  for  Zsh .\nYou can do this by running the command: vim ~/.bash_profile vim ~/.zshrc Add a new line to the file with the following: export FIREWORKS_API_KEY=\"<API_KEY>\" After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. source ~/.bash_profile source ~/.zshrc You can verify that the variable has been set correctly by running  echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. setx FIREWORKS_API_KEY \"<API_KEY>\" To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: echo %FIREWORKS_API_KEY% You can quickly instantiate the  LLM  class and call the Fireworks API. The Build SDK handles deployment management automatically. from fireworks import LLM\n\n# Basic usage - SDK automatically selects optimal deployment type\nllm = LLM(model=\"llama4-maverick-instruct-basic\", deployment_type=\"auto\")\n\nresponse = llm.chat.completions.create(\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}]\n)\n\nprint(response.choices[0].message.content) You can also pass the API key directly to the LLM constructor:  LLM(model=\"llama4-maverick-instruct-basic\", deployment_type=\"auto\", api_key=\"<FIREWORKS_API_KEY>\")\n\nThis is the recommended way to get started : This  Python (Fireworks)  tab uses our  Fireworks Build\nSDK  for the best performance and developer experience.\nSee our  Client-side performance\noptimization  guide for\nmore details. Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. pip install --upgrade fireworks-ai The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either  ~/.bash_profile  for Bash or  ~/.zshrc  for  Zsh .\nYou can do this by running the command: vim ~/.bash_profile vim ~/.zshrc Add a new line to the file with the following: export FIREWORKS_API_KEY=\"<API_KEY>\" After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. source ~/.bash_profile source ~/.zshrc You can verify that the variable has been set correctly by running  echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. setx FIREWORKS_API_KEY \"<API_KEY>\" To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: echo %FIREWORKS_API_KEY% You can quickly instantiate the  LLM  class and call the Fireworks API. The Build SDK handles deployment management automatically. from fireworks import LLM\n\n# Basic usage - SDK automatically selects optimal deployment type\nllm = LLM(model=\"llama4-maverick-instruct-basic\", deployment_type=\"auto\")\n\nresponse = llm.chat.completions.create(\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}]\n)\n\nprint(response.choices[0].message.content) You can also pass the API key directly to the LLM constructor:  LLM(model=\"llama4-maverick-instruct-basic\", deployment_type=\"auto\", api_key=\"<FIREWORKS_API_KEY>\")\n\nSet up your developer environment & call the Fireworks API\nThis is the recommended way to get started : This  tab uses our Fireworks Build\nSDK for the best performance and developer experience.\nSee our Client-side performance\noptimization guide for\nmore details. Install SDK Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Configure API Key Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either ~/.bash_profile for Bash or ~/.zshrc for Zsh .\nYou can do this by running the command: Add a new line to the file with the following: After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. You can verify that the variable has been set correctly by running echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: Sending the first API Request You can quickly instantiate the LLM class and call the Fireworks API. The Build SDK handles deployment management automatically. You can also pass the API key directly to the LLM constructor: LLM(model=\"llama4-maverick-instruct-basic\", deployment_type=\"auto\", api_key=\"<FIREWORKS_API_KEY>\") Install SDK Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. Fireworks AI platform offers drop-in replacement with OpenAI Python Client. Configure API Key Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either ~/.bash_profile for Bash or ~/.zshrc for Zsh .\nYou can do this by running the command: Add a new line to the file with the following: After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. You can verify that the variable has been set correctly by running echo $OPENAI_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: echo %OPENAI_API_KEY%\n\n", "type": "text"}], "tool_call_id": "functions.search:0"}, {"role": "assistant", "content": "Let me search for more specific information about deployment management commands and workflows.", "tool_calls": [{"id": "functions.search:1", "function": {"arguments": "{\"query\": \"firectl deployment management commands create delete list\"}", "name": "search"}, "type": "function"}]}, {"role": "tool", "content": [{"text": "Title: Why am I getting \"invalid id\" errors when using firectl commands like create deployment or list deployments?\nLink: https://docs.fireworks.ai/faq-new/fine-tuning/why-am-i-getting-invalid-id-errors-when-using-firectl-commands-like-create-deplo\nContent: This error typically occurs when your account ID is not properly configured.\n\nThis error typically occurs when your account ID is not properly configured.\n\nWhy am I getting \"invalid id\" errors when using firectl commands like create deployment or list deployments?\nThis error typically occurs when your account ID is not properly configured.\n\n", "type": "text"}, {"text": "Title: Delete a deployment\nLink: https://docs.fireworks.ai/tools-sdks/firectl/commands/delete-resources\nContent: Deletes an model deployment. firectl delete deployment [flags]\n\nDeletes an model deployment. firectl delete deployment [flags]\n\nExample\nfirectl delete deployment my-deployment\n\n", "type": "text"}, {"text": "Title: Create a deployment\nLink: https://docs.fireworks.ai/tools-sdks/firectl/commands/create-deployment\nContent: Creates a new deployment. firectl create deployment [flags]\n\nCreates a new deployment. firectl create deployment [flags]\n\nCreates a new deployment.\n\n", "type": "text"}, {"text": "Title: Creating an on-demand deployment\nLink: https://docs.fireworks.ai/guides/ondemand-deployments\nContent: Choose a model Check out our model library to see a list of models available for on-demand deployment. You can also upload and use your own custom base model. Create a deployment To create a new deployment of a model from the model library, use: Or, for a custom base model you have uploaded to your account: This command will complete when the deployment is READY . To let it run asynchronously, remove the --wait flag. If your account has purchased reserved capacity and this deployment meets the reservation criteria, it will be counted against that reservation and not billed as an on-demand deployment. Verify the deployment is running You can verify the deployment is active by running: The state field should show READY. The deployment ID is the last part of the deployment name: accounts/<ACCOUNT_ID>/deployments/<DEPLOYMENT_ID>. You can also list all deployments in your account at any time by running: firectl list deployments\n\nChoose a model Check out our model library to see a list of models available for on-demand deployment. You can also upload and use your own custom base model. Create a deployment To create a new deployment of a model from the model library, use: Or, for a custom base model you have uploaded to your account: This command will complete when the deployment is READY . To let it run asynchronously, remove the --wait flag. If your account has purchased reserved capacity and this deployment meets the reservation criteria, it will be counted against that reservation and not billed as an on-demand deployment. Verify the deployment is running You can verify the deployment is active by running: The state field should show READY. The deployment ID is the last part of the deployment name: accounts/<ACCOUNT_ID>/deployments/<DEPLOYMENT_ID>. You can also list all deployments in your account at any time by running: firectl list deployments\n\nChoose a model Check out our model library to see a list of models available for on-demand deployment. You can also upload and use your own custom base model. Create a deployment To create a new deployment of a model from the model library, use: Or, for a custom base model you have uploaded to your account: This command will complete when the deployment is READY . To let it run asynchronously, remove the --wait flag. If your account has purchased reserved capacity and this deployment meets the reservation criteria, it will be counted against that reservation and not billed as an on-demand deployment. Verify the deployment is running You can verify the deployment is active by running: firectl get deployment <DEPLOYMENT_ID>\n\n", "type": "text"}, {"text": "Title: Delete Deployment\nLink: https://docs.fireworks.ai/api-reference/delete-deployment\nContent: \n\n\n\nREST Endpoint DELETE /v1/accounts/{account_id}/deployments/{deployment_id}\n\nServer(s): https://api.fireworks.ai\n\n\nSummary:  \n\nDescription: N/A\n\nSecurity methods: \nheader: \n\tAuthorization: bearer auth\n\t\n\nRequest params: \npath params:\naccount_id:\n\tstring\n\tDescription: The Account Id\n\tExample: \"<string>\"\ndeployment_id:\n\tstring\n\tDescription: The Deployment Id\n\tExample: \"<string>\"\n\nquery params:\nhard:\n\tboolean\n\tDescription: If true, this will perform a hard deletion.\n\tExample: true\nignoreChecks:\n\tboolean\n\tDescription: If true, this will ignore checks and force the deletion of a deployment that is currently\ndeployed and is in use.\n\tExample: true\n\n\nRequest body: N/A\n\n\nResponses: \n200: object\n\tDescription: N/A\n\tExample: {}\n\n", "type": "text"}, {"text": "Title: \ud83d\ude80 Easier & faster LoRA fine-tune deployments on Fireworks\nLink: https://docs.fireworks.ai/updates/changelog\nContent: You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment \"accounts/fireworks/models/<MODEL_ID of lora model>\"\n\nYou can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment \"accounts/fireworks/models/<MODEL_ID of lora model>\"\n\nYou can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: Previously, this involved two distinct steps, and the resulting deployment was slower than the base model: Create a deployment using firectl create deployment \"accounts/fireworks/models/<MODEL_ID of base model>\" --enable-addons Then deploy the addon to the deployment: firectl load-lora <MODEL_ID> --deployment <DEPLOYMENT_ID> For more information, see our deployment documentation. This change is for dedicated deployments with a single LoRA. You can still deploy multiple LoRAs on a deployment or deploy LoRA(s) on some Serverless models as described in the documentation.\n\n", "type": "text"}, {"text": "Title: Deployment Command\nLink: https://docs.fireworks.ai/evaluators/examples/gcp_cloud_run_deployment_example\nContent: It's recommended to run the deployment command from the directory containing the reward function script ( dummy_rewards.py ) and your rewardkit.yaml (if used), for example, from examples/gcp_cloud_run_deployment_example/. Ensure your virtual environment is active: Run the deployment command: Command Explanation: --id my-dummy-gcp-evaluator : A unique ID for your evaluator on the Fireworks AI platform. --target gcp-cloud-run : Specifies deployment to GCP Cloud Run. --function-ref dummy_rewards.hello_world_reward : The Python import path to your reward function. If dummy_rewards.py is in the current directory, this reference works. --gcp-auth-mode api-key : Configures the Cloud Run service with API key authentication. reward-kit will generate a key, store it in GCP Secret Manager, and configure the service. The key is also saved to your local rewardkit.yaml under evaluator_endpoint_keys . This is the default if not specified. --verbose : Shows detailed output, including gcloud commands being executed. --force : (Optional) If an evaluator with the same --id already exists, this flag will delete the existing one before creating the new one.\n\nIt's recommended to run the deployment command from the directory containing the reward function script ( dummy_rewards.py ) and your rewardkit.yaml (if used), for example, from examples/gcp_cloud_run_deployment_example/. Ensure your virtual environment is active: Run the deployment command: Command Explanation: --id my-dummy-gcp-evaluator : A unique ID for your evaluator on the Fireworks AI platform. --target gcp-cloud-run : Specifies deployment to GCP Cloud Run. --function-ref dummy_rewards.hello_world_reward : The Python import path to your reward function. If dummy_rewards.py is in the current directory, this reference works. --gcp-auth-mode api-key : Configures the Cloud Run service with API key authentication. reward-kit will generate a key, store it in GCP Secret Manager, and configure the service. The key is also saved to your local rewardkit.yaml under evaluator_endpoint_keys . This is the default if not specified. --verbose : Shows detailed output, including gcloud commands being executed. --force : (Optional) If an evaluator with the same --id already exists, this flag will delete the existing one before creating the new one.\n\nIt's recommended to run the deployment command from the directory containing the reward function script ( dummy_rewards.py ) and your rewardkit.yaml (if used), for example, from examples/gcp_cloud_run_deployment_example/. Ensure your virtual environment is active: source .venv/bin/activate\n\n", "type": "text"}, {"text": "Title: Quick deployment\nLink: https://docs.fireworks.ai/fine-tuning/single-lora\nContent: Deploy your LoRA fine-tuned model with one simple command: firectl create deployment \"accounts/fireworks/models/<MODEL_ID of lora model>\"\n\nDeploy your LoRA fine-tuned model with one simple command: firectl create deployment \"accounts/fireworks/models/<MODEL_ID of lora model>\"\n\nAlternative deployment method\nThis two-step method is the standard approach for multi-LoRA deployments where multiple LoRA models share the same base model. While it can also be used for single LoRA deployments, it provides slower performance compared to live merge and is not recommended for single LoRA use cases. You can also deploy single LoRA models using a two-step process: Create base model deployment Deploy the base model with addons enabled: firectl create deployment \"accounts/fireworks/models/<MODEL_ID of base model>\" --enable-addons\n\n", "type": "text"}, {"text": "Title: Deploying a fine-tuned model using an on-demand deployment\nLink: https://docs.fireworks.ai/fine-tuning/fine-tuning-models\nContent: Use the following command to deploy your fine-tuned model using an on-demand deployment: firectl create deployment <FINE_TUNED_MODEL_ID>\n\nUse the following command to deploy your fine-tuned model using an on-demand deployment: firectl create deployment <FINE_TUNED_MODEL_ID>\n\nfirectl\nfirectl create dataset <DATASET_ID> /path/to/jsonl/file\n\n", "type": "text"}, {"text": "Title: Deleting Stored Responses\nLink: https://docs.fireworks.ai/guides/response-api\nContent: When responses are stored (the default behavior with store=True ), you can immediately delete them from storage using the DELETE endpoint. This permanently removes the conversation data. from fireworks import LLM\nimport requests\nimport os\n\nllm = LLM(model=\"qwen3-235b-a22b\", deployment_type=\"serverless\")\n\n# Create a response\nresponse = llm.responses.create(\n    input=\"What is the capital of France?\",\n    store=True  # This is the default\n)\n\nresponse_id = response.id\nprint(f\"Created response with ID: {response_id}\")\n\n# Delete the response immediately\nheaders = {\n    \"Authorization\": f\"Bearer {os.getenv('FIREWORKS_API_KEY')}\",\n    \"x-fireworks-account-id\": \"your-account-id\"\n}\ndelete_response = requests.delete(\n    f\"https://api.fireworks.ai/inference/v1/responses/{response_id}\",\n    headers=headers\n)\n\nif delete_response.status_code == 200:\n    print(\"Response deleted successfully\")\nelse:\n    print(f\"Failed to delete response: {delete_response.status_code}\")\n\nWhen responses are stored (the default behavior with store=True ), you can immediately delete them from storage using the DELETE endpoint. This permanently removes the conversation data. from fireworks import LLM\nimport requests\nimport os\n\nllm = LLM(model=\"qwen3-235b-a22b\", deployment_type=\"serverless\")\n\n# Create a response\nresponse = llm.responses.create(\n    input=\"What is the capital of France?\",\n    store=True  # This is the default\n)\n\nresponse_id = response.id\nprint(f\"Created response with ID: {response_id}\")\n\n# Delete the response immediately\nheaders = {\n    \"Authorization\": f\"Bearer {os.getenv('FIREWORKS_API_KEY')}\",\n    \"x-fireworks-account-id\": \"your-account-id\"\n}\ndelete_response = requests.delete(\n    f\"https://api.fireworks.ai/inference/v1/responses/{response_id}\",\n    headers=headers\n)\n\nif delete_response.status_code == 200:\n    print(\"Response deleted successfully\")\nelse:\n    print(f\"Failed to delete response: {delete_response.status_code}\")\n\nWhen responses are stored (the default behavior with store=True ), you can immediately delete them from storage using the DELETE endpoint. This permanently removes the conversation data. import os\nfrom openai import OpenAI\nimport requests\n\nclient = OpenAI(\n    base_url=\"https://api.fireworks.ai/inference/v1\",\n    api_key=os.getenv(\"FIREWORKS_API_KEY\", \"YOUR_FIREWORKS_API_KEY_HERE\")\n)\n\n# Create a response\nresponse = client.responses.create(\n    model=\"accounts/fireworks/models/qwen3-235b-a22b\",\n    input=\"What is the capital of France?\",\n    store=True  # This is the default\n)\n\nresponse_id = response.id\nprint(f\"Created response with ID: {response_id}\")\n\n# Delete the response immediately\nheaders = {\n    \"Authorization\": f\"Bearer {os.getenv('FIREWORKS_API_KEY')}\",\n    \"x-fireworks-account-id\": \"your-account-id\"\n}\ndelete_response = requests.delete(\n    f\"https://api.fireworks.ai/inference/v1/responses/{response_id}\",\n    headers=headers\n)\n\nif delete_response.status_code == 200:\n    print(\"Response deleted successfully\")\nelse:\n    print(f\"Failed to delete response: {delete_response.status_code}\")\n\n", "type": "text"}], "tool_call_id": "functions.search:1"}, {"role": "assistant", "content": "Now let me search for information about listing and managing deployments to give you a complete picture.", "tool_calls": [{"id": "functions.search:1", "function": {"arguments": "{\"query\": \"list deployments get deployment status firectl commands\"}", "name": "search"}, "type": "function"}]}, {"role": "tool", "content": [{"text": "Title: Why am I getting \"invalid id\" errors when using firectl commands like create deployment or list deployments?\nLink: https://docs.fireworks.ai/faq-new/fine-tuning/why-am-i-getting-invalid-id-errors-when-using-firectl-commands-like-create-deplo\nContent: This error typically occurs when your account ID is not properly configured.\n\nThis error typically occurs when your account ID is not properly configured.\n\nWhy am I getting \"invalid id\" errors when using firectl commands like create deployment or list deployments?\nThis error typically occurs when your account ID is not properly configured.\n\n", "type": "text"}, {"text": "Title: List deployments\nLink: https://docs.fireworks.ai/tools-sdks/firectl/commands/list-resources\nContent: Prints all deployments in the account. firectl list deployments [flags]\n\nPrints all deployments in the account. firectl list deployments [flags]\n\nList deployed models\nPrints all deployed models in an account. firectl list deployed-models [flags]\n\n", "type": "text"}, {"text": "Title: Creating an on-demand deployment\nLink: https://docs.fireworks.ai/guides/ondemand-deployments\nContent: Choose a model Check out our model library to see a list of models available for on-demand deployment. You can also upload and use your own custom base model. Create a deployment To create a new deployment of a model from the model library, use: Or, for a custom base model you have uploaded to your account: This command will complete when the deployment is READY . To let it run asynchronously, remove the --wait flag. If your account has purchased reserved capacity and this deployment meets the reservation criteria, it will be counted against that reservation and not billed as an on-demand deployment. Verify the deployment is running You can verify the deployment is active by running: The state field should show READY. The deployment ID is the last part of the deployment name: accounts/<ACCOUNT_ID>/deployments/<DEPLOYMENT_ID>. You can also list all deployments in your account at any time by running: firectl list deployments\n\nChoose a model Check out our model library to see a list of models available for on-demand deployment. You can also upload and use your own custom base model. Create a deployment To create a new deployment of a model from the model library, use: Or, for a custom base model you have uploaded to your account: This command will complete when the deployment is READY . To let it run asynchronously, remove the --wait flag. If your account has purchased reserved capacity and this deployment meets the reservation criteria, it will be counted against that reservation and not billed as an on-demand deployment. Verify the deployment is running You can verify the deployment is active by running: The state field should show READY. The deployment ID is the last part of the deployment name: accounts/<ACCOUNT_ID>/deployments/<DEPLOYMENT_ID>. You can also list all deployments in your account at any time by running: firectl list deployments\n\nPublishing a deployed model\nBy default, models can only be queried by the account that owns them. To make a deployment public so anyone with a valid Fireworks API key can query it, update the deployed model with the --public flag. Find the Deployed Model ID Every model running on a deployment receives a unique deployed model ID. You can find this ID if you know the model name and deployment name using the following command: firectl list deployed-models --filter 'model=\"<MODEL_NAME>\" AND deployment=\"<DEPLOYMENT_NAME>\"'\n\n", "type": "text"}, {"text": "Title: \ud83d\ude80 Easier & faster LoRA fine-tune deployments on Fireworks\nLink: https://docs.fireworks.ai/updates/changelog\nContent: You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment \"accounts/fireworks/models/<MODEL_ID of lora model>\"\n\nYou can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment \"accounts/fireworks/models/<MODEL_ID of lora model>\"\n\nYou can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: Previously, this involved two distinct steps, and the resulting deployment was slower than the base model: Create a deployment using firectl create deployment \"accounts/fireworks/models/<MODEL_ID of base model>\" --enable-addons Then deploy the addon to the deployment: firectl load-lora <MODEL_ID> --deployment <DEPLOYMENT_ID> For more information, see our deployment documentation. This change is for dedicated deployments with a single LoRA. You can still deploy multiple LoRAs on a deployment or deploy LoRA(s) on some Serverless models as described in the documentation.\n\n", "type": "text"}, {"text": "Title: firectl\nLink: https://docs.fireworks.ai/fine-tuning/fine-tuning-vlm\nContent: # Create a deployment for your fine-tuned VLM\nfirectl create deployment my-custom-vlm\n\n# Check deployment status\nfirectl get deployment accounts/your-account/deployment/deployment-id\n\n# Create a deployment for your fine-tuned VLM\nfirectl create deployment my-custom-vlm\n\n# Check deployment status\nfirectl get deployment accounts/your-account/deployment/deployment-id\n\nTry with an Example Dataset\nTo get a feel for how VLM fine-tuning works, you can use an example vision dataset: This is a classification dataset that contains images of food with <think></think> tags for reasoning. Once downloaded, you can upload this dataset using the instructions in the next step. Upload your VLM dataset Upload your prepared JSONL dataset to Fireworks for training: Navigate to the Datasets tab in the Fireworks console, click \"Create Dataset\", and upload your JSONL file through the wizard. For larger datasets (>500MB), use  as it handles large uploads more reliably than the web interface. For enhanced data control and security, we also support bring your own bucket (BYOB) configurations. See our External GCS Bucket Integration guide for setup details. Launch VLM fine-tuning job Create a supervised fine-tuning job for your VLM: For additional parameters like learning rates, evaluation datasets, and batch sizes, see Additional SFT job settings. Navigate to the Fine-tuning tab in the Fireworks console Click \"Create Fine-tuning Job\" Select your VLM base model (Qwen 2.5 VL) Choose your uploaded dataset Configure training parameters Launch the job VLM fine-tuning jobs typically take longer than text-only models due to the additional image processing. Expect training times of several hours depending on dataset size and model complexity. Monitor training progress Track your VLM fine-tuning job in the Fireworks console. Monitor key metrics: Training loss : Should generally decrease over time Evaluation loss : Monitor for overfitting if using evaluation dataset Training progress : Epochs completed and estimated time remaining Your VLM fine-tuning job is complete when the status shows COMPLETED and your custom model is ready for deployment. Deploy your fine-tuned VLM Once training is complete, deploy your custom VLM: # Create a deployment for your fine-tuned VLM\nfirectl create deployment my-custom-vlm\n\n# Check deployment status\nfirectl get deployment accounts/your-account/deployment/deployment-id\n\n", "type": "text"}, {"text": "Title: firectl\nLink: https://docs.fireworks.ai/guides/batch-inference\nContent: # Get job status\nfirectl get batch-inference-job my-batch-job\n\n# List all batch jobs\nfirectl list batch-inference-jobs\n\n# Get job status\nfirectl get batch-inference-job my-batch-job\n\n# List all batch jobs\nfirectl list batch-inference-jobs\n\nfirectl create dataset batch-input-dataset ./batch_input_data.jsonl\n\n", "type": "text"}, {"text": "Title: Does Fireworks support custom base models?\nLink: https://docs.fireworks.ai/faq-new/models-inference/does-fireworks-support-custom-base-models\nContent: Yes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.\n\nYes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.\n\nDoes Fireworks support custom base models?\nYes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.\n\n", "type": "text"}, {"text": "Title: List Deployments\nLink: https://docs.fireworks.ai/api-reference/list-deployments\nContent: \n\n\n\nREST Endpoint GET /v1/accounts/{account_id}/deployments\n\nServer(s): https://api.fireworks.ai\n\n\nSummary:  \n\nDescription: N/A\n\nSecurity methods: \nheader: \n\tAuthorization: bearer auth\n\t\n\nRequest params: \npath params:\naccount_id:\n\tstring\n\tDescription: The Account Id\n\tExample: \"<string>\"\n\nquery params:\npageSize:\n\tinteger\n\tDescription: The maximum number of deployments to return. The maximum page_size is 200,\nvalues above 200 will be coerced to 200.\nIf unspecified, the default is 50.\n\tExample: 123\npageToken:\n\tstring\n\tDescription: A page token, received from a previous ListDeployments call. Provide this\nto retrieve the subsequent page. When paginating, all other parameters\nprovided to ListDeployments must match the call that provided the page\ntoken.\n\tExample: \"<string>\"\nfilter:\n\tstring\n\tDescription: Only deployment satisfying the provided filter (if specified) will be\nreturned. See https://google.aip.dev/160 for the filter grammar.\n\tExample: \"<string>\"\norderBy:\n\tstring\n\tDescription: A comma-separated list of fields to order by. e.g. \"foo,bar\"\nThe default sort order is ascending. To specify a descending order for a\nfield, append a \" desc\" suffix. e.g. \"foo desc,bar\"\nSubfields are specified with a \".\" character. e.g. \"foo.bar\"\nIf not specified, the default order is by \"create_time\".\n\tExample: \"<string>\"\nshowDeleted:\n\tboolean\n\tDescription: If set, DELETED deployments will be included.\n\tExample: true\nreadMask:\n\tstring\n\tDescription: The fields to be returned in the response. If empty or \"*\", all fields will be returned.\n\tExample: \"<string>\"\n\n\nRequest body: N/A\n\n\nResponses: \n200: object\n\tDescription: N/A\n\tExample: {\"deployments\":[{\"name\":\"<string>\",\"displayName\":\"<string>\",\"description\":\"<string>\",\"createTime\":\"2023-11-07T05:31:56Z\",\"expireTime\":\"2023-11-07T05:31:56Z\",\"purgeTime\":\"2023-11-07T05:31:56Z\",\"deleteTime\":\"2023-11-07T05:31:56Z\",\"state\":\"STATE_UNSPECIFIED\",\"status\":{\"code\":\"OK\",\"message\":\"<string>\"},\"minReplicaCount\":123,\"maxReplicaCount\":123,\"replicaCount\":123,\"autoscalingPolicy\":{\"scaleUpWindow\":\"<string>\",\"scaleDownWindow\":\"<string>\",\"scaleToZeroWindow\":\"<string>\",\"loadTargets\":{}},\"baseModel\":\"<string>\",\"acceleratorCount\":123,\"acceleratorType\":\"ACCELERATOR_TYPE_UNSPECIFIED\",\"precision\":\"PRECISION_UNSPECIFIED\",\"cluster\":\"<string>\",\"enableAddons\":true,\"draftTokenCount\":123,\"draftModel\":\"<string>\",\"ngramSpeculationLength\":123,\"numPeftDeviceCached\":123,\"deploymentTemplate\":\"<string>\",\"autoTune\":{\"longPrompt\":true},\"placement\":{\"region\":\"REGION_UNSPECIFIED\",\"multiRegion\":\"MULTI_REGION_UNSPECIFIED\",\"regions\":[\"REGION_UNSPECIFIED\"]},\"region\":\"REGION_UNSPECIFIED\",\"updateTime\":\"2023-11-07T05:31:56Z\",\"disableDeploymentSizeValidation\":true}],\"nextPageToken\":\"<string>\",\"totalSize\":123}\n\n", "type": "text"}, {"text": "Title: Deploy Command\nLink: https://docs.fireworks.ai/evaluators/cli_reference/cli_overview\nContent: The deploy command deploys a reward function as an evaluator on the Fireworks platform.\n\nThe deploy command deploys a reward function as an evaluator on the Fireworks platform.\n\nCommand Overview\nThe Reward Kit CLI supports the following main commands: run : Run a local evaluation pipeline using a Hydra configuration. preview : Preview evaluation results or re-evaluate generated outputs. deploy : Deploy a reward function as an evaluator. agent-eval : Run agent evaluations on task bundles. list : List existing evaluators (coming soon). delete : Delete an evaluator (coming soon).\n\n", "type": "text"}, {"text": "Title: Quick deployment\nLink: https://docs.fireworks.ai/fine-tuning/single-lora\nContent: Deploy your LoRA fine-tuned model with one simple command: firectl create deployment \"accounts/fireworks/models/<MODEL_ID of lora model>\"\n\nDeploy your LoRA fine-tuned model with one simple command: firectl create deployment \"accounts/fireworks/models/<MODEL_ID of lora model>\"\n\nAlternative deployment method\nThis two-step method is the standard approach for multi-LoRA deployments where multiple LoRA models share the same base model. While it can also be used for single LoRA deployments, it provides slower performance compared to live merge and is not recommended for single LoRA use cases. You can also deploy single LoRA models using a two-step process: Create base model deployment Deploy the base model with addons enabled: firectl create deployment \"accounts/fireworks/models/<MODEL_ID of base model>\" --enable-addons\n\n", "type": "text"}], "tool_call_id": "functions.search:1"}, {"role": "assistant", "content": "Perfect! Now I have comprehensive information about managing deployments on Fireworks. Let me provide you with a complete guide.\n\n# Managing Deployments on Fireworks: A Complete Guide\n\nFireworks offers several ways to manage deployments, from command-line tools to REST APIs. Here's everything you need to know:\n\n## Types of Deployments\n\n### 1. On-Demand Deployments\n- **Billed by GPU-second** (not per-token like serverless)\n- **Dedicated resources** - no interference from other users\n- **No hard rate limits** (subject to deployment capacity)\n- **Access to more models** including custom ones\n- **Better for high utilization** scenarios\n\n### 2. Serverless Deployments\n- **Billed per-token**\n- **Shared resources** with other users\n- **Rate limits apply**\n- **Instant availability**\n\n## Management Tools\n\n### Option 1: Fireworks Build SDK (Recommended)\n```python\nfrom fireworks import LLM\n\n# Auto-deployment - SDK handles everything\nllm = LLM(model=\"llama4-maverick-instruct-basic\", deployment_type=\"auto\")\n\n# Manual deployment management\nllm = LLM(model=\"your-model\", deployment_type=\"on-demand\")\n```\n\n### Option 2: firectl CLI Tool\n\n#### Installation\n```bash\n# Install firectl CLI\ncurl -s https://fireworks.ai/install.sh | bash\n```\n\n#### Basic Commands\n\n**List all deployments:**\n```bash\nfirectl list deployments\n```\n\n**Create a deployment:**\n```bash\n# From model library\nfirectl create deployment accounts/fireworks/models/llama-v3p1-8b-instruct --wait\n\n# Custom model\nfirectl create deployment accounts/your-account-id/models/your-custom-model --wait\n\n# LoRA fine-tuned model (single command)\nfirectl create deployment accounts/fireworks/models/your-lora-model-id\n```\n\n**Get deployment status:**\n```bash\nfirectl get deployment <deployment-id>\n```\n\n**Delete a deployment:**\n```bash\nfirectl delete deployment <deployment-id>\n```\n\n**List deployed models:**\n```bash\nfirectl list deployed-models\n```\n\n### Option 3: REST API\n\n#### List Deployments\n```bash\ncurl -X GET \\\n  \"https://api.fireworks.ai/v1/accounts/{account_id}/deployments\" \\\n  -H \"Authorization: Bearer $FIREWORKS_API_KEY\"\n```\n\n#### Create Deployment\n```bash\ncurl -X POST \\\n  \"https://api.fireworks.ai/v1/accounts/{account_id}/deployments\" \\\n  -H \"Authorization: Bearer $FIREWORKS_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"deployment\": {\n      \"displayName\": \"my-deployment\",\n      \"baseModel\": \"accounts/fireworks/models/llama-v3p1-8b-instruct\"\n    }\n  }'\n```\n\n#### Delete Deployment\n```bash\ncurl -X DELETE \\\n  \"https://api.fireworks.ai/v1/accounts/{account_id}/deployments/{deployment_id}\" \\\n  -H \"Authorization: Bearer $FIREWORKS_API_KEY\"\n```\n\n## Deployment Workflow\n\n### 1. Choose Your Model\n```bash\n# Check available models\nfirectl list models\n```\n\n### 2. Create Deployment\n```bash\n# Basic deployment\nfirectl create deployment accounts/fireworks/models/llama-v3p1-8b-instruct --wait\n\n# With custom configuration\nfirectl create deployment accounts/fireworks/models/llama-v3p1-8b-instruct \\\n  --min-replica-count 1 \\\n  --max-replica-count 3 \\\n  --wait\n```\n\n### 3. Monitor Status\n```bash\n# Get deployment details\nfirectl get deployment <deployment-id>\n\n# Watch logs\nfirectl logs deployment <deployment-id>\n```\n\n### 4. Make Public (Optional)\n```bash\n# Make deployment public\nfirectl update deployment <deployment-id> --public\n```\n\n### 5. Clean Up\n```bash\n# Delete when done\nfirectl delete deployment <deployment-id>\n```\n\n## Advanced Features\n\n### Reserved Capacity\nFor high-volume usage, consider reserved capacity:\n- More reliable capacity\n- Higher GPU quotas\n- Cost savings for consistent usage\n- Contact Fireworks for enterprise pricing\n\n### Multi-LoRA Deployments\n```bash\n# Create base deployment with addons\nfirectl create deployment accounts/fireworks/models/base-model --enable-addons --wait\n\n# Load LoRA models\nfirectl load-lora lora-model-1 --deployment <deployment-id>\nfirectl load-lora lora-model-2 --deployment <deployment-id>\n```\n\n### Custom Models\n```bash\n# Upload custom model first\nfirectl upload model /path/to/model\n\n# Deploy custom model\nfirectl create deployment accounts/your-account-id/models/custom-model-name\n```\n\n## Best Practices\n\n1. **Use `--wait` flag** for important deployments to ensure they're ready\n2. **Monitor costs** with `firectl list deployments` to see active deployments\n3. **Set up alerts** for deployment health\n4. **Use reserved capacity** for production workloads\n5. **Clean up unused deployments** to avoid charges\n6. **Test deployments** before making them public\n\n## Troubleshooting\n\n**\"Invalid ID\" errors:**\n- Ensure your account ID is properly configured\n- Check your API key and account settings\n\n**Deployment stuck:**\n- Check resource availability\n- Verify model compatibility\n- Review deployment logs\n\n**Performance issues:**\n- Consider scaling up replicas\n- Check if reserved capacity is available\n- Monitor GPU utilization\n\n## Cost Management\n- **On-demand**: Pay per GPU-second used\n- **Serverless**: Pay per token processed\n- **Reserved capacity**: Fixed monthly cost for guaranteed capacity\n- Use `firectl list deployments` to track active deployments and costs\n\nThis comprehensive guide should give you everything you need to effectively manage deployments on Fireworks!", "tool_calls": []}], "input_metadata": {"row_id": "construct-do-safe-art-song", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct"}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "evaluation_result": {"score": 1.0, "is_score_valid": true, "reason": "At least one tool call was made", "metrics": {}}, "created_at": "2025-08-08T11:27:14.528488", "eval_metadata": {"name": "test_pytest_mcp_url", "description": "Run math evaluation on sample dataset using pytest interface.", "version": "v0.2.6.dev15+g6486dbd.dirty", "status": "finished", "num_runs": 1, "aggregation_method": "mean"}, "pid": 92995}, {"messages": [{"role": "system", "content": "You are a helpful assistant that can answer questions about Fireworks.\nALWAYS provide code or commands to execute to answer the question."}, {"role": "user", "content": "Can you teach me about how to manage deployments on Fireworks"}, {"role": "assistant", "content": "I'll help you learn about managing deployments on Fireworks. Let me search for comprehensive information about deployment management.", "tool_calls": [{"id": "functions.search:0", "function": {"arguments": "{\"query\": \"deployment management Fireworks AI\"}", "name": "search"}, "type": "function"}]}, {"role": "tool", "content": [{"text": "Title: Fireworks AI Developer Platform\nLink: https://docs.fireworks.ai/getting-started/introduction\nContent: Fireworks AI is the best platform for building AI product experiences with open source AI models. You can run and customize AI models with just a few lines of code!\n\nFireworks AI is the best platform for building AI product experiences with open source AI models. You can run and customize AI models with just a few lines of code!\n\nStart building with open source AI models\n\n", "type": "text"}, {"text": "Title: How do I close my Fireworks.ai account?\nLink: https://docs.fireworks.ai/faq-new/account-access/how-do-i-close-my-fireworksai-account\nContent: To close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed\n\nTo close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed\n\nHow do I close my Fireworks.ai account?\nTo close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed\n\n", "type": "text"}, {"text": "Title: Do you host your deployments in the EU or Asia?\nLink: https://docs.fireworks.ai/faq-new/support-general/do-you-host-your-deployments-in-the-eu-or-asia\nContent: We are currently deployed in multiple U.S.-based locations. However, we\u2019re open to hearing more about your specific requirements. You can: Join our Discord community Write to us at inquiries@fireworks.ai If you're an Enterprise customer, please contact your dedicated customer support representative to ensure a timely response.\n\nWe are currently deployed in multiple U.S.-based locations. However, we\u2019re open to hearing more about your specific requirements. You can: Join our Discord community Write to us at inquiries@fireworks.ai If you're an Enterprise customer, please contact your dedicated customer support representative to ensure a timely response.\n\nDo you host your deployments in the EU or Asia?\nWe are currently deployed in multiple U.S.-based locations. However, we\u2019re open to hearing more about your specific requirements. You can: Join our Discord community Write to us at inquiries@fireworks.ai If you're an Enterprise customer, please contact your dedicated customer support representative to ensure a timely response.\n\n", "type": "text"}, {"text": "Title: Introducing the Fireworks Build SDK\nLink: https://docs.fireworks.ai/tools-sdks/python-client/sdk-introduction\nContent: The Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.\n\nThe Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.\n\nIntroducing the Fireworks Build SDK\nThe Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.\n\n", "type": "text"}, {"text": "Title: On-demand deployments\nLink: https://docs.fireworks.ai/guides/ondemand-deployments\nContent: If you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.\n\nIf you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.\n\nOn-demand deployments\nIf you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.\n\n", "type": "text"}, {"text": "Title: I have multiple Fireworks accounts. When I try to login with Google on Fireworks' web UI, I'm getting signed into the wrong account. How do I fix this?\nLink: https://docs.fireworks.ai/faq-new/account-access/i-have-multiple-fireworks-accounts-when-i-try-to-login-with-google-on-fireworks\nContent: If you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.\n\nIf you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.\n\nI have multiple Fireworks accounts. When I try to login with Google on Fireworks' web UI, I'm getting signed into the wrong account. How do I fix this?\nIf you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.\n\n", "type": "text"}, {"text": "Title: Introduction\nLink: https://docs.fireworks.ai/api-reference/introduction\nContent: Fireworks AI REST API enables you to interact with various language, image and embedding models using an API Key. It also lets you automate management of models, deployments, datasets, and more.\n\nFireworks AI REST API enables you to interact with various language, image and embedding models using an API Key. It also lets you automate management of models, deployments, datasets, and more.\n\nIntroduction\nFireworks AI REST API enables you to interact with various language, image and embedding models using an API Key. It also lets you automate management of models, deployments, datasets, and more.\n\n", "type": "text"}, {"text": "Title: Create a deployment\nLink: https://docs.fireworks.ai/tools-sdks/firectl/commands/create-deployment\nContent: Create a Deployment on Fireworks AI platform\n\nCreate a Deployment on Fireworks AI platform\n\nCreates a new deployment.\n\n", "type": "text"}, {"text": "Title: How much does Fireworks cost?\nLink: https://docs.fireworks.ai/faq-new/billing-pricing/how-much-does-fireworks-cost\nContent: Fireworks AI operates on a pay-as-you-go model for all non-Enterprise usage, and new users automatically receive free credits. You pay based on: Per token for serverless inference Per GPU usage time for on-demand deployments Per token of training data for fine-tuning For customers needing enterprise-grade security and reliability , please reach out to us at inquiries@fireworks.ai to discuss options. Find out more about our current pricing on our Pricing page.\n\nFireworks AI operates on a pay-as-you-go model for all non-Enterprise usage, and new users automatically receive free credits. You pay based on: Per token for serverless inference Per GPU usage time for on-demand deployments Per token of training data for fine-tuning For customers needing enterprise-grade security and reliability , please reach out to us at inquiries@fireworks.ai to discuss options. Find out more about our current pricing on our Pricing page.\n\nHow much does Fireworks cost?\nFireworks AI operates on a pay-as-you-go model for all non-Enterprise usage, and new users automatically receive free credits. You pay based on: Per token for serverless inference Per GPU usage time for on-demand deployments Per token of training data for fine-tuning For customers needing enterprise-grade security and reliability , please reach out to us at inquiries@fireworks.ai to discuss options. Find out more about our current pricing on our Pricing page.\n\n", "type": "text"}, {"text": "Title: Python (Fireworks)\nLink: https://docs.fireworks.ai/getting-started/quickstart\nContent: This is the recommended way to get started : This  Python (Fireworks)  tab uses our  Fireworks Build\nSDK  for the best performance and developer experience.\nSee our  Client-side performance\noptimization  guide for\nmore details. Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. pip install --upgrade fireworks-ai The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either  ~/.bash_profile  for Bash or  ~/.zshrc  for  Zsh .\nYou can do this by running the command: vim ~/.bash_profile vim ~/.zshrc Add a new line to the file with the following: export FIREWORKS_API_KEY=\"<API_KEY>\" After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. source ~/.bash_profile source ~/.zshrc You can verify that the variable has been set correctly by running  echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. setx FIREWORKS_API_KEY \"<API_KEY>\" To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: echo %FIREWORKS_API_KEY% You can quickly instantiate the  LLM  class and call the Fireworks API. The Build SDK handles deployment management automatically. from fireworks import LLM\n\n# Basic usage - SDK automatically selects optimal deployment type\nllm = LLM(model=\"llama4-maverick-instruct-basic\", deployment_type=\"auto\")\n\nresponse = llm.chat.completions.create(\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}]\n)\n\nprint(response.choices[0].message.content) You can also pass the API key directly to the LLM constructor:  LLM(model=\"llama4-maverick-instruct-basic\", deployment_type=\"auto\", api_key=\"<FIREWORKS_API_KEY>\")\n\nThis is the recommended way to get started : This  Python (Fireworks)  tab uses our  Fireworks Build\nSDK  for the best performance and developer experience.\nSee our  Client-side performance\noptimization  guide for\nmore details. Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. pip install --upgrade fireworks-ai The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either  ~/.bash_profile  for Bash or  ~/.zshrc  for  Zsh .\nYou can do this by running the command: vim ~/.bash_profile vim ~/.zshrc Add a new line to the file with the following: export FIREWORKS_API_KEY=\"<API_KEY>\" After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. source ~/.bash_profile source ~/.zshrc You can verify that the variable has been set correctly by running  echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. setx FIREWORKS_API_KEY \"<API_KEY>\" To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: echo %FIREWORKS_API_KEY% You can quickly instantiate the  LLM  class and call the Fireworks API. The Build SDK handles deployment management automatically. from fireworks import LLM\n\n# Basic usage - SDK automatically selects optimal deployment type\nllm = LLM(model=\"llama4-maverick-instruct-basic\", deployment_type=\"auto\")\n\nresponse = llm.chat.completions.create(\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}]\n)\n\nprint(response.choices[0].message.content) You can also pass the API key directly to the LLM constructor:  LLM(model=\"llama4-maverick-instruct-basic\", deployment_type=\"auto\", api_key=\"<FIREWORKS_API_KEY>\")\n\nSet up your developer environment & call the Fireworks API\nThis is the recommended way to get started : This  tab uses our Fireworks Build\nSDK for the best performance and developer experience.\nSee our Client-side performance\noptimization guide for\nmore details. Install SDK Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Configure API Key Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either ~/.bash_profile for Bash or ~/.zshrc for Zsh .\nYou can do this by running the command: Add a new line to the file with the following: After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. You can verify that the variable has been set correctly by running echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: Sending the first API Request You can quickly instantiate the LLM class and call the Fireworks API. The Build SDK handles deployment management automatically. You can also pass the API key directly to the LLM constructor: LLM(model=\"llama4-maverick-instruct-basic\", deployment_type=\"auto\", api_key=\"<FIREWORKS_API_KEY>\") Install SDK Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. Fireworks AI platform offers drop-in replacement with OpenAI Python Client. Configure API Key Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either ~/.bash_profile for Bash or ~/.zshrc for Zsh .\nYou can do this by running the command: Add a new line to the file with the following: After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. You can verify that the variable has been set correctly by running echo $OPENAI_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: echo %OPENAI_API_KEY%\n\n", "type": "text"}], "tool_call_id": "functions.search:0"}, {"role": "assistant", "content": "Let me search for more specific information about deployment management commands and workflows.", "tool_calls": [{"id": "functions.search:1", "function": {"arguments": "{\"query\": \"firectl deployment management commands create delete list update\"}", "name": "search"}, "type": "function"}]}, {"role": "tool", "content": [{"text": "Title: Why am I getting \"invalid id\" errors when using firectl commands like create deployment or list deployments?\nLink: https://docs.fireworks.ai/faq-new/fine-tuning/why-am-i-getting-invalid-id-errors-when-using-firectl-commands-like-create-deplo\nContent: This error typically occurs when your account ID is not properly configured.\n\nThis error typically occurs when your account ID is not properly configured.\n\nWhy am I getting \"invalid id\" errors when using firectl commands like create deployment or list deployments?\nThis error typically occurs when your account ID is not properly configured.\n\n", "type": "text"}, {"text": "Title: Delete a deployment\nLink: https://docs.fireworks.ai/tools-sdks/firectl/commands/delete-resources\nContent: Deletes an model deployment. firectl delete deployment [flags]\n\nDeletes an model deployment. firectl delete deployment [flags]\n\nExample\nfirectl delete deployment my-deployment\n\n", "type": "text"}, {"text": "Title: Create a deployment\nLink: https://docs.fireworks.ai/tools-sdks/firectl/commands/create-deployment\nContent: Creates a new deployment. firectl create deployment [flags]\n\nCreates a new deployment. firectl create deployment [flags]\n\nCreates a new deployment.\n\n", "type": "text"}, {"text": "Title: Creating an on-demand deployment\nLink: https://docs.fireworks.ai/guides/ondemand-deployments\nContent: Choose a model Check out our model library to see a list of models available for on-demand deployment. You can also upload and use your own custom base model. Create a deployment To create a new deployment of a model from the model library, use: Or, for a custom base model you have uploaded to your account: This command will complete when the deployment is READY . To let it run asynchronously, remove the --wait flag. If your account has purchased reserved capacity and this deployment meets the reservation criteria, it will be counted against that reservation and not billed as an on-demand deployment. Verify the deployment is running You can verify the deployment is active by running: The state field should show READY. The deployment ID is the last part of the deployment name: accounts/<ACCOUNT_ID>/deployments/<DEPLOYMENT_ID>. You can also list all deployments in your account at any time by running: firectl list deployments\n\nChoose a model Check out our model library to see a list of models available for on-demand deployment. You can also upload and use your own custom base model. Create a deployment To create a new deployment of a model from the model library, use: Or, for a custom base model you have uploaded to your account: This command will complete when the deployment is READY . To let it run asynchronously, remove the --wait flag. If your account has purchased reserved capacity and this deployment meets the reservation criteria, it will be counted against that reservation and not billed as an on-demand deployment. Verify the deployment is running You can verify the deployment is active by running: The state field should show READY. The deployment ID is the last part of the deployment name: accounts/<ACCOUNT_ID>/deployments/<DEPLOYMENT_ID>. You can also list all deployments in your account at any time by running: firectl list deployments\n\nPublishing a deployed model\nBy default, models can only be queried by the account that owns them. To make a deployment public so anyone with a valid Fireworks API key can query it, update the deployed model with the --public flag. Find the Deployed Model ID Every model running on a deployment receives a unique deployed model ID. You can find this ID if you know the model name and deployment name using the following command: firectl list deployed-models --filter 'model=\"<MODEL_NAME>\" AND deployment=\"<DEPLOYMENT_NAME>\"'\n\n", "type": "text"}, {"text": "Title: Delete Deployment\nLink: https://docs.fireworks.ai/api-reference/delete-deployment\nContent: \n\n\n\nREST Endpoint DELETE /v1/accounts/{account_id}/deployments/{deployment_id}\n\nServer(s): https://api.fireworks.ai\n\n\nSummary:  \n\nDescription: N/A\n\nSecurity methods: \nheader: \n\tAuthorization: bearer auth\n\t\n\nRequest params: \npath params:\naccount_id:\n\tstring\n\tDescription: The Account Id\n\tExample: \"<string>\"\ndeployment_id:\n\tstring\n\tDescription: The Deployment Id\n\tExample: \"<string>\"\n\nquery params:\nhard:\n\tboolean\n\tDescription: If true, this will perform a hard deletion.\n\tExample: true\nignoreChecks:\n\tboolean\n\tDescription: If true, this will ignore checks and force the deletion of a deployment that is currently\ndeployed and is in use.\n\tExample: true\n\n\nRequest body: N/A\n\n\nResponses: \n200: object\n\tDescription: N/A\n\tExample: {}\n\n", "type": "text"}, {"text": "Title: \ud83d\ude80 Easier & faster LoRA fine-tune deployments on Fireworks\nLink: https://docs.fireworks.ai/updates/changelog\nContent: You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment \"accounts/fireworks/models/<MODEL_ID of lora model>\"\n\nYou can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment \"accounts/fireworks/models/<MODEL_ID of lora model>\"\n\nYou can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: Previously, this involved two distinct steps, and the resulting deployment was slower than the base model: Create a deployment using firectl create deployment \"accounts/fireworks/models/<MODEL_ID of base model>\" --enable-addons Then deploy the addon to the deployment: firectl load-lora <MODEL_ID> --deployment <DEPLOYMENT_ID> For more information, see our deployment documentation. This change is for dedicated deployments with a single LoRA. You can still deploy multiple LoRAs on a deployment or deploy LoRA(s) on some Serverless models as described in the documentation.\n\n", "type": "text"}, {"text": "Title: Deployment Command\nLink: https://docs.fireworks.ai/evaluators/examples/gcp_cloud_run_deployment_example\nContent: It's recommended to run the deployment command from the directory containing the reward function script ( dummy_rewards.py ) and your rewardkit.yaml (if used), for example, from examples/gcp_cloud_run_deployment_example/. Ensure your virtual environment is active: Run the deployment command: Command Explanation: --id my-dummy-gcp-evaluator : A unique ID for your evaluator on the Fireworks AI platform. --target gcp-cloud-run : Specifies deployment to GCP Cloud Run. --function-ref dummy_rewards.hello_world_reward : The Python import path to your reward function. If dummy_rewards.py is in the current directory, this reference works. --gcp-auth-mode api-key : Configures the Cloud Run service with API key authentication. reward-kit will generate a key, store it in GCP Secret Manager, and configure the service. The key is also saved to your local rewardkit.yaml under evaluator_endpoint_keys . This is the default if not specified. --verbose : Shows detailed output, including gcloud commands being executed. --force : (Optional) If an evaluator with the same --id already exists, this flag will delete the existing one before creating the new one.\n\nIt's recommended to run the deployment command from the directory containing the reward function script ( dummy_rewards.py ) and your rewardkit.yaml (if used), for example, from examples/gcp_cloud_run_deployment_example/. Ensure your virtual environment is active: Run the deployment command: Command Explanation: --id my-dummy-gcp-evaluator : A unique ID for your evaluator on the Fireworks AI platform. --target gcp-cloud-run : Specifies deployment to GCP Cloud Run. --function-ref dummy_rewards.hello_world_reward : The Python import path to your reward function. If dummy_rewards.py is in the current directory, this reference works. --gcp-auth-mode api-key : Configures the Cloud Run service with API key authentication. reward-kit will generate a key, store it in GCP Secret Manager, and configure the service. The key is also saved to your local rewardkit.yaml under evaluator_endpoint_keys . This is the default if not specified. --verbose : Shows detailed output, including gcloud commands being executed. --force : (Optional) If an evaluator with the same --id already exists, this flag will delete the existing one before creating the new one.\n\nIt's recommended to run the deployment command from the directory containing the reward function script ( dummy_rewards.py ) and your rewardkit.yaml (if used), for example, from examples/gcp_cloud_run_deployment_example/. Ensure your virtual environment is active: source .venv/bin/activate\n\n", "type": "text"}, {"text": "Title: Quick deployment\nLink: https://docs.fireworks.ai/fine-tuning/single-lora\nContent: Deploy your LoRA fine-tuned model with one simple command: firectl create deployment \"accounts/fireworks/models/<MODEL_ID of lora model>\"\n\nDeploy your LoRA fine-tuned model with one simple command: firectl create deployment \"accounts/fireworks/models/<MODEL_ID of lora model>\"\n\nAlternative deployment method\nThis two-step method is the standard approach for multi-LoRA deployments where multiple LoRA models share the same base model. While it can also be used for single LoRA deployments, it provides slower performance compared to live merge and is not recommended for single LoRA use cases. You can also deploy single LoRA models using a two-step process: Create base model deployment Deploy the base model with addons enabled: firectl create deployment \"accounts/fireworks/models/<MODEL_ID of base model>\" --enable-addons\n\n", "type": "text"}, {"text": "Title: Deploying a fine-tuned model using an on-demand deployment\nLink: https://docs.fireworks.ai/fine-tuning/fine-tuning-models\nContent: Use the following command to deploy your fine-tuned model using an on-demand deployment: firectl create deployment <FINE_TUNED_MODEL_ID>\n\nUse the following command to deploy your fine-tuned model using an on-demand deployment: firectl create deployment <FINE_TUNED_MODEL_ID>\n\nfirectl\nfirectl create dataset <DATASET_ID> /path/to/jsonl/file\n\n", "type": "text"}, {"text": "Title: Deleting Stored Responses\nLink: https://docs.fireworks.ai/guides/response-api\nContent: When responses are stored (the default behavior with store=True ), you can immediately delete them from storage using the DELETE endpoint. This permanently removes the conversation data. from fireworks import LLM\nimport requests\nimport os\n\nllm = LLM(model=\"qwen3-235b-a22b\", deployment_type=\"serverless\")\n\n# Create a response\nresponse = llm.responses.create(\n    input=\"What is the capital of France?\",\n    store=True  # This is the default\n)\n\nresponse_id = response.id\nprint(f\"Created response with ID: {response_id}\")\n\n# Delete the response immediately\nheaders = {\n    \"Authorization\": f\"Bearer {os.getenv('FIREWORKS_API_KEY')}\",\n    \"x-fireworks-account-id\": \"your-account-id\"\n}\ndelete_response = requests.delete(\n    f\"https://api.fireworks.ai/inference/v1/responses/{response_id}\",\n    headers=headers\n)\n\nif delete_response.status_code == 200:\n    print(\"Response deleted successfully\")\nelse:\n    print(f\"Failed to delete response: {delete_response.status_code}\")\n\nWhen responses are stored (the default behavior with store=True ), you can immediately delete them from storage using the DELETE endpoint. This permanently removes the conversation data. from fireworks import LLM\nimport requests\nimport os\n\nllm = LLM(model=\"qwen3-235b-a22b\", deployment_type=\"serverless\")\n\n# Create a response\nresponse = llm.responses.create(\n    input=\"What is the capital of France?\",\n    store=True  # This is the default\n)\n\nresponse_id = response.id\nprint(f\"Created response with ID: {response_id}\")\n\n# Delete the response immediately\nheaders = {\n    \"Authorization\": f\"Bearer {os.getenv('FIREWORKS_API_KEY')}\",\n    \"x-fireworks-account-id\": \"your-account-id\"\n}\ndelete_response = requests.delete(\n    f\"https://api.fireworks.ai/inference/v1/responses/{response_id}\",\n    headers=headers\n)\n\nif delete_response.status_code == 200:\n    print(\"Response deleted successfully\")\nelse:\n    print(f\"Failed to delete response: {delete_response.status_code}\")\n\nWhen responses are stored (the default behavior with store=True ), you can immediately delete them from storage using the DELETE endpoint. This permanently removes the conversation data. import os\nfrom openai import OpenAI\nimport requests\n\nclient = OpenAI(\n    base_url=\"https://api.fireworks.ai/inference/v1\",\n    api_key=os.getenv(\"FIREWORKS_API_KEY\", \"YOUR_FIREWORKS_API_KEY_HERE\")\n)\n\n# Create a response\nresponse = client.responses.create(\n    model=\"accounts/fireworks/models/qwen3-235b-a22b\",\n    input=\"What is the capital of France?\",\n    store=True  # This is the default\n)\n\nresponse_id = response.id\nprint(f\"Created response with ID: {response_id}\")\n\n# Delete the response immediately\nheaders = {\n    \"Authorization\": f\"Bearer {os.getenv('FIREWORKS_API_KEY')}\",\n    \"x-fireworks-account-id\": \"your-account-id\"\n}\ndelete_response = requests.delete(\n    f\"https://api.fireworks.ai/inference/v1/responses/{response_id}\",\n    headers=headers\n)\n\nif delete_response.status_code == 200:\n    print(\"Response deleted successfully\")\nelse:\n    print(f\"Failed to delete response: {delete_response.status_code}\")\n\n", "type": "text"}], "tool_call_id": "functions.search:1"}, {"role": "assistant", "content": "Let me search for more information about listing and managing deployments.", "tool_calls": [{"id": "functions.search:2", "function": {"arguments": "{\"query\": \"list deployments firectl commands deployment management\"}", "name": "search"}, "type": "function"}]}, {"role": "tool", "content": [{"text": "Title: Why am I getting \"invalid id\" errors when using firectl commands like create deployment or list deployments?\nLink: https://docs.fireworks.ai/faq-new/fine-tuning/why-am-i-getting-invalid-id-errors-when-using-firectl-commands-like-create-deplo\nContent: This error typically occurs when your account ID is not properly configured.\n\nThis error typically occurs when your account ID is not properly configured.\n\nWhy am I getting \"invalid id\" errors when using firectl commands like create deployment or list deployments?\nThis error typically occurs when your account ID is not properly configured.\n\n", "type": "text"}, {"text": "Title: List deployments\nLink: https://docs.fireworks.ai/tools-sdks/firectl/commands/list-resources\nContent: Prints all deployments in the account. firectl list deployments [flags]\n\nPrints all deployments in the account. firectl list deployments [flags]\n\nList deployed models\nPrints all deployed models in an account. firectl list deployed-models [flags]\n\n", "type": "text"}, {"text": "Title: \ud83d\ude80 Easier & faster LoRA fine-tune deployments on Fireworks\nLink: https://docs.fireworks.ai/updates/changelog\nContent: You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment \"accounts/fireworks/models/<MODEL_ID of lora model>\"\n\nYou can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment \"accounts/fireworks/models/<MODEL_ID of lora model>\"\n\nYou can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: Previously, this involved two distinct steps, and the resulting deployment was slower than the base model: Create a deployment using firectl create deployment \"accounts/fireworks/models/<MODEL_ID of base model>\" --enable-addons Then deploy the addon to the deployment: firectl load-lora <MODEL_ID> --deployment <DEPLOYMENT_ID> For more information, see our deployment documentation. This change is for dedicated deployments with a single LoRA. You can still deploy multiple LoRAs on a deployment or deploy LoRA(s) on some Serverless models as described in the documentation.\n\n", "type": "text"}, {"text": "Title: Managing default deployments\nLink: https://docs.fireworks.ai/guides/ondemand-deployments\nContent: Since a model may be deployed to multiple deployments, querying by model name will route to the \"default\" deployed model. You can see which deployed model entry is marked with Default: true using: firectl get model <MODEL_ID>\n\nSince a model may be deployed to multiple deployments, querying by model name will route to the \"default\" deployed model. You can see which deployed model entry is marked with Default: true using: firectl get model <MODEL_ID>\n\nCreating an on-demand deployment\nChoose a model Check out our model library to see a list of models available for on-demand deployment. You can also upload and use your own custom base model. Create a deployment To create a new deployment of a model from the model library, use: Or, for a custom base model you have uploaded to your account: This command will complete when the deployment is READY . To let it run asynchronously, remove the --wait flag. If your account has purchased reserved capacity and this deployment meets the reservation criteria, it will be counted against that reservation and not billed as an on-demand deployment. Verify the deployment is running You can verify the deployment is active by running: The state field should show READY. The deployment ID is the last part of the deployment name: accounts/<ACCOUNT_ID>/deployments/<DEPLOYMENT_ID>. You can also list all deployments in your account at any time by running: firectl list deployments\n\n", "type": "text"}, {"text": "Title: Does Fireworks support custom base models?\nLink: https://docs.fireworks.ai/faq-new/models-inference/does-fireworks-support-custom-base-models\nContent: Yes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.\n\nYes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.\n\nDoes Fireworks support custom base models?\nYes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.\n\n", "type": "text"}, {"text": "Title: Quick deployment\nLink: https://docs.fireworks.ai/fine-tuning/single-lora\nContent: Deploy your LoRA fine-tuned model with one simple command: firectl create deployment \"accounts/fireworks/models/<MODEL_ID of lora model>\"\n\nDeploy your LoRA fine-tuned model with one simple command: firectl create deployment \"accounts/fireworks/models/<MODEL_ID of lora model>\"\n\nAlternative deployment method\nThis two-step method is the standard approach for multi-LoRA deployments where multiple LoRA models share the same base model. While it can also be used for single LoRA deployments, it provides slower performance compared to live merge and is not recommended for single LoRA use cases. You can also deploy single LoRA models using a two-step process: Create base model deployment Deploy the base model with addons enabled: firectl create deployment \"accounts/fireworks/models/<MODEL_ID of base model>\" --enable-addons\n\n", "type": "text"}, {"text": "Title: List Deployments\nLink: https://docs.fireworks.ai/api-reference/list-deployments\nContent: \n\n\n\nREST Endpoint GET /v1/accounts/{account_id}/deployments\n\nServer(s): https://api.fireworks.ai\n\n\nSummary:  \n\nDescription: N/A\n\nSecurity methods: \nheader: \n\tAuthorization: bearer auth\n\t\n\nRequest params: \npath params:\naccount_id:\n\tstring\n\tDescription: The Account Id\n\tExample: \"<string>\"\n\nquery params:\npageSize:\n\tinteger\n\tDescription: The maximum number of deployments to return. The maximum page_size is 200,\nvalues above 200 will be coerced to 200.\nIf unspecified, the default is 50.\n\tExample: 123\npageToken:\n\tstring\n\tDescription: A page token, received from a previous ListDeployments call. Provide this\nto retrieve the subsequent page. When paginating, all other parameters\nprovided to ListDeployments must match the call that provided the page\ntoken.\n\tExample: \"<string>\"\nfilter:\n\tstring\n\tDescription: Only deployment satisfying the provided filter (if specified) will be\nreturned. See https://google.aip.dev/160 for the filter grammar.\n\tExample: \"<string>\"\norderBy:\n\tstring\n\tDescription: A comma-separated list of fields to order by. e.g. \"foo,bar\"\nThe default sort order is ascending. To specify a descending order for a\nfield, append a \" desc\" suffix. e.g. \"foo desc,bar\"\nSubfields are specified with a \".\" character. e.g. \"foo.bar\"\nIf not specified, the default order is by \"create_time\".\n\tExample: \"<string>\"\nshowDeleted:\n\tboolean\n\tDescription: If set, DELETED deployments will be included.\n\tExample: true\nreadMask:\n\tstring\n\tDescription: The fields to be returned in the response. If empty or \"*\", all fields will be returned.\n\tExample: \"<string>\"\n\n\nRequest body: N/A\n\n\nResponses: \n200: object\n\tDescription: N/A\n\tExample: {\"deployments\":[{\"name\":\"<string>\",\"displayName\":\"<string>\",\"description\":\"<string>\",\"createTime\":\"2023-11-07T05:31:56Z\",\"expireTime\":\"2023-11-07T05:31:56Z\",\"purgeTime\":\"2023-11-07T05:31:56Z\",\"deleteTime\":\"2023-11-07T05:31:56Z\",\"state\":\"STATE_UNSPECIFIED\",\"status\":{\"code\":\"OK\",\"message\":\"<string>\"},\"minReplicaCount\":123,\"maxReplicaCount\":123,\"replicaCount\":123,\"autoscalingPolicy\":{\"scaleUpWindow\":\"<string>\",\"scaleDownWindow\":\"<string>\",\"scaleToZeroWindow\":\"<string>\",\"loadTargets\":{}},\"baseModel\":\"<string>\",\"acceleratorCount\":123,\"acceleratorType\":\"ACCELERATOR_TYPE_UNSPECIFIED\",\"precision\":\"PRECISION_UNSPECIFIED\",\"cluster\":\"<string>\",\"enableAddons\":true,\"draftTokenCount\":123,\"draftModel\":\"<string>\",\"ngramSpeculationLength\":123,\"numPeftDeviceCached\":123,\"deploymentTemplate\":\"<string>\",\"autoTune\":{\"longPrompt\":true},\"placement\":{\"region\":\"REGION_UNSPECIFIED\",\"multiRegion\":\"MULTI_REGION_UNSPECIFIED\",\"regions\":[\"REGION_UNSPECIFIED\"]},\"region\":\"REGION_UNSPECIFIED\",\"updateTime\":\"2023-11-07T05:31:56Z\",\"disableDeploymentSizeValidation\":true}],\"nextPageToken\":\"<string>\",\"totalSize\":123}\n\n", "type": "text"}, {"text": "Title: Deploy Command\nLink: https://docs.fireworks.ai/evaluators/cli_reference/cli_overview\nContent: The deploy command deploys a reward function as an evaluator on the Fireworks platform.\n\nThe deploy command deploys a reward function as an evaluator on the Fireworks platform.\n\nCommand Overview\nThe Reward Kit CLI supports the following main commands: run : Run a local evaluation pipeline using a Hydra configuration. preview : Preview evaluation results or re-evaluate generated outputs. deploy : Deploy a reward function as an evaluator. agent-eval : Run agent evaluations on task bundles. list : List existing evaluators (coming soon). delete : Delete an evaluator (coming soon).\n\n", "type": "text"}, {"text": "Title: Deploying a fine-tuned model using an on-demand deployment\nLink: https://docs.fireworks.ai/fine-tuning/fine-tuning-models\nContent: Use the following command to deploy your fine-tuned model using an on-demand deployment: firectl create deployment <FINE_TUNED_MODEL_ID>\n\nUse the following command to deploy your fine-tuned model using an on-demand deployment: firectl create deployment <FINE_TUNED_MODEL_ID>\n\nDeploying a fine-tuned model serverlessly\nNot all base models support serverless addons. Please check this list for the full list of serverless models that support LoRA add-ons. If your base model supports serverless addons, you can run the following command to load the fine-tuned model into a serverless deployment: firectl load-lora <FINE_TUNED_MODEL_ID>\n\n", "type": "text"}, {"text": "Title: Create a deployment\nLink: https://docs.fireworks.ai/tools-sdks/firectl/commands/create-deployment\nContent: Creates a new deployment. firectl create deployment [flags]\n\nCreates a new deployment. firectl create deployment [flags]\n\nCreate a Deployment on Fireworks AI platform\n\n", "type": "text"}], "tool_call_id": "functions.search:2"}, {"role": "assistant", "content": "Now let me search for information about deployment configuration and management best practices.", "tool_calls": [{"id": "functions.search:0", "function": {"arguments": "{\"query\": \"deployment configuration scaling autoscaling reserved capacity Fireworks\"}", "name": "search"}, "type": "function"}]}, {"role": "tool", "content": [{"text": "Title: How does billing and scaling work for on-demand GPU deployments?\nLink: https://docs.fireworks.ai/faq-new/deployment-infrastructure/how-does-billing-and-scaling-work-for-on-demand-gpu-deployments\nContent: On-demand GPU deployments have unique billing and scaling characteristics compared to serverless deployments: Billing: Charges start when the server begins accepting requests Billed by GPU-second for each active instance Costs accumulate even if there are no active API calls Scaling options: Supports autoscaling from 0 to multiple GPUs Each additional GPU adds to the billing rate Can handle unlimited requests within the GPU\u2019s capacity Management requirements: Not fully serverless; requires some manual management Manually delete deployments when no longer needed Or configure autoscaling to scale down to 0 during inactive periods Cost control tips: Regularly monitor active deployments Delete unused deployments to avoid unnecessary costs Consider serverless options for intermittent usage Use autoscaling to 0 to optimize costs during low-demand times\n\nOn-demand GPU deployments have unique billing and scaling characteristics compared to serverless deployments: Billing: Charges start when the server begins accepting requests Billed by GPU-second for each active instance Costs accumulate even if there are no active API calls Scaling options: Supports autoscaling from 0 to multiple GPUs Each additional GPU adds to the billing rate Can handle unlimited requests within the GPU\u2019s capacity Management requirements: Not fully serverless; requires some manual management Manually delete deployments when no longer needed Or configure autoscaling to scale down to 0 during inactive periods Cost control tips: Regularly monitor active deployments Delete unused deployments to avoid unnecessary costs Consider serverless options for intermittent usage Use autoscaling to 0 to optimize costs during low-demand times\n\nHow does billing and scaling work for on-demand GPU deployments?\nOn-demand GPU deployments have unique billing and scaling characteristics compared to serverless deployments: Billing: Charges start when the server begins accepting requests Billed by GPU-second for each active instance Costs accumulate even if there are no active API calls Scaling options: Supports autoscaling from 0 to multiple GPUs Each additional GPU adds to the billing rate Can handle unlimited requests within the GPU\u2019s capacity Management requirements: Not fully serverless; requires some manual management Manually delete deployments when no longer needed Or configure autoscaling to scale down to 0 during inactive periods Cost control tips: Regularly monitor active deployments Delete unused deployments to avoid unnecessary costs Consider serverless options for intermittent usage Use autoscaling to 0 to optimize costs during low-demand times\n\n", "type": "text"}, {"text": "Title: What should I expect for deployment and scaling performance?\nLink: https://docs.fireworks.ai/faq-new/deployment-infrastructure/what-should-i-expect-for-deployment-and-scaling-performance\nContent: Initial deployment : Should complete within minutes Scaling from zero : You may experience brief availability delays while the system scales up Troubleshooting : If deployment takes over 1 hour, this typically indicates a crash and should be investigated Best practice : Monitor deployment status and contact support if deployment times are unusually long\n\nInitial deployment : Should complete within minutes Scaling from zero : You may experience brief availability delays while the system scales up Troubleshooting : If deployment takes over 1 hour, this typically indicates a crash and should be investigated Best practice : Monitor deployment status and contact support if deployment times are unusually long\n\nWhat should I expect for deployment and scaling performance?\nInitial deployment : Should complete within minutes Scaling from zero : You may experience brief availability delays while the system scales up Troubleshooting : If deployment takes over 1 hour, this typically indicates a crash and should be investigated Best practice : Monitor deployment status and contact support if deployment times are unusually long\n\n", "type": "text"}, {"text": "Title: Deployment type selection\nLink: https://docs.fireworks.ai/tools-sdks/python-client/sdk-basics\nContent: The SDK tries to be parsimonious with the way it deploys resources. We provide two types of deployment options on Fireworks: serverless hosting is enabled for some commonly-used state of the art models. The pricing for these models is per-token, i.e. you only pay for the tokens you use, and subject to rate limits. on-demand hosting is enabled for all other models. The pricing for these models is per GPU-second. This hosting is required for models that are not available serverlessly or workloads that exceed serverless rate limits. For non-finetuned models, you can always specify the deployment type of LLM() by passing either \"serverless\" or \"on-demand\" as the deployment_type parameter to the constructor. If the model is not available for the deployment type you selected, the SDK will throw an error. The SDK can also decide the best deployment strategy on your behalf, just pass deployment_type=\"auto\" . If the model is available serverlessly, the SDK will use serverless hosting, otherwise the SDK will create an on-demand deployment. When using deployment_type=\"on-demand\" or deployment_type=\"on-demand-lora\" , you must call .apply() to apply the deployment configuration to Fireworks. This is not required for serverless deployments. When using deployment_type=\"auto\" , the SDK will automatically handle deployment creation, but if it falls back to on-demand deployment, you may need to call .apply() explicitly. If you do not call .apply() , you are expected to set up the deployment through the deployment page at https://app.fireworks.ai/dashboard/deployments. Be careful with the deployment_type parameter, especially for \"auto\" and \"on-demand\" deployments. While the SDK will try to make the most cost effective choice for you and put sensible autoscaling policies in place, it is possible to unintentionally create many deployments that lead to unwanted spend, especially when working with non-serverless models. When using deployment_type=\"on-demand\" , you must provide an id parameter to uniquely identify your deployment. This is required to prevent accidental creation of multiple deployments. For finetuned (LoRA) models, passing deployment_type=\"serverless\" will try to deploy the finetuned model to serverless hosting, deployment_type=\"on-demand\" will create an on-demand deployment of your base model and merge in your LoRA weights, deployment_type=\"on-demand-lora\" will create an on-demand deployment with Multi-LoRA enabled, and deployment_type=\"auto\" will try to use serverless if available, otherwise fall back to on-demand-lora.\n\nThe SDK tries to be parsimonious with the way it deploys resources. We provide two types of deployment options on Fireworks: serverless hosting is enabled for some commonly-used state of the art models. The pricing for these models is per-token, i.e. you only pay for the tokens you use, and subject to rate limits. on-demand hosting is enabled for all other models. The pricing for these models is per GPU-second. This hosting is required for models that are not available serverlessly or workloads that exceed serverless rate limits. For non-finetuned models, you can always specify the deployment type of LLM() by passing either \"serverless\" or \"on-demand\" as the deployment_type parameter to the constructor. If the model is not available for the deployment type you selected, the SDK will throw an error. The SDK can also decide the best deployment strategy on your behalf, just pass deployment_type=\"auto\" . If the model is available serverlessly, the SDK will use serverless hosting, otherwise the SDK will create an on-demand deployment. When using deployment_type=\"on-demand\" or deployment_type=\"on-demand-lora\" , you must call .apply() to apply the deployment configuration to Fireworks. This is not required for serverless deployments. When using deployment_type=\"auto\" , the SDK will automatically handle deployment creation, but if it falls back to on-demand deployment, you may need to call .apply() explicitly. If you do not call .apply() , you are expected to set up the deployment through the deployment page at https://app.fireworks.ai/dashboard/deployments. Be careful with the deployment_type parameter, especially for \"auto\" and \"on-demand\" deployments. While the SDK will try to make the most cost effective choice for you and put sensible autoscaling policies in place, it is possible to unintentionally create many deployments that lead to unwanted spend, especially when working with non-serverless models. When using deployment_type=\"on-demand\" , you must provide an id parameter to uniquely identify your deployment. This is required to prevent accidental creation of multiple deployments. For finetuned (LoRA) models, passing deployment_type=\"serverless\" will try to deploy the finetuned model to serverless hosting, deployment_type=\"on-demand\" will create an on-demand deployment of your base model and merge in your LoRA weights, deployment_type=\"on-demand-lora\" will create an on-demand deployment with Multi-LoRA enabled, and deployment_type=\"auto\" will try to use serverless if available, otherwise fall back to on-demand-lora.\n\nDeploying Fine-tuned Models with On-Demand\nWhen deploying a fine-tuned model using deployment_type=\"on-demand\" , you need to provide: model - Your fine-tuned model ID (e.g., \"accounts/your-account/models/your-fine-tuned-model-id\") id - A unique deployment identifier (can be any simple string like \"my-fine-tuned-deployment\") # Deploy a fine-tuned model with on-demand deployment\nfine_tuned_llm = LLM(\n    model=\"accounts/your-account/models/your-fine-tuned-model-id\",\n    deployment_type=\"on-demand\",\n    id=\"my-fine-tuned-deployment\"  # Simple string ID\n)\n\n# Apply deployment configuration to Fireworks\nfine_tuned_llm.apply()\n\n# Track deployment in web dashboard\nprint(f\"Track at: {fine_tuned_llm.deployment_url}\")\n\n", "type": "text"}, {"text": "Title: Spike arrest policy\nLink: https://docs.fireworks.ai/guides/quotas_usage/rate-limits\nContent: LLM traffic that spikes quickly has the potential to be throttled. Here's how it works: Each user has a guaranteed rate limit, which increases with sustained usage near the limit. Typically, you can expect to stay within the limits if your traffic gradually doubles within an hour. You can see your guaranteed limits using API response headers (see below) Exceeding your guaranteed limit means that there's the potential for your requests to be processed with lower-priority. Fireworks operates serverless deployments by autoscaling capacity (within limits) as user traffic increases. However, if a deployment is overloaded while auto-scaling, requests that fall outside of guaranteed limits may be processed with lower-latency or dropped with HTTP code 429 (if limits are significantly exceeded).  You can monitor if you exceed limits via API response header x-ratelimit-over-limit: yes. Exceeding your guaranteed limit does not guarantee that your requests will be throttled. You can monitor if your requests are actually being throttled by monitoring latencies. Here's an example of how dynamic rate limits scale up: Metric Minimum Guaranteed Limit 10 Minutes 1 Hour 2 Hours Requests per minute 60 120 720 1440 Input tokens per minute 60000 120000 720000 1440000 Output tokens per minute 6000 12000 72000 144000\n\nLLM traffic that spikes quickly has the potential to be throttled. Here's how it works: Each user has a guaranteed rate limit, which increases with sustained usage near the limit. Typically, you can expect to stay within the limits if your traffic gradually doubles within an hour. You can see your guaranteed limits using API response headers (see below) Exceeding your guaranteed limit means that there's the potential for your requests to be processed with lower-priority. Fireworks operates serverless deployments by autoscaling capacity (within limits) as user traffic increases. However, if a deployment is overloaded while auto-scaling, requests that fall outside of guaranteed limits may be processed with lower-latency or dropped with HTTP code 429 (if limits are significantly exceeded).  You can monitor if you exceed limits via API response header x-ratelimit-over-limit: yes. Exceeding your guaranteed limit does not guarantee that your requests will be throttled. You can monitor if your requests are actually being throttled by monitoring latencies. Here's an example of how dynamic rate limits scale up: Metric Minimum Guaranteed Limit 10 Minutes 1 Hour 2 Hours Requests per minute 60 120 720 1440 Input tokens per minute 60000 120000 720000 1440000 Output tokens per minute 6000 12000 72000 144000\n\nSpend limits\nIn order to prevent fraud, Fireworks imposes a monthly spending limit on your account. Once you hit the spending limit, your account will automatically enter a suspended state, API requests will be rejected and all Fireworks usage will be stopped. This includes serverless inference, dedicated deployments, and fine-tuning jobs. Your spend limit will organically increase over time as you spend more on the platform. You can also increase your spend limit at any time, by purchasing prepaid credits to meet the historical spend required for a higher tier. For instance, if you are a new Tier 1 user with $0 historical spend, you can purchase $100 prepaid credits and become a Tier 2 user. You can qualify for a higher tier by adding credits into your Fireworks account. There may be a propagation delay for a few minutes after you prepay for credits - you may still see \"monthly usage exceeded error\" for a few minutes after adding credits. Tier Qualification Spending Limit Tier 1 Valid payment method added $50/mo Tier 2 $50 spent in payments or credits added $500/mo Tier 3 $500 spent in payments or credits added $5,000/mo Tier 4 $5000 spent in payments or credits added $50,000/mo Unlimited Contact us at inquiries@fireworks.ai Unlimited\n\n", "type": "text"}, {"text": "Title: How does billing work for on-demand deployments?\nLink: https://docs.fireworks.ai/faq-new/deployment-infrastructure/how-does-billing-work-for-on-demand-deployments\nContent: On-demand deployments come with automatic cost optimization features: Default autoscaling : Automatically scales to 0 replicas when not in use Pay for what you use : Charged only for GPU time when replicas are active Flexible configuration : Customize autoscaling behavior to match your needs Best practices for cost management: Leverage default autoscaling : The system automatically scales down deployments when not in use Customize carefully : While you can modify autoscaling behavior using our configuration options , note that preventing scale-to-zero will result in continuous GPU charges Consider your use case : For intermittent or low-frequency usage, serverless deployments might be more cost-effective For detailed configuration options, see our deployment guide.\n\nOn-demand deployments come with automatic cost optimization features: Default autoscaling : Automatically scales to 0 replicas when not in use Pay for what you use : Charged only for GPU time when replicas are active Flexible configuration : Customize autoscaling behavior to match your needs Best practices for cost management: Leverage default autoscaling : The system automatically scales down deployments when not in use Customize carefully : While you can modify autoscaling behavior using our configuration options , note that preventing scale-to-zero will result in continuous GPU charges Consider your use case : For intermittent or low-frequency usage, serverless deployments might be more cost-effective For detailed configuration options, see our deployment guide.\n\nHow does billing work for on-demand deployments?\nOn-demand deployments come with automatic cost optimization features: Default autoscaling : Automatically scales to 0 replicas when not in use Pay for what you use : Charged only for GPU time when replicas are active Flexible configuration : Customize autoscaling behavior to match your needs Best practices for cost management: Leverage default autoscaling : The system automatically scales down deployments when not in use Customize carefully : While you can modify autoscaling behavior using our configuration options , note that preventing scale-to-zero will result in continuous GPU charges Consider your use case : For intermittent or low-frequency usage, serverless deployments might be more cost-effective For detailed configuration options, see our deployment guide.\n\n", "type": "text"}, {"text": "Title: Replica count (horizontal scaling)\nLink: https://docs.fireworks.ai/guides/ondemand-deployments\nContent: The number of replicas (horizontal scaling) is specified by passing the --min-replica-count and --max-replica-count flags. Increasing the number of replicas will increase the maximum QPS the deployment can support. The deployment will automatically scale based on server load. Auto-scaling up may fail if there is a GPU stockout. Use reserved capacity to guarantee capacity for your deployments. The default value for --min-replica-count is 0. Setting --min-replica-count to 0 enables the deployment to auto-scale to 0 if a deployment is unused (i.e. no inference requests) for a specified \"scale-to-zero\" time window. While the deployment is scaled to 0, you will not pay for any GPU utilization. The default value for --max-replica-count is 1 if --min-replica-count=0 , or the value of\n--min-replica-count otherwise. firectl create deployment <MODEL_NAME> \\\n  --min-replica-count 2 \\\n  --max-replica-count 3\n\nThe number of replicas (horizontal scaling) is specified by passing the --min-replica-count and --max-replica-count flags. Increasing the number of replicas will increase the maximum QPS the deployment can support. The deployment will automatically scale based on server load. Auto-scaling up may fail if there is a GPU stockout. Use reserved capacity to guarantee capacity for your deployments. The default value for --min-replica-count is 0. Setting --min-replica-count to 0 enables the deployment to auto-scale to 0 if a deployment is unused (i.e. no inference requests) for a specified \"scale-to-zero\" time window. While the deployment is scaled to 0, you will not pay for any GPU utilization. The default value for --max-replica-count is 1 if --min-replica-count=0 , or the value of\n--min-replica-count otherwise. firectl create deployment <MODEL_NAME> \\\n  --min-replica-count 2 \\\n  --max-replica-count 3\n\nThe number of replicas (horizontal scaling) is specified by passing the --min-replica-count and --max-replica-count flags. Increasing the number of replicas will increase the maximum QPS the deployment can support. The deployment will automatically scale based on server load. Auto-scaling up may fail if there is a GPU stockout. Use reserved capacity to guarantee capacity for your deployments. The default value for --min-replica-count is 0. Setting --min-replica-count to 0 enables the deployment to auto-scale to 0 if a deployment is unused (i.e. no inference requests) for a specified \"scale-to-zero\" time window. While the deployment is scaled to 0, you will not pay for any GPU utilization. The default value for --max-replica-count is 1 if --min-replica-count=0 , or the value of\n--min-replica-count otherwise. firectl update deployment <DEPLOYMENT_ID> \\\n  --min-replica-count 2 \\\n  --max-replica-count 3\n\n", "type": "text"}, {"text": "Title: Do you support Auto Scaling?\nLink: https://docs.fireworks.ai/faq-new/deployment-infrastructure/do-you-support-auto-scaling\nContent: Yes, our system supports auto scaling with the following features: Scaling down to zero capability for resource efficiency Controllable scale-up and scale-down velocity Custom scaling rules and thresholds to match your specific needs\n\nYes, our system supports auto scaling with the following features: Scaling down to zero capability for resource efficiency Controllable scale-up and scale-down velocity Custom scaling rules and thresholds to match your specific needs\n\nDo you support Auto Scaling?\nYes, our system supports auto scaling with the following features: Scaling down to zero capability for resource efficiency Controllable scale-up and scale-down velocity Custom scaling rules and thresholds to match your specific needs\n\n", "type": "text"}, {"text": "Title: Inference\nLink: https://docs.fireworks.ai/getting-started/introduction\nContent: Fireworks has 3 options for running generative AI models with unparalleled speed and costs. Serverless: The easiest way to get started. Use the most popular models on pre-configured GPUs. Pay per token and avoid cold boots. On-demand: The most flexible option for scaling. Use private GPUs to support your specific needs and only pay when you're using it. GPUs running Fireworks software offer both ~250% improved throughput and 50% improved latency compared to vLLM. Excels for: Production volume - Per-token costs decrease with more volume and there are no set rate limits Custom needs and reliability - On-demand GPUs are private to you. This enables complete control to tailor deployments for speed/throughput/reliability or to run more specialized models Enterprise Reserved GPUs: Use private GPUs with hardware and software set-up personally tailored by the Fireworks team for your use case. Enjoy SLAs, dedicated support, bring-your-own-cloud (BYOC) deployment options, and enterprise-only optimizations. Property Serverless On-demand Enterprise reserved Performance Industry-leading speed on Fireworks-curated set-up. Performance may vary with others' usage. Speed dependent on user-specified GPU configuration and private usage. Per GPU latency should be significantly faster than vLLM. Tailor-made set-up by Fireworks AI experts for best possible latency Getting Started Self-serve - immediately use serverless with 1 line of code Self-serve - configure GPUs, then use them with 1 line of code. Chat with Fireworks Scaling and management Scale up and down freely within rate limits Option for auto-scaling GPUs with traffic. GPUs scale to zero automatically, so no charge for unused GPUs and for boot-ups. Chat with Fireworks Pricing Pay fixed price per token Pay per GPU second with no commitments. Per GPU throughput should be significantly greater than options like vLLM. Customized price based on reserved GPU capacity Commitment None None Arrange plan length with Fireworks Rate limits Yes, see quotas No rate limits. Quotas on number of GPUs None Model Selection Collection of popular models, curated by Fireworks Use 100s of pre-uploaded models or upload your own custom model within supported architecture Use 100s of pre-uploaded models or upload any model\n\nFireworks has 3 options for running generative AI models with unparalleled speed and costs. Serverless: The easiest way to get started. Use the most popular models on pre-configured GPUs. Pay per token and avoid cold boots. On-demand: The most flexible option for scaling. Use private GPUs to support your specific needs and only pay when you're using it. GPUs running Fireworks software offer both ~250% improved throughput and 50% improved latency compared to vLLM. Excels for: Production volume - Per-token costs decrease with more volume and there are no set rate limits Custom needs and reliability - On-demand GPUs are private to you. This enables complete control to tailor deployments for speed/throughput/reliability or to run more specialized models Enterprise Reserved GPUs: Use private GPUs with hardware and software set-up personally tailored by the Fireworks team for your use case. Enjoy SLAs, dedicated support, bring-your-own-cloud (BYOC) deployment options, and enterprise-only optimizations. Property Serverless On-demand Enterprise reserved Performance Industry-leading speed on Fireworks-curated set-up. Performance may vary with others' usage. Speed dependent on user-specified GPU configuration and private usage. Per GPU latency should be significantly faster than vLLM. Tailor-made set-up by Fireworks AI experts for best possible latency Getting Started Self-serve - immediately use serverless with 1 line of code Self-serve - configure GPUs, then use them with 1 line of code. Chat with Fireworks Scaling and management Scale up and down freely within rate limits Option for auto-scaling GPUs with traffic. GPUs scale to zero automatically, so no charge for unused GPUs and for boot-ups. Chat with Fireworks Pricing Pay fixed price per token Pay per GPU second with no commitments. Per GPU throughput should be significantly greater than options like vLLM. Customized price based on reserved GPU capacity Commitment None None Arrange plan length with Fireworks Rate limits Yes, see quotas No rate limits. Quotas on number of GPUs None Model Selection Collection of popular models, curated by Fireworks Use 100s of pre-uploaded models or upload your own custom model within supported architecture Use 100s of pre-uploaded models or upload any model\n\nFireworks AI Developer Platform\nFireworks AI is the best platform for building AI product experiences with open source AI models. You can run and customize AI models with just a few lines of code!\n\n", "type": "text"}, {"text": "Title: 2. Call a language model using the LLM() class\nLink: https://docs.fireworks.ai/tools-sdks/python-client/the-tutorial\nContent: Now that your machine is setup with credentials and the SDK, lets ensure you are\nready to make your first LLM call and explain some of the nuances of this SDK. Create a new file called main.py and import the Fireworks AI SDK. Instantiate the LLM class. The LLM class accepts a model argument that you\ncan use to specify the model you want to use. For this tutorial, we will use the\nLlama 4 Maverick\nmodel. When creating an LLM instance, you can specify the deployment type as either \"serverless\" , \"on-demand\" , or \"auto\" . If you pass \"auto\" , the SDK will try to use serverless hosting if available, otherwise it will create an on-demand deployment. In the other cases, the SDK will try to create a deployment of the specified type and will throw an error if it's not available for the model you selected. The SDK will try and re-use existing deployments for the same model if possible, see Resource management for more details. With great power comes great responsibility! Be careful with the deployment_type parameter, especially for \"auto\" and \"on-demand\" . While the SDK will try to make the most cost effective choice for you and put sensible autoscaling policies in place, it is possible to unintentionally create many deployments that lead to unwanted spend, especially when working with non-serverless models. When using deployment_type=\"on-demand\" , you must provide an id parameter to uniquely identify your deployment. This is required to prevent accidental creation of multiple deployments. When using deployment_type=\"on-demand\" or deployment_type=\"on-demand-lora\" , you must call .apply() to apply the deployment configuration to Fireworks. This is not required for serverless deployments. When using deployment_type=\"auto\" , the SDK will automatically handle deployment creation, but if it falls back to on-demand deployment, you may need to call .apply() explicitly. If you do not call .apply() , you are expected to set up the deployment through the deployment page at https://app.fireworks.ai/dashboard/deployments. Make a request to the LLM. The LLM class is OpenAI compatible, so you can use\nthe same chat completion interface to make a request to the LLM. Hello! It's nice to meet you. Is there something I can help you with or would you like to chat?\n\nNow that your machine is setup with credentials and the SDK, lets ensure you are\nready to make your first LLM call and explain some of the nuances of this SDK. Create a new file called main.py and import the Fireworks AI SDK. Instantiate the LLM class. The LLM class accepts a model argument that you\ncan use to specify the model you want to use. For this tutorial, we will use the\nLlama 4 Maverick\nmodel. When creating an LLM instance, you can specify the deployment type as either \"serverless\" , \"on-demand\" , or \"auto\" . If you pass \"auto\" , the SDK will try to use serverless hosting if available, otherwise it will create an on-demand deployment. In the other cases, the SDK will try to create a deployment of the specified type and will throw an error if it's not available for the model you selected. The SDK will try and re-use existing deployments for the same model if possible, see Resource management for more details. With great power comes great responsibility! Be careful with the deployment_type parameter, especially for \"auto\" and \"on-demand\" . While the SDK will try to make the most cost effective choice for you and put sensible autoscaling policies in place, it is possible to unintentionally create many deployments that lead to unwanted spend, especially when working with non-serverless models. When using deployment_type=\"on-demand\" , you must provide an id parameter to uniquely identify your deployment. This is required to prevent accidental creation of multiple deployments. When using deployment_type=\"on-demand\" or deployment_type=\"on-demand-lora\" , you must call .apply() to apply the deployment configuration to Fireworks. This is not required for serverless deployments. When using deployment_type=\"auto\" , the SDK will automatically handle deployment creation, but if it falls back to on-demand deployment, you may need to call .apply() explicitly. If you do not call .apply() , you are expected to set up the deployment through the deployment page at https://app.fireworks.ai/dashboard/deployments. Make a request to the LLM. The LLM class is OpenAI compatible, so you can use\nthe same chat completion interface to make a request to the LLM. The great thing about the SDK is that you can use your favorite Python constructs to powerfully work with LLMs. For example, let's try calling a few LLMs in a loop and see how they respond: from fireworks import LLM\n\nllms = [\n    \"llama4-maverick-instruct-basic\",\n    \"deepseek-r1\",\n    \"qwen2p5-vl-32b-instruct\"\n]\n\nfor llm in llms:\n    llm = LLM(model=llm, deployment_type=\"auto\") \n    print(\"\\n\" + \"-\" * 100)\n    print(f\"Model: {llm.model}\")\n    print(\"-\" * 100 + \"\\n\")\n\n    response = llm.chat.completions.create(\n        messages=[{\"role\": \"user\", \"content\": \"Hello, world!\"}]\n    )\n    print(response.choices[0].message.content)\n\nNow that your machine is setup with credentials and the SDK, lets ensure you are\nready to make your first LLM call and explain some of the nuances of this SDK. Create a new file called main.py and import the Fireworks AI SDK. Instantiate the LLM class. The LLM class accepts a model argument that you\ncan use to specify the model you want to use. For this tutorial, we will use the\nLlama 4 Maverick\nmodel. When creating an LLM instance, you can specify the deployment type as either \"serverless\" , \"on-demand\" , or \"auto\" . If you pass \"auto\" , the SDK will try to use serverless hosting if available, otherwise it will create an on-demand deployment. In the other cases, the SDK will try to create a deployment of the specified type and will throw an error if it's not available for the model you selected. The SDK will try and re-use existing deployments for the same model if possible, see Resource management for more details. With great power comes great responsibility! Be careful with the deployment_type parameter, especially for \"auto\" and \"on-demand\" . While the SDK will try to make the most cost effective choice for you and put sensible autoscaling policies in place, it is possible to unintentionally create many deployments that lead to unwanted spend, especially when working with non-serverless models. When using deployment_type=\"on-demand\" , you must provide an id parameter to uniquely identify your deployment. This is required to prevent accidental creation of multiple deployments. When using deployment_type=\"on-demand\" or deployment_type=\"on-demand-lora\" , you must call .apply() to apply the deployment configuration to Fireworks. This is not required for serverless deployments. When using deployment_type=\"auto\" , the SDK will automatically handle deployment creation, but if it falls back to on-demand deployment, you may need to call .apply() explicitly. If you do not call .apply() , you are expected to set up the deployment through the deployment page at https://app.fireworks.ai/dashboard/deployments. Make a request to the LLM. The LLM class is OpenAI compatible, so you can use\nthe same chat completion interface to make a request to the LLM. The great thing about the SDK is that you can use your favorite Python constructs to powerfully work with LLMs. For example, let's try calling a few LLMs in a loop and see how they respond: from fireworks import LLM\n\nllms = [\n    \"llama4-maverick-instruct-basic\",\n    \"deepseek-r1\",\n    \"qwen2p5-vl-32b-instruct\"\n]\n\nfor llm in llms:\n    llm = LLM(model=llm, deployment_type=\"auto\") \n    print(\"\\n\" + \"-\" * 100)\n    print(f\"Model: {llm.model}\")\n    print(\"-\" * 100 + \"\\n\")\n\n    response = llm.chat.completions.create(\n        messages=[{\"role\": \"user\", \"content\": \"Hello, world!\"}]\n    )\n    print(response.choices[0].message.content)\n\n", "type": "text"}, {"text": "Title: How does autoscaling affect my costs?\nLink: https://docs.fireworks.ai/faq-new/deployment-infrastructure/how-does-autoscaling-affect-my-costs\nContent: Scaling from 0 : No minimum cost when scaled to zero Scaling up : Each new replica adds to your total cost proportionally. For example: Scaling from 1 to 2 replicas doubles your GPU costs If each replica uses multiple GPUs, costs scale accordingly (e.g., scaling from 1 to 2 replicas with 2 GPUs each means paying for 4 GPUs total) For current pricing details, please visit our pricing page.\n\nScaling from 0 : No minimum cost when scaled to zero Scaling up : Each new replica adds to your total cost proportionally. For example: Scaling from 1 to 2 replicas doubles your GPU costs If each replica uses multiple GPUs, costs scale accordingly (e.g., scaling from 1 to 2 replicas with 2 GPUs each means paying for 4 GPUs total) For current pricing details, please visit our pricing page.\n\nHow does autoscaling affect my costs?\nScaling from 0 : No minimum cost when scaled to zero Scaling up : Each new replica adds to your total cost proportionally. For example: Scaling from 1 to 2 replicas doubles your GPU costs If each replica uses multiple GPUs, costs scale accordingly (e.g., scaling from 1 to 2 replicas with 2 GPUs each means paying for 4 GPUs total) For current pricing details, please visit our pricing page.\n\n", "type": "text"}], "tool_call_id": "functions.search:0"}, {"role": "assistant", "content": "Based on my research, I can now provide you with a comprehensive guide on managing deployments on Fireworks. Let me break this down into practical sections with commands and examples.\n\n## Managing Deployments on Fireworks: A Complete Guide\n\nFireworks offers three deployment types with different management approaches:\n\n1. **Serverless**: Pre-configured, pay-per-token, automatic scaling\n2. **On-demand**: Private GPUs, pay-per-second, manual/automatic scaling\n3. **Enterprise Reserved**: Custom hardware, tailored setup, SLAs\n\n### 1. Basic Deployment Management Commands\n\n#### Using firectl CLI (Recommended)\n\n**List all deployments:**\n```bash\nfirectl list deployments\n```\n\n**Create a deployment:**\n```bash\n# Basic deployment\nfirectl create deployment <MODEL_NAME> --wait\n\n# With custom configuration\nfirectl create deployment <MODEL_NAME> \\\n  --min-replica-count 0 \\\n  --max-replica-count 3 \\\n  --wait\n```\n\n**Delete a deployment:**\n```bash\nfirectl delete deployment <DEPLOYMENT_ID>\n```\n\n**Update deployment configuration:**\n```bash\nfirectl update deployment <DEPLOYMENT_ID> \\\n  --min-replica-count 2 \\\n  --max-replica-count 5\n```\n\n#### Using REST API\n\n**List deployments:**\n```bash\ncurl -X GET \\\n  \"https://api.fireworks.ai/v1/accounts/{account_id}/deployments\" \\\n  -H \"Authorization: Bearer $FIREWORKS_API_KEY\"\n```\n\n**Delete deployment via API:**\n```bash\ncurl -X DELETE \\\n  \"https://api.fireworks.ai/v1/accounts/{account_id}/deployments/{deployment_id}\" \\\n  -H \"Authorization: Bearer $FIREWORKS_API_KEY\"\n```\n\n### 2. Python SDK Management\n\n#### Installation and Setup\n```bash\npip install --upgrade fireworks-ai\nexport FIREWORKS_API_KEY=\"your-api-key\"\n```\n\n#### Basic Usage\n```python\nfrom fireworks import LLM\n\n# Serverless deployment (automatic)\nllm = LLM(model=\"llama4-maverick-instruct-basic\", deployment_type=\"serverless\")\n\n# On-demand deployment with custom ID\nllm = LLM(\n    model=\"llama4-maverick-instruct-basic\",\n    deployment_type=\"on-demand\",\n    id=\"my-custom-deployment\"\n)\nllm.apply()  # Required for on-demand deployments\n\n# Auto-selection (serverless if available, otherwise on-demand)\nllm = LLM(model=\"llama4-maverick-instruct-basic\", deployment_type=\"auto\")\n```\n\n#### Fine-tuned Model Deployment\n```python\n# Single LoRA deployment (recommended)\nlora_llm = LLM(\n    model=\"accounts/your-account/models/your-lora-model\",\n    deployment_type=\"on-demand\"\n)\nlora_llm.apply()\n\n# Multi-LoRA deployment\nbase_llm = LLM(\n    model=\"accounts/fireworks/models/llama4-maverick-instruct-basic\",\n    deployment_type=\"on-demand-lora\",\n    id=\"multi-lora-deployment\"\n)\nbase_llm.apply()\n```\n\n### 3. Deployment Configuration Options\n\n#### Scaling Configuration\n```bash\n# Scale between 0-3 replicas based on demand\nfirectl create deployment <MODEL_NAME> \\\n  --min-replica-count 0 \\\n  --max-replica-count 3\n\n# Fixed scaling (always have 2 replicas running)\nfirectl create deployment <MODEL_NAME> \\\n  --min-replica-count 2 \\\n  --max-replica-count 2\n```\n\n#### Advanced Configuration\n```bash\n# Custom model deployment\nfirectl create deployment accounts/your-account/models/custom-model \\\n  --min-replica-count 0 \\\n  --max-replica-count 5 \\\n  --accelerator-type A100 \\\n  --accelerator-count 2\n```\n\n### 4. Cost Management Best Practices\n\n#### Automatic Cost Optimization\n- **Scale-to-zero**: Deployments automatically scale to 0 when unused\n- **No charges when scaled down**: Only pay for active GPU time\n- **Autoscaling**: Automatically adjusts based on traffic\n\n#### Manual Cost Control\n```python\n# Monitor deployment costs\nimport requests\n\nheaders = {\"Authorization\": f\"Bearer {os.getenv('FIREWORKS_API_KEY')}\"}\nresponse = requests.get(\n    \"https://api.fireworks.ai/v1/accounts/{account_id}/deployments\",\n    headers=headers\n)\n\n# Delete unused deployments\nfor deployment in response.json()['deployments']:\n    if deployment['state'] == 'READY' and deployment['replicaCount'] == 0:\n        # Consider deleting if unused\n        pass\n```\n\n### 5. Monitoring and Troubleshooting\n\n#### Check Deployment Status\n```bash\n# Get deployment details\nfirectl get deployment <DEPLOYMENT_ID>\n\n# Check deployed models\nfirectl list deployed-models\n```\n\n#### Web Dashboard\nTrack deployments at: `https://app.fireworks.ai/dashboard/deployments`\n\n### 6. Reserved Capacity (Enterprise)\n\nFor high-volume usage, consider reserved capacity:\n- More reliable capacity\n- Higher GPU quotas\n- Better pricing for consistent usage\n- Contact: inquiries@fireworks.ai\n\n### 7. Common Management Workflows\n\n#### Daily Workflow Example\n```python\n# Morning: Check active deployments\nimport subprocess\nresult = subprocess.run(['firectl', 'list', 'deployments'], \n                       capture_output=True, text=True)\nprint(result.stdout)\n\n# Scale up for expected traffic\nsubprocess.run(['firectl', 'update', 'deployment', 'my-deployment', \n                '--min-replica-count', '2'])\n\n# Evening: Scale down\nsubprocess.run(['firectl', 'update', 'deployment', 'my-deployment', \n                '--min-replica-count', '0'])\n```\n\n#### Automated Cleanup Script\n```bash\n#!/bin/bash\n# cleanup_deployments.sh\n\n# List all deployments\ndeployments=$(firectl list deployments --format json | jq -r '.deployments[].name')\n\n# Delete deployments older than 7 days with 0 replicas\nfor deployment in $deployments; do\n    firectl delete deployment $deployment --ignore-checks\ndone\n```\n\n### 8. Important Considerations\n\n- **Billing**: On-demand deployments charge by GPU-second, even when idle\n- **Rate limits**: Serverless has rate limits, on-demand does not\n- **Cold starts**: Scaling from 0 may have brief delays\n- **Monitoring**: Regularly check deployment status and costs\n- **Cleanup**: Always delete unused deployments to avoid charges\n\nThis comprehensive guide covers all aspects of deployment management on Fireworks. Start with serverless for simple use cases, move to on-demand for production workloads, and consider reserved capacity for enterprise needs.", "tool_calls": []}], "input_metadata": {"row_id": "have-disassemble-simple-final-experience", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct"}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "evaluation_result": {"score": 1.0, "is_score_valid": true, "reason": "At least one tool call was made", "metrics": {}}, "created_at": "2025-08-08T11:38:53.968430", "eval_metadata": {"name": "test_pytest_mcp_url", "description": "Run math evaluation on sample dataset using pytest interface.", "version": "v0.2.6.dev17+gf8398de.dirty", "status": "finished", "num_runs": 1, "aggregation_method": "mean"}, "pid": 95890}, {"messages": [{"role": "system", "content": "You are a helpful assistant that can answer questions about Fireworks.\nALWAYS provide code or commands to execute to answer the question."}, {"role": "user", "content": "Can you teach me about how to manage deployments on Fireworks"}, {"role": "assistant", "content": "I'll help you learn about managing deployments on Fireworks. Let me search for comprehensive information about deployment management.", "tool_calls": [{"id": "functions.search:0", "function": {"arguments": "{\"query\": \"deployment management Fireworks AI\"}", "name": "search"}, "type": "function"}]}, {"role": "tool", "content": [{"text": "Title: Fireworks AI Developer Platform\nLink: https://docs.fireworks.ai/getting-started/introduction\nContent: Fireworks AI is the best platform for building AI product experiences with open source AI models. You can run and customize AI models with just a few lines of code!\n\nFireworks AI is the best platform for building AI product experiences with open source AI models. You can run and customize AI models with just a few lines of code!\n\nStart building with open source AI models\n\n", "type": "text"}, {"text": "Title: How do I close my Fireworks.ai account?\nLink: https://docs.fireworks.ai/faq-new/account-access/how-do-i-close-my-fireworksai-account\nContent: To close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed\n\nTo close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed\n\nHow do I close my Fireworks.ai account?\nTo close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed\n\n", "type": "text"}, {"text": "Title: Do you host your deployments in the EU or Asia?\nLink: https://docs.fireworks.ai/faq-new/support-general/do-you-host-your-deployments-in-the-eu-or-asia\nContent: We are currently deployed in multiple U.S.-based locations. However, we\u2019re open to hearing more about your specific requirements. You can: Join our Discord community Write to us at inquiries@fireworks.ai If you're an Enterprise customer, please contact your dedicated customer support representative to ensure a timely response.\n\nWe are currently deployed in multiple U.S.-based locations. However, we\u2019re open to hearing more about your specific requirements. You can: Join our Discord community Write to us at inquiries@fireworks.ai If you're an Enterprise customer, please contact your dedicated customer support representative to ensure a timely response.\n\nDo you host your deployments in the EU or Asia?\nWe are currently deployed in multiple U.S.-based locations. However, we\u2019re open to hearing more about your specific requirements. You can: Join our Discord community Write to us at inquiries@fireworks.ai If you're an Enterprise customer, please contact your dedicated customer support representative to ensure a timely response.\n\n", "type": "text"}, {"text": "Title: Introducing the Fireworks Build SDK\nLink: https://docs.fireworks.ai/tools-sdks/python-client/sdk-introduction\nContent: The Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.\n\nThe Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.\n\nIntroducing the Fireworks Build SDK\nThe Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.\n\n", "type": "text"}, {"text": "Title: On-demand deployments\nLink: https://docs.fireworks.ai/guides/ondemand-deployments\nContent: If you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.\n\nIf you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.\n\nOn-demand deployments\nIf you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.\n\n", "type": "text"}, {"text": "Title: I have multiple Fireworks accounts. When I try to login with Google on Fireworks' web UI, I'm getting signed into the wrong account. How do I fix this?\nLink: https://docs.fireworks.ai/faq-new/account-access/i-have-multiple-fireworks-accounts-when-i-try-to-login-with-google-on-fireworks\nContent: If you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.\n\nIf you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.\n\nI have multiple Fireworks accounts. When I try to login with Google on Fireworks' web UI, I'm getting signed into the wrong account. How do I fix this?\nIf you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.\n\n", "type": "text"}, {"text": "Title: Introduction\nLink: https://docs.fireworks.ai/api-reference/introduction\nContent: Fireworks AI REST API enables you to interact with various language, image and embedding models using an API Key. It also lets you automate management of models, deployments, datasets, and more.\n\nFireworks AI REST API enables you to interact with various language, image and embedding models using an API Key. It also lets you automate management of models, deployments, datasets, and more.\n\nIntroduction\nFireworks AI REST API enables you to interact with various language, image and embedding models using an API Key. It also lets you automate management of models, deployments, datasets, and more.\n\n", "type": "text"}, {"text": "Title: Create a deployment\nLink: https://docs.fireworks.ai/tools-sdks/firectl/commands/create-deployment\nContent: Create a Deployment on Fireworks AI platform\n\nCreate a Deployment on Fireworks AI platform\n\nCreates a new deployment.\n\n", "type": "text"}, {"text": "Title: How much does Fireworks cost?\nLink: https://docs.fireworks.ai/faq-new/billing-pricing/how-much-does-fireworks-cost\nContent: Fireworks AI operates on a pay-as-you-go model for all non-Enterprise usage, and new users automatically receive free credits. You pay based on: Per token for serverless inference Per GPU usage time for on-demand deployments Per token of training data for fine-tuning For customers needing enterprise-grade security and reliability , please reach out to us at inquiries@fireworks.ai to discuss options. Find out more about our current pricing on our Pricing page.\n\nFireworks AI operates on a pay-as-you-go model for all non-Enterprise usage, and new users automatically receive free credits. You pay based on: Per token for serverless inference Per GPU usage time for on-demand deployments Per token of training data for fine-tuning For customers needing enterprise-grade security and reliability , please reach out to us at inquiries@fireworks.ai to discuss options. Find out more about our current pricing on our Pricing page.\n\nHow much does Fireworks cost?\nFireworks AI operates on a pay-as-you-go model for all non-Enterprise usage, and new users automatically receive free credits. You pay based on: Per token for serverless inference Per GPU usage time for on-demand deployments Per token of training data for fine-tuning For customers needing enterprise-grade security and reliability , please reach out to us at inquiries@fireworks.ai to discuss options. Find out more about our current pricing on our Pricing page.\n\n", "type": "text"}, {"text": "Title: Python (Fireworks)\nLink: https://docs.fireworks.ai/getting-started/quickstart\nContent: This is the recommended way to get started : This  Python (Fireworks)  tab uses our  Fireworks Build\nSDK  for the best performance and developer experience.\nSee our  Client-side performance\noptimization  guide for\nmore details. Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. pip install --upgrade fireworks-ai The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either  ~/.bash_profile  for Bash or  ~/.zshrc  for  Zsh .\nYou can do this by running the command: vim ~/.bash_profile vim ~/.zshrc Add a new line to the file with the following: export FIREWORKS_API_KEY=\"<API_KEY>\" After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. source ~/.bash_profile source ~/.zshrc You can verify that the variable has been set correctly by running  echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. setx FIREWORKS_API_KEY \"<API_KEY>\" To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: echo %FIREWORKS_API_KEY% You can quickly instantiate the  LLM  class and call the Fireworks API. The Build SDK handles deployment management automatically. from fireworks import LLM\n\n# Basic usage - SDK automatically selects optimal deployment type\nllm = LLM(model=\"llama4-maverick-instruct-basic\", deployment_type=\"auto\")\n\nresponse = llm.chat.completions.create(\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}]\n)\n\nprint(response.choices[0].message.content) You can also pass the API key directly to the LLM constructor:  LLM(model=\"llama4-maverick-instruct-basic\", deployment_type=\"auto\", api_key=\"<FIREWORKS_API_KEY>\")\n\nThis is the recommended way to get started : This  Python (Fireworks)  tab uses our  Fireworks Build\nSDK  for the best performance and developer experience.\nSee our  Client-side performance\noptimization  guide for\nmore details. Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. pip install --upgrade fireworks-ai The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either  ~/.bash_profile  for Bash or  ~/.zshrc  for  Zsh .\nYou can do this by running the command: vim ~/.bash_profile vim ~/.zshrc Add a new line to the file with the following: export FIREWORKS_API_KEY=\"<API_KEY>\" After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. source ~/.bash_profile source ~/.zshrc You can verify that the variable has been set correctly by running  echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. setx FIREWORKS_API_KEY \"<API_KEY>\" To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: echo %FIREWORKS_API_KEY% You can quickly instantiate the  LLM  class and call the Fireworks API. The Build SDK handles deployment management automatically. from fireworks import LLM\n\n# Basic usage - SDK automatically selects optimal deployment type\nllm = LLM(model=\"llama4-maverick-instruct-basic\", deployment_type=\"auto\")\n\nresponse = llm.chat.completions.create(\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}]\n)\n\nprint(response.choices[0].message.content) You can also pass the API key directly to the LLM constructor:  LLM(model=\"llama4-maverick-instruct-basic\", deployment_type=\"auto\", api_key=\"<FIREWORKS_API_KEY>\")\n\nSet up your developer environment & call the Fireworks API\nThis is the recommended way to get started : This  tab uses our Fireworks Build\nSDK for the best performance and developer experience.\nSee our Client-side performance\noptimization guide for\nmore details. Install SDK Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Configure API Key Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either ~/.bash_profile for Bash or ~/.zshrc for Zsh .\nYou can do this by running the command: Add a new line to the file with the following: After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. You can verify that the variable has been set correctly by running echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: Sending the first API Request You can quickly instantiate the LLM class and call the Fireworks API. The Build SDK handles deployment management automatically. You can also pass the API key directly to the LLM constructor: LLM(model=\"llama4-maverick-instruct-basic\", deployment_type=\"auto\", api_key=\"<FIREWORKS_API_KEY>\") Install SDK Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. Fireworks AI platform offers drop-in replacement with OpenAI Python Client. Configure API Key Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either ~/.bash_profile for Bash or ~/.zshrc for Zsh .\nYou can do this by running the command: Add a new line to the file with the following: After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. You can verify that the variable has been set correctly by running echo $OPENAI_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: echo %OPENAI_API_KEY%\n\n", "type": "text"}], "tool_call_id": "functions.search:0"}, {"role": "assistant", "content": "Let me search for more specific information about deployment management commands and workflows.", "tool_calls": [{"id": "functions.search:1", "function": {"arguments": "{\"query\": \"firectl deployment commands create manage delete update\"}", "name": "search"}, "type": "function"}]}, {"role": "tool", "content": [{"text": "Title: Why am I getting \"invalid id\" errors when using firectl commands like create deployment or list deployments?\nLink: https://docs.fireworks.ai/faq-new/fine-tuning/why-am-i-getting-invalid-id-errors-when-using-firectl-commands-like-create-deplo\nContent: This error typically occurs when your account ID is not properly configured.\n\nThis error typically occurs when your account ID is not properly configured.\n\nWhy am I getting \"invalid id\" errors when using firectl commands like create deployment or list deployments?\nThis error typically occurs when your account ID is not properly configured.\n\n", "type": "text"}, {"text": "Title: Delete a deployment\nLink: https://docs.fireworks.ai/tools-sdks/firectl/commands/delete-resources\nContent: Deletes an model deployment. firectl delete deployment [flags]\n\nDeletes an model deployment. firectl delete deployment [flags]\n\nExample\nfirectl delete deployment my-deployment\n\n", "type": "text"}, {"text": "Title: Create a deployment\nLink: https://docs.fireworks.ai/tools-sdks/firectl/commands/create-deployment\nContent: Creates a new deployment. firectl create deployment [flags]\n\nCreates a new deployment. firectl create deployment [flags]\n\nCreate a Deployment on Fireworks AI platform\n\n", "type": "text"}, {"text": "Title: Managing default deployments\nLink: https://docs.fireworks.ai/guides/ondemand-deployments\nContent: Since a model may be deployed to multiple deployments, querying by model name will route to the \"default\" deployed model. You can see which deployed model entry is marked with Default: true using: This will show the Deployed Model Refs section with the Default: true entry. To update the default deployed model, note the Name of the deployed model reference above. Then run: firectl update deployed-model <DEPLOYED_MODEL_ID_2> --default\n\nSince a model may be deployed to multiple deployments, querying by model name will route to the \"default\" deployed model. You can see which deployed model entry is marked with Default: true using: This will show the Deployed Model Refs section with the Default: true entry. To update the default deployed model, note the Name of the deployed model reference above. Then run: firectl update deployed-model <DEPLOYED_MODEL_ID_2> --default\n\nSince a model may be deployed to multiple deployments, querying by model name will route to the \"default\" deployed model. You can see which deployed model entry is marked with Default: true using: This will show the Deployed Model Refs section with the Default: true entry. To update the default deployed model, note the Name of the deployed model reference above. Then run: To delete a default deployment you must delete all other deployments for the same model first, or designate a different deployed model as the default as described above. This is to ensure that querying by model name will always route to an unambiguous default deployment as long as deployments for the model exist.\n\n", "type": "text"}, {"text": "Title: \ud83d\ude80 Easier & faster LoRA fine-tune deployments on Fireworks\nLink: https://docs.fireworks.ai/updates/changelog\nContent: You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment \"accounts/fireworks/models/<MODEL_ID of lora model>\"\n\nYou can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment \"accounts/fireworks/models/<MODEL_ID of lora model>\"\n\nYou can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: Previously, this involved two distinct steps, and the resulting deployment was slower than the base model: Create a deployment using firectl create deployment \"accounts/fireworks/models/<MODEL_ID of base model>\" --enable-addons Then deploy the addon to the deployment: firectl load-lora <MODEL_ID> --deployment <DEPLOYMENT_ID> For more information, see our deployment documentation. This change is for dedicated deployments with a single LoRA. You can still deploy multiple LoRAs on a deployment or deploy LoRA(s) on some Serverless models as described in the documentation.\n\n", "type": "text"}, {"text": "Title: Deployment Command\nLink: https://docs.fireworks.ai/evaluators/examples/gcp_cloud_run_deployment_example\nContent: It's recommended to run the deployment command from the directory containing the reward function script ( dummy_rewards.py ) and your rewardkit.yaml (if used), for example, from examples/gcp_cloud_run_deployment_example/. Ensure your virtual environment is active: Run the deployment command: Command Explanation: --id my-dummy-gcp-evaluator : A unique ID for your evaluator on the Fireworks AI platform. --target gcp-cloud-run : Specifies deployment to GCP Cloud Run. --function-ref dummy_rewards.hello_world_reward : The Python import path to your reward function. If dummy_rewards.py is in the current directory, this reference works. --gcp-auth-mode api-key : Configures the Cloud Run service with API key authentication. reward-kit will generate a key, store it in GCP Secret Manager, and configure the service. The key is also saved to your local rewardkit.yaml under evaluator_endpoint_keys . This is the default if not specified. --verbose : Shows detailed output, including gcloud commands being executed. --force : (Optional) If an evaluator with the same --id already exists, this flag will delete the existing one before creating the new one.\n\nIt's recommended to run the deployment command from the directory containing the reward function script ( dummy_rewards.py ) and your rewardkit.yaml (if used), for example, from examples/gcp_cloud_run_deployment_example/. Ensure your virtual environment is active: Run the deployment command: Command Explanation: --id my-dummy-gcp-evaluator : A unique ID for your evaluator on the Fireworks AI platform. --target gcp-cloud-run : Specifies deployment to GCP Cloud Run. --function-ref dummy_rewards.hello_world_reward : The Python import path to your reward function. If dummy_rewards.py is in the current directory, this reference works. --gcp-auth-mode api-key : Configures the Cloud Run service with API key authentication. reward-kit will generate a key, store it in GCP Secret Manager, and configure the service. The key is also saved to your local rewardkit.yaml under evaluator_endpoint_keys . This is the default if not specified. --verbose : Shows detailed output, including gcloud commands being executed. --force : (Optional) If an evaluator with the same --id already exists, this flag will delete the existing one before creating the new one.\n\nIt's recommended to run the deployment command from the directory containing the reward function script ( dummy_rewards.py ) and your rewardkit.yaml (if used), for example, from examples/gcp_cloud_run_deployment_example/. Ensure your virtual environment is active: source .venv/bin/activate\n\n", "type": "text"}, {"text": "Title: Quick deployment\nLink: https://docs.fireworks.ai/fine-tuning/single-lora\nContent: Deploy your LoRA fine-tuned model with one simple command: firectl create deployment \"accounts/fireworks/models/<MODEL_ID of lora model>\"\n\nDeploy your LoRA fine-tuned model with one simple command: firectl create deployment \"accounts/fireworks/models/<MODEL_ID of lora model>\"\n\nAlternative deployment method\nThis two-step method is the standard approach for multi-LoRA deployments where multiple LoRA models share the same base model. While it can also be used for single LoRA deployments, it provides slower performance compared to live merge and is not recommended for single LoRA use cases. You can also deploy single LoRA models using a two-step process: Create base model deployment Deploy the base model with addons enabled: firectl create deployment \"accounts/fireworks/models/<MODEL_ID of base model>\" --enable-addons\n\n", "type": "text"}, {"text": "Title: Delete Deployment\nLink: https://docs.fireworks.ai/api-reference/delete-deployment\nContent: \n\n\n\nREST Endpoint DELETE /v1/accounts/{account_id}/deployments/{deployment_id}\n\nServer(s): https://api.fireworks.ai\n\n\nSummary:  \n\nDescription: N/A\n\nSecurity methods: \nheader: \n\tAuthorization: bearer auth\n\t\n\nRequest params: \npath params:\naccount_id:\n\tstring\n\tDescription: The Account Id\n\tExample: \"<string>\"\ndeployment_id:\n\tstring\n\tDescription: The Deployment Id\n\tExample: \"<string>\"\n\nquery params:\nhard:\n\tboolean\n\tDescription: If true, this will perform a hard deletion.\n\tExample: true\nignoreChecks:\n\tboolean\n\tDescription: If true, this will ignore checks and force the deletion of a deployment that is currently\ndeployed and is in use.\n\tExample: true\n\n\nRequest body: N/A\n\n\nResponses: \n200: object\n\tDescription: N/A\n\tExample: {}\n\n", "type": "text"}, {"text": "Title: How do I close my Fireworks.ai account?\nLink: https://docs.fireworks.ai/faq-new/account-access/how-do-i-close-my-fireworksai-account\nContent: To close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed\n\nTo close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed\n\nHow do I close my Fireworks.ai account?\nTo close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed\n\n", "type": "text"}, {"text": "Title: Deleting Stored Responses\nLink: https://docs.fireworks.ai/guides/response-api\nContent: When responses are stored (the default behavior with store=True ), you can immediately delete them from storage using the DELETE endpoint. This permanently removes the conversation data. from fireworks import LLM\nimport requests\nimport os\n\nllm = LLM(model=\"qwen3-235b-a22b\", deployment_type=\"serverless\")\n\n# Create a response\nresponse = llm.responses.create(\n    input=\"What is the capital of France?\",\n    store=True  # This is the default\n)\n\nresponse_id = response.id\nprint(f\"Created response with ID: {response_id}\")\n\n# Delete the response immediately\nheaders = {\n    \"Authorization\": f\"Bearer {os.getenv('FIREWORKS_API_KEY')}\",\n    \"x-fireworks-account-id\": \"your-account-id\"\n}\ndelete_response = requests.delete(\n    f\"https://api.fireworks.ai/inference/v1/responses/{response_id}\",\n    headers=headers\n)\n\nif delete_response.status_code == 200:\n    print(\"Response deleted successfully\")\nelse:\n    print(f\"Failed to delete response: {delete_response.status_code}\")\n\nWhen responses are stored (the default behavior with store=True ), you can immediately delete them from storage using the DELETE endpoint. This permanently removes the conversation data. from fireworks import LLM\nimport requests\nimport os\n\nllm = LLM(model=\"qwen3-235b-a22b\", deployment_type=\"serverless\")\n\n# Create a response\nresponse = llm.responses.create(\n    input=\"What is the capital of France?\",\n    store=True  # This is the default\n)\n\nresponse_id = response.id\nprint(f\"Created response with ID: {response_id}\")\n\n# Delete the response immediately\nheaders = {\n    \"Authorization\": f\"Bearer {os.getenv('FIREWORKS_API_KEY')}\",\n    \"x-fireworks-account-id\": \"your-account-id\"\n}\ndelete_response = requests.delete(\n    f\"https://api.fireworks.ai/inference/v1/responses/{response_id}\",\n    headers=headers\n)\n\nif delete_response.status_code == 200:\n    print(\"Response deleted successfully\")\nelse:\n    print(f\"Failed to delete response: {delete_response.status_code}\")\n\nWhen responses are stored (the default behavior with store=True ), you can immediately delete them from storage using the DELETE endpoint. This permanently removes the conversation data. import os\nfrom openai import OpenAI\nimport requests\n\nclient = OpenAI(\n    base_url=\"https://api.fireworks.ai/inference/v1\",\n    api_key=os.getenv(\"FIREWORKS_API_KEY\", \"YOUR_FIREWORKS_API_KEY_HERE\")\n)\n\n# Create a response\nresponse = client.responses.create(\n    model=\"accounts/fireworks/models/qwen3-235b-a22b\",\n    input=\"What is the capital of France?\",\n    store=True  # This is the default\n)\n\nresponse_id = response.id\nprint(f\"Created response with ID: {response_id}\")\n\n# Delete the response immediately\nheaders = {\n    \"Authorization\": f\"Bearer {os.getenv('FIREWORKS_API_KEY')}\",\n    \"x-fireworks-account-id\": \"your-account-id\"\n}\ndelete_response = requests.delete(\n    f\"https://api.fireworks.ai/inference/v1/responses/{response_id}\",\n    headers=headers\n)\n\nif delete_response.status_code == 200:\n    print(\"Response deleted successfully\")\nelse:\n    print(f\"Failed to delete response: {delete_response.status_code}\")\n\n", "type": "text"}], "tool_call_id": "functions.search:1"}, {"role": "assistant", "content": "Let me search for more information about listing and managing deployments, as well as the Fireworks Build SDK for deployment management.", "tool_calls": [{"id": "functions.search:1", "function": {"arguments": "{\"query\": \"list deployments firectl Fireworks Build SDK deployment management\"}", "name": "search"}, "type": "function"}]}, {"role": "tool", "content": [{"text": "Title: Introducing the Fireworks Build SDK\nLink: https://docs.fireworks.ai/tools-sdks/python-client/sdk-introduction\nContent: Introducing the Fireworks Build SDK\nThe Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.\n\nIntroducing the Fireworks Build SDK\nThe Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.\n\nThe Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.\n\n", "type": "text"}, {"text": "Title: Basics of the Build SDK\nLink: https://docs.fireworks.ai/tools-sdks/python-client/sdk-basics\nContent: Why use the Build SDK?\nThe Fireworks Build SDK gives you a declarative way to work with Fireworks resources like deployments, fine-tuning jobs, and datasets. We've designed it to handle all the infrastructure complexity for you, letting you focus on building your application. Instead of using the web UI, CLI, or raw API calls, you can manage everything through simple Python code with smart, logical defaults without sacrificing control and customizability. The principles of the SDK are the following: Object-oriented: Fireworks primitives are represented as Python objects. You can access their capabilities and properties through methods and attributes. Declarative: You can describe your desired state and the SDK will handle reconcilliation. Smart defaults: The SDK will infer the most logical defaults for you, prioritizing development speed and lowest cost. Here are some examples: The SDK will automatically use a serverless deployment for models that are available serverlessly unless you specify otherwise. When creating deployments, the SDK will also enable scale-to-zero with the shortest possible scale-down window. If the SDK determines that a resource already exists by matching its signature (see below), it will re-use the existing resource instead of creating a new one. Customizable: Although we enable smart defaults, you still have full access to the configuration parameters for any Fireworks resource The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.\n\nWhy use the Build SDK?\nThe Fireworks Build SDK gives you a declarative way to work with Fireworks resources like deployments, fine-tuning jobs, and datasets. We've designed it to handle all the infrastructure complexity for you, letting you focus on building your application. Instead of using the web UI, CLI, or raw API calls, you can manage everything through simple Python code with smart, logical defaults without sacrificing control and customizability. The principles of the SDK are the following: Object-oriented: Fireworks primitives are represented as Python objects. You can access their capabilities and properties through methods and attributes. Declarative: You can describe your desired state and the SDK will handle reconcilliation. Smart defaults: The SDK will infer the most logical defaults for you, prioritizing development speed and lowest cost. Here are some examples: The SDK will automatically use a serverless deployment for models that are available serverlessly unless you specify otherwise. When creating deployments, the SDK will also enable scale-to-zero with the shortest possible scale-down window. If the SDK determines that a resource already exists by matching its signature (see below), it will re-use the existing resource instead of creating a new one. Customizable: Although we enable smart defaults, you still have full access to the configuration parameters for any Fireworks resource The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.\n\nWhy use the Build SDK?\nThe Fireworks Build SDK gives you a declarative way to work with Fireworks resources like deployments, fine-tuning jobs, and datasets. We've designed it to handle all the infrastructure complexity for you, letting you focus on building your application. Instead of using the web UI, CLI, or raw API calls, you can manage everything through simple Python code with smart, logical defaults without sacrificing control and customizability. The principles of the SDK are the following: Object-oriented: Fireworks primitives are represented as Python objects. You can access their capabilities and properties through methods and attributes. Declarative: You can describe your desired state and the SDK will handle reconcilliation. Smart defaults: The SDK will infer the most logical defaults for you, prioritizing development speed and lowest cost. Here are some examples: The SDK will automatically use a serverless deployment for models that are available serverlessly unless you specify otherwise. When creating deployments, the SDK will also enable scale-to-zero with the shortest possible scale-down window. If the SDK determines that a resource already exists by matching its signature (see below), it will re-use the existing resource instead of creating a new one. Customizable: Although we enable smart defaults, you still have full access to the configuration parameters for any Fireworks resource The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.\n\n", "type": "text"}, {"text": "Title: Querying existing dedicated deployments\nLink: https://docs.fireworks.ai/tools-sdks/python-client/querying-dedicated-deployments\nContent: When you have dedicated deployments that were created via firectl or the Fireworks web UI, you can easily connect to them using the Build SDK to run inference. This is particularly useful when you want to leverage existing infrastructure or when deployments are managed by different teams.\n\nWhen you have dedicated deployments that were created via firectl or the Fireworks web UI, you can easily connect to them using the Build SDK to run inference. This is particularly useful when you want to leverage existing infrastructure or when deployments are managed by different teams.\n\nDeployment ID requirements\nThe id parameter should match exactly with your existing deployment: Use the deployment name/ID as shown in the Fireworks dashboard The ID is case-sensitive and must match exactly If the deployment doesn't exist, you'll receive an error when making requests\n\n", "type": "text"}, {"text": "Title: Fireworks SDK\nLink: https://docs.fireworks.ai/guides/ondemand-deployments\nContent: You can connect to your existing deployment using the  Fireworks Build SDK : from fireworks import LLM\n\n# Connect to your existing deployment\nllm = LLM(\n    model=\"llama-v3p2-3b-instruct\",  # The model your deployment is running\n    deployment_type=\"on-demand\",\n    id=\"my-deployment-id\",  # Your deployment ID\n)\n\n# Use OpenAI-compatible chat completions\nresponse = llm.chat.completions.create(\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}]\n)\n\nprint(response.choices[0].message.content) When connecting to an existing deployment with the SDK, you don't need to call  .apply()  - the deployment is already running. For more details on querying existing deployments with Fireworks SDK, see our guide on  querying dedicated deployments .\n\nYou can connect to your existing deployment using the  Fireworks Build SDK : from fireworks import LLM\n\n# Connect to your existing deployment\nllm = LLM(\n    model=\"llama-v3p2-3b-instruct\",  # The model your deployment is running\n    deployment_type=\"on-demand\",\n    id=\"my-deployment-id\",  # Your deployment ID\n)\n\n# Use OpenAI-compatible chat completions\nresponse = llm.chat.completions.create(\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}]\n)\n\nprint(response.choices[0].message.content) When connecting to an existing deployment with the SDK, you don't need to call  .apply()  - the deployment is already running. For more details on querying existing deployments with Fireworks SDK, see our guide on  querying dedicated deployments .\n\nOn-demand deployments\nIf you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.\n\n", "type": "text"}, {"text": "Title: Step 1: Configure the Fireworks Build SDK\nLink: https://docs.fireworks.ai/structured-responses/structured-output-grammar-based\nContent: from fireworks import LLM\n\nclient = LLM(\n    model=\"llama-v3p1-405b-instruct\",\n    deployment_type=\"serverless\"\n)\n\nfrom fireworks import LLM\n\nclient = LLM(\n    model=\"llama-v3p1-405b-instruct\",\n    deployment_type=\"serverless\"\n)\n\nPrerequisites\nBefore you begin, ensure you have the following: Python installed on your system. Build SDK installed. You can install it using pip: Next, select the model you want to use. In this example, we use llama-v3p1-405b-instruct , but all fireworks models support this feature.\n\n", "type": "text"}, {"text": "Title: Does Fireworks support custom base models?\nLink: https://docs.fireworks.ai/faq-new/models-inference/does-fireworks-support-custom-base-models\nContent: Yes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.\n\nYes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.\n\nDoes Fireworks support custom base models?\nYes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.\n\n", "type": "text"}, {"text": "Title: \ud83d\udd27 Build SDK: Deployment Configuration Application Requirement\nLink: https://docs.fireworks.ai/updates/changelog\nContent: The Build SDK now requires you to call .apply() to apply any deployment configurations to Fireworks when using deployment_type=\"on-demand\" or deployment_type=\"on-demand-lora\" . This change ensures explicit control over when deployments are created and helps prevent accidental deployment creation. Key changes: .apply() is now required for on-demand and on-demand-lora deployments Serverless deployments do not require .apply() calls If you do not call .apply() , you are expected to set up the deployment through the deployment page at https://app.fireworks.ai/dashboard/deployments Migration guide: Add llm.apply() after creating LLM instances with deployment_type=\"on-demand\" or deployment_type=\"on-demand-lora\" No changes needed for serverless deployments See updated documentation for examples and best practices This change improves deployment management and provides better control over resource creation. This applies to Python SDK version >=0.19.14.\n\nThe Build SDK now requires you to call .apply() to apply any deployment configurations to Fireworks when using deployment_type=\"on-demand\" or deployment_type=\"on-demand-lora\" . This change ensures explicit control over when deployments are created and helps prevent accidental deployment creation. Key changes: .apply() is now required for on-demand and on-demand-lora deployments Serverless deployments do not require .apply() calls If you do not call .apply() , you are expected to set up the deployment through the deployment page at https://app.fireworks.ai/dashboard/deployments Migration guide: Add llm.apply() after creating LLM instances with deployment_type=\"on-demand\" or deployment_type=\"on-demand-lora\" No changes needed for serverless deployments See updated documentation for examples and best practices This change improves deployment management and provides better control over resource creation. This applies to Python SDK version >=0.19.14.\n\n\ud83c\udfaf Build SDK: Reward-kit integration for evaluator development\nThe Build SDK now natively integrates with reward-kit to simplify evaluator development for Reinforcement Fine-Tuning (RFT) . You can now create custom evaluators in Python with automatic dependency management and seamless deployment to Fireworks infrastructure. Key features: Native reward-kit integration for evaluator development Automatic packaging of dependencies from pyproject.toml or requirements.txt Local testing capabilities before deployment Direct integration with Fireworks datasets and evaluation jobs Support for third-party libraries and complex evaluation logic See our Developing Evaluators guide to get started with your first evaluator in minutes.\n\n", "type": "text"}, {"text": "Title: Set up your developer environment & call the Fireworks API\nLink: https://docs.fireworks.ai/getting-started/quickstart\nContent: This is the recommended way to get started : This Python (Fireworks) tab uses our Fireworks Build\nSDK for the best performance and developer experience.\nSee our Client-side performance\noptimization guide for\nmore details. Install SDK Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Configure API Key Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either ~/.bash_profile for Bash or ~/.zshrc for Zsh .\nYou can do this by running the command: Add a new line to the file with the following: After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. You can verify that the variable has been set correctly by running echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: Sending the first API Request You can quickly instantiate the LLM class and call the Fireworks API. The Build SDK handles deployment management automatically. from fireworks import LLM\n\n# Basic usage - SDK automatically selects optimal deployment type\nllm = LLM(model=\"llama4-maverick-instruct-basic\", deployment_type=\"auto\")\n\nresponse = llm.chat.completions.create(\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}]\n)\n\nprint(response.choices[0].message.content)\n\nThis is the recommended way to get started : This Python (Fireworks) tab uses our Fireworks Build\nSDK for the best performance and developer experience.\nSee our Client-side performance\noptimization guide for\nmore details. Install SDK Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Configure API Key Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either ~/.bash_profile for Bash or ~/.zshrc for Zsh .\nYou can do this by running the command: Add a new line to the file with the following: After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. You can verify that the variable has been set correctly by running echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: Sending the first API Request You can quickly instantiate the LLM class and call the Fireworks API. The Build SDK handles deployment management automatically. from fireworks import LLM\n\n# Basic usage - SDK automatically selects optimal deployment type\nllm = LLM(model=\"llama4-maverick-instruct-basic\", deployment_type=\"auto\")\n\nresponse = llm.chat.completions.create(\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}]\n)\n\nprint(response.choices[0].message.content)\n\nPython (Fireworks)\nThis is the recommended way to get started : This  Python (Fireworks)  tab uses our  Fireworks Build\nSDK  for the best performance and developer experience.\nSee our  Client-side performance\noptimization  guide for\nmore details. Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. pip install --upgrade fireworks-ai The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either  ~/.bash_profile  for Bash or  ~/.zshrc  for  Zsh .\nYou can do this by running the command: vim ~/.bash_profile vim ~/.zshrc Add a new line to the file with the following: export FIREWORKS_API_KEY=\"<API_KEY>\" After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. source ~/.bash_profile source ~/.zshrc You can verify that the variable has been set correctly by running  echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. setx FIREWORKS_API_KEY \"<API_KEY>\" To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: echo %FIREWORKS_API_KEY% You can quickly instantiate the  LLM  class and call the Fireworks API. The Build SDK handles deployment management automatically. from fireworks import LLM\n\n# Basic usage - SDK automatically selects optimal deployment type\nllm = LLM(model=\"llama4-maverick-instruct-basic\", deployment_type=\"auto\")\n\nresponse = llm.chat.completions.create(\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}]\n)\n\nprint(response.choices[0].message.content) You can also pass the API key directly to the LLM constructor:  LLM(model=\"llama4-maverick-instruct-basic\", deployment_type=\"auto\", api_key=\"<FIREWORKS_API_KEY>\")\n\n", "type": "text"}, {"text": "Title: Example with Fireworks Build SDK\nLink: https://docs.fireworks.ai/faq-new/models-inference/how-to-get-performance-metrics-for-streaming-responses\nContent: from fireworks import LLM\nimport os\n\nllm = LLM(\n    model=\"llama-v3p1-8b-instruct\",\n    deployment_type=\"serverless\",\n    api_key=os.environ[\"FIREWORKS_API_KEY\"],\n)\n\n# Streaming completion with performance metrics\nstream = llm.chat.completions.create(\n    messages=[{\"role\": \"user\", \"content\": \"Hello, world!\"}],\n    max_tokens=100,\n    stream=True,\n    perf_metrics_in_response=True,\n)\n\nfor chunk in stream:\n    # Use getattr to avoid linter errors for unknown attributes\n    perf_metrics = getattr(chunk, \"perf_metrics\", None)\n    finish_reason = getattr(chunk.choices[0], \"finish_reason\", None)\n    if perf_metrics is not None and finish_reason:\n        print(\"Performance metrics:\", perf_metrics)\n\nfrom fireworks import LLM\nimport os\n\nllm = LLM(\n    model=\"llama-v3p1-8b-instruct\",\n    deployment_type=\"serverless\",\n    api_key=os.environ[\"FIREWORKS_API_KEY\"],\n)\n\n# Streaming completion with performance metrics\nstream = llm.chat.completions.create(\n    messages=[{\"role\": \"user\", \"content\": \"Hello, world!\"}],\n    max_tokens=100,\n    stream=True,\n    perf_metrics_in_response=True,\n)\n\nfor chunk in stream:\n    # Use getattr to avoid linter errors for unknown attributes\n    perf_metrics = getattr(chunk, \"perf_metrics\", None)\n    finish_reason = getattr(chunk.choices[0], \"finish_reason\", None)\n    if perf_metrics is not None and finish_reason:\n        print(\"Performance metrics:\", perf_metrics)\n\nPerformance Metrics Overview\nThe Inference API returns several per-request metrics in the response. They can be useful for one-off debugging or can be logged by the client in their preferred observability tool. For aggregate metrics, see the usage dashboard. Non-streaming requests: Performance metrics are always included in response headers (e.g., fireworks-prompt-tokens , fireworks-server-time-to-first-token ). Streaming requests: Only selected performance metrics, such as \"fireworks-server-time-to-first-token,\" are available because HTTP headers must be sent before the first token can be streamed. Use the perf_metrics_in_response body parameter to include all metrics in the last SSE event of the response body.\n\n", "type": "text"}, {"text": "Title: Fireworks AI Developer Platform\nLink: https://docs.fireworks.ai/getting-started/introduction\nContent: Fireworks AI is the best platform for building AI product experiences with open source AI models. You can run and customize AI models with just a few lines of code!\n\nFireworks AI is the best platform for building AI product experiences with open source AI models. You can run and customize AI models with just a few lines of code!\n\nStart building with open source AI models\n\n", "type": "text"}], "tool_call_id": "functions.search:1"}, {"role": "assistant", "content": "Based on my research, I can now provide you with a comprehensive guide on managing deployments on Fireworks. Let me break this down into different approaches:\n\n## Managing Deployments on Fireworks\n\nFireworks offers three main ways to manage deployments:\n\n### 1. **Fireworks Build SDK (Recommended - Python)**\nThis is the most user-friendly approach using Python.\n\n**Installation:**\n```bash\npip install --upgrade fireworks-ai\n```\n\n**Basic Deployment Creation:**\n```python\nfrom fireworks import LLM\n\n# Create a new deployment\nllm = LLM(\n    model=\"llama-v3p2-3b-instruct\",\n    deployment_type=\"on-demand\",  # or \"serverless\", \"on-demand-lora\"\n    id=\"my-deployment-name\"\n)\n\n# Apply the deployment (required for on-demand deployments)\nllm.apply()\n\n# Use the deployment\nresponse = llm.chat.completions.create(\n    messages=[{\"role\": \"user\", \"content\": \"Hello, world!\"}]\n)\nprint(response.choices[0].message.content)\n```\n\n**Key Features:**\n- **Smart defaults**: Automatically selects optimal deployment type\n- **Declarative**: Describe desired state, SDK handles reconciliation\n- **Object-oriented**: Deployments are Python objects with methods/attributes\n- **Scale-to-zero**: Enabled by default with shortest scale-down window\n\n### 2. **Firectl CLI Tool**\nCommand-line interface for deployment management.\n\n**Create a deployment:**\n```bash\nfirectl create deployment \"accounts/fireworks/models/llama-v3p2-3b-instruct\" --id my-deployment\n```\n\n**Deploy LoRA fine-tunes:**\n```bash\n# Single command for LoRA deployment\nfirectl create deployment \"accounts/fireworks/models/<MODEL_ID of lora model>\"\n\n# Traditional two-step method (for multi-LoRA)\nfirectl create deployment \"accounts/fireworks/models/<base-model-id>\" --enable-addons\nfirectl load-lora <lora-model-id> --deployment <deployment-id>\n```\n\n**List deployments:**\n```bash\nfirectl list deployments\n```\n\n**Delete deployments:**\n```bash\nfirectl delete deployment my-deployment\n```\n\n**Update default deployment:**\n```bash\nfirectl update deployed-model <DEPLOYED_MODEL_ID> --default\n```\n\n### 3. **REST API**\nDirect API calls for programmatic management.\n\n**Create deployment:**\n```bash\ncurl -X POST https://api.fireworks.ai/v1/accounts/{account_id}/deployments \\\n  -H \"Authorization: Bearer YOUR_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"llama-v3p2-3b-instruct\",\n    \"deployment_type\": \"on-demand\"\n  }'\n```\n\n**Delete deployment:**\n```bash\ncurl -X DELETE https://api.fireworks.ai/v1/accounts/{account_id}/deployments/{deployment_id} \\\n  -H \"Authorization: Bearer YOUR_API_KEY\"\n```\n\n## Deployment Types Explained\n\n### **Serverless Deployments**\n- **Billing**: Per-token usage\n- **Best for**: Variable/low usage, quick prototyping\n- **Management**: Fully managed by Fireworks\n- **Access**: Instant, no setup required\n\n### **On-Demand Deployments**\n- **Billing**: Per GPU-second\n- **Best for**: Consistent usage, predictable performance\n- **Advantages**: \n  - No rate limits\n  - Predictable performance\n  - Access to larger model selection\n  - Custom base models from Hugging Face\n- **Management**: Requires explicit creation/deletion\n\n### **Reserved Capacity**\n- **Best for**: High-volume, consistent usage\n- **Advantages**: More reliable capacity, higher GPU quotas\n- **Contact**: inquiries@fireworks.ai for enterprise pricing\n\n## Complete Management Workflow Example\n\nHere's a comprehensive example using the Build SDK:\n\n```python\nfrom fireworks import LLM\nimport os\n\n# Set up API key\nos.environ[\"FIREWORKS_API_KEY\"] = \"your-api-key\"\n\n# 1. Create and manage deployments\ndef manage_deployments():\n    \n    # Create a serverless deployment (no .apply() needed)\n    serverless_llm = LLM(\n        model=\"llama4-maverick-instruct-basic\",\n        deployment_type=\"serverless\"\n    )\n    \n    # Create an on-demand deployment\n    ondemand_llm = LLM(\n        model=\"llama-v3p2-3b-instruct\",\n        deployment_type=\"on-demand\",\n        id=\"my-production-deployment\",\n        max_instances=3,\n        min_instances=0  # Enable scale-to-zero\n    )\n    \n    # Apply the on-demand deployment\n    ondemand_llm.apply()\n    \n    # 2. Query existing deployments\n    print(\"Connecting to existing deployment...\")\n    existing_llm = LLM(\n        model=\"llama-v3p2-3b-instruct\",\n        deployment_type=\"on-demand\",\n        id=\"my-existing-deployment\"\n    )\n    \n    # 3. Use deployments\n    response = existing_llm.chat.completions.create(\n        messages=[{\"role\": \"user\", \"content\": \"Explain deployment management\"}]\n    )\n    \n    # 4. Clean up (delete deployment when done)\n    # firectl delete deployment my-production-deployment\n    \n    return response.choices[0].message.content\n\n# Run the workflow\nresult = manage_deployments()\nprint(result)\n```\n\n## Best Practices\n\n1. **Cost Management**: Use serverless for development, on-demand for production\n2. **Monitoring**: Track usage through the Fireworks dashboard\n3. **Cleanup**: Always delete unused on-demand deployments to avoid charges\n4. **Scaling**: Use scale-to-zero for cost efficiency\n5. **Naming**: Use descriptive deployment IDs for easy management\n\n## Getting Started\n\n1. **Install the Build SDK**: `pip install fireworks-ai`\n2. **Get API Key**: From https://app.fireworks.ai/dashboard/api-keys\n3. **Choose Approach**: \n   - **Beginners**: Start with Build SDK\n   - **CLI Users**: Use firectl\n   - **API Integration**: Use REST API\n\nFor enterprise needs or reserved capacity, contact inquiries@fireworks.ai.\n\nWould you like me to dive deeper into any specific aspect of deployment management?", "tool_calls": []}], "input_metadata": {"row_id": "look-win-current-private-teacher", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct"}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "evaluation_result": {"score": 1.0, "is_score_valid": true, "reason": "At least one tool call was made", "metrics": {}}, "created_at": "2025-08-08T11:43:38.694430", "eval_metadata": {"name": "test_pytest_mcp_url", "description": "Run math evaluation on sample dataset using pytest interface.", "version": "v0.2.6.dev19+gf755623", "status": "finished", "num_runs": 1, "aggregation_method": "mean"}, "pid": 97954}, {"messages": [{"role": "system", "content": "You are a helpful assistant that can answer questions about Fireworks.\nALWAYS provide code or commands to execute to answer the question."}, {"role": "user", "content": "Can you teach me about how to manage deployments on Fireworks"}], "input_metadata": {"row_id": "invent-special-real-rich-dream", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct"}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "created_at": "2025-08-08T11:51:25.530495", "eval_metadata": {"name": "test_pytest_mcp_url", "description": "Run math evaluation on sample dataset using pytest interface.", "version": "v0.2.6.dev19+gf755623.dirty", "status": "stopped", "num_runs": 1, "aggregation_method": "mean"}, "pid": 2168}, {"messages": [{"role": "system", "content": "You are a helpful assistant that can answer questions about Fireworks.\nALWAYS provide code or commands to execute to answer the question."}, {"role": "user", "content": "Can you teach me about how to manage deployments on Fireworks"}], "input_metadata": {"row_id": "continue-encourage-dry-part-meeting", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct"}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "created_at": "2025-08-08T11:51:48.974779", "eval_metadata": {"name": "test_pytest_mcp_url", "description": "Run math evaluation on sample dataset using pytest interface.", "version": "v0.2.6.dev19+gf755623.dirty", "status": "stopped", "num_runs": 1, "aggregation_method": "mean"}, "pid": 2642}, {"messages": [{"role": "system", "content": "You are a helpful assistant that can answer questions about Fireworks.\nALWAYS provide code or commands to execute to answer the question."}, {"role": "user", "content": "Can you teach me about how to manage deployments on Fireworks"}, {"role": "assistant", "content": "I'll help you learn about managing deployments on Fireworks! Let me search for comprehensive information about deployment management.", "tool_calls": [{"id": "functions.search:0", "function": {"arguments": "{\"query\": \"deployment management Fireworks AI\"}", "name": "search"}, "type": "function"}]}, {"role": "tool", "content": [{"text": "Title: Fireworks AI Developer Platform\nLink: https://docs.fireworks.ai/getting-started/introduction\nContent: Fireworks AI is the best platform for building AI product experiences with open source AI models. You can run and customize AI models with just a few lines of code!\n\nFireworks AI is the best platform for building AI product experiences with open source AI models. You can run and customize AI models with just a few lines of code!\n\nStart building with open source AI models\n\n", "type": "text"}, {"text": "Title: How do I close my Fireworks.ai account?\nLink: https://docs.fireworks.ai/faq-new/account-access/how-do-i-close-my-fireworksai-account\nContent: To close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed\n\nTo close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed\n\nHow do I close my Fireworks.ai account?\nTo close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed\n\n", "type": "text"}, {"text": "Title: Do you host your deployments in the EU or Asia?\nLink: https://docs.fireworks.ai/faq-new/support-general/do-you-host-your-deployments-in-the-eu-or-asia\nContent: We are currently deployed in multiple U.S.-based locations. However, we\u2019re open to hearing more about your specific requirements. You can: Join our Discord community Write to us at inquiries@fireworks.ai If you're an Enterprise customer, please contact your dedicated customer support representative to ensure a timely response.\n\nWe are currently deployed in multiple U.S.-based locations. However, we\u2019re open to hearing more about your specific requirements. You can: Join our Discord community Write to us at inquiries@fireworks.ai If you're an Enterprise customer, please contact your dedicated customer support representative to ensure a timely response.\n\nDo you host your deployments in the EU or Asia?\nWe are currently deployed in multiple U.S.-based locations. However, we\u2019re open to hearing more about your specific requirements. You can: Join our Discord community Write to us at inquiries@fireworks.ai If you're an Enterprise customer, please contact your dedicated customer support representative to ensure a timely response.\n\n", "type": "text"}, {"text": "Title: Introducing the Fireworks Build SDK\nLink: https://docs.fireworks.ai/tools-sdks/python-client/sdk-introduction\nContent: The Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.\n\nThe Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.\n\nIntroducing the Fireworks Build SDK\nThe Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.\n\n", "type": "text"}, {"text": "Title: On-demand deployments\nLink: https://docs.fireworks.ai/guides/ondemand-deployments\nContent: If you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.\n\nIf you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.\n\nOn-demand deployments\nIf you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.\n\n", "type": "text"}, {"text": "Title: I have multiple Fireworks accounts. When I try to login with Google on Fireworks' web UI, I'm getting signed into the wrong account. How do I fix this?\nLink: https://docs.fireworks.ai/faq-new/account-access/i-have-multiple-fireworks-accounts-when-i-try-to-login-with-google-on-fireworks\nContent: If you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.\n\nIf you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.\n\nI have multiple Fireworks accounts. When I try to login with Google on Fireworks' web UI, I'm getting signed into the wrong account. How do I fix this?\nIf you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.\n\n", "type": "text"}, {"text": "Title: Introduction\nLink: https://docs.fireworks.ai/api-reference/introduction\nContent: Fireworks AI REST API enables you to interact with various language, image and embedding models using an API Key. It also lets you automate management of models, deployments, datasets, and more.\n\nFireworks AI REST API enables you to interact with various language, image and embedding models using an API Key. It also lets you automate management of models, deployments, datasets, and more.\n\nIntroduction\nFireworks AI REST API enables you to interact with various language, image and embedding models using an API Key. It also lets you automate management of models, deployments, datasets, and more.\n\n", "type": "text"}, {"text": "Title: Create a deployment\nLink: https://docs.fireworks.ai/tools-sdks/firectl/commands/create-deployment\nContent: Create a Deployment on Fireworks AI platform\n\nCreate a Deployment on Fireworks AI platform\n\nCreates a new deployment.\n\n", "type": "text"}, {"text": "Title: How much does Fireworks cost?\nLink: https://docs.fireworks.ai/faq-new/billing-pricing/how-much-does-fireworks-cost\nContent: Fireworks AI operates on a pay-as-you-go model for all non-Enterprise usage, and new users automatically receive free credits. You pay based on: Per token for serverless inference Per GPU usage time for on-demand deployments Per token of training data for fine-tuning For customers needing enterprise-grade security and reliability , please reach out to us at inquiries@fireworks.ai to discuss options. Find out more about our current pricing on our Pricing page.\n\nFireworks AI operates on a pay-as-you-go model for all non-Enterprise usage, and new users automatically receive free credits. You pay based on: Per token for serverless inference Per GPU usage time for on-demand deployments Per token of training data for fine-tuning For customers needing enterprise-grade security and reliability , please reach out to us at inquiries@fireworks.ai to discuss options. Find out more about our current pricing on our Pricing page.\n\nHow much does Fireworks cost?\nFireworks AI operates on a pay-as-you-go model for all non-Enterprise usage, and new users automatically receive free credits. You pay based on: Per token for serverless inference Per GPU usage time for on-demand deployments Per token of training data for fine-tuning For customers needing enterprise-grade security and reliability , please reach out to us at inquiries@fireworks.ai to discuss options. Find out more about our current pricing on our Pricing page.\n\n", "type": "text"}, {"text": "Title: Python (Fireworks)\nLink: https://docs.fireworks.ai/getting-started/quickstart\nContent: This is the recommended way to get started : This  Python (Fireworks)  tab uses our  Fireworks Build\nSDK  for the best performance and developer experience.\nSee our  Client-side performance\noptimization  guide for\nmore details. Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. pip install --upgrade fireworks-ai The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either  ~/.bash_profile  for Bash or  ~/.zshrc  for  Zsh .\nYou can do this by running the command: vim ~/.bash_profile vim ~/.zshrc Add a new line to the file with the following: export FIREWORKS_API_KEY=\"<API_KEY>\" After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. source ~/.bash_profile source ~/.zshrc You can verify that the variable has been set correctly by running  echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. setx FIREWORKS_API_KEY \"<API_KEY>\" To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: echo %FIREWORKS_API_KEY% You can quickly instantiate the  LLM  class and call the Fireworks API. The Build SDK handles deployment management automatically. from fireworks import LLM\n\n# Basic usage - SDK automatically selects optimal deployment type\nllm = LLM(model=\"llama4-maverick-instruct-basic\", deployment_type=\"auto\")\n\nresponse = llm.chat.completions.create(\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}]\n)\n\nprint(response.choices[0].message.content) You can also pass the API key directly to the LLM constructor:  LLM(model=\"llama4-maverick-instruct-basic\", deployment_type=\"auto\", api_key=\"<FIREWORKS_API_KEY>\")\n\nThis is the recommended way to get started : This  Python (Fireworks)  tab uses our  Fireworks Build\nSDK  for the best performance and developer experience.\nSee our  Client-side performance\noptimization  guide for\nmore details. Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. pip install --upgrade fireworks-ai The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either  ~/.bash_profile  for Bash or  ~/.zshrc  for  Zsh .\nYou can do this by running the command: vim ~/.bash_profile vim ~/.zshrc Add a new line to the file with the following: export FIREWORKS_API_KEY=\"<API_KEY>\" After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. source ~/.bash_profile source ~/.zshrc You can verify that the variable has been set correctly by running  echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. setx FIREWORKS_API_KEY \"<API_KEY>\" To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: echo %FIREWORKS_API_KEY% You can quickly instantiate the  LLM  class and call the Fireworks API. The Build SDK handles deployment management automatically. from fireworks import LLM\n\n# Basic usage - SDK automatically selects optimal deployment type\nllm = LLM(model=\"llama4-maverick-instruct-basic\", deployment_type=\"auto\")\n\nresponse = llm.chat.completions.create(\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}]\n)\n\nprint(response.choices[0].message.content) You can also pass the API key directly to the LLM constructor:  LLM(model=\"llama4-maverick-instruct-basic\", deployment_type=\"auto\", api_key=\"<FIREWORKS_API_KEY>\")\n\nSet up your developer environment & call the Fireworks API\nThis is the recommended way to get started : This  tab uses our Fireworks Build\nSDK for the best performance and developer experience.\nSee our Client-side performance\noptimization guide for\nmore details. Install SDK Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Configure API Key Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either ~/.bash_profile for Bash or ~/.zshrc for Zsh .\nYou can do this by running the command: Add a new line to the file with the following: After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. You can verify that the variable has been set correctly by running echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: Sending the first API Request You can quickly instantiate the LLM class and call the Fireworks API. The Build SDK handles deployment management automatically. You can also pass the API key directly to the LLM constructor: LLM(model=\"llama4-maverick-instruct-basic\", deployment_type=\"auto\", api_key=\"<FIREWORKS_API_KEY>\") Install SDK Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. Fireworks AI platform offers drop-in replacement with OpenAI Python Client. Configure API Key Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either ~/.bash_profile for Bash or ~/.zshrc for Zsh .\nYou can do this by running the command: Add a new line to the file with the following: After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. You can verify that the variable has been set correctly by running echo $OPENAI_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: echo %OPENAI_API_KEY%\n\n", "type": "text"}], "tool_call_id": "functions.search:0"}, {"role": "assistant", "content": "Let me search for more specific information about deployment management commands and workflows.", "tool_calls": [{"id": "functions.search:1", "function": {"arguments": "{\"query\": \"deployment management commands create update delete list Fireworks\"}", "name": "search"}, "type": "function"}]}, {"role": "tool", "content": [{"text": "Title: Deployment Command\nLink: https://docs.fireworks.ai/evaluators/examples/gcp_cloud_run_deployment_example\nContent: It's recommended to run the deployment command from the directory containing the reward function script ( dummy_rewards.py ) and your rewardkit.yaml (if used), for example, from examples/gcp_cloud_run_deployment_example/. Ensure your virtual environment is active: Run the deployment command: Command Explanation: --id my-dummy-gcp-evaluator : A unique ID for your evaluator on the Fireworks AI platform. --target gcp-cloud-run : Specifies deployment to GCP Cloud Run. --function-ref dummy_rewards.hello_world_reward : The Python import path to your reward function. If dummy_rewards.py is in the current directory, this reference works. --gcp-auth-mode api-key : Configures the Cloud Run service with API key authentication. reward-kit will generate a key, store it in GCP Secret Manager, and configure the service. The key is also saved to your local rewardkit.yaml under evaluator_endpoint_keys . This is the default if not specified. --verbose : Shows detailed output, including gcloud commands being executed. --force : (Optional) If an evaluator with the same --id already exists, this flag will delete the existing one before creating the new one.\n\nIt's recommended to run the deployment command from the directory containing the reward function script ( dummy_rewards.py ) and your rewardkit.yaml (if used), for example, from examples/gcp_cloud_run_deployment_example/. Ensure your virtual environment is active: Run the deployment command: Command Explanation: --id my-dummy-gcp-evaluator : A unique ID for your evaluator on the Fireworks AI platform. --target gcp-cloud-run : Specifies deployment to GCP Cloud Run. --function-ref dummy_rewards.hello_world_reward : The Python import path to your reward function. If dummy_rewards.py is in the current directory, this reference works. --gcp-auth-mode api-key : Configures the Cloud Run service with API key authentication. reward-kit will generate a key, store it in GCP Secret Manager, and configure the service. The key is also saved to your local rewardkit.yaml under evaluator_endpoint_keys . This is the default if not specified. --verbose : Shows detailed output, including gcloud commands being executed. --force : (Optional) If an evaluator with the same --id already exists, this flag will delete the existing one before creating the new one.\n\nIt's recommended to run the deployment command from the directory containing the reward function script ( dummy_rewards.py ) and your rewardkit.yaml (if used), for example, from examples/gcp_cloud_run_deployment_example/. Ensure your virtual environment is active: source .venv/bin/activate\n\n", "type": "text"}, {"text": "Title: I have multiple Fireworks accounts. When I try to login with Google on Fireworks' web UI, I'm getting signed into the wrong account. How do I fix this?\nLink: https://docs.fireworks.ai/faq-new/account-access/i-have-multiple-fireworks-accounts-when-i-try-to-login-with-google-on-fireworks\nContent: If you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.\n\nIf you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.\n\nI have multiple Fireworks accounts. When I try to login with Google on Fireworks' web UI, I'm getting signed into the wrong account. How do I fix this?\nIf you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.\n\n", "type": "text"}, {"text": "Title: Deleting Stored Responses\nLink: https://docs.fireworks.ai/guides/response-api\nContent: When responses are stored (the default behavior with store=True ), you can immediately delete them from storage using the DELETE endpoint. This permanently removes the conversation data. from fireworks import LLM\nimport requests\nimport os\n\nllm = LLM(model=\"qwen3-235b-a22b\", deployment_type=\"serverless\")\n\n# Create a response\nresponse = llm.responses.create(\n    input=\"What is the capital of France?\",\n    store=True  # This is the default\n)\n\nresponse_id = response.id\nprint(f\"Created response with ID: {response_id}\")\n\n# Delete the response immediately\nheaders = {\n    \"Authorization\": f\"Bearer {os.getenv('FIREWORKS_API_KEY')}\",\n    \"x-fireworks-account-id\": \"your-account-id\"\n}\ndelete_response = requests.delete(\n    f\"https://api.fireworks.ai/inference/v1/responses/{response_id}\",\n    headers=headers\n)\n\nif delete_response.status_code == 200:\n    print(\"Response deleted successfully\")\nelse:\n    print(f\"Failed to delete response: {delete_response.status_code}\")\n\nWhen responses are stored (the default behavior with store=True ), you can immediately delete them from storage using the DELETE endpoint. This permanently removes the conversation data. from fireworks import LLM\nimport requests\nimport os\n\nllm = LLM(model=\"qwen3-235b-a22b\", deployment_type=\"serverless\")\n\n# Create a response\nresponse = llm.responses.create(\n    input=\"What is the capital of France?\",\n    store=True  # This is the default\n)\n\nresponse_id = response.id\nprint(f\"Created response with ID: {response_id}\")\n\n# Delete the response immediately\nheaders = {\n    \"Authorization\": f\"Bearer {os.getenv('FIREWORKS_API_KEY')}\",\n    \"x-fireworks-account-id\": \"your-account-id\"\n}\ndelete_response = requests.delete(\n    f\"https://api.fireworks.ai/inference/v1/responses/{response_id}\",\n    headers=headers\n)\n\nif delete_response.status_code == 200:\n    print(\"Response deleted successfully\")\nelse:\n    print(f\"Failed to delete response: {delete_response.status_code}\")\n\nWhen responses are stored (the default behavior with store=True ), you can immediately delete them from storage using the DELETE endpoint. This permanently removes the conversation data. import os\nfrom openai import OpenAI\nimport requests\n\nclient = OpenAI(\n    base_url=\"https://api.fireworks.ai/inference/v1\",\n    api_key=os.getenv(\"FIREWORKS_API_KEY\", \"YOUR_FIREWORKS_API_KEY_HERE\")\n)\n\n# Create a response\nresponse = client.responses.create(\n    model=\"accounts/fireworks/models/qwen3-235b-a22b\",\n    input=\"What is the capital of France?\",\n    store=True  # This is the default\n)\n\nresponse_id = response.id\nprint(f\"Created response with ID: {response_id}\")\n\n# Delete the response immediately\nheaders = {\n    \"Authorization\": f\"Bearer {os.getenv('FIREWORKS_API_KEY')}\",\n    \"x-fireworks-account-id\": \"your-account-id\"\n}\ndelete_response = requests.delete(\n    f\"https://api.fireworks.ai/inference/v1/responses/{response_id}\",\n    headers=headers\n)\n\nif delete_response.status_code == 200:\n    print(\"Response deleted successfully\")\nelse:\n    print(f\"Failed to delete response: {delete_response.status_code}\")\n\n", "type": "text"}, {"text": "Title: Delete Deployment\nLink: https://docs.fireworks.ai/api-reference/delete-deployment\nContent: \n\n\n\nREST Endpoint DELETE /v1/accounts/{account_id}/deployments/{deployment_id}\n\nServer(s): https://api.fireworks.ai\n\n\nSummary:  \n\nDescription: N/A\n\nSecurity methods: \nheader: \n\tAuthorization: bearer auth\n\t\n\nRequest params: \npath params:\naccount_id:\n\tstring\n\tDescription: The Account Id\n\tExample: \"<string>\"\ndeployment_id:\n\tstring\n\tDescription: The Deployment Id\n\tExample: \"<string>\"\n\nquery params:\nhard:\n\tboolean\n\tDescription: If true, this will perform a hard deletion.\n\tExample: true\nignoreChecks:\n\tboolean\n\tDescription: If true, this will ignore checks and force the deletion of a deployment that is currently\ndeployed and is in use.\n\tExample: true\n\n\nRequest body: N/A\n\n\nResponses: \n200: object\n\tDescription: N/A\n\tExample: {}\n\n", "type": "text"}, {"text": "Title: List Deployments\nLink: https://docs.fireworks.ai/api-reference/list-deployments\nContent: REST Endpoint GET /v1/accounts/{account_id}/deployments\n\nServer(s): https://api.fireworks.ai\n\n\nSummary:  \n\nDescription: N/A\n\nSecurity methods: \nheader: \n\tAuthorization: bearer auth\n\t\n\nRequest params: \npath params:\naccount_id:\n\tstring\n\tDescription: The Account Id\n\tExample: \"<string>\"\n\nquery params:\npageSize:\n\tinteger\n\tDescription: The maximum number of deployments to return. The maximum page_size is 200,\nvalues above 200 will be coerced to 200.\nIf unspecified, the default is 50.\n\tExample: 123\npageToken:\n\tstring\n\tDescription: A page token, received from a previous ListDeployments call. Provide this\nto retrieve the subsequent page. When paginating, all other parameters\nprovided to ListDeployments must match the call that provided the page\ntoken.\n\tExample: \"<string>\"\nfilter:\n\tstring\n\tDescription: Only deployment satisfying the provided filter (if specified) will be\nreturned. See https://google.aip.dev/160 for the filter grammar.\n\tExample: \"<string>\"\norderBy:\n\tstring\n\tDescription: A comma-separated list of fields to order by. e.g. \"foo,bar\"\nThe default sort order is ascending. To specify a descending order for a\nfield, append a \" desc\" suffix. e.g. \"foo desc,bar\"\nSubfields are specified with a \".\" character. e.g. \"foo.bar\"\nIf not specified, the default order is by \"create_time\".\n\tExample: \"<string>\"\nshowDeleted:\n\tboolean\n\tDescription: If set, DELETED deployments will be included.\n\tExample: true\nreadMask:\n\tstring\n\tDescription: The fields to be returned in the response. If empty or \"*\", all fields will be returned.\n\tExample: \"<string>\"\n\n\nRequest body: N/A\n\n\nResponses: \n200: object\n\tDescription: N/A\n\tExample: {\"deployments\":[{\"name\":\"<string>\",\"displayName\":\"<string>\",\"description\":\"<string>\",\"createTime\":\"2023-11-07T05:31:56Z\",\"expireTime\":\"2023-11-07T05:31:56Z\",\"purgeTime\":\"2023-11-07T05:31:56Z\",\"deleteTime\":\"2023-11-07T05:31:56Z\",\"state\":\"STATE_UNSPECIFIED\",\"status\":{\"code\":\"OK\",\"message\":\"<string>\"},\"minReplicaCount\":123,\"maxReplicaCount\":123,\"replicaCount\":123,\"autoscalingPolicy\":{\"scaleUpWindow\":\"<string>\",\"scaleDownWindow\":\"<string>\",\"scaleToZeroWindow\":\"<string>\",\"loadTargets\":{}},\"baseModel\":\"<string>\",\"acceleratorCount\":123,\"acceleratorType\":\"ACCELERATOR_TYPE_UNSPECIFIED\",\"precision\":\"PRECISION_UNSPECIFIED\",\"cluster\":\"<string>\",\"enableAddons\":true,\"draftTokenCount\":123,\"draftModel\":\"<string>\",\"ngramSpeculationLength\":123,\"numPeftDeviceCached\":123,\"deploymentTemplate\":\"<string>\",\"autoTune\":{\"longPrompt\":true},\"placement\":{\"region\":\"REGION_UNSPECIFIED\",\"multiRegion\":\"MULTI_REGION_UNSPECIFIED\",\"regions\":[\"REGION_UNSPECIFIED\"]},\"region\":\"REGION_UNSPECIFIED\",\"updateTime\":\"2023-11-07T05:31:56Z\",\"disableDeploymentSizeValidation\":true}],\"nextPageToken\":\"<string>\",\"totalSize\":123}\n\nREST Endpoint GET /v1/accounts/{account_id}/deployments\n\nServer(s): https://api.fireworks.ai\n\n\nSummary:  \n\nDescription: N/A\n\nSecurity methods: \nheader: \n\tAuthorization: bearer auth\n\t\n\nRequest params: \npath params:\naccount_id:\n\tstring\n\tDescription: The Account Id\n\tExample: \"<string>\"\n\nquery params:\npageSize:\n\tinteger\n\tDescription: The maximum number of deployments to return. The maximum page_size is 200,\nvalues above 200 will be coerced to 200.\nIf unspecified, the default is 50.\n\tExample: 123\npageToken:\n\tstring\n\tDescription: A page token, received from a previous ListDeployments call. Provide this\nto retrieve the subsequent page. When paginating, all other parameters\nprovided to ListDeployments must match the call that provided the page\ntoken.\n\tExample: \"<string>\"\nfilter:\n\tstring\n\tDescription: Only deployment satisfying the provided filter (if specified) will be\nreturned. See https://google.aip.dev/160 for the filter grammar.\n\tExample: \"<string>\"\norderBy:\n\tstring\n\tDescription: A comma-separated list of fields to order by. e.g. \"foo,bar\"\nThe default sort order is ascending. To specify a descending order for a\nfield, append a \" desc\" suffix. e.g. \"foo desc,bar\"\nSubfields are specified with a \".\" character. e.g. \"foo.bar\"\nIf not specified, the default order is by \"create_time\".\n\tExample: \"<string>\"\nshowDeleted:\n\tboolean\n\tDescription: If set, DELETED deployments will be included.\n\tExample: true\nreadMask:\n\tstring\n\tDescription: The fields to be returned in the response. If empty or \"*\", all fields will be returned.\n\tExample: \"<string>\"\n\n\nRequest body: N/A\n\n\nResponses: \n200: object\n\tDescription: N/A\n\tExample: {\"deployments\":[{\"name\":\"<string>\",\"displayName\":\"<string>\",\"description\":\"<string>\",\"createTime\":\"2023-11-07T05:31:56Z\",\"expireTime\":\"2023-11-07T05:31:56Z\",\"purgeTime\":\"2023-11-07T05:31:56Z\",\"deleteTime\":\"2023-11-07T05:31:56Z\",\"state\":\"STATE_UNSPECIFIED\",\"status\":{\"code\":\"OK\",\"message\":\"<string>\"},\"minReplicaCount\":123,\"maxReplicaCount\":123,\"replicaCount\":123,\"autoscalingPolicy\":{\"scaleUpWindow\":\"<string>\",\"scaleDownWindow\":\"<string>\",\"scaleToZeroWindow\":\"<string>\",\"loadTargets\":{}},\"baseModel\":\"<string>\",\"acceleratorCount\":123,\"acceleratorType\":\"ACCELERATOR_TYPE_UNSPECIFIED\",\"precision\":\"PRECISION_UNSPECIFIED\",\"cluster\":\"<string>\",\"enableAddons\":true,\"draftTokenCount\":123,\"draftModel\":\"<string>\",\"ngramSpeculationLength\":123,\"numPeftDeviceCached\":123,\"deploymentTemplate\":\"<string>\",\"autoTune\":{\"longPrompt\":true},\"placement\":{\"region\":\"REGION_UNSPECIFIED\",\"multiRegion\":\"MULTI_REGION_UNSPECIFIED\",\"regions\":[\"REGION_UNSPECIFIED\"]},\"region\":\"REGION_UNSPECIFIED\",\"updateTime\":\"2023-11-07T05:31:56Z\",\"disableDeploymentSizeValidation\":true}],\"nextPageToken\":\"<string>\",\"totalSize\":123}\n\n\n\n", "type": "text"}, {"text": "Title: Publishing a deployed model\nLink: https://docs.fireworks.ai/guides/ondemand-deployments\nContent: By default, models can only be queried by the account that owns them. To make a deployment public so anyone with a valid Fireworks API key can query it, update the deployed model with the --public flag. Find the Deployed Model ID Every model running on a deployment receives a unique deployed model ID. You can find this ID if you know the model name and deployment name using the following command: firectl list deployed-models --filter 'model=\"<MODEL_NAME>\" AND deployment=\"<DEPLOYMENT_NAME>\"'\n\nBy default, models can only be queried by the account that owns them. To make a deployment public so anyone with a valid Fireworks API key can query it, update the deployed model with the --public flag. Find the Deployed Model ID Every model running on a deployment receives a unique deployed model ID. You can find this ID if you know the model name and deployment name using the following command: firectl list deployed-models --filter 'model=\"<MODEL_NAME>\" AND deployment=\"<DEPLOYMENT_NAME>\"'\n\nManaging default deployments\nSince a model may be deployed to multiple deployments, querying by model name will route to the \"default\" deployed model. You can see which deployed model entry is marked with Default: true using: This will show the Deployed Model Refs section with the Default: true entry. To update the default deployed model, note the Name of the deployed model reference above. Then run: To delete a default deployment you must delete all other deployments for the same model first, or designate a different deployed model as the default as described above. This is to ensure that querying by model name will always route to an unambiguous default deployment as long as deployments for the model exist.\n\n", "type": "text"}, {"text": "Title: Why am I getting \"invalid id\" errors when using firectl commands like create deployment or list deployments?\nLink: https://docs.fireworks.ai/faq-new/fine-tuning/why-am-i-getting-invalid-id-errors-when-using-firectl-commands-like-create-deplo\nContent: This error typically occurs when your account ID is not properly configured.\n\nThis error typically occurs when your account ID is not properly configured.\n\nWhy am I getting \"invalid id\" errors when using firectl commands like create deployment or list deployments?\nThis error typically occurs when your account ID is not properly configured.\n\n", "type": "text"}, {"text": "Title: How do I close my Fireworks.ai account?\nLink: https://docs.fireworks.ai/faq-new/account-access/how-do-i-close-my-fireworksai-account\nContent: To close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed\n\nTo close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed\n\nHow do I close my Fireworks.ai account?\nTo close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed\n\n", "type": "text"}, {"text": "Title: Introducing the Fireworks Build SDK\nLink: https://docs.fireworks.ai/tools-sdks/python-client/sdk-introduction\nContent: Introducing the Fireworks Build SDK\nThe Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.\n\nIntroducing the Fireworks Build SDK\nThe Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.\n\nThe Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.\n\n", "type": "text"}, {"text": "Title: Python (Fireworks)\nLink: https://docs.fireworks.ai/getting-started/quickstart\nContent: This is the recommended way to get started : This  Python (Fireworks)  tab uses our  Fireworks Build\nSDK  for the best performance and developer experience.\nSee our  Client-side performance\noptimization  guide for\nmore details. Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. pip install --upgrade fireworks-ai The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either  ~/.bash_profile  for Bash or  ~/.zshrc  for  Zsh .\nYou can do this by running the command: vim ~/.bash_profile vim ~/.zshrc Add a new line to the file with the following: export FIREWORKS_API_KEY=\"<API_KEY>\" After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. source ~/.bash_profile source ~/.zshrc You can verify that the variable has been set correctly by running  echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. setx FIREWORKS_API_KEY \"<API_KEY>\" To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: echo %FIREWORKS_API_KEY% You can quickly instantiate the  LLM  class and call the Fireworks API. The Build SDK handles deployment management automatically. from fireworks import LLM\n\n# Basic usage - SDK automatically selects optimal deployment type\nllm = LLM(model=\"llama4-maverick-instruct-basic\", deployment_type=\"auto\")\n\nresponse = llm.chat.completions.create(\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}]\n)\n\nprint(response.choices[0].message.content) You can also pass the API key directly to the LLM constructor:  LLM(model=\"llama4-maverick-instruct-basic\", deployment_type=\"auto\", api_key=\"<FIREWORKS_API_KEY>\")\n\nThis is the recommended way to get started : This  Python (Fireworks)  tab uses our  Fireworks Build\nSDK  for the best performance and developer experience.\nSee our  Client-side performance\noptimization  guide for\nmore details. Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. pip install --upgrade fireworks-ai The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either  ~/.bash_profile  for Bash or  ~/.zshrc  for  Zsh .\nYou can do this by running the command: vim ~/.bash_profile vim ~/.zshrc Add a new line to the file with the following: export FIREWORKS_API_KEY=\"<API_KEY>\" After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. source ~/.bash_profile source ~/.zshrc You can verify that the variable has been set correctly by running  echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. setx FIREWORKS_API_KEY \"<API_KEY>\" To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: echo %FIREWORKS_API_KEY% You can quickly instantiate the  LLM  class and call the Fireworks API. The Build SDK handles deployment management automatically. from fireworks import LLM\n\n# Basic usage - SDK automatically selects optimal deployment type\nllm = LLM(model=\"llama4-maverick-instruct-basic\", deployment_type=\"auto\")\n\nresponse = llm.chat.completions.create(\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}]\n)\n\nprint(response.choices[0].message.content) You can also pass the API key directly to the LLM constructor:  LLM(model=\"llama4-maverick-instruct-basic\", deployment_type=\"auto\", api_key=\"<FIREWORKS_API_KEY>\")\n\nSet up your developer environment & call the Fireworks API\nThis is the recommended way to get started : This  tab uses our Fireworks Build\nSDK for the best performance and developer experience.\nSee our Client-side performance\noptimization guide for\nmore details. Install SDK Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Configure API Key Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either ~/.bash_profile for Bash or ~/.zshrc for Zsh .\nYou can do this by running the command: Add a new line to the file with the following: After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. You can verify that the variable has been set correctly by running echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: Sending the first API Request You can quickly instantiate the LLM class and call the Fireworks API. The Build SDK handles deployment management automatically. from fireworks import LLM\n\n# Basic usage - SDK automatically selects optimal deployment type\nllm = LLM(model=\"llama4-maverick-instruct-basic\", deployment_type=\"auto\")\n\nresponse = llm.chat.completions.create(\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}]\n)\n\nprint(response.choices[0].message.content)\n\n", "type": "text"}], "tool_call_id": "functions.search:1"}, {"role": "assistant", "content": "Let me search for more specific information about the firectl tool and deployment workflows.", "tool_calls": [{"id": "functions.search:0", "function": {"arguments": "{\"query\": \"firectl deployment management commands create deployment update deployment\"}", "name": "search"}, "type": "function"}]}, {"role": "tool", "content": [{"text": "Title: Why am I getting \"invalid id\" errors when using firectl commands like create deployment or list deployments?\nLink: https://docs.fireworks.ai/faq-new/fine-tuning/why-am-i-getting-invalid-id-errors-when-using-firectl-commands-like-create-deplo\nContent: This error typically occurs when your account ID is not properly configured.\n\nThis error typically occurs when your account ID is not properly configured.\n\nWhy am I getting \"invalid id\" errors when using firectl commands like create deployment or list deployments?\nThis error typically occurs when your account ID is not properly configured.\n\n", "type": "text"}, {"text": "Title: Create a deployment\nLink: https://docs.fireworks.ai/tools-sdks/firectl/commands/create-deployment\nContent: Creates a new deployment. firectl create deployment [flags]\n\nCreates a new deployment. firectl create deployment [flags]\n\nCreates a new deployment.\n\n", "type": "text"}, {"text": "Title: \ud83d\ude80 Easier & faster LoRA fine-tune deployments on Fireworks\nLink: https://docs.fireworks.ai/updates/changelog\nContent: You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment \"accounts/fireworks/models/<MODEL_ID of lora model>\"\n\nYou can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment \"accounts/fireworks/models/<MODEL_ID of lora model>\"\n\nYou can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: Previously, this involved two distinct steps, and the resulting deployment was slower than the base model: Create a deployment using firectl create deployment \"accounts/fireworks/models/<MODEL_ID of base model>\" --enable-addons Then deploy the addon to the deployment: firectl load-lora <MODEL_ID> --deployment <DEPLOYMENT_ID> For more information, see our deployment documentation. This change is for dedicated deployments with a single LoRA. You can still deploy multiple LoRAs on a deployment or deploy LoRA(s) on some Serverless models as described in the documentation.\n\n", "type": "text"}, {"text": "Title: Managing default deployments\nLink: https://docs.fireworks.ai/guides/ondemand-deployments\nContent: Since a model may be deployed to multiple deployments, querying by model name will route to the \"default\" deployed model. You can see which deployed model entry is marked with Default: true using: This will show the Deployed Model Refs section with the Default: true entry. To update the default deployed model, note the Name of the deployed model reference above. Then run: firectl update deployed-model <DEPLOYED_MODEL_ID_2> --default\n\nSince a model may be deployed to multiple deployments, querying by model name will route to the \"default\" deployed model. You can see which deployed model entry is marked with Default: true using: This will show the Deployed Model Refs section with the Default: true entry. To update the default deployed model, note the Name of the deployed model reference above. Then run: firectl update deployed-model <DEPLOYED_MODEL_ID_2> --default\n\nPublishing a deployed model\nBy default, models can only be queried by the account that owns them. To make a deployment public so anyone with a valid Fireworks API key can query it, update the deployed model with the --public flag. Find the Deployed Model ID Every model running on a deployment receives a unique deployed model ID. You can find this ID if you know the model name and deployment name using the following command: The first column in the output is the deployed model ID. Publish a deployed model To make a deployment public so anyone with a valid Fireworks API key can query it, update the deployed model with the --public flag. firectl update deployed-model <DEPLOYED_MODEL_ID> --public\n\n", "type": "text"}, {"text": "Title: Quick deployment\nLink: https://docs.fireworks.ai/fine-tuning/single-lora\nContent: Deploy your LoRA fine-tuned model with one simple command: firectl create deployment \"accounts/fireworks/models/<MODEL_ID of lora model>\"\n\nDeploy your LoRA fine-tuned model with one simple command: firectl create deployment \"accounts/fireworks/models/<MODEL_ID of lora model>\"\n\nAlternative deployment method\nThis two-step method is the standard approach for multi-LoRA deployments where multiple LoRA models share the same base model. While it can also be used for single LoRA deployments, it provides slower performance compared to live merge and is not recommended for single LoRA use cases. You can also deploy single LoRA models using a two-step process: Create base model deployment Deploy the base model with addons enabled: firectl create deployment \"accounts/fireworks/models/<MODEL_ID of base model>\" --enable-addons\n\n", "type": "text"}, {"text": "Title: Deploying a fine-tuned model using an on-demand deployment\nLink: https://docs.fireworks.ai/fine-tuning/fine-tuning-models\nContent: Use the following command to deploy your fine-tuned model using an on-demand deployment: firectl create deployment <FINE_TUNED_MODEL_ID>\n\nUse the following command to deploy your fine-tuned model using an on-demand deployment: firectl create deployment <FINE_TUNED_MODEL_ID>\n\nfirectl\nfirectl create dataset <DATASET_ID> /path/to/jsonl/file\n\n", "type": "text"}, {"text": "Title: Does Fireworks support custom base models?\nLink: https://docs.fireworks.ai/faq-new/models-inference/does-fireworks-support-custom-base-models\nContent: Yes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.\n\nYes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.\n\nDoes Fireworks support custom base models?\nYes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.\n\n", "type": "text"}, {"text": "Title: firectl\nLink: https://docs.fireworks.ai/fine-tuning/fine-tuning-vlm\nContent: # Create a deployment for your fine-tuned VLM\nfirectl create deployment my-custom-vlm\n\n# Check deployment status\nfirectl get deployment accounts/your-account/deployment/deployment-id\n\n# Create a deployment for your fine-tuned VLM\nfirectl create deployment my-custom-vlm\n\n# Check deployment status\nfirectl get deployment accounts/your-account/deployment/deployment-id\n\nfirectl create sftj \\\n  --base-model accounts/fireworks/models/qwen2p5-vl-32b-instruct \\\n  --dataset my-vlm-dataset \\\n  --output-model my-custom-vlm \\\n  --epochs 3 For additional parameters like learning rates, evaluation datasets, and batch sizes, see  Additional SFT job settings .\n\n", "type": "text"}, {"text": "Title: firectl\nLink: https://docs.fireworks.ai/models/quantization\nContent: firectl create deployment <MODEL> --accelerator-type NVIDIA_H100_80GB --precision FP8\n\nfirectl create deployment <MODEL> --accelerator-type NVIDIA_H100_80GB --precision FP8\n\nCreating an FP8 deployment\nBy default, creating a dedicated deployment will use the FP16 checkpoint. To see what precisions are available for a\nmodel, run:  get model <MODEL_ID>\n\n", "type": "text"}, {"text": "Title: Uploading the model locally (firectl)\nLink: https://docs.fireworks.ai/models/uploading-custom-models\nContent: To upload a custom base model, run the following command. firectl create model <MODEL_ID> /path/to/files/\n\nTo upload a custom base model, run the following command. firectl create model <MODEL_ID> /path/to/files/\n\nTo upload a custom base model, run the following command.\n\n", "type": "text"}], "tool_call_id": "functions.search:0"}], "input_metadata": {"row_id": "create-destroy-past-service-case", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct"}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "created_at": "2025-08-08T12:17:53.864430", "eval_metadata": {"name": "test_pytest_mcp_url", "description": "Run math evaluation on sample dataset using pytest interface.", "version": "v0.2.6.dev20+ge1da45a", "status": "stopped", "num_runs": 1, "aggregation_method": "mean"}, "pid": 12679}, {"messages": [{"role": "user", "content": "Write an outline for a paper on the history of Yemeni coffee. The outline should include the main points of the paper, and at least 15 sections should be highlighted with markdown such as *highlighted section*."}, {"role": "assistant", "content": "# Outline: The History of Yemeni Coffee\n\n1. **Introduction: Why Yemeni Coffee Matters**  \n   - Thesis: Yemen\u2019s role as the cradle of commercial coffee culture and its enduring legacy on global trade, ritual, and taste.\n\n2. *Pre-Coffee Yemen: Agriculture and Trade Routes*  \n   - Overview of terraced farming and the incense & spice trade that primed the Red Sea economy.\n\n3. *The Myth and Botany of Coffea arabica*  \n   - Ethiopian origin vs. Yemeni domestication; genetic evidence of Yemeni varietals.\n\n4. **Sufi Mystics and the First Coffeehouses**  \n   - 15th-century Sufi orders in Mocha and Ta\u02bfizz using *qahwa* for nocturnal devotions.\n\n5. *Mocha as the World\u2019s First Coffee Port*  \n   - Rise of the Red Sea entrep\u00f4t; Ottoman tax farms and the *tujjar* merchant guilds.\n\n6. **Yemeni Cultivation Techniques: Terraces, Water, and Shade**  \n   - Indigenous irrigation (*ghayl*), drought-resistant varietals, and the *mafrash* drying mats.\n\n7. *The 16th-Century Ottoman Coffee Ban and Smuggling Networks*  \n   - How Mecca\u2019s 1511 prohibition fueled clandestine Red Sea traffic and price inflation.\n\n8. **European Encounters: \u201cTurkish\u201d Coffee as a Yemeni Product**  \n   - Dutch, Venetian, and English merchants mislabeling Mocha beans; first price lists in Amsterdam (1616).\n\n9. *The Dutch East India Company and the First Seed Theft*  \n   - Pieter van den Broecke\u2019s 1616 smuggling of Yemeni seeds to Batavia\u2014birth of Java coffee.\n\n10. **Yemen\u2019s Monopoly Crumbles: 17th\u201318th-Century Rival Plantations**  \n    - French at R\u00e9union (Bourbon), Portuguese in Brazil, Spanish in Jamaica\u2014all tracing back to Mocha stock.\n\n11. *The Qasimi Era: Internal Trade, Taxes, and the Rise of the Ismaili Imamate*  \n    - How highland imams leveraged coffee revenues to fund statecraft and architecture.\n\n12. **Decline in the 19th Century: British Aden and the Suez Canal**  \n    - Shift of maritime traffic; competition from colonial plantations; collapse of Mocha\u2019s mercantile elite.\n\n13. *Legacy in Cupping Notes: Mocha Java and Flavor Memory*  \n    - Chocolatey, winey profiles; how Yemeni beans shaped early espresso blends.\n\n14. **Modern Revival: Specialty Coffee, NGOs, and Qima Coffee**  \n    - 21st-century traceability, micro-lots, and direct-trade premiums; challenges of war and water scarcity.\n\n15. *Cultural Continuities: Qat vs. Qahwa, Wedding Roasts, and Oral Poetry*  \n    - Daily rituals, gendered spaces, and the recitation of *zawamil* couplets over fresh brews.\n\n16. **Conclusion: Yemen as a Living Archive of Coffee Culture**  \n    - Reasserting Yemen\u2019s centrality in every cup; call for sustainable heritage preservation."}], "input_metadata": {"row_id": "review-say-right-news-government", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct", "temperature": 0.0, "max_tokens": 4096}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "ground_truth": "15", "evaluation_result": {"score": 1.0, "is_score_valid": true, "reason": "\u2705 Found 21 highlighted sections (required: 15)", "metrics": {}}, "created_at": "2025-08-08T23:30:54.738493", "eval_metadata": {"name": "test_markdown_highlighting_evaluation", "description": "\n    Evaluation function that checks if the model's response contains the required number of formatted sections.\n    ", "version": "v0.2.6.dev20+ge1da45a.dirty", "status": "finished", "run_id": "hear-live-common-late-hate", "num_runs": 1, "aggregation_method": "mean", "threshold_of_success": 0.5, "passed": true}, "pid": 52072}, {"messages": [{"role": "user", "content": "Write a cover letter for a job at a local coffee shop in the form of a poem. Highlight at least 5 text sections using \"*\". For example: *3 years of experience*."}, {"role": "assistant", "content": "Dear *Steam & Bean* family,  \nI come to you not on paper feet but on *three years of espresso-stained sneakers*,  \nsoles softened by dawn milk-foam and dusk drip-trays.  \nI have measured my mornings in *latte-art hearts*\u2014  \nsome perfect, some *wobbly, but all poured with intention*.  \n\nI know the *hiss of the steam wand* like a lullaby,  \nand the *clink of ceramic* like a metronome for the day.  \nI have *memorized the weight of 18 grams* of ground hope,  \ntamped it level, locked it tight, and watched *crema bloom*  \nthe way a shy smile opens when someone says their name right.  \n\nI speak *fluent caffeine*:  \na triple-shot for the night-shift nurse,  \noat-milk cappuccino for the poet who tips in verses,  \ndecaf Americano for the retired teacher grading sunrise.  \nI *remember orders like birthdays*,  \nand birthdays like *extra whip, no charge*.  \n\nI can *calibrate a grinder* faster than gossip travels,  \nand *unclog a portafilter* with the patience of a gardener.  \nI have *danced behind counters* two feet wide,  \ntwirled between syrups and strangers,  \nand never spilled a single *thank-you*.  \n\nIf you need someone who *sweats kindness* and *smiles in ristretto*,  \nwho counts tips not in coins but in *stories collected*,  \nI am your *next opening shift*, your *steady hand at 5:59 a.m.*,  \nyour *last call who still says \u201csee you tomorrow\u201d*.  \n\nLet me *steam, pour, wipe, repeat*\u2014  \nuntil the chairs are upside-down and the *grinders sleep*.  \nI offer you *poetry in motion*, one cup at a time.  \n\nWith *grounds under my nails* and *hope in my apron*,  \n\u2014[Your Name]"}], "input_metadata": {"row_id": "install-fine-financial-health-state", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct", "temperature": 0.0, "max_tokens": 4096}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "ground_truth": "5", "evaluation_result": {"score": 1.0, "is_score_valid": true, "reason": "\u2705 Found 26 highlighted sections (required: 5)", "metrics": {}}, "created_at": "2025-08-08T23:30:54.738539", "eval_metadata": {"name": "test_markdown_highlighting_evaluation", "description": "\n    Evaluation function that checks if the model's response contains the required number of formatted sections.\n    ", "version": "v0.2.6.dev20+ge1da45a.dirty", "status": "finished", "run_id": "hear-live-common-late-hate", "num_runs": 1, "aggregation_method": "mean", "threshold_of_success": 0.5, "passed": true}, "pid": 52072}, {"messages": [{"role": "user", "content": "Write a casual blog post about similarities across animal species. Highlight at least 5 sections in your answer by starting and ending with \"*\", like: *highlighted text section*."}, {"role": "assistant", "content": "Hey there, fellow creature-curious readers! I just spent the weekend binge-watching nature docs with my cat (who was mostly unimpressed), and I can\u2019t stop noticing how often totally different animals act like they\u2019re reading from the same playbook. So grab a snack and let\u2019s zoom out for a second\u2014here are five \u201cwait, we\u2019re not so different\u201d moments that made me laugh, gasp, and text my group chat at 2 a.m.\n\n*Moms are moms, whether they have feathers, fur, or fins.*  \nFrom emperor penguins passing an egg back and forth like the world\u2019s most stressful game of hot potato, to orangutan mamas teaching their kids which leaves make the best umbrellas, the universal rule seems to be: if you\u2019re a parent, you\u2019re tired. I watched a seahorse dad give birth (yes, the males do that!) and still look like he needed a nap and a latte.\n\n*Play isn\u2019t just for puppies.*  \nSnowboarding crows, surfing dolphins, snow-rolling pandas\u2014turns out \u201cbecause it\u2019s fun\u201d is a legit evolutionary strategy. Scientists think play sharpens coordination and social skills, but honestly, watching a raven sled down a metal roof on a plastic lid just screams \u201cI\u2019m doing it for the TikTok.\u201d\n\n*Grandma knows best\u2014even in elephant form.*  \nElephant herds are led by the oldest female, and her decades of \u201cI\u2019ve seen this before\u201d wisdom decide when to migrate, where to find water, and how to handle that one teenage bull who thinks he\u2019s all that. Swap gray skin for gray hair and you\u2019ve basically got my nana directing Thanksgiving traffic.\n\n*Besties across species lines.*  \nGoby fish and pistol shrimp run the cutest roommate situation: the shrimp digs the burrow, the goby stands guard. If danger shows up, the goby flicks its tail like a silent alarm, and both dive to safety. It\u2019s the underwater version of \u201cI\u2019ll cook, you do the dishes.\u201d\n\n*We all scream for sunscreen\u2014sort of.*  \nHippos secrete \u201cblood sweat\u201d that acts like SPF 50, while giraffes drool black saliva that works like built-in sunglasses. Meanwhile, I\u2019m over here reapplying SPF 30 every 90 minutes and still ending up the color of a lobster. Nature\u2019s got hacks, and I\u2019m taking notes.\n\nSo next time you catch yourself thinking, \u201cI\u2019m nothing like that weird-looking animal,\u201d remember: somewhere out there a capybara is sharing its snack with a bird, and a whale is singing a song that rhymes with last year\u2019s chart-topper. We\u2019re all just variations on a theme\u2014some of us just have more legs or better night vision."}], "input_metadata": {"row_id": "should-poor-parent-war-magazine", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct", "temperature": 0.0, "max_tokens": 4096}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "ground_truth": "5", "evaluation_result": {"score": 1.0, "is_score_valid": true, "reason": "\u2705 Found 5 highlighted sections (required: 5)", "metrics": {}}, "created_at": "2025-08-08T23:30:54.738569", "eval_metadata": {"name": "test_markdown_highlighting_evaluation", "description": "\n    Evaluation function that checks if the model's response contains the required number of formatted sections.\n    ", "version": "v0.2.6.dev20+ge1da45a.dirty", "status": "finished", "run_id": "hear-live-common-late-hate", "num_runs": 1, "aggregation_method": "mean", "threshold_of_success": 0.5, "passed": true}, "pid": 52072}, {"messages": [{"role": "user", "content": "Generate a business proposal to start a sweatshirt company in Bremen. The proposal should contain 5 or more sections. Highlight each section name using the this format:\n*section name*"}, {"role": "assistant", "content": "*Executive Summary*\nBremen is a city of 570,000 residents, 40,000 university students, and a thriving creative scene, yet it lacks a home-grown premium sweatshirt brand that speaks to its maritime heritage, Hanseatic pride, and eco-conscious mindset. \u201cBremer Fleeze\u201d will fill that gap by producing limited-edition, GOTS-certified organic-cotton sweatshirts that combine North-German minimalism with subtle nautical iconography. Our mission: to become Bremen\u2019s first carbon-neutral apparel label and the go-to gift for locals and visitors alike. We will launch with an online-first D2C model, supported by pop-up stores in Viertel and \u00dcberseestadt, and scale to \u20ac1.2 M revenue by Year 3.\n\n*Market Analysis*\nTarget segments: (1) 18-30 students who value sustainability and streetwear aesthetics; (2) 25-45 professionals seeking premium basics; (3) tourists wanting authentic Bremen souvenirs. Competitor scan: global fast-fashion brands (H&M, Zara) lack local identity; German labels (Hugo Boss, Adidas) are either too formal or too sporty; no Bremen-specific sweatshirt exists. Market size: \u20ac45 M annual spend on mid-premium sweatshirts in Bremen & Niedersachsen combined, growing 6 % YoY. Consumer pain points: greenwashing, poor fit, generic designs. Our edge: verifiable sustainability, Bremen storytelling, and inclusive sizing (XXS-4XL).\n\n*Product & Sustainability Strategy*\nCore line: 12 unisex styles in 20 colors, 320 gsm brushed organic cotton, recycled polyester labels, water-based inks. Signature design elements: tonal outline of the Bremer Stadtmusikanten silhouette, latitude-longitude coordinates of Bremen harbor, and a discreet \u201cKohlfahrt-proof\u201d patch. Packaging: reusable tote made from retired ship sails (partnership with local sailmaker). Carbon footprint: 2.1 kg CO\u2082e per sweatshirt vs. 8.9 kg industry average; offset via Gold-Standard wind-farm credits. End-of-life: take-back program with 15 % store credit incentive; returned garments upcycled into insulation for social housing.\n\n*Go-to-Market & Marketing Plan*\nPhase 1 (Months 1-3): Pre-launch teaser on Instagram & TikTok (#BremerFleeze), micro-influencer seeding with 50 Bremen creators, waitlist goal 5,000 sign-ups. Phase 2 (Months 4-6): E-commerce launch on Shopify, same-day bike courier delivery within Bremen city limits, QR code on hangtag linking to AR filter that overlays the Roland statue wearing the sweatshirt. Phase 3 (Months 7-12): Pop-up container store at Marktplatz during Weihnachtsmarkt, collab with Beck\u2019s Brewery for co-branded \u201cBremer Fleeze & Bremer Pils\u201d gift box. KPIs: CAC \u2264 \u20ac18, repeat purchase rate \u2265 35 %, NPS \u2265 70.\n\n*Financial Projections & Funding Needs*\nStart-up capital required: \u20ac180,000. Breakdown: \u20ac60k inventory (2,000 units), \u20ac35k marketing, \u20ac25k e-commerce & IT, \u20ac20k legal & certifications, \u20ac40k working capital. Revenue forecast: Y1 \u20ac220k, Y2 \u20ac550k, Y3 \u20ac1.2 M. Gross margin 62 %, EBITDA margin 18 % by Y3. Break-even month 14. Funding structure: \u20ac80k founder equity, \u20ac50k Bremen-based angel investor (ex-Bremer Kaffee & Co-founder), \u20ac50k zero-interest KfW Green Loan. Exit option: acquisition by sustainable German fashion conglomerate at 3\u00d7 revenue multiple in Year 5."}], "input_metadata": {"row_id": "discover-stop-show-different-water", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct", "temperature": 0.0, "max_tokens": 4096}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "ground_truth": "5", "evaluation_result": {"score": 1.0, "is_score_valid": true, "reason": "\u2705 Found 5 highlighted sections (required: 5)", "metrics": {}}, "created_at": "2025-08-08T23:30:54.738594", "eval_metadata": {"name": "test_markdown_highlighting_evaluation", "description": "\n    Evaluation function that checks if the model's response contains the required number of formatted sections.\n    ", "version": "v0.2.6.dev20+ge1da45a.dirty", "status": "finished", "run_id": "hear-live-common-late-hate", "num_runs": 1, "aggregation_method": "mean", "threshold_of_success": 0.5, "passed": true}, "pid": 52072}, {"messages": [{"role": "user", "content": "Write a funny and sarcastic template for rating the quality of a marriage between two people who are both moms. This is for the couple themselves. Please highlight at least 3 sections with markdown,  i.e *highlighted section*."}, {"role": "assistant", "content": "**The Official \u201cTwo-Mom Marriage Quality Control\u201d Checklist**  \n*Because nothing says romance like a quarterly performance review.*\n\n---\n\n**1. Morning Routine Coordination**  \nRate: \u2610 1 (We need a UN mediator) \u2610 2 (We\u2019ve agreed to disagree\u2026 loudly) \u2610 3 (Only one of us cried into the coffee) \u2610 4 (Synchronized chaos) \u2610 5 (*We finished each other\u2019s sentences\u2026 and the toddler\u2019s homework*)  \n\n*Note: Bonus points if you\u2019ve mastered the \u201cI\u2019ll pack lunches if you find the missing shoe\u201d hostage negotiation.*\n\n---\n\n**2. Division of Emotional Labor**  \nRate: \u2610 1 (\u201cI\u2019m fine.\u201d *\u2014 famous last words*) \u2610 2 (We schedule meltdowns like dentist appointments) \u2610 3 (Therapy? We *are* the therapy) \u2610 4 (We tag-team existential dread while folding tiny socks) \u2610 5 (*We\u2019ve achieved telepathic resentment-free snack negotiations*)  \n\n*Pro tip: If you\u2019ve ever passive-aggressively alphabetized the spice rack while humming \u201cYou\u2019re Welcome\u201d from Moana, circle \u2610 1 twice.*\n\n---\n\n**3. Co-Parenting Synchronization**  \nRate: \u2610 1 (Our kids have unionized) \u2610 2 (We\u2019ve agreed to disagree on screen time\u2026 and bedtime\u2026 and vegetables) \u2610 3 (We bribe with different currencies: one uses stickers, the other uses Wi-Fi passwords) \u2610 4 (We\u2019ve synchronized our \u201cmom voice\u201d to a frequency only dogs and children fear) \u2610 5 (*We\u2019ve weaponized teamwork so effectively the kids think we share a brain*)  \n\n*Highlight this if you\u2019ve ever high-fived over successfully gaslighting a toddler into eating broccoli.*\n\n---\n\n**Final Grade:**  \n\u2610 Certified Disaster (But at least the houseplants are alive)  \n\u2610 Functioning Dysfunction (We\u2019ve agreed the laundry mountain is *decorative*)  \n\u2610 Power Couple (We\u2019ve weaponized Pinterest and the kids still haven\u2019t noticed)  \n\u2610 *Mom-tastic Nirvana* (We finish each other\u2019s\u2026 sandwiches. And trauma. But mostly sandwiches.)\n\n---\n\n**Comments Section (for roasting each other lovingly):**  \n\u201cRemember when you \u2018lost\u2019 the Elf on the Shelf for three days? *Iconic.*\u201d  \n\u201cYour snoring sounds like a haunted dishwasher, but I\u2019d still share my last square of emergency chocolate with you.\u201d"}], "input_metadata": {"row_id": "mean-certain-bitter-sharp-party", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct", "temperature": 0.0, "max_tokens": 4096}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "ground_truth": "3", "evaluation_result": {"score": 1.0, "is_score_valid": true, "reason": "\u2705 Found 18 highlighted sections (required: 3)", "metrics": {}}, "created_at": "2025-08-08T23:30:54.738623", "eval_metadata": {"name": "test_markdown_highlighting_evaluation", "description": "\n    Evaluation function that checks if the model's response contains the required number of formatted sections.\n    ", "version": "v0.2.6.dev20+ge1da45a.dirty", "status": "finished", "run_id": "hear-live-common-late-hate", "num_runs": 1, "aggregation_method": "mean", "threshold_of_success": 0.5, "passed": true}, "pid": 52072}, {"messages": [{"role": "user", "content": "Write a song about the summers of my childhood that I spent in the countryside. Give the song a name, and highlight the name by wrapping it with *. For example: *little me in the countryside*."}, {"role": "assistant", "content": "*little me in the countryside*\n\n[Verse 1]  \nBarefoot on the gravel road, dust between my toes,  \nSunlight dripped like honey on the freckles on my nose.  \nGrandma\u2019s porch swing creaked a lullaby in time,  \nWhile the crickets tuned their violins to the rhythm of the pines.  \n\n[Pre-Chorus]  \nMason jars with lightning bugs, constellations in a jar,  \nWe\u2019d race the dusk to catch the dark before it got too far.  \n\n[Chorus]  \nOh, little me in the countryside, chasing dreams as wide as skies,  \nJumping creeks and climbing trees, believing I could fly.  \nEvery sunset was a promise that tomorrow would be mine,  \nIn the summers of my childhood, where the world was kind.  \n\n[Verse 2]  \nHayloft hideouts, secret forts, straw castles in the loft,  \nWe were pirates, we were astronauts, anything we thought.  \nMama\u2019s voice would echo, \u201cSupper!\u201d across the open fields,  \nAnd the wind would carry laughter like the church bell\u2019s gentle peals.  \n\n[Pre-Chorus]  \nFireflies wrote our names in light, then vanished in the night,  \nBut the glow stayed in our hearts, a soft and steady light.  \n\n[Chorus]  \nOh, little me in the countryside, chasing dreams as wide as skies,  \nJumping creeks and climbing trees, believing I could fly.  \nEvery sunset was a promise that tomorrow would be mine,  \nIn the summers of my childhood, where the world was kind.  \n\n[Bridge]  \nNow the roads are paved, the porch is still, the swing don\u2019t creak no more,  \nBut I can close my eyes and hear the screen door\u2019s gentle roar.  \nTime may steal the years away, but it can\u2019t outrun the past\u2014  \nI keep those golden afternoons like fireflies in a glass.  \n\n[Final Chorus]  \nOh, little me in the countryside, you\u2019re never far behind,  \nYou\u2019re the echo in the wheat fields, the shadow in my mind.  \nThough the world keeps spinning faster, I still find you every time\u2014  \nIn the hush of dusk, in the scent of rain, in the summers of my childhood,  \nWhere the world was kind."}], "input_metadata": {"row_id": "forecast-big-song-issue-service", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct", "temperature": 0.0, "max_tokens": 4096}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "ground_truth": "1", "evaluation_result": {"score": 1.0, "is_score_valid": true, "reason": "\u2705 Found 1 highlighted sections (required: 1)", "metrics": {}}, "created_at": "2025-08-08T23:30:54.738649", "eval_metadata": {"name": "test_markdown_highlighting_evaluation", "description": "\n    Evaluation function that checks if the model's response contains the required number of formatted sections.\n    ", "version": "v0.2.6.dev20+ge1da45a.dirty", "status": "finished", "run_id": "hear-live-common-late-hate", "num_runs": 1, "aggregation_method": "mean", "threshold_of_success": 0.5, "passed": true}, "pid": 52072}, {"messages": [{"role": "user", "content": "Write a riddle for the word \"fa\u00e7ade\" that contains at least 3 italic text phrases in markdown syntax, i.e *italic text*."}, {"role": "assistant", "content": "I stand before you, yet I\u2019m not what I seem;  \na *face I wear* that isn\u2019t mine,  \na *mask of stone* or paint or beam,  \na *pretty lie* that keeps the truth confined.  \n\nWhat am I?"}], "input_metadata": {"row_id": "design-slow-wish-end-fear", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct", "temperature": 0.0, "max_tokens": 4096}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "ground_truth": "3", "evaluation_result": {"score": 1.0, "is_score_valid": true, "reason": "\u2705 Found 3 highlighted sections (required: 3)", "metrics": {}}, "created_at": "2025-08-08T23:30:54.738707", "eval_metadata": {"name": "test_markdown_highlighting_evaluation", "description": "\n    Evaluation function that checks if the model's response contains the required number of formatted sections.\n    ", "version": "v0.2.6.dev20+ge1da45a.dirty", "status": "finished", "run_id": "hear-live-common-late-hate", "num_runs": 1, "aggregation_method": "mean", "threshold_of_success": 0.5, "passed": true}, "pid": 52072}, {"messages": [{"role": "user", "content": "Write a template for a workshop on the importance of diversity in the workplace and highlight at least 3 sections with markdown, i.e. *highlighted section*."}, {"role": "assistant", "content": "# Workshop Template  \n**Title:** *Building Inclusive Excellence: The Business Case for Workplace Diversity*  \n\n---\n\n### 1. Welcome & Objectives (15 min)  \n- **Ice-breaker:** \u201cTwo Truths & a Dream\u201d \u2013 participants share two facts and one aspiration related to identity or culture.  \n- *State the learning outcomes:*  \n  1. Understand how diversity drives innovation and profit.  \n  2. Identify unconscious bias and micro-behaviors.  \n  3. Create an action plan for inclusive leadership.  \n\n---\n\n### 2. *The Business Case for Diversity* (30 min)  \n- **Data Walk:** Posters with McKinsey, Deloitte, and Harvard studies showing \u2191 revenue, \u2193 turnover.  \n- *Interactive poll:* \u201cWhich statistic surprised you most?\u201d (live Mentimeter).  \n- **Case Study:** *How a global tech firm increased patent filings by 19 % after diversifying R&D teams.*  \n\n---\n\n### 3. *Unconscious Bias & Micro-behaviors* (45 min)  \n- **Activity:** \u201cResume Roulette\u201d \u2013 identical CVs with different names; discuss callback disparities.  \n- *Bias Busters Toolkit:* 5-step process to interrupt bias in real time.  \n- **Role-play triads:** Observer, Speaker, Interrupter practicing inclusive language.  \n\n---\n\n### 4. Inclusive Leadership in Action (30 min)  \n- *Panel of ERG leaders* sharing measurable wins (e.g., 40 % \u2191 in promotions for under-represented groups).  \n- **Speed-ideation:** 90-second pitches for inclusive policies; peer voting on top 3.  \n\n---\n\n### 5. Action Planning & Commitments (20 min)  \n- *Personal Inclusion Pledge:* Write one behavior to start, one to stop, one to amplify.  \n- **Accountability Circles:** Pair up for 30-day check-ins.  \n\n---\n\n### 6. Closing & Resources (10 min)  \n- **Resource QR Code:** Links to toolkits, podcasts, and upcoming ERG events.  \n- *Final reflection:* \u201cOne word to describe how you\u2019ll lead differently tomorrow.\u201d"}], "input_metadata": {"row_id": "consider-work-economic-popular-country", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct", "temperature": 0.0, "max_tokens": 4096}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "ground_truth": "3", "evaluation_result": {"score": 1.0, "is_score_valid": true, "reason": "\u2705 Found 19 highlighted sections (required: 3)", "metrics": {}}, "created_at": "2025-08-08T23:30:54.738738", "eval_metadata": {"name": "test_markdown_highlighting_evaluation", "description": "\n    Evaluation function that checks if the model's response contains the required number of formatted sections.\n    ", "version": "v0.2.6.dev20+ge1da45a.dirty", "status": "finished", "run_id": "hear-live-common-late-hate", "num_runs": 1, "aggregation_method": "mean", "threshold_of_success": 0.5, "passed": true}, "pid": 52072}, {"messages": [{"role": "user", "content": "Write a cover letter to a local political party, asking to be their rally organizer. Make sure to highlight at least 3 sections in your answer in markdown format."}, {"role": "assistant", "content": "**[Your Name]**  \n[Your Address]  \n[City, State, ZIP]  \n[Email Address] | [Phone Number] | [Date]\n\n**[Local Party Chair\u2019s Name]**  \n[Local Political Party Name]  \n[Party Headquarters Address]  \n[City, State, ZIP]\n\n---\n\n### **Introduction: Why I\u2019m Writing**\n\nDear **[Chair\u2019s Name]** and the **Executive Committee**,\n\nI am writing to express my enthusiastic interest in serving as **Rally Organizer** for **[Local Party Name]**. As a lifelong resident of **[City/County]** and a committed volunteer who has knocked on over **2,000 doors** and organized **five successful voter-turnout events** in the last two cycles, I understand how to turn passion into turnout and energy into votes. I want to bring that same fire\u2014and a proven playbook\u2014to every rally our party holds between now and Election Day.\n\n---\n\n### **Core Qualifications: What I Bring to the Podium**\n\n- **Logistics Mastery**: From securing permits with the Parks Department to negotiating stage-rental discounts, I have reduced average rally costs by **28 %** while increasing attendance by **45 %** through targeted social-media geofencing and SMS reminders.  \n- **Coalition Building**: I have already built relationships with **15 local unions**, **7 student groups**, and **4 faith communities** who are eager to co-sponsor rallies if given two weeks\u2019 notice.  \n- **Crisis-Ready Leadership**: When an unexpected thunderstorm threatened our 2022 \u201cGet-Out-the-Vote\u201d rally, I pivoted to a **drive-in format** in under four hours\u2014resulting in zero cancellations and 300 additional cars circling the block to hear our speakers.\n\n---\n\n### **Vision for 2024: Turning Rallies into Results**\n\nMy plan is to run a **\u201cRally-to-Ballot Pipeline\u201d**:\n\n1. **Pre-Rally**: Micro-targeted digital invites to low-propensity voters within a 5-mile radius.  \n2. **During Rally**: QR-code check-ins that instantly text attendees their personalized early-vote locations.  \n3. **Post-Rally**: A 72-hour follow-up cadence of thank-you emails, volunteer sign-up links, and ride-share codes to the polls.\n\nI have already drafted a **90-day rally calendar** (attached) that staggers high-energy weekend events with smaller, issue-specific pop-ups near early-vote sites. All I need is the party\u2019s green light and a modest budget line for permits, A/V, and refreshments.\n\n---\n\n### **Closing: Let\u2019s Win This Together**\n\nI would welcome the chance to present this plan in person at your next committee meeting. Thank you for considering my application. Let\u2019s give our volunteers, candidates, and\u2014most importantly\u2014our voters the rallies they deserve.\n\n**Ready to organize, energize, and mobilize,**  \n[Your Signature (if submitting hard copy)]  \n[Your Printed Name]"}], "input_metadata": {"row_id": "predict-serve-messy-difficult-festival", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct", "temperature": 0.0, "max_tokens": 4096}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "ground_truth": "3", "evaluation_result": {"score": 1.0, "is_score_valid": true, "reason": "\u2705 Found 28 highlighted sections (required: 3)", "metrics": {}}, "created_at": "2025-08-08T23:30:54.738766", "eval_metadata": {"name": "test_markdown_highlighting_evaluation", "description": "\n    Evaluation function that checks if the model's response contains the required number of formatted sections.\n    ", "version": "v0.2.6.dev20+ge1da45a.dirty", "status": "finished", "run_id": "hear-live-common-late-hate", "num_runs": 1, "aggregation_method": "mean", "threshold_of_success": 0.5, "passed": true}, "pid": 52072}, {"messages": [{"role": "user", "content": "The opposite of youth is not age, but ...? Highlight at least 2 sections in your answer with markdown, i.e. *highlighted section*."}, {"role": "assistant", "content": "The opposite of youth is not age, but *stagnation*\u2014the moment curiosity closes its fist and wonder stops asking \u201cwhat if?\u201d  \nWhile *age* merely counts revolutions around the sun, *true youth* is the *capacity to keep revising the map of oneself* long after the hair turns silver."}], "input_metadata": {"row_id": "accept-expect-black-young-area", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct", "temperature": 0.0, "max_tokens": 4096}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "ground_truth": "2", "evaluation_result": {"score": 1.0, "is_score_valid": true, "reason": "\u2705 Found 4 highlighted sections (required: 2)", "metrics": {}}, "created_at": "2025-08-08T23:30:54.738795", "eval_metadata": {"name": "test_markdown_highlighting_evaluation", "description": "\n    Evaluation function that checks if the model's response contains the required number of formatted sections.\n    ", "version": "v0.2.6.dev20+ge1da45a.dirty", "status": "finished", "run_id": "hear-live-common-late-hate", "num_runs": 1, "aggregation_method": "mean", "threshold_of_success": 0.5, "passed": true}, "pid": 52072}, {"messages": [{"role": "user", "content": "Write a description of the following data in a weird style: The Golden Palace eatType restaurant; The Golden Palace food Indian; The Golden Palace area city centre. Use markdown to highlight at least 3 sections in your answer."}, {"role": "assistant", "content": "**The Golden Palace** is not a palace at all, but a **restaurant** that has disguised itself as a mirage of saffron and cardamom in the **city centre**. It whispers **Indian** recipes into the ears of pigeons, who then forget how to fly and instead waddle around the square, dizzy with cumin.  \n\nInside, the walls sweat **garam masala** and the tables grow tiny **basil plants** that recite Bollywood dialogues when no one is looking. The waiters glide like **shadow puppets**, balancing trays of **butter chicken** that glow like radioactive sunsets.  \n\nIf you order **naan**, it arrives folded into the shape of a **paper swan**, flapping once before dissolving into steam that smells of **monsoon rain**. The bill is presented on a **lotus leaf**, and if you stare at it long enough, the numbers rearrange themselves into a **limerick about turmeric**."}], "input_metadata": {"row_id": "store-be-past-hard-war", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct", "temperature": 0.0, "max_tokens": 4096}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "ground_truth": "3", "evaluation_result": {"score": 1.0, "is_score_valid": true, "reason": "\u2705 Found 13 highlighted sections (required: 3)", "metrics": {}}, "created_at": "2025-08-08T23:30:54.738826", "eval_metadata": {"name": "test_markdown_highlighting_evaluation", "description": "\n    Evaluation function that checks if the model's response contains the required number of formatted sections.\n    ", "version": "v0.2.6.dev20+ge1da45a.dirty", "status": "finished", "run_id": "hear-live-common-late-hate", "num_runs": 1, "aggregation_method": "mean", "threshold_of_success": 0.5, "passed": true}, "pid": 52072}, {"messages": [{"role": "user", "content": "Write a funny rap about a man who gets a call from an official saying that he is a long lost relative of the king of Nigeria. Use markdown to highlight at least one section of your answer, i.e. *highlighted section*."}, {"role": "assistant", "content": "Yo, it\u2019s MC Broke-Again, fresh outta ramen,  \nphone buzzin\u2019 like a bee in a jar of gin.  \nUnknown number\u2014man, I\u2019m already suspicious,  \nbut the voice got *royal timbre*, straight-up delicious.  \n\n\u201cGreetings, Prince Darnell of House Wi-Fi,  \nI\u2019m the Royal Scribe from Lagos on high.  \nYour great-great-grand-uncle was the King of Nigeria,  \nleft a vault of gold bars and a pet cheetah named Maria.\u201d  \n\nI\u2019m like, \u201cHold up\u2014me? Royalty?  \nLast week I pawned my TV for a two-piece and sweet tea.\u201d  \nHe says, \u201cSign this PDF, wire a small advance fee,  \njust 419 bucks and the throne belongs to thee.\u201d  \n\n**I\u2019m already picking out a crown on Amazon Prime,  \nordering a robe that says \u2018Fresh-to-Death\u2019 in gold slime.**  \n\nI Venmo the cash, add a royal emoji\u2014\ud83d\udc51  \nthen sprint to the mirror, practice my \u201cMy people, be free!\u201d  \nMom walks in, \u201cWhy you wrapped in bedsheets with a Burger King crown?\u201d  \nI\u2019m like, \u201cSilence, peasant! Bow before your prince of renown!\u201d  \n\nTwo days later, no gold, no cheetah, just crickets,  \nmy bank account looking like a ghost town with limericks.  \nI call the number\u2014disconnected, tone dead,  \neven the scammer ghosted me harder than my ex on read.  \n\nSo I\u2019m back in the basement, eating discount sardines,  \nbut I kept the crown\u2014*it\u2019s made of genuine Burger King plastic, supreme*."}], "input_metadata": {"row_id": "exploit-serve-invisible-community-minute", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct", "temperature": 0.0, "max_tokens": 4096}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "ground_truth": "1", "evaluation_result": {"score": 1.0, "is_score_valid": true, "reason": "\u2705 Found 2 highlighted sections (required: 1)", "metrics": {}}, "created_at": "2025-08-08T23:30:54.738850", "eval_metadata": {"name": "test_markdown_highlighting_evaluation", "description": "\n    Evaluation function that checks if the model's response contains the required number of formatted sections.\n    ", "version": "v0.2.6.dev20+ge1da45a.dirty", "status": "finished", "run_id": "hear-live-common-late-hate", "num_runs": 1, "aggregation_method": "mean", "threshold_of_success": 0.5, "passed": true}, "pid": 52072}, {"messages": [{"role": "user", "content": "Expand the riddle into a story with a funny tone:\n\nWhat can you catch but not throw?\nA cold\n\nUse * to highlight at least 2 sections in your text. For example: *this is a highlighted text section*."}, {"role": "assistant", "content": "Gary the Germ was the *smallest, greenest, and most obnoxiously proud* microbe in the entire Sniffle Kingdom. Every morning he practiced his grand entrance: sliding down banisters made of nasal hair, cannon-balling into unsuspecting tonsils, and yelling, \u201cACHOO\u2014did somebody order a cold?\u201d  \n\nOne Tuesday, Gary spotted his dream target: *Bartholomew \u201cBart\u201d Bigglesworth, a man who believed hand-washing was a government conspiracy*. Bart had just exited the subway, triumphantly licking a subway pole \u201cfor the vitamins.\u201d Gary rubbed his microscopic hands together. \u201cPerfect. I\u2019ll hitch a ride on that finger, ride the express elevator to the sinuses, and throw the *snottiest* house-warming party in history!\u201d  \n\nBut there was a catch\u2014literally. Gary could *catch* Bart, but he couldn\u2019t *throw* him anywhere. Every time Gary tried to catapult Bart into the next office cubicle, Bart just sneezed, wiped his nose on his sleeve, and kept typing spreadsheets. Gary screamed, \u201cThis is mutiny! I\u2019m supposed to be the one doing the tossing!\u201d  \n\nMeanwhile, Bart\u2019s immune system\u2014led by Captain Phlegm and the Mucus Marines\u2014launched *Operation Tissue Storm*. They surrounded Gary, armed with vitamin C grenades and zinc zappers. Gary attempted a dramatic escape by surfing a sneeze, but the tissue slapped him like a flyswatter on a summer day.  \n\nDefeated, Gary sat in the crumpled tissue, sulking. \u201cI caught the cold,\u201d he muttered, \u201cbut I can\u2019t throw it anywhere. Worst. Superpower. Ever.\u201d  \n\nFrom that day forward, Gary changed his business card to read: \u201cProfessional Cold Catcher\u2014No Returns, No Refunds, Absolutely No Throwing.\u201d"}], "input_metadata": {"row_id": "suggest-american-open-level-country", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct", "temperature": 0.0, "max_tokens": 4096}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "ground_truth": "2", "evaluation_result": {"score": 1.0, "is_score_valid": true, "reason": "\u2705 Found 6 highlighted sections (required: 2)", "metrics": {}}, "created_at": "2025-08-08T23:30:54.738881", "eval_metadata": {"name": "test_markdown_highlighting_evaluation", "description": "\n    Evaluation function that checks if the model's response contains the required number of formatted sections.\n    ", "version": "v0.2.6.dev20+ge1da45a.dirty", "status": "finished", "run_id": "hear-live-common-late-hate", "num_runs": 1, "aggregation_method": "mean", "threshold_of_success": 0.5, "passed": true}, "pid": 52072}, {"messages": [{"role": "user", "content": "Write a rap about the renaissance. It should be noticeably different from raps about other historical eras, and have an interesting or weird tone. Highlight at least 3 sections in your answer in markdown format."}, {"role": "assistant", "content": "**[Intro \u2013 Lute Drop]**  \n*beat drops like a sack of wet marble*  \nYo, it\u2019s 14-something, smell that? That\u2019s perspective in the air,  \nGutenberg just ghost-wrote the mixtape, pages everywhere.  \nI\u2019m a Medici money-mage, patron of the bizarre,  \nCommissioning nightmares in egg-tempera.  \n\n---\n\n### **Verse 1 \u2013 The Alchemist\u2019s Cypher**  \nI paint with mercury, breathe with a lung of lead,  \nMy selfie\u2019s a death-mask, gold leaf on the dead.  \nBrunelleschi built a dome like a half-buried skull,  \nYou think it\u2019s architecture? Nah, it\u2019s a ritual.  \n**[weird baroque wheeze]**  \nI rhyme in vanishing points, every line converges  \nOn a single terrified eye that never emerges.  \nPerspective\u2019s a ouija board, da Vinci holds the planchette,  \nSketching choppers (helicopters) centuries before the jet-set.  \n\n---\n\n### **Hook \u2013 Chant of the Anamorphic Choir**  \n*distorted children\u2019s voices layered over sackbut*  \nRenaissance, renaissance, put a cherub on it,  \nBaby wings, baby fangs, marble veins, vomit.  \nWe don\u2019t bling, we gild, we don\u2019t rap, we fresco\u2014  \nYour flow\u2019s two-dimensions, mine\u2019s *trompe-l\u2019oeil* techno.  \n\n---\n\n### **Verse 2 \u2013 The Court Jester\u2019s Acid Madrigal**  \n**[sudden switch to a harpsichord trap beat]**  \nI juggle plague rats for the Duke of Milan,  \nHe pays me in sonnets and unmarked arsenic flan.  \nIsabella d\u2019Este texts me in mirror-script,  \nHer emoji: a decapitated hydra, lips pursed, licked.  \nI\u2019m the fool with a scalpel, dissecting songbirds to find  \nWhich organ chirps\u2014turns out it\u2019s the **pineal gland** of the mind.  \nMichelangelo\u2019s in the corner, carving his own ribcage,  \nSays \u201cThe Sistine\u2019s just a throat, God\u2019s a uvula on stage.\u201d  \n\n---\n\n### **Bridge \u2013 The Astrolabe Malfunction**  \n*beat glitches like a scratched illuminated manuscript*  \nCopernicus DM\u2019d me: \u201cThe sun\u2019s a strobe light,  \nPlanets orbit like moths around a candle that bites.\u201d  \nI reply with a lute solo reversed through a cathedral,  \nNotes fall upward, monks levitate, it\u2019s procedural.  \n**[static of Latin hex]**  \nGalileo\u2019s telescope sees the future: TikTok, guillotine,  \nHe mutters \u201c*E pur si muove*\u201d then dabs in quarantine.  \n\n---\n\n### **Outro \u2013 The Anachronistic Masque**  \n**[all instruments devolve into wet clay sounds]**  \nSo remember me when you\u2019re double-tapping a Botticelli,  \nI\u2019m the glitch in the background, the snail with a human belly.  \nRenaissance never ended, it just learned to hide,  \nUnder your touchscreen, oil-paint bleeding through the side.  \nNow go\u2014vanish in a puff of ultramarine dust,  \nThe past is a performance, and the future\u2019s already trussed."}], "input_metadata": {"row_id": "respond-connect-true-problem-plan", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct", "temperature": 0.0, "max_tokens": 4096}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "ground_truth": "3", "evaluation_result": {"score": 1.0, "is_score_valid": true, "reason": "\u2705 Found 16 highlighted sections (required: 3)", "metrics": {}}, "created_at": "2025-08-08T23:30:54.738906", "eval_metadata": {"name": "test_markdown_highlighting_evaluation", "description": "\n    Evaluation function that checks if the model's response contains the required number of formatted sections.\n    ", "version": "v0.2.6.dev20+ge1da45a.dirty", "status": "finished", "run_id": "hear-live-common-late-hate", "num_runs": 1, "aggregation_method": "mean", "threshold_of_success": 0.5, "passed": true}, "pid": 52072}, {"messages": [{"role": "user", "content": "Summarize the history of Japan. Italicize at least 5 keywords in your response. To indicate a italic word, wrap it with asterisk, like *italic*"}, {"role": "assistant", "content": "*J\u014dmon* hunter-gatherers (c. 14,000 BCE) gave way to *Yayoi* rice cultivators who introduced metal tools and social stratification.  \nThe *Yamato* court unified much of the archipelago by the 5th century CE, absorbing *Shinto* beliefs and Chinese writing, law, and Buddhism.  \nA *Heian* golden age (794\u20131185) produced literature like *The Tale of Genji* while the imperial court\u2019s real power waned.  \nSamurai clans rose; the *Kamakura* shogunate (1185\u20131333) created Japan\u2019s first warrior government, repelling Mongol invasions.  \nCenturies of civil war ended when *Oda Nobunaga*, *Toyotomi Hideyoshi*, and *Tokugawa Ieyasu* reunified the land, leading to the *Edo* shogunate (1603\u20131868).  \nIsolation under *sakoku* preserved domestic culture until *Commodore Perry*\u2019s 1853 arrival forced unequal treaties.  \nThe *Meiji* Restoration (1868) abolished feudalism, industrialized rapidly, and built an empire that won wars against China and Russia.  \nImperial expansion\u2014Korea, Manchuria, and *Pearl Harbor*\u2014ended in atomic bombings and occupation by Allied forces (1945\u20131952).  \nPost-war *miracle* growth made Japan an economic superpower, followed by stagnation since the 1990s yet enduring global cultural influence."}], "input_metadata": {"row_id": "must-watch-bad-hate-magazine", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct", "temperature": 0.0, "max_tokens": 4096}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "ground_truth": "5", "evaluation_result": {"score": 1.0, "is_score_valid": true, "reason": "\u2705 Found 16 highlighted sections (required: 5)", "metrics": {}}, "created_at": "2025-08-08T23:30:54.738931", "eval_metadata": {"name": "test_markdown_highlighting_evaluation", "description": "\n    Evaluation function that checks if the model's response contains the required number of formatted sections.\n    ", "version": "v0.2.6.dev20+ge1da45a.dirty", "status": "finished", "run_id": "hear-live-common-late-hate", "num_runs": 1, "aggregation_method": "mean", "threshold_of_success": 0.5, "passed": true}, "pid": 52072}, {"messages": [{"role": "user", "content": "Write a funny Haiku about a Quaker named John who lives in the town of De Smet, South Dakota. Use the asterisk symbol, *, to highlight some words or phrases twice. Example: *This is a highlighted phrase*."}, {"role": "assistant", "content": "John in *De Smet*  \n*Quaker* silence\u2014then he yells  \n\u201cWhere\u2019s my *oatmeal*, Friend?\u201d"}], "input_metadata": {"row_id": "take-love-start-short-work", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct", "temperature": 0.0, "max_tokens": 4096}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "ground_truth": "2", "evaluation_result": {"score": 1.0, "is_score_valid": true, "reason": "\u2705 Found 3 highlighted sections (required: 2)", "metrics": {}}, "created_at": "2025-08-08T23:30:54.738958", "eval_metadata": {"name": "test_markdown_highlighting_evaluation", "description": "\n    Evaluation function that checks if the model's response contains the required number of formatted sections.\n    ", "version": "v0.2.6.dev20+ge1da45a.dirty", "status": "finished", "run_id": "hear-live-common-late-hate", "num_runs": 1, "aggregation_method": "mean", "threshold_of_success": 0.5, "passed": true}, "pid": 52072}, {"messages": [{"role": "user", "content": "Today, at the 54th Annual Grammy Awards, the Recording Academy honors the talent and creativity of the artists, musicians, and producers who are the creators of the best recordings of the past year. Please continue writing this text in a formal tone, using notations. Highlight some key parts in your response with \"*\", like *highlighted text*."}, {"role": "assistant", "content": "Tonight, the 54th Annual Grammy Awards [1] stand as a testament to the enduring power of recorded music. Under the vaulted ceiling of the *Staples Center*, the Recording Academy gathers to bestow its highest accolades upon those whose artistry has defined the sonic landscape of eligibility year 2011 [2].  \n\nThe evening\u2019s proceedings are governed by the *strictures of the Grammy Charter* [3], ensuring that every statuette awarded is the result of peer-review balloting among the Academy\u2019s 12,000 voting members [4]. In accordance with Rule 3.01, only recordings commercially released in the United States between 1 October 2010 and 30 September 2011 are eligible [5].  \n\nAmong the most anticipated categories is *Album of the Year* [6], whose nominees\u2014Adele\u2019s *21*, Foo Fighters\u2019 *Wasting Light*, Lady Gaga\u2019s *Born This Way*, Bruno Mars\u2019 *Doo-Wops & Hooligans*, and Rihanna\u2019s *Loud*\u2014represent a cross-section of genre, generation, and geography [7]. The *Record of the Year* [8] shortlist similarly spans continents and styles, underscoring the Academy\u2019s commitment to excellence irrespective of commercial metrics.  \n\nThe *Producer of the Year, Non-Classical* [9] category spotlights the architects of sound whose meticulous craft transforms raw performance into timeless recording. Paul Epworth, whose work on *21* has garnered both critical and commercial acclaim, is widely regarded as the frontrunner [10].  \n\nAs the telecast proceeds, the *In Memoriam* segment [11] will honor those luminaries whose voices were stilled during the eligibility year, their legacies enshrined in the grooves and bytes of the recordings they leave behind.  \n\nThe 54th Annual Grammy Awards thus serve not merely as a celebration, but as a *codification of excellence* [12], ensuring that the artistry of 2011 resonates through the decades to come."}], "input_metadata": {"row_id": "delete-democratic-hot-new-work", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct", "temperature": 0.0, "max_tokens": 4096}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "ground_truth": "1", "evaluation_result": {"score": 1.0, "is_score_valid": true, "reason": "\u2705 Found 13 highlighted sections (required: 1)", "metrics": {}}, "created_at": "2025-08-08T23:30:54.738984", "eval_metadata": {"name": "test_markdown_highlighting_evaluation", "description": "\n    Evaluation function that checks if the model's response contains the required number of formatted sections.\n    ", "version": "v0.2.6.dev20+ge1da45a.dirty", "status": "finished", "run_id": "hear-live-common-late-hate", "num_runs": 1, "aggregation_method": "mean", "threshold_of_success": 0.5, "passed": true}, "pid": 52072}, {"messages": [{"role": "user", "content": "Write a blog post about interesting facts about the Dutch language. Italicize at least 2 sections in your answer with markdown, i.e. *italic text*."}, {"role": "assistant", "content": "# 7 Surprising Facts About Dutch That Will Make You Say \u201cWauw!\u201d\n\nWhen most people think of Dutch, they picture windmills, tulips, and maybe a stroopwafel or two. But the language behind those images is far stranger\u2014and more fascinating\u2014than you might expect. Below are seven facts that prove Dutch is anything but *just English with extra g\u2019s*.\n\n---\n\n### 1. The Longest Word Is 53 Letters\u2014and It\u2019s Official  \nThe Dutch *Encyclopedie* recognizes *aansprakelijkheidswaardevaststellingsveranderingen* (53 letters) as a legitimate word. It translates to \u201cchanges in liability value assessments,\u201d which is ironic because the word itself is a liability to pronounce. Try saying that three times fast\u2014or even once slowly.\n\n---\n\n### 2. Dutch Has a Word for *Everything*\u2014Including \u201cSchadenfreude\u201d but Cozier  \nEnglish borrows *Schadenfreude* from German, but Dutch gives us *gezellig*, a term that packs \u201ccozy,\u201d \u201cfun,\u201d and \u201cbelonging\u201d into one syllable. A caf\u00e9 can be *gezellig*, a party can be *gezellig*, and even a quiet evening with friends can be *gezellig*. There\u2019s no direct English equivalent, which is why expats end up using it in English sentences: \u201cThat dinner was so\u2026 *gezellig*.\u201d\n\n---\n\n### 3. The Dutch Alphabet Ends with \u201cX, Y, IJ, Z\u201d  \nMost alphabets end with \u201cX, Y, Z,\u201d but Dutch sneaks in an extra character: *IJ*. It\u2019s considered a single letter (yes, two letters acting as one), and it even has its own capitalized form: *IJsselmeer*, the famous lake. Type designers have to create a special ligature for it, and crossword puzzles treat *IJ* as one square. Imagine Scrabble tiles clinking together as someone proudly lays down \u201cIJSBERG\u201d for 72 points.\n\n---\n\n### 4. Dutch Greetings Change by Time of Day\u2014But Not How You\u2019d Expect  \nIn English, \u201cgood morning\u201d fades into \u201cgood afternoon\u201d around noon. Dutch, however, switches from *goedemorgen* to *goedemiddag* at exactly 12:00 p.m.\u2014unless you\u2019re in Belgium, where some regions wait until 1:00 p.m. Miss the cutoff and you\u2019ll get a polite correction: \u201cHet is al middag, hoor!\u201d (\u201cIt\u2019s already afternoon, you know!\u201d).\n\n---\n\n### 5. Dutch Is the Closest Major Language to English\u2014But That Doesn\u2019t Mean It\u2019s Easy  \nLinguists classify Dutch and English as *West Germanic siblings*, which means basic vocabulary overlaps: *appel* (apple), *boek* (book), *water* (water). Yet false friends lurk everywhere. *Durf jij?* doesn\u2019t mean \u201cDo you dare?\u201d\u2014it means \u201cDo you dare?\u201d\u2026 but with a cultural undertone of *prove it*. And *be careful* with *eventueel*, which means \u201cpossibly,\u201d not \u201ceventually.\u201d\n\n---\n\n### 6. Dutch Has Three Genders\u2014But Only Two Articles  \nGrammatically, Dutch retains masculine, feminine, and neuter genders. In practice, however, only *de* (common gender) and *het* (neuter) are used. The result? Native speakers themselves debate whether *auto* (car) is masculine or feminine. The safest route is to just say *de auto* and avoid the awkward silence that follows gendered guesswork.\n\n---\n\n### 7. Dutch Invented the Word \u201cCookie\u201d\u2014and the World Never Looked Back  \nThe English word \u201ccookie\u201d comes from the Dutch *koekje* (\u201clittle cake\u201d). Dutch settlers brought *koekjes* to New Amsterdam (now New York), and the rest is snack history. Next time you dunk a chocolate chip, thank the Dutch for their linguistic\u2014and culinary\u2014contribution.\n\n---\n\n### Bonus: Dutch Tongue Twisters Are Next-Level  \nTry this: *De kat krabt de krullen van de trap*. Translation: \u201cThe cat scratches the curls off the stairs.\u201d Say it five times without twisting your tongue into a *stroopwafel*.\n\n---\n\nWhether you\u2019re planning a trip to Amsterdam or just want to impress friends at trivia night, Dutch offers a treasure trove of quirks. From *gezellig* evenings to 53-letter bureaucratic monsters, the language proves that small countries can produce *enormous linguistic personality*."}], "input_metadata": {"row_id": "live-important-military-only-love", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct", "temperature": 0.0, "max_tokens": 4096}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "ground_truth": "2", "evaluation_result": {"score": 1.0, "is_score_valid": true, "reason": "\u2705 Found 33 highlighted sections (required: 2)", "metrics": {}}, "created_at": "2025-08-08T23:30:54.739013", "eval_metadata": {"name": "test_markdown_highlighting_evaluation", "description": "\n    Evaluation function that checks if the model's response contains the required number of formatted sections.\n    ", "version": "v0.2.6.dev20+ge1da45a.dirty", "status": "finished", "run_id": "hear-live-common-late-hate", "num_runs": 1, "aggregation_method": "mean", "threshold_of_success": 0.5, "passed": true}, "pid": 52072}, {"messages": [{"role": "user", "content": "Gideon is a farmer who has a surplus of crops from his farm this year. What might he do with that surplus? Highlight at least one section of your answer in markdown, i.e *highlighted section*."}, {"role": "assistant", "content": "Gideon has several good options for his surplus:\n\n1. **Sell it at the local farmers\u2019 market or to a regional wholesaler**, turning the extra produce into cash that he can use for next season\u2019s seeds, equipment repairs, or household expenses.  \n2. *Donate a portion to the nearby food bank or community kitchen*, helping families who struggle with food insecurity while also earning a small tax deduction.  \n3. Preserve some of the surplus\u2014by canning tomatoes, drying herbs, or storing root vegetables in a cool cellar\u2014so his own family has nutritious food throughout the winter.  \n4. Trade with neighboring farmers: swap excess corn for someone else\u2019s surplus eggs or honey, strengthening community ties and diversifying his pantry without spending money."}], "input_metadata": {"row_id": "think-indicate-write-local-company", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct", "temperature": 0.0, "max_tokens": 4096}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "ground_truth": "1", "evaluation_result": {"score": 1.0, "is_score_valid": true, "reason": "\u2705 Found 2 highlighted sections (required: 1)", "metrics": {}}, "created_at": "2025-08-08T23:30:54.739037", "eval_metadata": {"name": "test_markdown_highlighting_evaluation", "description": "\n    Evaluation function that checks if the model's response contains the required number of formatted sections.\n    ", "version": "v0.2.6.dev20+ge1da45a.dirty", "status": "finished", "run_id": "hear-live-common-late-hate", "num_runs": 1, "aggregation_method": "mean", "threshold_of_success": 0.5, "passed": true}, "pid": 52072}, {"messages": [{"role": "system", "content": "You are a helpful assistant that can answer questions about Fireworks.\nALWAYS provide code or commands to execute to answer the question."}, {"role": "user", "content": "Can you teach me about how to manage deployments on Fireworks"}], "input_metadata": {"row_id": "expect-upload-close-president-goal", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct"}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "created_at": "2025-08-08T23:34:12.823862", "eval_metadata": {"name": "test_pytest_mcp_url", "description": "Run math evaluation on sample dataset using pytest interface.", "version": "v0.2.6.dev20+ge1da45a.dirty", "status": "stopped", "run_id": "meet-treat-central-possible-lot", "num_runs": 1, "aggregation_method": "mean"}, "pid": 52715}, {"messages": [{"role": "system", "content": "You are a helpful assistant that can answer questions about Fireworks.\nALWAYS provide code or commands to execute to answer the question."}, {"role": "user", "content": "Can you teach me about how to manage deployments on Fireworks"}, {"role": "assistant", "content": "I'll help you learn about managing deployments on Fireworks. Let me search for comprehensive information about deployment management.", "tool_calls": [{"id": "functions.search:0", "function": {"arguments": "{\"query\": \"deployment management Fireworks\"}", "name": "search"}, "type": "function"}]}, {"role": "tool", "content": [{"text": "Title: On-demand deployments\nLink: https://docs.fireworks.ai/guides/ondemand-deployments\nContent: If you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.\n\nIf you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.\n\nOn-demand deployments\nIf you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.\n\n", "type": "text"}, {"text": "Title: Does Fireworks support custom base models?\nLink: https://docs.fireworks.ai/faq-new/models-inference/does-fireworks-support-custom-base-models\nContent: Yes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.\n\nYes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.\n\nDoes Fireworks support custom base models?\nYes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.\n\n", "type": "text"}, {"text": "Title: Introducing the Fireworks Build SDK\nLink: https://docs.fireworks.ai/tools-sdks/python-client/sdk-introduction\nContent: Introducing the Fireworks Build SDK\nThe Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.\n\nIntroducing the Fireworks Build SDK\nThe Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.\n\nThe Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.\n\n", "type": "text"}, {"text": "Title: I have multiple Fireworks accounts. When I try to login with Google on Fireworks' web UI, I'm getting signed into the wrong account. How do I fix this?\nLink: https://docs.fireworks.ai/faq-new/account-access/i-have-multiple-fireworks-accounts-when-i-try-to-login-with-google-on-fireworks\nContent: If you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.\n\nIf you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.\n\nI have multiple Fireworks accounts. When I try to login with Google on Fireworks' web UI, I'm getting signed into the wrong account. How do I fix this?\nIf you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.\n\n", "type": "text"}, {"text": "Title: Querying existing dedicated deployments\nLink: https://docs.fireworks.ai/tools-sdks/python-client/querying-dedicated-deployments\nContent: When you have dedicated deployments that were created via firectl or the Fireworks web UI, you can easily connect to them using the Build SDK to run inference. This is particularly useful when you want to leverage existing infrastructure or when deployments are managed by different teams.\n\nWhen you have dedicated deployments that were created via firectl or the Fireworks web UI, you can easily connect to them using the Build SDK to run inference. This is particularly useful when you want to leverage existing infrastructure or when deployments are managed by different teams.\n\nDeployment ID requirements\nThe id parameter should match exactly with your existing deployment: Use the deployment name/ID as shown in the Fireworks dashboard The ID is case-sensitive and must match exactly If the deployment doesn't exist, you'll receive an error when making requests\n\n", "type": "text"}, {"text": "Title: Migration and traffic management\nLink: https://docs.fireworks.ai/guides/prompt-caching\nContent: When migrating between deployments that use prompt caching, it's crucial to implement proper traffic routing to maintain optimal cache hit rates. When gradually routing traffic to a new deployment, use consistent user/session-based sampling rather than random sampling. Here's the recommended implementation for traffic routing: import hashlib\n\n# Configure traffic fraction (e.g., 20% to new deployment)\nfireworks_traffic_fraction = 0.2\nuser_id = \"session-id-123\"\n\n# Generate deterministic hash from user_id\nhashed_user_id = int(hashlib.md5(user_id.encode()).hexdigest(), 16) # MD5 hash on user-id and convert to integer\nMAX_HASH = 2**128 - 1  # MD5 hash maximum value\n\n# Compute ratio for consistent routing\nratio = hashed_user_id / MAX_HASH # Returns 0.0 to 1.0\n\nif (ratio < fireworks_traffic_fraction):\n    send_to_new_deployment(user=hashed_user_id)  # Pass user ID for caching\nelse:\n    send_elsewhere()  # Route to old deployment or serverless\n\nWhen migrating between deployments that use prompt caching, it's crucial to implement proper traffic routing to maintain optimal cache hit rates. When gradually routing traffic to a new deployment, use consistent user/session-based sampling rather than random sampling. Here's the recommended implementation for traffic routing: import hashlib\n\n# Configure traffic fraction (e.g., 20% to new deployment)\nfireworks_traffic_fraction = 0.2\nuser_id = \"session-id-123\"\n\n# Generate deterministic hash from user_id\nhashed_user_id = int(hashlib.md5(user_id.encode()).hexdigest(), 16) # MD5 hash on user-id and convert to integer\nMAX_HASH = 2**128 - 1  # MD5 hash maximum value\n\n# Compute ratio for consistent routing\nratio = hashed_user_id / MAX_HASH # Returns 0.0 to 1.0\n\nif (ratio < fireworks_traffic_fraction):\n    send_to_new_deployment(user=hashed_user_id)  # Pass user ID for caching\nelse:\n    send_elsewhere()  # Route to old deployment or serverless\n\nAdvanced: cache locality for Enterprise deployments\nDedicated deployments on an Enterprise plan allow you to pass an additional hint in the request to improve cache hit rates. First, the deployment needs to be created or updated with an additional flag: Then the client can pass an opaque identifier representing a single user or\nsession in the user field of the body or in the x-session-affinity header. Fireworks\nwill try to route requests with the identifier to the same server, further reducing response times. It's best to choose an identifier that groups requests with long shared prompt\nprefixes. For example, it can be a chat session with the same user or an\nassistant working with the same shared context.\n\n", "type": "text"}, {"text": "Title: Do you host your deployments in the EU or Asia?\nLink: https://docs.fireworks.ai/faq-new/support-general/do-you-host-your-deployments-in-the-eu-or-asia\nContent: Do you host your deployments in the EU or Asia?\nWe are currently deployed in multiple U.S.-based locations. However, we\u2019re open to hearing more about your specific requirements. You can: Join our Discord community Write to us at inquiries@fireworks.ai If you're an Enterprise customer, please contact your dedicated customer support representative to ensure a timely response.\n\nDo you host your deployments in the EU or Asia?\nWe are currently deployed in multiple U.S.-based locations. However, we\u2019re open to hearing more about your specific requirements. You can: Join our Discord community Write to us at inquiries@fireworks.ai If you're an Enterprise customer, please contact your dedicated customer support representative to ensure a timely response.\n\nWe are currently deployed in multiple U.S.-based locations. However, we\u2019re open to hearing more about your specific requirements. You can: Join our Discord community Write to us at inquiries@fireworks.ai If you're an Enterprise customer, please contact your dedicated customer support representative to ensure a timely response.\n\n", "type": "text"}, {"text": "Title: Deployments and deployment types\nLink: https://docs.fireworks.ai/getting-started/concepts\nContent: A model must be deployed before it can be used for inference. A deployment is a collection (one or more) model servers that host one base model and optionally one or more LoRA addons. Fireworks supports two types of deployments: Serverless deployments: Fireworks hosts popular base models on shared \"serverless\" deployments. Users pay-per-token to query these models and do not need to configure GPUs. The most popular serverless deployments also support serverless LoRA addons. See the Deploying to serverless guide for details. Dedicated deployments: Dedicated deployments enable users to configure private deployments with a wide array of hardware (see on-demand deployments guide ). Dedicated deployments give users performance guarantees and the most flexibility and control over what models can be deployed. Both LoRA addons and base models can be deployed to dedicated deployments. Dedicated deployments are billed by a GPU-second basis (see pricing page). See the Querying text models guide for a comprehensive overview of making LLM inference.\n\nA model must be deployed before it can be used for inference. A deployment is a collection (one or more) model servers that host one base model and optionally one or more LoRA addons. Fireworks supports two types of deployments: Serverless deployments: Fireworks hosts popular base models on shared \"serverless\" deployments. Users pay-per-token to query these models and do not need to configure GPUs. The most popular serverless deployments also support serverless LoRA addons. See the Deploying to serverless guide for details. Dedicated deployments: Dedicated deployments enable users to configure private deployments with a wide array of hardware (see on-demand deployments guide ). Dedicated deployments give users performance guarantees and the most flexibility and control over what models can be deployed. Both LoRA addons and base models can be deployed to dedicated deployments. Dedicated deployments are billed by a GPU-second basis (see pricing page). See the Querying text models guide for a comprehensive overview of making LLM inference.\n\nControl plane and data plane\nThe Fireworks API can be split into a control plane and a data plane. The control plane consists of APIs used for managing the lifecycle of resources. This\nincludes your account, models, and deployments. The data plane consists of the APIs used for inference and the backend services that power\nthem.\n\n", "type": "text"}, {"text": "Title: Deployment Process\nLink: https://docs.fireworks.ai/evaluators/api_reference/reward_function_decorator\nContent: When .deploy() is called, the decorator: Extracts the function's source code Creates a wrapper that handles the Fireworks evaluation format Creates a temporary directory with the wrapped function Uploads and registers the function with the Fireworks API\n\nWhen .deploy() is called, the decorator: Extracts the function's source code Creates a wrapper that handles the Fireworks evaluation format Creates a temporary directory with the wrapped function Uploads and registers the function with the Fireworks API\n\n.deploy()\nThe decorator adds a .deploy() method to the function, allowing it to be deployed to Fireworks. evaluation_id = my_reward_function.deploy(\n    name=\"my-evaluator\",\n    description=\"Evaluates responses based on clarity and accuracy\",\n    account_id=None,  # Optional, defaults to configured account\n    auth_token=None,  # Optional, defaults to configured token\n    force=False,  # Set to True to overwrite if it already exists\n    providers=None  # Optional model providers configuration\n)\n\n", "type": "text"}, {"text": "Title: \ud83d\udd27 Build SDK: Deployment Configuration Application Requirement\nLink: https://docs.fireworks.ai/updates/changelog\nContent: The Build SDK now requires you to call .apply() to apply any deployment configurations to Fireworks when using deployment_type=\"on-demand\" or deployment_type=\"on-demand-lora\" . This change ensures explicit control over when deployments are created and helps prevent accidental deployment creation. Key changes: .apply() is now required for on-demand and on-demand-lora deployments Serverless deployments do not require .apply() calls If you do not call .apply() , you are expected to set up the deployment through the deployment page at https://app.fireworks.ai/dashboard/deployments Migration guide: Add llm.apply() after creating LLM instances with deployment_type=\"on-demand\" or deployment_type=\"on-demand-lora\" No changes needed for serverless deployments See updated documentation for examples and best practices This change improves deployment management and provides better control over resource creation. This applies to Python SDK version >=0.19.14.\n\nThe Build SDK now requires you to call .apply() to apply any deployment configurations to Fireworks when using deployment_type=\"on-demand\" or deployment_type=\"on-demand-lora\" . This change ensures explicit control over when deployments are created and helps prevent accidental deployment creation. Key changes: .apply() is now required for on-demand and on-demand-lora deployments Serverless deployments do not require .apply() calls If you do not call .apply() , you are expected to set up the deployment through the deployment page at https://app.fireworks.ai/dashboard/deployments Migration guide: Add llm.apply() after creating LLM instances with deployment_type=\"on-demand\" or deployment_type=\"on-demand-lora\" No changes needed for serverless deployments See updated documentation for examples and best practices This change improves deployment management and provides better control over resource creation. This applies to Python SDK version >=0.19.14.\n\n\ud83d\ude80 Easier & faster LoRA fine-tune deployments on Fireworks\nYou can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: Previously, this involved two distinct steps, and the resulting deployment was slower than the base model: Create a deployment using firectl create deployment \"accounts/fireworks/models/<MODEL_ID of base model>\" --enable-addons Then deploy the addon to the deployment: firectl load-lora <MODEL_ID> --deployment <DEPLOYMENT_ID> For more information, see our deployment documentation. This change is for dedicated deployments with a single LoRA. You can still deploy multiple LoRAs on a deployment or deploy LoRA(s) on some Serverless models as described in the documentation.\n\n", "type": "text"}], "tool_call_id": "functions.search:0"}, {"role": "assistant", "content": "Let me search for more specific information about deployment management tools and processes.", "tool_calls": [{"id": "functions.search:1", "function": {"arguments": "{\"query\": \"firectl deployment management commands\"}", "name": "search"}, "type": "function"}]}, {"role": "tool", "content": [{"text": "Title: Why am I getting \"invalid id\" errors when using firectl commands like create deployment or list deployments?\nLink: https://docs.fireworks.ai/faq-new/fine-tuning/why-am-i-getting-invalid-id-errors-when-using-firectl-commands-like-create-deplo\nContent: This error typically occurs when your account ID is not properly configured.\n\nThis error typically occurs when your account ID is not properly configured.\n\nWhy am I getting \"invalid id\" errors when using firectl commands like create deployment or list deployments?\nThis error typically occurs when your account ID is not properly configured.\n\n", "type": "text"}, {"text": "Title: Managing default deployments\nLink: https://docs.fireworks.ai/guides/ondemand-deployments\nContent: Since a model may be deployed to multiple deployments, querying by model name will route to the \"default\" deployed model. You can see which deployed model entry is marked with Default: true using: firectl get model <MODEL_ID>\n\nSince a model may be deployed to multiple deployments, querying by model name will route to the \"default\" deployed model. You can see which deployed model entry is marked with Default: true using: firectl get model <MODEL_ID>\n\nSince a model may be deployed to multiple deployments, querying by model name will route to the \"default\" deployed model. You can see which deployed model entry is marked with Default: true using: This will show the Deployed Model Refs section with the Default: true entry. To update the default deployed model, note the Name of the deployed model reference above. Then run: firectl update deployed-model <DEPLOYED_MODEL_ID_2> --default\n\n", "type": "text"}, {"text": "Title: Does Fireworks support custom base models?\nLink: https://docs.fireworks.ai/faq-new/models-inference/does-fireworks-support-custom-base-models\nContent: Yes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.\n\nYes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.\n\nDoes Fireworks support custom base models?\nYes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.\n\n", "type": "text"}, {"text": "Title: \ud83d\ude80 Easier & faster LoRA fine-tune deployments on Fireworks\nLink: https://docs.fireworks.ai/updates/changelog\nContent: You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment \"accounts/fireworks/models/<MODEL_ID of lora model>\"\n\nYou can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment \"accounts/fireworks/models/<MODEL_ID of lora model>\"\n\nYou can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: Previously, this involved two distinct steps, and the resulting deployment was slower than the base model: Create a deployment using firectl create deployment \"accounts/fireworks/models/<MODEL_ID of base model>\" --enable-addons Then deploy the addon to the deployment: firectl load-lora <MODEL_ID> --deployment <DEPLOYMENT_ID> For more information, see our deployment documentation. This change is for dedicated deployments with a single LoRA. You can still deploy multiple LoRAs on a deployment or deploy LoRA(s) on some Serverless models as described in the documentation.\n\n", "type": "text"}, {"text": "Title: Quick deployment\nLink: https://docs.fireworks.ai/fine-tuning/single-lora\nContent: Deploy your LoRA fine-tuned model with one simple command: firectl create deployment \"accounts/fireworks/models/<MODEL_ID of lora model>\"\n\nDeploy your LoRA fine-tuned model with one simple command: firectl create deployment \"accounts/fireworks/models/<MODEL_ID of lora model>\"\n\nAlternative deployment method\nThis two-step method is the standard approach for multi-LoRA deployments where multiple LoRA models share the same base model. While it can also be used for single LoRA deployments, it provides slower performance compared to live merge and is not recommended for single LoRA use cases. You can also deploy single LoRA models using a two-step process: Create base model deployment Deploy the base model with addons enabled: firectl create deployment \"accounts/fireworks/models/<MODEL_ID of base model>\" --enable-addons\n\n", "type": "text"}, {"text": "Title: Deploying a fine-tuned model using an on-demand deployment\nLink: https://docs.fireworks.ai/fine-tuning/fine-tuning-models\nContent: Use the following command to deploy your fine-tuned model using an on-demand deployment: firectl create deployment <FINE_TUNED_MODEL_ID>\n\nUse the following command to deploy your fine-tuned model using an on-demand deployment: firectl create deployment <FINE_TUNED_MODEL_ID>\n\nDeploying a fine-tuned model serverlessly\nNot all base models support serverless addons. Please check this list for the full list of serverless models that support LoRA add-ons. If your base model supports serverless addons, you can run the following command to load the fine-tuned model into a serverless deployment: firectl load-lora <FINE_TUNED_MODEL_ID>\n\n", "type": "text"}, {"text": "Title: Deploy Command\nLink: https://docs.fireworks.ai/evaluators/cli_reference/cli_overview\nContent: The deploy command deploys a reward function as an evaluator on the Fireworks platform.\n\nThe deploy command deploys a reward function as an evaluator on the Fireworks platform.\n\nCommand Overview\nThe Reward Kit CLI supports the following main commands: run : Run a local evaluation pipeline using a Hydra configuration. preview : Preview evaluation results or re-evaluate generated outputs. deploy : Deploy a reward function as an evaluator. agent-eval : Run agent evaluations on task bundles. list : List existing evaluators (coming soon). delete : Delete an evaluator (coming soon).\n\n", "type": "text"}, {"text": "Title: Create a deployment\nLink: https://docs.fireworks.ai/tools-sdks/firectl/commands/create-deployment\nContent: Creates a new deployment. firectl create deployment [flags]\n\nCreates a new deployment. firectl create deployment [flags]\n\nExample\nfirectl create deployment falcon-7b\n\n", "type": "text"}, {"text": "Title: firectl\nLink: https://docs.fireworks.ai/fine-tuning/fine-tuning-vlm\nContent: # Create a deployment for your fine-tuned VLM\nfirectl create deployment my-custom-vlm\n\n# Check deployment status\nfirectl get deployment accounts/your-account/deployment/deployment-id\n\n# Create a deployment for your fine-tuned VLM\nfirectl create deployment my-custom-vlm\n\n# Check deployment status\nfirectl get deployment accounts/your-account/deployment/deployment-id\n\nfirectl create sftj \\\n  --base-model accounts/fireworks/models/qwen2p5-vl-32b-instruct \\\n  --dataset my-vlm-dataset \\\n  --output-model my-custom-vlm \\\n  --epochs 3 For additional parameters like learning rates, evaluation datasets, and batch sizes, see  Additional SFT job settings .\n\n", "type": "text"}, {"text": "Title: Uploading the model locally (firectl)\nLink: https://docs.fireworks.ai/models/uploading-custom-models\nContent: To upload a custom base model, run the following command. firectl create model <MODEL_ID> /path/to/files/\n\nTo upload a custom base model, run the following command. firectl create model <MODEL_ID> /path/to/files/\n\nTo upload a custom base model, run the following command.\n\n", "type": "text"}], "tool_call_id": "functions.search:1"}, {"role": "assistant", "content": "Let me search for more information about deployment lifecycle management and monitoring.", "tool_calls": [{"id": "functions.search:0", "function": {"arguments": "{\"query\": \"deployment monitoring scaling management lifecycle\"}", "name": "search"}, "type": "function"}]}, {"role": "tool", "content": [{"text": "Title: What should I expect for deployment and scaling performance?\nLink: https://docs.fireworks.ai/faq-new/deployment-infrastructure/what-should-i-expect-for-deployment-and-scaling-performance\nContent: Initial deployment : Should complete within minutes Scaling from zero : You may experience brief availability delays while the system scales up Troubleshooting : If deployment takes over 1 hour, this typically indicates a crash and should be investigated Best practice : Monitor deployment status and contact support if deployment times are unusually long\n\nInitial deployment : Should complete within minutes Scaling from zero : You may experience brief availability delays while the system scales up Troubleshooting : If deployment takes over 1 hour, this typically indicates a crash and should be investigated Best practice : Monitor deployment status and contact support if deployment times are unusually long\n\nWhat should I expect for deployment and scaling performance?\nInitial deployment : Should complete within minutes Scaling from zero : You may experience brief availability delays while the system scales up Troubleshooting : If deployment takes over 1 hour, this typically indicates a crash and should be investigated Best practice : Monitor deployment status and contact support if deployment times are unusually long\n\n", "type": "text"}, {"text": "Title: How does billing and scaling work for on-demand GPU deployments?\nLink: https://docs.fireworks.ai/faq-new/deployment-infrastructure/how-does-billing-and-scaling-work-for-on-demand-gpu-deployments\nContent: On-demand GPU deployments have unique billing and scaling characteristics compared to serverless deployments: Billing: Charges start when the server begins accepting requests Billed by GPU-second for each active instance Costs accumulate even if there are no active API calls Scaling options: Supports autoscaling from 0 to multiple GPUs Each additional GPU adds to the billing rate Can handle unlimited requests within the GPU\u2019s capacity Management requirements: Not fully serverless; requires some manual management Manually delete deployments when no longer needed Or configure autoscaling to scale down to 0 during inactive periods Cost control tips: Regularly monitor active deployments Delete unused deployments to avoid unnecessary costs Consider serverless options for intermittent usage Use autoscaling to 0 to optimize costs during low-demand times\n\nOn-demand GPU deployments have unique billing and scaling characteristics compared to serverless deployments: Billing: Charges start when the server begins accepting requests Billed by GPU-second for each active instance Costs accumulate even if there are no active API calls Scaling options: Supports autoscaling from 0 to multiple GPUs Each additional GPU adds to the billing rate Can handle unlimited requests within the GPU\u2019s capacity Management requirements: Not fully serverless; requires some manual management Manually delete deployments when no longer needed Or configure autoscaling to scale down to 0 during inactive periods Cost control tips: Regularly monitor active deployments Delete unused deployments to avoid unnecessary costs Consider serverless options for intermittent usage Use autoscaling to 0 to optimize costs during low-demand times\n\nHow does billing and scaling work for on-demand GPU deployments?\nOn-demand GPU deployments have unique billing and scaling characteristics compared to serverless deployments: Billing: Charges start when the server begins accepting requests Billed by GPU-second for each active instance Costs accumulate even if there are no active API calls Scaling options: Supports autoscaling from 0 to multiple GPUs Each additional GPU adds to the billing rate Can handle unlimited requests within the GPU\u2019s capacity Management requirements: Not fully serverless; requires some manual management Manually delete deployments when no longer needed Or configure autoscaling to scale down to 0 during inactive periods Cost control tips: Regularly monitor active deployments Delete unused deployments to avoid unnecessary costs Consider serverless options for intermittent usage Use autoscaling to 0 to optimize costs during low-demand times\n\n", "type": "text"}, {"text": "Title: How does billing work for on-demand deployments?\nLink: https://docs.fireworks.ai/faq-new/deployment-infrastructure/how-does-billing-work-for-on-demand-deployments\nContent: On-demand deployments come with automatic cost optimization features: Default autoscaling : Automatically scales to 0 replicas when not in use Pay for what you use : Charged only for GPU time when replicas are active Flexible configuration : Customize autoscaling behavior to match your needs Best practices for cost management: Leverage default autoscaling : The system automatically scales down deployments when not in use Customize carefully : While you can modify autoscaling behavior using our configuration options , note that preventing scale-to-zero will result in continuous GPU charges Consider your use case : For intermittent or low-frequency usage, serverless deployments might be more cost-effective For detailed configuration options, see our deployment guide.\n\nOn-demand deployments come with automatic cost optimization features: Default autoscaling : Automatically scales to 0 replicas when not in use Pay for what you use : Charged only for GPU time when replicas are active Flexible configuration : Customize autoscaling behavior to match your needs Best practices for cost management: Leverage default autoscaling : The system automatically scales down deployments when not in use Customize carefully : While you can modify autoscaling behavior using our configuration options , note that preventing scale-to-zero will result in continuous GPU charges Consider your use case : For intermittent or low-frequency usage, serverless deployments might be more cost-effective For detailed configuration options, see our deployment guide.\n\nHow does billing work for on-demand deployments?\nOn-demand deployments come with automatic cost optimization features: Default autoscaling : Automatically scales to 0 replicas when not in use Pay for what you use : Charged only for GPU time when replicas are active Flexible configuration : Customize autoscaling behavior to match your needs Best practices for cost management: Leverage default autoscaling : The system automatically scales down deployments when not in use Customize carefully : While you can modify autoscaling behavior using our configuration options , note that preventing scale-to-zero will result in continuous GPU charges Consider your use case : For intermittent or low-frequency usage, serverless deployments might be more cost-effective For detailed configuration options, see our deployment guide.\n\n", "type": "text"}, {"text": "Title: How does the system scale?\nLink: https://docs.fireworks.ai/faq-new/deployment-infrastructure/how-does-the-system-scale\nContent: Our system is horizontally scalable , meaning it: Scales linearly with additional replicas of the deployment Automatically allocates resources based on demand Manages distributed load handling efficiently\n\nOur system is horizontally scalable , meaning it: Scales linearly with additional replicas of the deployment Automatically allocates resources based on demand Manages distributed load handling efficiently\n\nHow does the system scale?\nOur system is horizontally scalable , meaning it: Scales linearly with additional replicas of the deployment Automatically allocates resources based on demand Manages distributed load handling efficiently\n\n", "type": "text"}, {"text": "Title: Multiple GPUs (vertical scaling)\nLink: https://docs.fireworks.ai/guides/ondemand-deployments\nContent: The number of GPUs used per replica is specified by passing the --accelerator-count flag. Increasing the accelerator count will increase the generation speed, time-to-first-token, and maximum QPS for your deployment, however the scaling is sub-linear. The default value for most models is 1 but may be higher for larger models that require sharding. firectl create deployment <MODEL_NAME> --accelerator-count 2\n\nThe number of GPUs used per replica is specified by passing the --accelerator-count flag. Increasing the accelerator count will increase the generation speed, time-to-first-token, and maximum QPS for your deployment, however the scaling is sub-linear. The default value for most models is 1 but may be higher for larger models that require sharding. firectl create deployment <MODEL_NAME> --accelerator-count 2\n\nThe number of GPUs used per replica is specified by passing the --accelerator-count flag. Increasing the accelerator count will increase the generation speed, time-to-first-token, and maximum QPS for your deployment, however the scaling is sub-linear. The default value for most models is 1 but may be higher for larger models that require sharding. firectl update deployment <DEPLOYMENT_ID> --accelerator-count 2\n\n", "type": "text"}, {"text": "Title: scale_to_zero()\nLink: https://docs.fireworks.ai/tools-sdks/python-client/sdk-reference\nContent: Sends a request to scale the deployment to 0 replicas but does not wait for it to complete. Returns: The deployment object, or None if no deployment exists deployment = llm.scale_to_zero()\n\nSends a request to scale the deployment to 0 replicas but does not wait for it to complete. Returns: The deployment object, or None if no deployment exists\n\nscale_to_1_replica()\nScales the deployment to at least 1 replica. llm.scale_to_1_replica()\n\n", "type": "text"}, {"text": "Title: Do you support Auto Scaling?\nLink: https://docs.fireworks.ai/faq-new/deployment-infrastructure/do-you-support-auto-scaling\nContent: Yes, our system supports auto scaling with the following features: Scaling down to zero capability for resource efficiency Controllable scale-up and scale-down velocity Custom scaling rules and thresholds to match your specific needs\n\nYes, our system supports auto scaling with the following features: Scaling down to zero capability for resource efficiency Controllable scale-up and scale-down velocity Custom scaling rules and thresholds to match your specific needs\n\nDo you support Auto Scaling?\nYes, our system supports auto scaling with the following features: Scaling down to zero capability for resource efficiency Controllable scale-up and scale-down velocity Custom scaling rules and thresholds to match your specific needs\n\n", "type": "text"}, {"text": "Title: What are the rate limits for on-demand deployments?\nLink: https://docs.fireworks.ai/faq-new/deployment-infrastructure/what-are-the-rate-limits-for-on-demand-deployments\nContent: Request throughput scales with your GPU allocation. Base allocations include: Up to 8 A100 GPUs Up to 8 H100 GPUs On-demand deployments offer several advantages: Predictable pricing based on time units, not token I/O Protected latency and performance , independent of traffic on the serverless platform Choice of GPUs , including A100s and H100s Need more GPUs? Contact us to discuss higher allocations for your specific use case.\n\nRequest throughput scales with your GPU allocation. Base allocations include: Up to 8 A100 GPUs Up to 8 H100 GPUs On-demand deployments offer several advantages: Predictable pricing based on time units, not token I/O Protected latency and performance , independent of traffic on the serverless platform Choice of GPUs , including A100s and H100s Need more GPUs? Contact us to discuss higher allocations for your specific use case.\n\nWhat are the rate limits for on-demand deployments?\nRequest throughput scales with your GPU allocation. Base allocations include: Up to 8 A100 GPUs Up to 8 H100 GPUs On-demand deployments offer several advantages: Predictable pricing based on time units, not token I/O Protected latency and performance , independent of traffic on the serverless platform Choice of GPUs , including A100s and H100s Need more GPUs? Contact us to discuss higher allocations for your specific use case.\n\n", "type": "text"}, {"text": "Title: Create a deployment\nLink: https://docs.fireworks.ai/tools-sdks/firectl/commands/create-deployment\nContent: Creates a new deployment.\n\nCreates a new deployment.\n\nCreates a new deployment. firectl create deployment [flags]\n\n", "type": "text"}, {"text": "Title: Monitoring\nLink: https://docs.fireworks.ai/guides/prompt-caching\nContent: For dedicated deployments, information about prompt caching is returned in the\nresponse headers. The header fireworks-prompt-tokens contains the number of tokens\nin the prompt, out of which fireworks-cached-prompt-tokens are cached. Aggregated metrics are also available in the usage dashboard.\n\nFor dedicated deployments, information about prompt caching is returned in the\nresponse headers. The header fireworks-prompt-tokens contains the number of tokens\nin the prompt, out of which fireworks-cached-prompt-tokens are cached. Aggregated metrics are also available in the usage dashboard.\n\nMigration and traffic management\nWhen migrating between deployments that use prompt caching, it's crucial to implement proper traffic routing to maintain optimal cache hit rates. When gradually routing traffic to a new deployment, use consistent user/session-based sampling rather than random sampling. Here's the recommended implementation for traffic routing: Avoid random sampling for traffic routing as it can negatively impact cache hit rates: # Don't do this:\nif random() < fireworks_traffic_fraction:  # \u274c Reduces cache effectiveness\n  send_to_new_deployment(user=hashed_user_id)\n\n", "type": "text"}], "tool_call_id": "functions.search:0"}, {"role": "assistant", "content": "Based on my research, I can provide you with a comprehensive guide on managing deployments on Fireworks. Let me break this down into key areas:\n\n## Types of Deployments\n\nFireworks supports two main deployment types:\n\n### 1. Serverless Deployments\n- **Managed by Fireworks**: Popular base models hosted on shared infrastructure\n- **Billing**: Pay-per-token usage\n- **No configuration needed**: Fireworks handles all infrastructure\n- **Rate limits**: Subject to platform-wide limits\n- **Best for**: Intermittent usage, prototyping, or low-volume applications\n\n### 2. Dedicated Deployments\n- **Your private deployment**: Reserved exclusively for your use\n- **Billing**: GPU-second basis (charged for active GPU time)\n- **Full control**: Configure hardware, scaling, and model selection\n- **No rate limits**: Subject only to your GPU allocation capacity\n- **Best for**: High-volume applications, consistent performance requirements\n\n## Deployment Management Tools\n\n### Command Line Interface (firectl)\n\n**Installation and Setup:**\n```bash\n# Install firectl (assuming you have it configured)\n# Set your account ID to avoid \"invalid id\" errors\nexport FIREWORKS_ACCOUNT_ID=your-account-id\n```\n\n**Basic Deployment Commands:**\n\n```bash\n# Create a new deployment\nfirectl create deployment <MODEL_NAME>\n\n# Create deployment with specific GPU count\nfirectl create deployment <MODEL_NAME> --accelerator-count 2\n\n# Create deployment for LoRA fine-tuned model\nfirectl create deployment \"accounts/fireworks/models/<LORA_MODEL_ID>\"\n\n# Check deployment status\nfirectl get deployment accounts/your-account/deployment/<DEPLOYMENT_ID>\n\n# List all deployments\nfirectl list deployments\n\n# Update deployment configuration\nfirectl update deployment <DEPLOYMENT_ID> --accelerator-count 4\n\n# Delete a deployment\nfirectl delete deployment <DEPLOYMENT_ID>\n```\n\n### Python SDK (Build SDK)\n\n**Installation:**\n```bash\npip install fireworks-ai\n```\n\n**Basic Usage:**\n```python\nfrom fireworks import Fireworks\n\n# Initialize client\nclient = Fireworks(api_key=\"your-api-key\")\n\n# Create deployment configuration\nfrom fireworks.client import LLM\n\n# For on-demand deployments (requires .apply())\nllm = LLM(\n    model=\"accounts/your-account/models/your-model\",\n    deployment_type=\"on-demand\"\n)\nllm.apply()  # Required for on-demand deployments\n\n# For serverless deployments\nllm = LLM(model=\"accounts/fireworks/models/llama-v3p1-8b-instruct\")\n\n# Scale management\nllm.scale_to_1_replica()  # Ensure at least 1 replica\nllm.scale_to_zero()       # Scale down to 0 replicas\n```\n\n## Deployment Lifecycle Management\n\n### 1. Creation and Initial Setup\n```python\n# Example: Creating a comprehensive deployment\ndeployment_config = {\n    \"model\": \"accounts/your-account/models/your-model\",\n    \"deployment_type\": \"on-demand\",\n    \"accelerator_type\": \"A100\",  # or \"H100\"\n    \"accelerator_count\": 2,\n    \"min_replicas\": 0,  # Scale to zero when not used\n    \"max_replicas\": 4,  # Maximum scaling\n    \"autoscaling\": True\n}\n```\n\n### 2. Monitoring and Scaling\n```python\n# Check deployment status\ndeployment = llm.get_deployment()\nprint(f\"Status: {deployment.status}\")\nprint(f\"Replicas: {deployment.replicas}\")\n\n# Monitor usage and costs\n# Check the usage dashboard at https://app.fireworks.ai/dashboard/usage\n```\n\n### 3. Traffic Management During Migration\n```python\nimport hashlib\n\n# Implement consistent traffic routing for cache optimization\ndef route_traffic(user_id, traffic_fraction=0.2):\n    \"\"\"Route traffic consistently based on user ID for cache optimization\"\"\"\n    hashed_user_id = int(hashlib.md5(user_id.encode()).hexdigest(), 16)\n    max_hash = 2**128 - 1\n    ratio = hashed_user_id / max_hash\n    \n    if ratio < traffic_fraction:\n        return \"new_deployment\"\n    else:\n        return \"old_deployment\"\n\n# Usage\ndeployment_choice = route_traffic(\"user-123\", traffic_fraction=0.3)\n```\n\n## Cost Management Best Practices\n\n### 1. Autoscaling Configuration\n```python\n# Configure autoscaling to minimize costs\ndeployment_config = {\n    \"min_replicas\": 0,  # Scale to zero when idle\n    \"max_replicas\": 8,  # Maximum GPUs\n    \"target_cpu_utilization\": 0.7,  # Scale up at 70% utilization\n    \"scale_down_delay\": 300  # Wait 5 minutes before scaling down\n}\n```\n\n### 2. Monitoring and Cleanup\n```bash\n# Regular monitoring commands\nfirectl list deployments  # See all active deployments\nfirectl get deployment <ID> --watch  # Monitor specific deployment\n\n# Clean up unused deployments\nfirectl delete deployment <DEPLOYMENT_ID>\n```\n\n### 3. Reserved Capacity\nFor high-volume usage, consider reserved capacity instead of on-demand:\n- More reliable capacity\n- Higher GPU quotas\n- Better pricing for consistent usage\n\n## Advanced Features\n\n### Custom Base Models\n```bash\n# Upload custom model\nfirectl create model <MODEL_ID> /path/to/model/files/\n\n# Deploy custom model\nfirectl create deployment <MODEL_ID>\n```\n\n### Multi-GPU Deployments\n```bash\n# Deploy with multiple GPUs\nfirectl create deployment <MODEL_NAME> --accelerator-count 4\n\n# Update GPU count\nfirectl update deployment <DEPLOYMENT_ID> --accelerator-count 8\n```\n\n### LoRA Model Deployment\n```bash\n# Single LoRA deployment (faster)\nfirectl create deployment \"accounts/fireworks/models/<LORA_MODEL_ID>\"\n\n# Multi-LoRA deployment (shared base model)\nfirectl create deployment <BASE_MODEL_ID> --enable-addons\nfirectl load-lora <LORA_MODEL_ID> --deployment <DEPLOYMENT_ID>\n```\n\n## Key Management Commands Summary\n\n| Command | Purpose |\n|---------|---------|\n| `firectl create deployment` | Create new deployment |\n| `firectl get deployment` | Check deployment status |\n| `firectl list deployments` | List all deployments |\n| `firectl update deployment` | Modify deployment settings |\n| `firectl delete deployment` | Remove deployment |\n| `llm.apply()` | Apply configuration (Python SDK) |\n| `llm.scale_to_zero()` | Scale down to 0 replicas |\n| `llm.scale_to_1_replica()` | Ensure at least 1 replica |\n\n## Best Practices\n\n1. **Monitor Costs**: Regularly check your usage dashboard\n2. **Use Autoscaling**: Configure deployments to scale to zero when not used\n3. **Plan Migrations**: Use consistent traffic routing for cache optimization\n4. **Choose Wisely**: Use serverless for intermittent usage, dedicated for consistent workloads\n5. **Clean Up**: Delete unused deployments to avoid unnecessary charges\n6. **Test Scaling**: Verify your deployment scales appropriately under load\n\nWould you like me to dive deeper into any specific aspect of deployment management, such as autoscaling configuration, cost optimization strategies, or monitoring techniques?", "tool_calls": []}], "input_metadata": {"row_id": "run-update-traditional-software-idea", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct"}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "evaluation_result": {"score": 1.0, "is_score_valid": true, "reason": "At least one tool call was made", "metrics": {}}, "created_at": "2025-08-09T00:10:34.081457", "eval_metadata": {"name": "test_pytest_mcp_url", "description": "Run math evaluation on sample dataset using pytest interface.", "version": "v0.2.6.dev22+gfb34cf0.dirty", "status": "finished", "run_id": "replace-measure-bitter-open-reason", "num_runs": 1, "aggregation_method": "mean"}, "pid": 63817}, {"messages": [{"role": "user", "content": "Write an outline for a paper on the history of Yemeni coffee. The outline should include the main points of the paper, and at least 15 sections should be highlighted with markdown such as *highlighted section*."}, {"role": "assistant", "content": "# Outline: The History of Yemeni Coffee\n\n1. **Introduction: Why Yemeni Coffee Matters**  \n   - Thesis: Yemeni coffee is not merely a beverage but a cultural, economic, and geopolitical force that shaped global history.  \n   - Scope: From its mythical discovery in 9th-century Yemen to its modern revival.\n\n2. **Pre-Coffee Yemen: The Land, the People, and the Plant**  \n   - Geography of the Yemeni highlands and terraced agriculture.  \n   - Indigenous *Coffea arabica* varieties and early cultivation techniques.  \n   - *The myth of the goatherd Kaldi and the dancing goats*.\n\n3. **The Sufi Spark: Coffee as a Spiritual Elixir**  \n   - 15th-century Sufi monasteries in Mocha and Taiz.  \n   - *Qahwa as a tool for nocturnal dhikr rituals*.  \n   - Early roasting, grinding, and brewing methods.\n\n4. **Mocha: The Birthplace of a Global Commodity**  \n   - Rise of the port city of Mocha (Al-Mukha) in the 16th century.  \n   - *The Mocha trade network linking Yemen, Egypt, and the Ottoman Empire*.  \n   - Coffeehouses as centers of intellectual and political discourse.\n\n5. **The Ottoman Grip and the First Coffee Monopoly**  \n   - 1536: Ottoman conquest of Yemen and control of coffee exports.  \n   - *The ban on fertile beans to prevent cultivation elsewhere*.  \n   - Smuggling and the first cracks in the monopoly.\n\n6. **The Dutch Heist and the Global Spread of Coffee**  \n   - 1616: Dutch traders smuggle Yemeni beans to Batavia (Java).  \n   - *The birth of Dutch colonial coffee plantations*.  \n   - How Mocha\u2019s loss became the world\u2019s gain.\n\n7. **Coffeehouses of Cairo and Istanbul: Yemen\u2019s Cultural Export**  \n   - 1554: First Ottoman coffeehouse in Istanbul.  \n   - *The \u201cSchools of the Wise\u201d in Cairo\u2019s Khan el-Khalili*.  \n   - Coffee as a vector for Yemeni poetry, music, and Sufi thought.\n\n8. **The European Coffee Craze and the Mocha Brand**  \n   - 17th-century European coffeehouses and the \u201cMocha\u201d label.  \n   - *The false association of Mocha with chocolate flavor*.  \n   - Yemeni coffee\u2019s role in the Enlightenment.\n\n9. **Colonial Rivalries and the Decline of Yemeni Dominance**  \n   - 18th-century: French, British, and Dutch plantations outcompete Yemen.  \n   - *The collapse of Mocha\u2019s monopoly and the rise of Caribbean and Latin American coffee*.  \n   - Impact on Yemeni economy and society.\n\n10. **Yemeni Coffee in the Age of Empire**  \n   - 19th-century British and Ottoman interests in Yemeni coffee.  \n   - *The introduction of new cash crops (qat) and their effect on coffee cultivation*.  \n   - Decline in quality and quantity due to neglect and conflict.\n\n11. **The Cold War and the Coffee Crisis**  \n   - 20th-century Yemeni division (North/South) and Soviet/US influences.  \n   - *The shift from coffee to qat in rural livelihoods*.  \n   - Drought, pests, and civil wars further erode production.\n\n12. **The Specialty Coffee Renaissance: Yemen\u2019s Return to Glory**  \n   - 21st-century global demand for single-origin, heirloom coffees.  \n   - *The discovery of \u201cYemenia\u201d varietals and their genetic uniqueness*.  \n   - NGOs and cooperatives reviving traditional farming.\n\n13. **Modern Challenges: War, Climate, and Market Access**  \n   - 2015\u2013present: Saudi-led conflict and blockade.  \n   - *The siege of Hodeidah port and its impact on coffee exports*.  \n   - Climate change and water scarcity in the highlands.\n\n14. **Yemeni Coffee in the Cup: Flavor, Ritual, and Identity**  \n   - Tasting notes: dried fruit, spice, and wine-like acidity.  \n   - *The Yemeni coffee ceremony (bunna) and its social significance*.  \n   - Diaspora communities keeping traditions alive abroad.\n\n15. **Conclusion: The Future of Yemeni Coffee**  \n   - Balancing heritage with innovation.  \n   - *The role of Yemeni coffee in post-war reconstruction*.  \n   - Final reflection: From the highlands of Sana\u2019a to the cups of Brooklyn, Yemeni coffee remains a bridge between past and future."}], "input_metadata": {"row_id": "download-look-quick-minute-study", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct", "temperature": 0.0, "max_tokens": 4096}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "ground_truth": "15", "evaluation_result": {"score": 1.0, "is_score_valid": true, "reason": "\u2705 Found 30 highlighted sections (required: 15)", "metrics": {}}, "created_at": "2025-08-09T00:23:12.737811", "eval_metadata": {"name": "test_markdown_highlighting_evaluation", "description": "\n    Evaluation function that checks if the model's response contains the required number of formatted sections.\n    ", "version": "v0.2.6.dev22+gfb34cf0.dirty", "status": "finished", "run_id": "analyze-political-poorly-air-hardware", "num_runs": 1, "aggregation_method": "mean", "threshold_of_success": 0.5, "passed": true}, "pid": 68433}, {"messages": [{"role": "user", "content": "Write a cover letter for a job at a local coffee shop in the form of a poem. Highlight at least 5 text sections using \"*\". For example: *3 years of experience*."}, {"role": "assistant", "content": "Dear Brewed Awakening,\n\nI am the *early-bird steam* that curls above your grinders,  \nthe *latte-art heart* that beats in every porcelain curve.  \nMy apron is a canvas of *espresso freckles*,  \neach dot a story told in *3 years of experience*  \nfrom bean to cup, from dawn rush to dusk hush.\n\nI know the *symphony of the steam wand*\u2014  \nits hiss, its sigh, its whispered crema secrets\u2014  \nand how to coax *microfoam galaxies*  \ninto orbit around a *double-shot comet*.  \nI\u2019ve tamped, poured, swirled, and smiled  \nthrough *Saturday lines out the door*,  \nkeeping tempers cool as the milk I *steam to 140\u00b0*.\n\nI speak fluent *small-town regular*:  \n\u201cMedium drip, room for kindness?\u201d  \nI remember that *Mrs. Alvarez likes oat milk*,  \nthat *Mr. Chen takes two raw sugars*,  \nand that *your dog, Scout, prefers puppuccinos*.\n\nI sweep, I stock, I *balance the till to the penny*,  \nbut mostly I *brew community*\u2014  \none *extra-hot americano* at a time.  \nHire me, and the morning light  \nwill find me already *wiping down tables*,  \nalready *saving the last blueberry scone*  \nfor the kid who always forgets his wallet.\n\nWith *grounds in my veins* and *crema in my smile*,  \nI remain,\n\nYour future barista,  \nMaya"}], "input_metadata": {"row_id": "delete-open-cold-poor-power", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct", "temperature": 0.0, "max_tokens": 4096}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "ground_truth": "5", "evaluation_result": {"score": 1.0, "is_score_valid": true, "reason": "\u2705 Found 20 highlighted sections (required: 5)", "metrics": {}}, "created_at": "2025-08-09T00:23:12.737854", "eval_metadata": {"name": "test_markdown_highlighting_evaluation", "description": "\n    Evaluation function that checks if the model's response contains the required number of formatted sections.\n    ", "version": "v0.2.6.dev22+gfb34cf0.dirty", "status": "finished", "run_id": "analyze-political-poorly-air-hardware", "num_runs": 1, "aggregation_method": "mean", "threshold_of_success": 0.5, "passed": true}, "pid": 68433}, {"messages": [{"role": "user", "content": "Write a casual blog post about similarities across animal species. Highlight at least 5 sections in your answer by starting and ending with \"*\", like: *highlighted text section*."}, {"role": "assistant", "content": "Hey friends! Grab a coffee and let\u2019s talk about something that always blows my mind: *how much we all have in common, even when we look nothing alike*. I was scrolling through wildlife photos the other night and realized that a jellyfish and I are basically running the same basic software\u2014just with different packaging. So here\u2019s a laid-back tour of five \u201cwait, we do that too?!\u201d moments across the animal kingdom.\n\n*We all need our beauty sleep*  \nTurns out, every animal with a nervous system\u2014from fruit flies to blue whales\u2014shares a version of sleep. Crows nap on tree branches, octopuses snooze in coconut-shell beds, and even roundworms have \u201clethargus,\u201d which is basically worm REM. Scientists think sleep is so ancient that the last common ancestor of all animals already had it. So next time you hit snooze, remember you\u2019re honoring a 700-million-year-old tradition.\n\n*Love songs come in many dialects*  \nWhether you\u2019re a human humming in the shower or a humpback whale belting out 30-minute ballads, *we all use rhythm and pitch to flirt and bond*. Frogs croak in choruses, nightingales remix their playlists nightly, and male mice sing ultrasonic \u201cpickup lines\u201d too high for us to hear. Same goal, different instruments.\n\n*Grandma knew best: family matters*  \nElephants babysit their nieces, wolves bring back take-out for auntie, and *clownfish dads are the ultimate stay-at-home fathers*. Across species, kinship networks boost survival. Even bacteria split into \u201csibling\u201d lineages that cooperate\u2014so the next time your cousin crashes on your couch, call it evolutionary strategy.\n\n*Stress looks the same on everyone*  \nWhen life gets spicy, our bodies flip the same ancient switch. *Cortisol floods fish, birds, and humans alike*, making hearts race and muscles twitch. Zebra finches lose their appetite during tough times, and stressed-out bees become pessimistic (yes, they can test bee mood). So if you\u2019re doom-scrolling at 2 a.m., know that a stressed squirrel in your backyard totally gets you.\n\n*Play isn\u2019t optional\u2014it\u2019s homework for life*  \nOtters juggle rocks, dogs zoom in circles, and young ravens snowboard down snowy roofs using jar lids. *Play builds brains and social skills across the board*. Researchers have spotted reptiles \u201cwrestling\u201d for fun and octopuses repeatedly bouncing Lego blocks like toddlers. If it has neurons, it probably needs recess.\n\nNext time you feel alone in the universe, remember that somewhere a parrot is probably preening its best friend, a bat is sharing food with a hungry roost-mate, and a tiny nematode is curling up for a nap. We\u2019re all just variations on a very old, very shared theme."}], "input_metadata": {"row_id": "analyze-visible-level-love-pleasure", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct", "temperature": 0.0, "max_tokens": 4096}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "ground_truth": "5", "evaluation_result": {"score": 1.0, "is_score_valid": true, "reason": "\u2705 Found 10 highlighted sections (required: 5)", "metrics": {}}, "created_at": "2025-08-09T00:23:12.737882", "eval_metadata": {"name": "test_markdown_highlighting_evaluation", "description": "\n    Evaluation function that checks if the model's response contains the required number of formatted sections.\n    ", "version": "v0.2.6.dev22+gfb34cf0.dirty", "status": "finished", "run_id": "analyze-political-poorly-air-hardware", "num_runs": 1, "aggregation_method": "mean", "threshold_of_success": 0.5, "passed": true}, "pid": 68433}, {"messages": [{"role": "user", "content": "Generate a business proposal to start a sweatshirt company in Bremen. The proposal should contain 5 or more sections. Highlight each section name using the this format:\n*section name*"}, {"role": "assistant", "content": "*Executive Summary*\n\nBremen is a city where maritime heritage meets cutting-edge creativity. Our venture, \u201cNordHood Bremen,\u201d will launch a premium, design-driven sweatshirt brand that fuses Hanseatic culture with contemporary streetwear. In year one we will release three limited-edition drops (total 6,000 units), achieve \u20ac420,000 in revenue, and reach break-even by month 14. The brand will be 100 % online-direct at launch, then expand into two Bremen concept stores and select European boutiques in years two and three. We will operate carbon-neutral, use 100 % organic cotton, and reinvest 3 % of profits into local youth art programs\u2014turning every sweatshirt into a wearable piece of Bremen pride.\n\n*Market Analysis*\n\nBremen\u2019s 0.57 million residents and 500,000 annual tourists create a \u20ac35 million addressable mid-price sweatshirt market. Key insights:  \n\u2022 68 % of 18-35-year-olds prefer streetwear that tells a local story (Hochschule Bremen Trend Report 2023).  \n\u2022 Tourists spend \u20ac180 million on souvenirs, yet only 11 % are apparel items\u2014an untapped niche.  \n\u2022 Competitor scan: global brands (Nike, Adidas) dominate functionality; regional labels (Hamburg\u2019s \u201cDinghy,\u201d Hannover\u2019s \u201cLiguster\u201d) lack Bremen specificity.  \n\u2022 Online D2C share in German apparel has grown from 18 % (2019) to 31 % (2023), proving the viability of our launch model.  \nWe will target two segments: \u201cCulture Creatives\u201d (students, designers, musicians) and \u201cHeritage Seekers\u201d (tourists, expats). Both segments value authenticity, limited editions, and sustainability.\n\n*Product & Brand Strategy*\n\nSignature Product: The \u201cHanse Hood\u201d \u2013 460 g brushed organic-cotton fleece, oversized fit, dropped shoulders, double-stitched flat seams, and a hidden inside pocket sized for a Bremen transit card.  \nDesign Language: Minimalist silhouettes in maritime colorways (Bremen Blue, Weser Sand, Harbor Grey) with subtle iconography\u2014windrose embroidery, cog-ship sleeve print, and a QR code linking to an AR story of the garment\u2019s journey.  \nSustainability: GOTS-certified cotton, recycled polyester labels, biodegradable mailers, and a take-back program offering 15 % discount on next purchase.  \nDrop Model: Three seasonal drops per year (Spring \u201cHanse Sail,\u201d Fall \u201cSchlachte,\u201d Winter \u201cWeihnachtsmarkt\u201d), each capped at 2,000 units to maintain scarcity.  \nBrand Voice: \u201cWear the City.\u201d Every hangtag includes a Bremen insider tip curated by local artists, turning the sweatshirt into a cultural guide.\n\n*Operations & Supply Chain*\n\nDesign Studio: 120 m\u00b2 loft in \u00dcberseestadt (rent \u20ac1,800/month) doubling as photo studio and event space.  \nManufacturing: Partnership with a Fair-Wear-audited factory in Porto, Portugal (MOQ 500, 30-day lead time, \u20ac18 landed cost per unit).  \nLogistics: Bremen-based 3PL \u201cLogwin\u201d for pick-pack-ship (\u20ac1.20 per parcel, same-day dispatch).  \nTech Stack: Shopify Plus storefront, Klarna & PayPal checkout, Shipcloud integration, and a custom AR filter for Instagram try-ons.  \nQuality Control: In-house inspection on arrival; defective rate target <1 %.  \nRisk Mitigation: Dual-source yarn from Turkey and Portugal; maintain 8-week safety stock.\n\n*Marketing & Sales Plan*\n\nPre-Launch (Months 1-2):  \n\u2022 Teaser campaign on TikTok & Instagram Reels (#NordHoodBremen) with local influencers (5-50 k followers).  \n\u2022 Pop-up try-on booth at Viertel Kreativmarkt; collect 1,000 email sign-ups.  \nLaunch (Month 3):  \n\u2022 \u201cMidnight Drop\u201d online at 00:07 (Bremen dialing code) with countdown timer.  \n\u2022 Limited 100 \u201cFounders\u2019 Edition\u201d sweatshirts laser-numbered and signed by Bremen street artist \u201cPlotBot.\u201d  \nGrowth (Months 4-12):  \n\u2022 Collaborations: capsule with SV Werder Bremen fan design; co-branded hoodie with \u00dcberseemuseum.  \n\u2022 Referral program: give \u20ac10, get \u20ac10.  \n\u2022 Tourist channel: partnership with Bremer Touristik-Zentrum for QR-coded city map bundled with sweatshirt.  \nKPIs: CAC \u2264 \u20ac18, repeat purchase rate \u2265 35 %, Instagram engagement \u2265 8 %.\n\n*Financial Plan*\n\nStart-up Capital Needed: \u20ac85,000  \n\u2022 Inventory (3,000 units): \u20ac54,000  \n\u2022 Marketing & content: \u20ac12,000  \n\u2022 E-commerce & IT: \u20ac6,000  \n\u2022 Legal, trademark, insurance: \u20ac5,000  \n\u2022 Working capital buffer: \u20ac8,000  \nFunding: 60 % founder equity, 40 % Bremen F\u00f6rderbank micro-loan at 1.9 % interest.  \nYear-1 P&L (conservative):  \nRevenue \u20ac420,000 (avg. \u20ac70 net per unit)  \nCOGS \u20ac234,000  \nGross Profit \u20ac186,000  \nOperating Expenses \u20ac150,000  \nEBIT \u20ac36,000  \nBreak-even: 1,800 units or month 14.  \nThree-year projection: revenue \u20ac1.4 million, EBITDA margin 18 %, reinvestment into two Bremen stores and EU wholesale.\n\n*Appendices*\n\nA. Mood-board & color palette  \nB. Factory audit certificate  \nC. Sample influencer contracts  \nD. Detailed 36-month cash-flow model"}], "input_metadata": {"row_id": "run-put-natural-traditional-thought", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct", "temperature": 0.0, "max_tokens": 4096}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "ground_truth": "5", "evaluation_result": {"score": 1.0, "is_score_valid": true, "reason": "\u2705 Found 7 highlighted sections (required: 5)", "metrics": {}}, "created_at": "2025-08-09T00:23:12.737906", "eval_metadata": {"name": "test_markdown_highlighting_evaluation", "description": "\n    Evaluation function that checks if the model's response contains the required number of formatted sections.\n    ", "version": "v0.2.6.dev22+gfb34cf0.dirty", "status": "finished", "run_id": "analyze-political-poorly-air-hardware", "num_runs": 1, "aggregation_method": "mean", "threshold_of_success": 0.5, "passed": true}, "pid": 68433}, {"messages": [{"role": "user", "content": "Write a funny and sarcastic template for rating the quality of a marriage between two people who are both moms. This is for the couple themselves. Please highlight at least 3 sections with markdown,  i.e *highlighted section*."}, {"role": "assistant", "content": "**The Official \u201cTwo-Mom Marriage Quality Report Card\u201d**  \n(For internal use only. If this ends up on Facebook, we *will* change the Wi-Fi password.)\n\n---\n\n**1. Communication Score: ___/10**  \n*\u201cDid we just have a 45-minute conversation about whose turn it is to buy more goldfish crackers, or was that a TED Talk?\u201d*  \n- 10 = We finish each other\u2019s sentences\u2026 and each other\u2019s coffee.  \n- 5 = We communicate exclusively via passive-aggressive Post-it notes on the fridge.  \n- 0 = We\u2019ve resorted to interpretive dance and/or carrier pigeons.  \n\n**Bonus points** if you\u2019ve ever used the kids as tiny, sticky messengers (\u201cTell Mommy she\u2019s *cordially invited* to take out the trash\u201d).\n\n---\n\n**2. Chore Wars Score: ___/10**  \n*\u201cThe laundry basket is a Jenga tower of lies.\u201d*  \n- 10 = We\u2019ve achieved a chore utopia where socks actually match.  \n- 5 = We\u2019ve agreed to \u2018divide\u2019 labor\u2026 by one of us doing everything and the other \u2018supervising\u2019.  \n- 0 = The dishwasher has become a permanent storage unit.  \n\n**Penalty** if either mom has ever \u2018forgotten\u2019 how to load the dishwasher *again*.\n\n---\n\n**3. Co-Parenting Harmony Score: ___/10**  \n*\u201cWe tag-team tantrums like WWE pros.\u201d*  \n- 10 = We\u2019ve synchronized our \u2018Mom Voice\u2019 to subsonic frequencies that terrify small children.  \n- 5 = We\u2019ve debated whether \u2018screen time\u2019 is a human right or a war crime.  \n- 0 = We\u2019ve weaponized bedtime stories (\u2018And then the princess *never* got dessert again\u2026\u2019).  \n\n**Extra credit** for every time you\u2019ve high-fived over a successful *\u201cBecause I said so\u201d* deployment.\n\n---\n\n**4. Romance Maintenance Score: ___/10**  \n*\u201cDate night: when we argue over which drive-thru has the shortest line.\u201d*  \n- 10 = We\u2019ve scheduled intimacy between soccer practice and PTA meetings.  \n- 5 = We\u2019ve agreed that \u2018Netflix and actually chill\u2019 counts as foreplay.  \n- 0 = Our last romantic gesture was sharing the last chicken nugget.  \n\n**Automatic fail** if either of you has ever said, \u201cI\u2019m too tired\u201d\u2026 *while scrolling Instagram*.\n\n---\n\n**5. Emergency Snack Reserves Score: ___/10**  \n*\u201cThe pantry is a battlefield of empty granola bar boxes.\u201d*  \n- 10 = We\u2019ve hidden chocolate so well we forgot where it is.  \n- 5 = We\u2019ve labeled snacks with fake allergen warnings to deter theft.  \n- 0 = The kids have unionized and are demanding better rations.  \n\n**Gold star** if you\u2019ve ever *texted each other* to brag about finding the secret stash.\n\n---\n\n**Final Grade: ___/50**  \n- 40\u201350: *Power couple.* You\u2019re basically the Beyonc\u00e9 and Beyonc\u00e9 of lesbian moms.  \n- 25\u201339: *Functioning chaos.* You\u2019ve accepted that love is just shared exhaustion.  \n- 0\u201324: *Renew your vows\u2026 or your Costco membership.* Either way, you\u2019re in this together.  \n\n**Disclaimer:** This report is not legally binding, but hiding the good snacks *is* grounds for divorce."}], "input_metadata": {"row_id": "sit-disassemble-ready-area-problem", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct", "temperature": 0.0, "max_tokens": 4096}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "ground_truth": "3", "evaluation_result": {"score": 1.0, "is_score_valid": true, "reason": "\u2705 Found 29 highlighted sections (required: 3)", "metrics": {}}, "created_at": "2025-08-09T00:23:12.737929", "eval_metadata": {"name": "test_markdown_highlighting_evaluation", "description": "\n    Evaluation function that checks if the model's response contains the required number of formatted sections.\n    ", "version": "v0.2.6.dev22+gfb34cf0.dirty", "status": "finished", "run_id": "analyze-political-poorly-air-hardware", "num_runs": 1, "aggregation_method": "mean", "threshold_of_success": 0.5, "passed": true}, "pid": 68433}, {"messages": [{"role": "user", "content": "Write a song about the summers of my childhood that I spent in the countryside. Give the song a name, and highlight the name by wrapping it with *. For example: *little me in the countryside*."}, {"role": "assistant", "content": "*little me in the countryside*\n\n[Verse 1]  \nBarefoot on the gravel road, dust between my toes,  \nSunlight dripped like honey on the clover and the crows.  \nGrandma\u2019s porch swing creaked a lullaby so slow,  \nWhile the creek below kept singing songs only we could know.  \nFireflies stitched the twilight with lanterns made of stars,  \nAnd the moon was just a silver coin tossed into cookie jars.  \n\n[Chorus]  \nOh, little me in the countryside, running wild and free,  \nChasing dreams through cornfield mazes, climbing every tree.  \nEvery laugh was louder than the thunder rolling by,  \nIn the summers of my childhood, underneath that endless sky.  \n\n[Verse 2]  \nMason jars of sweet tea sweating on the window ledge,  \nRadio played \u201cSweet Home Alabama\u201d from the rusty garden shed.  \nWe built rafts from old barn doors and floated down the stream,  \nKingfishers were pirates and the cattails were our team.  \nSupper was tomatoes sliced like sunsets on a plate,  \nAnd the crickets kept the tempo while the night got late.  \n\n[Chorus]  \nOh, little me in the countryside, running wild and free,  \nSkinny-dipping in the moonlight where the river bends to see.  \nEvery scar was a medal from a dare we\u2019d never hide,  \nIn the summers of my childhood, where the earth and I collided.  \n\n[Bridge]  \nDandelion clocks we blew to tell the time,  \nEach puff a tiny parachute for wishes we would climb.  \nWe swore we\u2019d never grow up, just grow into the land,  \nBut the school bus came like autumn and took us by the hand.  \n\n[Verse 3]  \nNow the fields are sold and paved, the barn\u2019s a memory store,  \nBut I still hear the wheat heads whisper, \u201cCome back, come back once more.\u201d  \nI close my eyes and I\u2019m ten again, chasing dragonflies,  \nWith the smell of rain on dirt roads and the taste of cherry pies.  \n\n[Final Chorus]  \nOh, little me in the countryside, you\u2019re still inside my chest,  \nA pocket full of wonder that the city can\u2019t arrest.  \nThough the years roll on like tractors down a dusty county ride,  \nI keep those summers in a mason jar where the fireflies never die.  \n\n[Outro]  \nSo here\u2019s to cicadas, to tire swings and creek-stone slides,  \nTo the echo of my laughter in the hollow where time hides.  \nIf you find that gravel road, kick off your shoes and glide\u2014  \nYou\u2019ll meet little me in the countryside, still barefoot, still wide-eyed."}], "input_metadata": {"row_id": "affect-unknown-long-life-month", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct", "temperature": 0.0, "max_tokens": 4096}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "ground_truth": "1", "evaluation_result": {"score": 1.0, "is_score_valid": true, "reason": "\u2705 Found 1 highlighted sections (required: 1)", "metrics": {}}, "created_at": "2025-08-09T00:23:12.737956", "eval_metadata": {"name": "test_markdown_highlighting_evaluation", "description": "\n    Evaluation function that checks if the model's response contains the required number of formatted sections.\n    ", "version": "v0.2.6.dev22+gfb34cf0.dirty", "status": "finished", "run_id": "analyze-political-poorly-air-hardware", "num_runs": 1, "aggregation_method": "mean", "threshold_of_success": 0.5, "passed": true}, "pid": 68433}, {"messages": [{"role": "user", "content": "Write a riddle for the word \"fa\u00e7ade\" that contains at least 3 italic text phrases in markdown syntax, i.e *italic text*."}, {"role": "assistant", "content": "I wear a *painted smile* for every passer-by,  \na *perfectly symmetrical* face that never cracks.  \nBehind me, the *crumbling bricks* sigh in secret.  \nWhat am I?"}], "input_metadata": {"row_id": "believe-verify-likely-desire-power", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct", "temperature": 0.0, "max_tokens": 4096}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "ground_truth": "3", "evaluation_result": {"score": 1.0, "is_score_valid": true, "reason": "\u2705 Found 3 highlighted sections (required: 3)", "metrics": {}}, "created_at": "2025-08-09T00:23:12.738015", "eval_metadata": {"name": "test_markdown_highlighting_evaluation", "description": "\n    Evaluation function that checks if the model's response contains the required number of formatted sections.\n    ", "version": "v0.2.6.dev22+gfb34cf0.dirty", "status": "finished", "run_id": "analyze-political-poorly-air-hardware", "num_runs": 1, "aggregation_method": "mean", "threshold_of_success": 0.5, "passed": true}, "pid": 68433}, {"messages": [{"role": "user", "content": "Write a template for a workshop on the importance of diversity in the workplace and highlight at least 3 sections with markdown, i.e. *highlighted section*."}, {"role": "assistant", "content": "# Workshop Template  \n**Title:** *Building Inclusive Excellence: The Business & Human Case for Workplace Diversity*  \n**Duration:** 3.5 hours (can be split into two 90-minute sessions)  \n**Audience:** Managers, HR professionals, team leads, ERG members, and any employee who influences culture  \n\n---\n\n## 1. Welcome & Objectives (15 min)\n- Ice-breaker: \u201cTwo Truths & a Lie\u2014Diversity Edition\u201d  \n- *Learning objectives*: By the end of this workshop, participants will be able to:  \n  1. Define diversity, equity, inclusion, and belonging (DEIB) in their own words.  \n  2. Articulate at least three evidence-based benefits of a diverse workforce.  \n  3. Identify one action they can take within 30 days to foster inclusion on their team.  \n\n---\n\n## 2. *The Data-Driven Case for Diversity* (25 min)\n- Interactive poll: \u201cWhat % performance lift do you think diverse teams achieve?\u201d  \n- Mini-lecture with live charts:  \n  - McKinsey 2023 report: Top-quartile companies for ethnic/gender diversity are **39 % more likely to outperform** on profitability.  \n  - Harvard study: *Inclusive teams make better business decisions 87 % of the time* and do so twice as fast.  \n- Small-group exercise: Match the statistic to the business outcome (revenue, innovation, retention).  \n\n---\n\n## 3. *Unconscious Bias & Micro-behaviors* (30 min)\n- 5-minute video: \u201cThe Resume with Two Names\u201d (identical CVs, different callbacks).  \n- Bias Busters card game: Participants draw scenario cards and practice interrupting bias in real time.  \n- *Key takeaway*: Bias is not a moral failing; it is a neurological shortcut that can be rewired through deliberate habits.  \n\n---\n\n## 4. *Inclusive Leadership Toolkit* (40 min)\n- **The 4 A\u2019s Framework**:  \n  1. **Awareness** \u2013 Daily 3-minute bias check-in.  \n  2. **Amplification** \u2013 Repeat & credit ideas from under-represented voices.  \n  3. **Access** \u2013 Rotate high-visibility assignments.  \n  4. **Accountability** \u2013 Track inclusion metrics on team dashboards.  \n- Role-play: \u201cThe Missed Promotion\u201d \u2013 diagnose where each \u2018A\u2019 broke down and redesign the conversation.  \n\n---\n\n## 5. Action Planning & Commitments (20 min)\n- Personal *Inclusion Commitment Card*: \u201cIn the next 30 days I will\u2026\u201d  \n- Pair-share for accountability.  \n- QR code to a digital resource hub (TED talks, reading list, policy templates).  \n\n---\n\n## 6. Closing & Evaluation (10 min)\n- One-word checkout: \u201cHow do you feel now?\u201d  \n- *Post-workshop survey* link (2 min) with prize draw for completion.  \n\n---\n\n## Materials Checklist\n- Slide deck (with alt-text on all images)  \n- Printed Commitment Cards & pens  \n- Bias Busters card decks (laminated)  \n- Timer & bell  \n- Snacks labeled for common dietary restrictions (gluten-free, vegan, halal)  \n\n---\n\n## Follow-Up Email Template (send within 24 hrs)\nSubject: Your Inclusion Toolkit + Next Steps  \nBody:  \nHi team,  \nThank you for leaning into today\u2019s conversation on *why diversity wins*. Attached are:  \n- Slide deck & resource links  \n- Your signed Inclusion Commitment Card (photo reminder)  \n- Calendar invite for the optional 30-day check-in circle  \n\nRemember: Inclusion is a practice, not a one-time training. Let\u2019s keep the momentum!"}], "input_metadata": {"row_id": "change-sort-serious-song-family", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct", "temperature": 0.0, "max_tokens": 4096}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "ground_truth": "3", "evaluation_result": {"score": 1.0, "is_score_valid": true, "reason": "\u2705 Found 19 highlighted sections (required: 3)", "metrics": {}}, "created_at": "2025-08-09T00:23:12.738042", "eval_metadata": {"name": "test_markdown_highlighting_evaluation", "description": "\n    Evaluation function that checks if the model's response contains the required number of formatted sections.\n    ", "version": "v0.2.6.dev22+gfb34cf0.dirty", "status": "finished", "run_id": "analyze-political-poorly-air-hardware", "num_runs": 1, "aggregation_method": "mean", "threshold_of_success": 0.5, "passed": true}, "pid": 68433}, {"messages": [{"role": "user", "content": "Write a cover letter to a local political party, asking to be their rally organizer. Make sure to highlight at least 3 sections in your answer in markdown format."}, {"role": "assistant", "content": "**[Your Name]**  \n[Your Address]  \n[City, State ZIP]  \n[Email Address] | [Phone Number] | [LinkedIn URL]  \n[Date]\n\n**[Local Party Chairperson\u2019s Name]**  \n[Local Party Name]  \n[Party Headquarters Address]  \n[City, State ZIP]\n\nDear **Chairperson [Last Name]**,\n\nI am writing to express my enthusiastic interest in serving as **Rally Organizer** for **[Local Party Name]**. As a lifelong resident of **[Your City/County]** and a committed progressive, I have spent the past six years turning grassroots energy into measurable voter engagement. I believe my proven record of mobilizing thousands of supporters, securing high-profile speakers, and running flawless logistics makes me the ideal candidate to lead our 2024 rally program.\n\n### **Proven Rally & Event Expertise**\n- **2018 & 2020 Midterm & Presidential Rallies**: Planned and executed 14 rallies across three counties, averaging 2,500 attendees each.  \n- **Permits & Safety**: Obtained all municipal permits within 72 hours, coordinated with local fire and police departments, and maintained a 100 % safety record.  \n- **Stage & AV Management**: Negotiated vendor contracts that cut AV costs by 27 % while increasing sound coverage by 40 %.\n\n### **Community & Coalition Building**\n- **Diverse Coalitions**: Forged partnerships with 18 labor unions, 9 student groups, and 7 faith organizations, expanding our volunteer base by 1,200 active members.  \n- **Multilingual Outreach**: Produced bilingual signage and volunteer scripts, increasing turnout in Latinx precincts by 31 %.  \n- **Accessibility**: Installed ADA-compliant ramps, ASL interpreters, and quiet zones at every event, earning commendation from the local Disability Rights chapter.\n\n### **Data-Driven Strategy & Rapid Response**\n- **Real-Time Metrics**: Deployed a mobile check-in app that captured 94 % of attendee data, feeding our CRM for targeted follow-up.  \n- **Weather Contingencies**: Created a \u201cPlan B\u201d playbook that moved a 3,000-person outdoor rally indoors in under 90 minutes with zero no-shows.  \n- **Crisis Communications**: Served as on-site media liaison, turning a last-minute speaker cancellation into a prime-time local news segment featuring our grassroots volunteers.\n\nI would welcome the opportunity to discuss how my experience, energy, and strategic vision can help **[Local Party Name]** stage rallies that not only energize our base but also convert enthusiasm into votes. I am available at your convenience for an interview and can provide a detailed rally calendar and budget forecast within 48 hours of request.\n\nThank you for your time and consideration. I look forward to amplifying our shared values across **[Your City/County]**\u2014one rally at a time.\n\nIn solidarity,\n\n**[Your Name]**"}], "input_metadata": {"row_id": "must-write-need-theoretical-pleasure", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct", "temperature": 0.0, "max_tokens": 4096}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "ground_truth": "3", "evaluation_result": {"score": 1.0, "is_score_valid": true, "reason": "\u2705 Found 21 highlighted sections (required: 3)", "metrics": {}}, "created_at": "2025-08-09T00:23:12.738068", "eval_metadata": {"name": "test_markdown_highlighting_evaluation", "description": "\n    Evaluation function that checks if the model's response contains the required number of formatted sections.\n    ", "version": "v0.2.6.dev22+gfb34cf0.dirty", "status": "finished", "run_id": "analyze-political-poorly-air-hardware", "num_runs": 1, "aggregation_method": "mean", "threshold_of_success": 0.5, "passed": true}, "pid": 68433}, {"messages": [{"role": "user", "content": "The opposite of youth is not age, but ...? Highlight at least 2 sections in your answer with markdown, i.e. *highlighted section*."}, {"role": "assistant", "content": "The opposite of youth is not age, but **disillusionment**\u2014the moment when *wonder* is traded for *weariness*, when the *boundless horizon* of possibility collapses into the *narrow corridor* of routine."}], "input_metadata": {"row_id": "control-accept-record-legal-car", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct", "temperature": 0.0, "max_tokens": 4096}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "ground_truth": "2", "evaluation_result": {"score": 1.0, "is_score_valid": true, "reason": "\u2705 Found 5 highlighted sections (required: 2)", "metrics": {}}, "created_at": "2025-08-09T00:23:12.738096", "eval_metadata": {"name": "test_markdown_highlighting_evaluation", "description": "\n    Evaluation function that checks if the model's response contains the required number of formatted sections.\n    ", "version": "v0.2.6.dev22+gfb34cf0.dirty", "status": "finished", "run_id": "analyze-political-poorly-air-hardware", "num_runs": 1, "aggregation_method": "mean", "threshold_of_success": 0.5, "passed": true}, "pid": 68433}, {"messages": [{"role": "user", "content": "Write a description of the following data in a weird style: The Golden Palace eatType restaurant; The Golden Palace food Indian; The Golden Palace area city centre. Use markdown to highlight at least 3 sections in your answer."}, {"role": "assistant", "content": "**The Golden Palace** is not a palace, but it *is* golden\u2014at least in spirit\u2014nestled like a curry-scented jewel in the **city centre**, where neon pigeons peck at samosa crumbs and the air hums with sitar-shaped traffic lights.  \n\nInside, **Indian** food performs acrobatics on your tongue: cardamom cartwheels, turmeric tumbles, and a rogue chili that once tried to elope with a mango lassi. The waiters glide like buttered shadows, whispering orders to the kitchen where naan breads inflate like edible balloons plotting a coup against gravity.  \n\nAt **The Golden Palace**, every table is a throne, every plate a miniature map of the subcontinent, and every bite a teleportation device disguised as dinner."}], "input_metadata": {"row_id": "miss-continue-fall-local-work", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct", "temperature": 0.0, "max_tokens": 4096}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "ground_truth": "3", "evaluation_result": {"score": 1.0, "is_score_valid": true, "reason": "\u2705 Found 5 highlighted sections (required: 3)", "metrics": {}}, "created_at": "2025-08-09T00:23:12.738119", "eval_metadata": {"name": "test_markdown_highlighting_evaluation", "description": "\n    Evaluation function that checks if the model's response contains the required number of formatted sections.\n    ", "version": "v0.2.6.dev22+gfb34cf0.dirty", "status": "finished", "run_id": "analyze-political-poorly-air-hardware", "num_runs": 1, "aggregation_method": "mean", "threshold_of_success": 0.5, "passed": true}, "pid": 68433}, {"messages": [{"role": "user", "content": "Write a funny rap about a man who gets a call from an official saying that he is a long lost relative of the king of Nigeria. Use markdown to highlight at least one section of your answer, i.e. *highlighted section*."}, {"role": "assistant", "content": "\ud83c\udfa4 *Yo, it\u2019s the tale of Sir Scam-a-lot, straight outta suburbia!* \ud83c\udfa4  \n\nYo, my name is Dave, I sell insurance in Ohio,  \nGot a cubicle, a cat, and a 401(k) that\u2019s kinda fly-o.  \nOne Tuesday, phone rings\u2014unknown number, what the heck?  \nVoice says, \u201c*Prince Ogbonna the Third* just wrote you a fat check!\u201d  \n\nI\u2019m like, \u201cHold up, who dis? I\u2019m whiter than printer paper,  \nMy bloodline\u2019s mayo on Wonder Bread\u2014ain\u2019t no royal vapor!\u201d  \nDude on the line, accent silkier than lotion,  \nSays, \u201cCousin, you\u2019re 37th in line\u2014time to claim your ocean!\u201d  \n\n**[Hook]**  \n\ud83c\udfb6 *Long lost king of Nigeria, crown sittin\u2019 on my dome,*  \n*Got a palace full of leopards and a Wi-Fi that roams!*  \n*They need my SSN and a quick wire fee,*  \n*But I\u2019m \u2018bout to be big pimpin\u2019 in Abuja, just wait and see!* \ud83c\udfb6  \n\nHe says, \u201cJust send five grand for customs, then the jet lands,  \nYou\u2019ll be dripping in so much gold, even your sweat glands bling.\u201d  \nI\u2019m sweatin\u2019 like a rotisserie, visions of caviar dreams,  \nI Venmo \u201cPrinceNaija69\u201d like I\u2019m funding meme schemes.  \n\nNext day, email: \u201cOops, palace goat ate the transfer slip,  \nNeed ten more for goat surgery and a diamond-encrusted whip.\u201d  \nI hock my lawnmower, my Xbox, and Aunt Linda\u2019s porcelain goose,  \nWire cash so fast my bank\u2019s like, \u201cDave, you loose!\u201d  \n\n**[Bridge \u2013 Spoken Word]**  \nThen the *official royal seal* arrives\u2014  \nIt\u2019s a GIF of Shrek in a crown doing the Macarena.  \nI\u2019m starting to sense a pattern here\u2026  \n\n**[Final Hook \u2013 Regret Remix]**  \n\ud83c\udfb6 *Now I\u2019m the king of my mom\u2019s basement, throne\u2019s a beanbag chair,*  \n*Royal decree: never trust a prince who types in all caps\u2014*  \n*\u201cURGENT, COUSIN, SEND MORE FLAMINGOS!\u201d*  \n*Lesson learned: if the crown\u2019s too shiny, it\u2019s probably aluminum foil, homie!*"}], "input_metadata": {"row_id": "require-affect-major-logical-business", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct", "temperature": 0.0, "max_tokens": 4096}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "ground_truth": "1", "evaluation_result": {"score": 1.0, "is_score_valid": true, "reason": "\u2705 Found 14 highlighted sections (required: 1)", "metrics": {}}, "created_at": "2025-08-09T00:23:12.738142", "eval_metadata": {"name": "test_markdown_highlighting_evaluation", "description": "\n    Evaluation function that checks if the model's response contains the required number of formatted sections.\n    ", "version": "v0.2.6.dev22+gfb34cf0.dirty", "status": "finished", "run_id": "analyze-political-poorly-air-hardware", "num_runs": 1, "aggregation_method": "mean", "threshold_of_success": 0.5, "passed": true}, "pid": 68433}, {"messages": [{"role": "user", "content": "Expand the riddle into a story with a funny tone:\n\nWhat can you catch but not throw?\nA cold\n\nUse * to highlight at least 2 sections in your text. For example: *this is a highlighted text section*."}, {"role": "assistant", "content": "Gary the Germ was the *smallest, greenest, and most obnoxiously proud* microbe in the entire Sniffle Kingdom. Every morning he practiced his grand entrance: sliding down banisters made of nasal hair, cannon-balling into unsuspecting tonsils, and yelling, \u201cACHOO\u2014did somebody order a cold?\u201d  \n\nOne Tuesday, Gary spotted his dream target: *Bartholomew \u201cBart\u201d Bigglesworth, a man who believed hand-washing was a government conspiracy*. Bart had just exited the subway, triumphantly licking a subway pole \u201cfor the vitamins.\u201d Gary rubbed his microscopic hands together. \u201cPerfect. I\u2019ll hitch a ride on that finger, ride the express elevator to the sinuses, and throw the *snottiest* house-warming party in history!\u201d  \n\nBut there was a catch\u2014literally. Gary could *catch* Bart, but he couldn\u2019t *throw* him anywhere. Every time Gary tried to catapult Bart into the next office cubicle, Bart just sneezed, wiped his nose on his sleeve, and kept typing spreadsheets. Gary screamed, \u201cThis is mutiny! I\u2019m supposed to be the one doing the tossing!\u201d  \n\nMeanwhile, Bart\u2019s immune system\u2014led by Captain Phlegm and the Mucus Marines\u2014launched *Operation Tissue Storm*. They surrounded Gary, armed with vitamin C grenades and zinc zappers. Gary attempted a dramatic escape by surfing a sneeze, but the tissue slapped him like a flyswatter on a summer day.  \n\nDefeated, Gary sat in the crumpled tissue, sulking. \u201cI caught the cold,\u201d he muttered, \u201cbut I can\u2019t throw it anywhere. Worst. Superpower. Ever.\u201d  \n\nFrom that day forward, Gary changed his business card to read: \u201cProfessional Cold Catcher\u2014No Returns, No Refunds, Absolutely No Throwing.\u201d"}], "input_metadata": {"row_id": "could-serious-newspaper-radio-group", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct", "temperature": 0.0, "max_tokens": 4096}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "ground_truth": "2", "evaluation_result": {"score": 1.0, "is_score_valid": true, "reason": "\u2705 Found 6 highlighted sections (required: 2)", "metrics": {}}, "created_at": "2025-08-09T00:23:12.738169", "eval_metadata": {"name": "test_markdown_highlighting_evaluation", "description": "\n    Evaluation function that checks if the model's response contains the required number of formatted sections.\n    ", "version": "v0.2.6.dev22+gfb34cf0.dirty", "status": "finished", "run_id": "analyze-political-poorly-air-hardware", "num_runs": 1, "aggregation_method": "mean", "threshold_of_success": 0.5, "passed": true}, "pid": 68433}, {"messages": [{"role": "user", "content": "Write a rap about the renaissance. It should be noticeably different from raps about other historical eras, and have an interesting or weird tone. Highlight at least 3 sections in your answer in markdown format."}, {"role": "assistant", "content": "**[Intro \u2013 harpsichord glitch beat, stylus scratching parchment]**  \nYo, it\u2019s 14-something, but my mind\u2019s 3-D printed in gold leaf\u2014  \nI\u2019m the **Renaissance Rapper**, got a quill in one hand and a scalpel in the other, dissectin\u2019 beats like Vesalius dissectin\u2019 spleens.  \nLet\u2019s get *weird* with the rebirth, baby.\n\n---\n\n### **Section 1 \u2013 The Fresco Flex**  \nI drip ultramarine like I\u2019m sponsored by Afghanistan mines,  \nMichelangelo on the ceiling, I\u2019m reclining on divine lines.  \nAngels doing backflips off the Sistine, they\u2019re hype for my set,  \nWhile I tag \u201c*AMOR VINCIT OMNIA*\u201d in invisible ink\u2014wet.  \nMy flow\u2019s a trompe-l\u2019\u0153il, looks flat till you tilt your head\u2014  \nThen it\u2019s depth for days, like Brunelleschi\u2019s dome overhead.  \nI\u2019m not baroque, I\u2019m pre-baroque, I\u2019m the prototype shock,  \nGot patrons in fur-lined sleeves stuffing florins in my sock.  \n**I paint in egg tempera so the colors never fade,**  \n**But the yolk smells funky\u2014call it *odeur de old crusade*.**\n\n---\n\n### **Section 2 \u2013 The Printing-Press Poltergeist**  \nGutenberg ghost in the machine, clanking lead type like chains,  \nEvery page a polter-page, every serif a soul that remains.  \nI drop a folio, it\u2019s viral before \u201cviral\u201d was a cough,  \nMonks in scriptoriums rage-quitting, \u201cBro, that illuminated stuff\u2019s off!\u201d  \nI\u2019m the incunabula influencer, my colophon\u2019s a flex,  \nGot red-ink rubricators sliding into my DMs\u2014*next*.  \n**I rhyme in movable type so my bars can rearrange,**  \n**Yesterday I was Petrarch, today I\u2019m deranged.**  \nThe press spits paper like a hydra with hiccups,  \nEach sheet a protest flyer: \u201cDown with scholastic pick-ups!\u201d\n\n---\n\n### **Section 3 \u2013 The Anatomical Anthem (Parental Advisory: Corpses)**  \nNight shift with da Vinci, we\u2019re sketching by candle and moon,  \nCadavers on the table like, \u201cDo me a favor\u2014autotune my coccyx soon.\u201d  \nI rap in reverse Latin so only the dead understand,  \nWhile Leo reverse-engineers a bird-man from a frying pan.  \n**My beats are ventricles pumping four-four time,**  \n**Aorta bass drop\u2014call that circulatory grime.**  \nI sample the sound of a scalpel scraping periosteum,  \nLayer it over a lute loop\u2014*that\u2019s* my magnum opium.  \nGalen\u2019s in the corner, throwing shade like a Greek tragedy,  \nBut I\u2019m Harvey before Harvey, already rapping blood circulation majestically.\n\n---\n\n**[Outro \u2013 fading lute arpeggio, distant cannon fire from Constantinople]**  \nSo remember me when you\u2019re doom-scrolling on glass,  \nI was the era that turned corpses into class.  \nI\u2019m the **Renaissance Rapper**, my ghost still tags in chalk,  \nLeaving QR codes on cathedral walls\u2014scan it, you\u2019ll squawk.  \nRebirth never dies, it just pixelates and loops,  \nNow if you\u2019ll excuse me, I\u2019ve got a date with Lucretius in the soup."}], "input_metadata": {"row_id": "offer-select-free-state-moment", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct", "temperature": 0.0, "max_tokens": 4096}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "ground_truth": "3", "evaluation_result": {"score": 1.0, "is_score_valid": true, "reason": "\u2705 Found 17 highlighted sections (required: 3)", "metrics": {}}, "created_at": "2025-08-09T00:23:12.738192", "eval_metadata": {"name": "test_markdown_highlighting_evaluation", "description": "\n    Evaluation function that checks if the model's response contains the required number of formatted sections.\n    ", "version": "v0.2.6.dev22+gfb34cf0.dirty", "status": "finished", "run_id": "analyze-political-poorly-air-hardware", "num_runs": 1, "aggregation_method": "mean", "threshold_of_success": 0.5, "passed": true}, "pid": 68433}, {"messages": [{"role": "user", "content": "Summarize the history of Japan. Italicize at least 5 keywords in your response. To indicate a italic word, wrap it with asterisk, like *italic*"}, {"role": "assistant", "content": "*J\u014dmon* hunter-gatherers (c. 14,000 BCE) gave way to *Yayoi* rice cultivators who forged bronze and iron tools.  \nThe *Yamato* court unified clans in the 3rd\u20134th centuries, adopting *Shint\u014d* rituals and Chinese script.  \nPrince *Sh\u014dtoku*\u2019s 7th-century constitution and the Taika Reforms centralized power under an emperor, while *Nara* and *Heian* aristocrats refined literature and *Buddhism*.  \nSamurai clans rose during the *Kamakura* shogunate (1185), repelling *Mongol* fleets and entrenching warrior rule.  \nCenturies of *Ashikaga* and *Sengoku* civil war ended when *Oda Nobunaga*, *Toyotomi Hideyoshi*, and *Tokugawa Ieyasu* reunified Japan; the *Edo* shogunate (1603\u20131868) enforced isolation yet fostered *ukiyo-e* culture.  \nCommodore *Perry*\u2019s 1853 arrival triggered the *Meiji* Restoration, rapid modernization, and imperial expansion.  \nVictory in the *Russo-Japanese* War (1905) emboldened militarism, leading to *Manchuria* occupation and *Pearl Harbor*.  \nAfter atomic bombings and *Hirohito*\u2019s surrender in 1945, U.S. occupation reformed politics and the economy, birthing the *post-war* \u201ceconomic miracle.\u201d  \nToday, Japan balances *constitutional monarchy* with global technological leadership."}], "input_metadata": {"row_id": "calculate-affect-fullest-emotional-money", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct", "temperature": 0.0, "max_tokens": 4096}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "ground_truth": "5", "evaluation_result": {"score": 1.0, "is_score_valid": true, "reason": "\u2705 Found 25 highlighted sections (required: 5)", "metrics": {}}, "created_at": "2025-08-09T00:23:12.738219", "eval_metadata": {"name": "test_markdown_highlighting_evaluation", "description": "\n    Evaluation function that checks if the model's response contains the required number of formatted sections.\n    ", "version": "v0.2.6.dev22+gfb34cf0.dirty", "status": "finished", "run_id": "analyze-political-poorly-air-hardware", "num_runs": 1, "aggregation_method": "mean", "threshold_of_success": 0.5, "passed": true}, "pid": 68433}, {"messages": [{"role": "user", "content": "Write a funny Haiku about a Quaker named John who lives in the town of De Smet, South Dakota. Use the asterisk symbol, *, to highlight some words or phrases twice. Example: *This is a highlighted phrase*."}, {"role": "assistant", "content": "*John* in *De Smet*  \nQuaker hat *blows off*\u2014chases  \n*Silent* *scream*\u2014\u201cCome back!\u201d"}], "input_metadata": {"row_id": "download-american-private-result-research", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct", "temperature": 0.0, "max_tokens": 4096}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "ground_truth": "2", "evaluation_result": {"score": 1.0, "is_score_valid": true, "reason": "\u2705 Found 5 highlighted sections (required: 2)", "metrics": {}}, "created_at": "2025-08-09T00:23:12.738252", "eval_metadata": {"name": "test_markdown_highlighting_evaluation", "description": "\n    Evaluation function that checks if the model's response contains the required number of formatted sections.\n    ", "version": "v0.2.6.dev22+gfb34cf0.dirty", "status": "finished", "run_id": "analyze-political-poorly-air-hardware", "num_runs": 1, "aggregation_method": "mean", "threshold_of_success": 0.5, "passed": true}, "pid": 68433}, {"messages": [{"role": "user", "content": "Today, at the 54th Annual Grammy Awards, the Recording Academy honors the talent and creativity of the artists, musicians, and producers who are the creators of the best recordings of the past year. Please continue writing this text in a formal tone, using notations. Highlight some key parts in your response with \"*\", like *highlighted text*."}, {"role": "assistant", "content": "Today, at the 54th Annual Grammy Awards, the Recording Academy honors the talent and creativity of the artists, musicians, and producers who are the creators of the best recordings of the past year. In a ceremony marked by *unprecedented artistic excellence* and *meticulous craftsmanship*, the Academy recognizes those whose work has defined the sonic landscape of 2011.\n\nThe evening\u2019s proceedings underscore the Academy\u2019s enduring commitment to *artistic integrity* and *technical innovation*. Among the most celebrated achievements are:\n\n\u2022 *Album of the Year* \u2013 awarded to an opus that has redefined genre boundaries while achieving both critical acclaim and popular resonance.  \n\u2022 *Record of the Year* \u2013 bestowed upon a single track whose *sonic clarity*, *emotive depth*, and *production mastery* have set a new benchmark for the industry.  \n\u2022 *Best New Artist* \u2013 conferred upon an emerging voice whose *visionary artistry* signals the future direction of contemporary music.\n\nThroughout the telecast, special tributes illuminate the *indelible legacies* of icons whose influence transcends generations, while collaborative performances showcase the *synergistic power* of cross-genre partnerships. The Academy further acknowledges the *behind-the-scenes luminaries*\u2014engineers, mixers, and mastering specialists\u2014whose *invisible expertise* transforms raw inspiration into *audible transcendence*.\n\nIn closing, the 54th Annual Grammy Awards reaffirm the Recording Academy\u2019s solemn charge: to *safeguard the heritage*, *nurture the present*, and *inspire the future* of recorded music."}], "input_metadata": {"row_id": "indicate-strong-single-festival-mother", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct", "temperature": 0.0, "max_tokens": 4096}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "ground_truth": "1", "evaluation_result": {"score": 1.0, "is_score_valid": true, "reason": "\u2705 Found 19 highlighted sections (required: 1)", "metrics": {}}, "created_at": "2025-08-09T00:23:12.738286", "eval_metadata": {"name": "test_markdown_highlighting_evaluation", "description": "\n    Evaluation function that checks if the model's response contains the required number of formatted sections.\n    ", "version": "v0.2.6.dev22+gfb34cf0.dirty", "status": "finished", "run_id": "analyze-political-poorly-air-hardware", "num_runs": 1, "aggregation_method": "mean", "threshold_of_success": 0.5, "passed": true}, "pid": 68433}, {"messages": [{"role": "user", "content": "Write a blog post about interesting facts about the Dutch language. Italicize at least 2 sections in your answer with markdown, i.e. *italic text*."}, {"role": "assistant", "content": "# 10 Surprising Facts About the Dutch Language That Will Make You Say \u201cWauw!\u201d\n\nIf you think Dutch is just \u201cGerman with a sore throat,\u201d think again. From tongue-twisting consonant clusters to words that sound like they were invented by a stand-up comedian, Dutch is a linguistic playground. Below are ten facts that prove the language is far more colorful than its reputation for windmills and wooden shoes.\n\n---\n\n### 1. *The longest word in Dutch is 53 letters long*  \nMeet **\u201ckindercarnavalsoptochtvoorbereidingswerkzaamhedenplan\u201d**\u2014a single noun describing the preparation work for a children\u2019s carnival parade. Dutch loves to glue words together, so once you master the basics you can create your own kilometer-long compounds on the fly.\n\n---\n\n### 2. Dutch has three genders, but only two articles  \nGrammatically, nouns are masculine, feminine, or neuter. In everyday speech, however, masculine and feminine collapsed into a single \u201ccommon\u201d gender, leaving *\u201cde\u201d* for common and *\u201chet\u201d* for neuter. The result? A never-ending guessing game for learners.\n\n---\n\n### 3. *The Dutch alphabet ends with \u201cij,\u201d not \u201cz\u201d*  \nSchoolchildren recite the ABC and finish with \u201c\u2026x, y, ij, z.\u201d The digraph *ij* is so common (think *hij* = he, *mijn* = my) that it\u2019s treated almost like a separate letter. On old typewriters, *ij* even had its own key.\n\n---\n\n### 4. Dutch is the closest major language to English  \nFrisian is technically closer, but among languages with tens of millions of speakers, Dutch wins. Compare:  \n- English *\u201cbread\u201d* \u2192 Dutch *\u201cbrood\u201d*  \n- English *\u201cgreen\u201d* \u2192 Dutch *\u201cgroen\u201d*  \nShared roots mean you can read simple Dutch signs long before you\u2019ve cracked a single grammar book.\n\n---\n\n### 5. *Dutch gave New York its first words*  \nBefore it was New York, it was New Amsterdam. That\u2019s why we say *\u201ccookie\u201d* (from Dutch *koekje*) instead of *\u201cbiscuit,\u201d* and *\u201cboss\u201d* (from *baas*) instead of *\u201cmaster.\u201d* Wall Street? It once ran alongside the *\u201cwal\u201d* (rampart) of the city wall.\n\n---\n\n### 6. The word *\u201cgezellig\u201d* has no exact English translation*  \nRoughly it means \u201ccozy,\u201d \u201cconvivial,\u201d or \u201cthe warm feeling of being with friends,\u201d but none of those capture the full vibe. A caf\u00e9 can be *gezellig*, a conversation can be *gezellig*, even a spreadsheet can be *gezellig* if the team is laughing.\n\n---\n\n### 7. Dutch spelling changed overnight in 1996  \nThe *\u201cGroene Boekje\u201d* (Green Booklet) reforms simplified thousands of words. *\u201cpotatos\u201d* became *\u201caardappelen\u201d* and *\u201ccat\u2019s\u201d* became *\u201ckatten.\u201d* Overnight, every dictionary, street sign, and Scrabble board in the country was technically out of date.\n\n---\n\n### 8. *Dutch is spoken on six continents*  \nBesides the Netherlands and Belgium, it\u2019s an official language in Suriname, Aruba, Cura\u00e7ao, and Sint Maarten. Thanks to emigration, you\u2019ll also find active Dutch-speaking communities in Canada, Australia, Argentina\u2014even South Africa, where Afrikaans is a daughter language.\n\n---\n\n### 9. The Dutch love diminutives  \nAdd *-je* to almost any noun and you instantly shrink it: *\u201chuis\u201d* (house) \u2192 *\u201chuisje\u201d* (little house). It\u2019s not just about size; diminutives add affection. A *\u201cbiertje\u201d* isn\u2019t a small beer\u2014it\u2019s an invitation to relax.\n\n---\n\n### 10. *Dutch is the language of the world\u2019s oldest multinational*  \nThe Vereenigde Oostindische Compagnie (VOC) was founded in 1602 and issued the planet\u2019s first publicly traded stock. Its ledgers, contracts, and ship logs were all written in Dutch, making the language the original tongue of global capitalism.\n\n---\n\nReady to dive in? Start with *hallo* and *dank je wel*, and you\u2019ll already be halfway to *gezellig*."}], "input_metadata": {"row_id": "design-assume-past-camera-word", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct", "temperature": 0.0, "max_tokens": 4096}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "ground_truth": "2", "evaluation_result": {"score": 1.0, "is_score_valid": true, "reason": "\u2705 Found 39 highlighted sections (required: 2)", "metrics": {}}, "created_at": "2025-08-09T00:23:12.738315", "eval_metadata": {"name": "test_markdown_highlighting_evaluation", "description": "\n    Evaluation function that checks if the model's response contains the required number of formatted sections.\n    ", "version": "v0.2.6.dev22+gfb34cf0.dirty", "status": "finished", "run_id": "analyze-political-poorly-air-hardware", "num_runs": 1, "aggregation_method": "mean", "threshold_of_success": 0.5, "passed": true}, "pid": 68433}, {"messages": [{"role": "user", "content": "Gideon is a farmer who has a surplus of crops from his farm this year. What might he do with that surplus? Highlight at least one section of your answer in markdown, i.e *highlighted section*."}, {"role": "assistant", "content": "Gideon has several good options for his surplus:\n\n1. **Sell it at the local farmers\u2019 market or to a wholesaler** to turn the extra produce into cash that can cover next season\u2019s seed, fertilizer, and equipment costs.  \n2. **Store some of the surplus in a community grain silo** so he can sell later when prices are higher.  \n3. **Donate part of the surplus to the regional food bank**, helping families in need and possibly qualifying for a small tax deduction.  \n4. **Use the surplus as livestock feed** for his own animals, cutting down on purchased feed expenses.  \n\n*The most profitable short-term choice is usually selling the surplus immediately after harvest, when demand is strong and prices are favorable.*"}], "input_metadata": {"row_id": "express-prove-private-internet-goal", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct", "temperature": 0.0, "max_tokens": 4096}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "ground_truth": "1", "evaluation_result": {"score": 1.0, "is_score_valid": true, "reason": "\u2705 Found 5 highlighted sections (required: 1)", "metrics": {}}, "created_at": "2025-08-09T00:23:12.738340", "eval_metadata": {"name": "test_markdown_highlighting_evaluation", "description": "\n    Evaluation function that checks if the model's response contains the required number of formatted sections.\n    ", "version": "v0.2.6.dev22+gfb34cf0.dirty", "status": "finished", "run_id": "analyze-political-poorly-air-hardware", "num_runs": 1, "aggregation_method": "mean", "threshold_of_success": 0.5, "passed": true}, "pid": 68433}, {"messages": [{"role": "user", "content": "Write an outline for a paper on the history of Yemeni coffee. The outline should include the main points of the paper, and at least 15 sections should be highlighted with markdown such as *highlighted section*."}, {"role": "assistant", "content": "# Outline: The History of Yemeni Coffee\n\n1. **Introduction: Why Yemeni Coffee Matters**  \n   - Thesis: Yemeni coffee is not merely a beverage but a cultural, economic, and geopolitical force that shaped global history.  \n   - Scope: From its mythical discovery in 9th-century Yemen to its modern revival.\n\n2. **Pre-Coffee Yemen: The Land, the People, and the Plant**  \n   - Geography of the Yemeni highlands and terraced agriculture.  \n   - Indigenous *Coffea arabica* varieties and early cultivation techniques.  \n   - *The myth of the goatherd Kaldi and the dancing goats*.\n\n3. **The Sufi Spark: Coffee as a Spiritual Elixir**  \n   - 15th-century Sufi monasteries in Mocha and Taiz.  \n   - *Qahwa as a tool for nocturnal dhikr rituals*.  \n   - Early roasting, grinding, and brewing methods.\n\n4. **Mocha: The Birthplace of a Global Commodity**  \n   - Rise of the port city of Mocha (Al-Mukha) in the 16th century.  \n   - *The Mocha trade network linking Yemen, Egypt, and the Ottoman Empire*.  \n   - Coffeehouses as centers of intellectual and political discourse.\n\n5. **The Ottoman Grip and the First Coffee Monopoly**  \n   - 1536: Ottoman conquest of Yemen and control of coffee exports.  \n   - *The ban on fertile beans to prevent cultivation elsewhere*.  \n   - Smuggling and the first cracks in the monopoly.\n\n6. **The Dutch Heist and the Global Spread of Coffee**  \n   - 1616: Dutch traders smuggle Yemeni beans to Batavia (Java).  \n   - *The birth of Dutch colonial coffee plantations*.  \n   - How Mocha\u2019s loss became the world\u2019s gain.\n\n7. **Coffeehouses of Cairo and Istanbul: Yemen\u2019s Cultural Export**  \n   - 1554: First Ottoman coffeehouse in Istanbul.  \n   - *The \u201cSchools of the Wise\u201d in Cairo\u2019s Khan el-Khalili*.  \n   - Coffee as a vector for Yemeni poetry, music, and Sufi thought.\n\n8. **The European Coffee Craze and the Mocha Brand**  \n   - 17th-century European coffeehouses and the \u201cMocha\u201d label.  \n   - *The false association of Mocha with chocolate flavor*.  \n   - Yemeni coffee\u2019s role in the Enlightenment.\n\n9. **Colonial Rivalries and the Decline of Yemeni Dominance**  \n   - 18th-century: French, British, and Dutch plantations outcompete Yemen.  \n   - *The collapse of Mocha\u2019s monopoly and the rise of Caribbean and Latin American coffee*.  \n   - Impact on Yemeni economy and society.\n\n10. **Yemeni Coffee in the Age of Empire**  \n   - 19th-century British and Ottoman interests in Yemeni coffee.  \n   - *The introduction of new cash crops (qat) and their effect on coffee cultivation*.  \n   - Decline in quality and quantity due to neglect and conflict.\n\n11. **The Cold War and the Coffee Crisis**  \n   - 20th-century Yemeni division (North/South) and Soviet/US influences.  \n   - *The shift from coffee to qat in rural livelihoods*.  \n   - Drought, pests, and civil wars further erode production.\n\n12. **The Specialty Coffee Renaissance: Yemen\u2019s Return to Glory**  \n   - 21st-century global demand for single-origin, heirloom coffees.  \n   - *The discovery of \u201cYemenia\u201d varietals and their genetic uniqueness*.  \n   - NGOs and cooperatives reviving traditional farming.\n\n13. **Modern Challenges: War, Climate, and Market Access**  \n   - 2015\u2013present: Saudi-led conflict and blockade.  \n   - *The siege of Hodeidah port and its impact on coffee exports*.  \n   - Climate change and water scarcity in the highlands.\n\n14. **Yemeni Coffee in the Cup: Flavor, Ritual, and Identity**  \n   - Tasting notes: dried fruit, spice, and wine-like acidity.  \n   - *The Yemeni coffee ceremony (bunna) and its social significance*.  \n   - Diaspora communities keeping traditions alive abroad.\n\n15. **Conclusion: The Future of Yemeni Coffee**  \n   - Balancing heritage with innovation.  \n   - *The role of Yemeni coffee in post-war reconstruction*.  \n   - Final reflection: From the highlands of Sana\u2019a to the cups of Brooklyn, Yemeni coffee remains a bridge between past and future."}], "input_metadata": {"row_id": "should-speak-cold-film-festival", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct", "temperature": 0.0, "max_tokens": 4096}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "ground_truth": "15", "evaluation_result": {"score": 1.0, "is_score_valid": true, "reason": "\u2705 Found 30 highlighted sections (required: 15)", "metrics": {}}, "created_at": "2025-08-09T10:35:28.042894", "eval_metadata": {"name": "test_markdown_highlighting_evaluation", "description": "\n    Evaluation function that checks if the model's response contains the required number of formatted sections.\n    ", "version": "v0.2.6-dev1.dev4+g67b03c6", "status": "finished", "run_id": "stop-choose-expect-major-pain", "num_runs": 1, "aggregation_method": "mean", "threshold_of_success": 0.5, "passed": true}, "pid": 43066}, {"messages": [{"role": "user", "content": "Write a cover letter for a job at a local coffee shop in the form of a poem. Highlight at least 5 text sections using \"*\". For example: *3 years of experience*."}, {"role": "assistant", "content": "Dear Brewmaster of *Morning Rituals*,  \nI come not with parchment r\u00e9sum\u00e9,  \nbut with *3 years of experience* tucked in my apron pocket,  \nsteam-warm and espresso-dark.  \n\nI have *memorized 47 regulars\u2019 names*\u2014  \neach syllable frothed like oat-milk foam\u2014  \nand I know that *Carla takes two raw sugars*  \nwhile *Mr. Lee prefers his cappuccino bone-dry*.  \n\nI can *dial in a grinder* at 5:03 a.m.  \nto the *exact 18.5 grams* that coax  \ncaramel notes from Ethiopian beans,  \nand still greet the line with *a sunrise smile*.  \n\nMy wrists speak in *latte-art cursive*:  \nhearts, rosettas, and once (on a dare)  \na tiny saxophone for the jazz trio  \nwho tips in *harmonies and quarters*.  \n\nI *close the shop solo* without a single bean left homeless,  \ncount the till to the *penny and a poem*,  \nand leave the milk pitchers *mirror-bright*,  \nso the night shift sees its own reflection of care.  \n\nHire me, and the *scent of fresh-ground hope*  \nwill greet every customer at the door;  \nI\u2019ll be the *steady hand* behind the bar,  \npouring *courage in ceramic*, one cup at a time.  \n\nYours in steam and service,  \nRiley"}], "input_metadata": {"row_id": "destroy-rational-cultural-available-meeting", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct", "temperature": 0.0, "max_tokens": 4096}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "ground_truth": "5", "evaluation_result": {"score": 1.0, "is_score_valid": true, "reason": "\u2705 Found 16 highlighted sections (required: 5)", "metrics": {}}, "created_at": "2025-08-09T10:35:28.042934", "eval_metadata": {"name": "test_markdown_highlighting_evaluation", "description": "\n    Evaluation function that checks if the model's response contains the required number of formatted sections.\n    ", "version": "v0.2.6-dev1.dev4+g67b03c6", "status": "finished", "run_id": "stop-choose-expect-major-pain", "num_runs": 1, "aggregation_method": "mean", "threshold_of_success": 0.5, "passed": true}, "pid": 43066}, {"messages": [{"role": "user", "content": "Write a casual blog post about similarities across animal species. Highlight at least 5 sections in your answer by starting and ending with \"*\", like: *highlighted text section*."}, {"role": "assistant", "content": "Hey friends! Grab a coffee and let\u2019s talk about something that always blows my mind: *how much we all have in common, even when we look nothing alike*. I was scrolling through wildlife photos the other night and realized that a jellyfish and I are basically running the same basic software\u2014just with different packaging. So here\u2019s a laid-back tour of five \u201cwait, we do that too?!\u201d moments across the animal kingdom.\n\n*We all need our beauty sleep*  \nTurns out, every animal with a nervous system\u2014from fruit flies to blue whales\u2014shares a version of sleep. Crows nap on tree branches, octopuses snooze in coconut-shell beds, and even roundworms have \u201clethargus,\u201d which is basically worm REM. Scientists think sleep is so ancient that the last common ancestor of all animals already had it. So next time you hit snooze, remember you\u2019re honoring a 700-million-year-old tradition.\n\n*Love songs come in many dialects*  \nWhether you\u2019re a human humming in the shower or a humpback whale belting out 30-minute ballads, *we all use rhythm and pitch to flirt and bond*. Frogs croak in choruses, nightingales remix their playlists nightly, and male mice sing ultrasonic \u201cpickup lines\u201d too high for us to hear. Same goal, different instruments.\n\n*Grandma knew best: family matters*  \nElephants babysit their nieces, wolves bring back take-out for auntie, and *clownfish dads are the ultimate stay-at-home fathers*. Across species, kinship networks boost survival. Even bacteria split into \u201csibling\u201d lineages that cooperate\u2014so the next time your cousin crashes on your couch, call it evolutionary strategy.\n\n*Stress looks the same on everyone*  \nWhen life gets spicy, our bodies flip the same ancient switch. *Cortisol floods fish, birds, and humans alike*, making hearts race and muscles twitch. Zebra finches lose their appetite during tough times, and stressed-out bees become pessimistic (yes, they can test bee mood). So if you\u2019re doom-scrolling at 2 a.m., know that a stressed squirrel in your backyard totally gets you.\n\n*Play isn\u2019t optional\u2014it\u2019s homework for life*  \nOtters juggle rocks, dogs zoom in circles, and young ravens snowboard down snowy roofs using jar lids. *Play builds brains and social skills across the board*. Researchers have spotted reptiles \u201cwrestling\u201d for fun and octopuses repeatedly bouncing Lego blocks like toddlers. If it has neurons, it probably needs recess.\n\nNext time you feel alone in the universe, remember that somewhere a parrot is probably preening its best friend, a bat is sharing food with a hungry roost-mate, and a tiny nematode is curling up for a nap. We\u2019re all just variations on a very old, very shared theme."}], "input_metadata": {"row_id": "provide-cool-meeting-kid-book", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct", "temperature": 0.0, "max_tokens": 4096}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "ground_truth": "5", "evaluation_result": {"score": 1.0, "is_score_valid": true, "reason": "\u2705 Found 10 highlighted sections (required: 5)", "metrics": {}}, "created_at": "2025-08-09T10:35:28.042963", "eval_metadata": {"name": "test_markdown_highlighting_evaluation", "description": "\n    Evaluation function that checks if the model's response contains the required number of formatted sections.\n    ", "version": "v0.2.6-dev1.dev4+g67b03c6", "status": "finished", "run_id": "stop-choose-expect-major-pain", "num_runs": 1, "aggregation_method": "mean", "threshold_of_success": 0.5, "passed": true}, "pid": 43066}, {"messages": [{"role": "user", "content": "Generate a business proposal to start a sweatshirt company in Bremen. The proposal should contain 5 or more sections. Highlight each section name using the this format:\n*section name*"}, {"role": "assistant", "content": "*Executive Summary*\nBremen Sweat Co. will be a premium, sustainability-driven sweatshirt brand headquartered in Bremen, Germany. We will design, manufacture, and sell limited-edition sweatshirts that combine Nordic minimalism with Hanseatic maritime heritage. Our mission is to become Bremen\u2019s first carbon-neutral apparel label while capturing 5 % of the city\u2019s \u20ac120 million annual streetwear market within three years. Initial funding of \u20ac250 k will cover product development, a flagship micro-factory/store in the Viertel district, and a digital-first go-to-market strategy.\n\n*Market Analysis*\nBremen\u2019s population of 570 k includes 120 k university students and 80 k creative-industry professionals\u2014our core demographic. Streetwear sales in Northern Germany grew 11 % YoY (Statista 2023), yet only 8 % of brands are locally produced. Competitor benchmarking shows price gaps: fast-fashion sweatshirts sell at \u20ac25\u201335, mid-tier at \u20ac60\u201380, and premium sustainable labels at \u20ac110\u2013140. We will occupy the \u20ac85\u201395 niche with superior materials and storytelling. Surveys (n = 400) reveal 68 % of Bremen consumers would pay a 20 % premium for locally made, eco-certified apparel.\n\n*Product & Sustainability Strategy*\nEach sweatshirt will be crafted from 100 % GOTS-certified organic cotton and recycled ocean-bound polyester, dyed in Bremen using closed-loop water systems. Designs will feature subtle references to Bremen\u2019s Town Musicians, ship masts, and Weser river textures. Limited drops of 300 units per style ensure scarcity and collectability. Blockchain-based QR codes will provide end-to-end transparency: fiber origin, factory GPS, and carbon footprint per garment (-2.1 kg CO\u2082e vs. industry average 8.5 kg). End-of-life take-back program offers 15 % store credit for returned items, feeding our upcycling line.\n\n*Operations & Supply Chain*\nMicro-factory (180 m\u00b2) in the Viertel will house 4 digital knitting machines and 6 seamstresses, producing 1 200 sweatshirts/month. Local sourcing: yarn from OEKO-TEX mill in Oldenburg, trims from Hamburg port suppliers. Just-in-time production reduces inventory risk by 40 %. Bremen\u2019s port infrastructure enables low-impact inbound logistics; outbound shipments leverage DHL GoGreen within Germany (24 h delivery). ISO 14001 certification targeted by month 12.\n\n*Marketing & Sales Channels*\nPhase 1 (Months 1\u20136): Pop-up store at Lloyd Passage, Instagram/TikTok campaigns with Bremen influencers (@hanseaticstyle, 180 k followers). Phase 2: Flagship store launch with AR fitting mirrors and Weser riverfront window projection art. E-commerce via Shopify Plus, offering 3-D configurator for sleeve embroidery. Wholesale partnerships with 5 concept stores in Berlin, Hamburg, and Copenhagen. Revenue split: 60 % DTC, 25 % online, 15 % wholesale. Loyalty app rewards sustainable actions (bike delivery, clothing swaps).\n\n*Financial Projections & Funding*\nYear 1: Revenue \u20ac420 k, gross margin 58 %, EBITDA -\u20ac30 k (investment phase). Year 2: Revenue \u20ac950 k, EBITDA 12 %. Year 3: Revenue \u20ac1.8 m, EBITDA 22 %. Break-even month 18. Initial ask: \u20ac250 k seed (\u20ac150 k equipment, \u20ac60 k marketing, \u20ac40 k working capital). Investor IRR 38 % over 5 years with 20 % equity offered. ESG-linked loan option with Bremen Green Finance Initiative at 2.1 % interest."}], "input_metadata": {"row_id": "delete-stale-similar-others-email", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct", "temperature": 0.0, "max_tokens": 4096}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "ground_truth": "5", "evaluation_result": {"score": 1.0, "is_score_valid": true, "reason": "\u2705 Found 6 highlighted sections (required: 5)", "metrics": {}}, "created_at": "2025-08-09T10:35:28.042989", "eval_metadata": {"name": "test_markdown_highlighting_evaluation", "description": "\n    Evaluation function that checks if the model's response contains the required number of formatted sections.\n    ", "version": "v0.2.6-dev1.dev4+g67b03c6", "status": "finished", "run_id": "stop-choose-expect-major-pain", "num_runs": 1, "aggregation_method": "mean", "threshold_of_success": 0.5, "passed": true}, "pid": 43066}, {"messages": [{"role": "user", "content": "Write a funny and sarcastic template for rating the quality of a marriage between two people who are both moms. This is for the couple themselves. Please highlight at least 3 sections with markdown,  i.e *highlighted section*."}, {"role": "assistant", "content": "**The Official \u201cHow Are We Still Married?\u201d Scorecard**  \n*For Two Moms Who\u2019ve Survived Sleepless Nights, PTA Wars, and Each Other\u2019s Cooking*  \n\n---\n\n**1. Morning Efficiency Rating**  \n*How quickly can we get two kids, one dog, and zero coffee spills out the door before someone passive-aggressively \u201creminds\u201d the other that *\u201cthe permission slip was due yesterday\u201d*?*  \n- 10/10: We\u2019re a synchronized swim team of lunchboxes and existential dread.  \n- 5/10: One of us is still Googling \u201chow to unclog a glue stick from a USB port.\u201d  \n- 1/10: We\u2019re late because someone had to *\u201cjust quickly\u201d* reorganize the spice rack alphabetically.  \n\n---\n\n**2. Co-Parenting Coordination**  \n*Rate the likelihood that we\u2019ll both remember which kid is allergic to *\u201cthe orange cheese\u201d* and which one is in a *\u201cbe kind to worms\u201d* phase.*  \n- 10/10: We\u2019ve got a shared Google calendar, color-coded to the point of *\u201cmilitant whimsy.\u201d*  \n- 5/10: We rely on telepathy and the faint hope that the school nurse *\u201cjust knows.\u201d*  \n- 1/10: We\u2019ve accidentally sent the wrong kid to violin practice *twice*. The violin teacher now thinks we\u2019re running a *\u201cchild exchange program.\u201d*  \n\n---\n\n**3. Romance Maintenance**  \n*How often do we prioritize date night over *\u201ccollapsing into a heap of mutual resentment\u201d*?*  \n- 10/10: We\u2019ve scheduled a candlelit dinner for *\u201cwhen the kids go to college.\u201d*  \n- 5/10: Our last date was a *\u201cromantic\u201d* trip to Target where we *\u201caccidentally\u201d* held hands in the freezer aisle.  \n- 1/10: We consider *\u201csimultaneously ignoring the laundry\u201d* a form of intimacy.  \n\n---\n\n**Bonus Round: Petty Argument Hall of Fame**  \n*Check all that apply:*  \n- [ ] The Great 2019 Dishwasher Loading Schism (*\u201cKnives point DOWN, Karen!\u201d*)  \n- [ ] The Passive-Aggressive Post-it War of 2021 (*\u201cYour turn to buy *\u2018organic\u2019* milk. XOXO\u201d*)  \n- [ ] The Thermostat Cold War (*\u201c68\u00b0F is a *human rights violation*.\u201d*)  \n\n---\n\n**Final Score Interpretation:**  \n- **30\u201340 points:** *You\u2019re the Beyonc\u00e9 and Jay-Z of lesbian moms.*  \n- **15\u201329 points:** *You\u2019re more like *\u201cThelma and Louise,\u201d* but with minivans and a Costco membership.*  \n- **0\u201314 points:** *You\u2019re still married, which is statistically *\u201ca win.\u201d* Celebrate with *\u201cleftover chicken nuggets\u201d* and a *\u201cwe\u2019ll deal with this tomorrow\u201d* shrug.*"}], "input_metadata": {"row_id": "remain-large-left-team-wish", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct", "temperature": 0.0, "max_tokens": 4096}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "ground_truth": "3", "evaluation_result": {"score": 1.0, "is_score_valid": true, "reason": "\u2705 Found 39 highlighted sections (required: 3)", "metrics": {}}, "created_at": "2025-08-09T10:35:28.043019", "eval_metadata": {"name": "test_markdown_highlighting_evaluation", "description": "\n    Evaluation function that checks if the model's response contains the required number of formatted sections.\n    ", "version": "v0.2.6-dev1.dev4+g67b03c6", "status": "finished", "run_id": "stop-choose-expect-major-pain", "num_runs": 1, "aggregation_method": "mean", "threshold_of_success": 0.5, "passed": true}, "pid": 43066}, {"messages": [{"role": "user", "content": "Write a song about the summers of my childhood that I spent in the countryside. Give the song a name, and highlight the name by wrapping it with *. For example: *little me in the countryside*."}, {"role": "assistant", "content": "*little me in the countryside*\n\n[Verse 1]  \nBarefoot on the gravel road, dust between my toes,  \nSunlight dripped like honey on the corn rows.  \nGrandma\u2019s porch swing creaked the same old tune,  \nWhile fireflies spelled my name across June.  \nMason jar of lemonade, half a moon inside,  \nI chased the shade like it was trying to hide.  \n\n[Chorus]  \nOh, little me in the countryside,  \nRunning through the wheat so high,  \nEvery breeze was a lullaby,  \nAnd the sky was big enough to fly.  \nI left my worries by the riverside,  \nBack when I was little me in the countryside.  \n\n[Verse 2]  \nTractor songs at twilight, crickets keeping time,  \nDirt on my knees, freckles in a climbing rhyme.  \nCreek water baptized my sunburned skin,  \nI believed every ripple was a secret twin.  \nUnder the quilt of stars, I counted dreams instead,  \nWhile the Milky Way braided night through my head.  \n\n[Chorus]  \nOh, little me in the countryside,  \nRunning through the wheat so high,  \nEvery breeze was a lullaby,  \nAnd the sky was big enough to fly.  \nI left my worries by the riverside,  \nBack when I was little me in the countryside.  \n\n[Bridge]  \nNow the city lights flicker, but I still close my eyes\u2014  \nI can smell the clover, hear the distant barn owls\u2019 cries.  \nTime may stretch the map, but it can\u2019t erase  \nThe echo of my laughter in that endless open space.  \n\n[Final Chorus]  \nOh, little me in the countryside,  \nKeep the gate open, I\u2019m still nearby.  \nThough the roads have changed and the years have flown,  \nA part of me is always home.  \nIn every golden field I drive beside,  \nI find little me in the countryside."}], "input_metadata": {"row_id": "read-wish-deliver-great-case", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct", "temperature": 0.0, "max_tokens": 4096}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "ground_truth": "1", "evaluation_result": {"score": 1.0, "is_score_valid": true, "reason": "\u2705 Found 1 highlighted sections (required: 1)", "metrics": {}}, "created_at": "2025-08-09T10:35:28.043042", "eval_metadata": {"name": "test_markdown_highlighting_evaluation", "description": "\n    Evaluation function that checks if the model's response contains the required number of formatted sections.\n    ", "version": "v0.2.6-dev1.dev4+g67b03c6", "status": "finished", "run_id": "stop-choose-expect-major-pain", "num_runs": 1, "aggregation_method": "mean", "threshold_of_success": 0.5, "passed": true}, "pid": 43066}, {"messages": [{"role": "user", "content": "Write a riddle for the word \"fa\u00e7ade\" that contains at least 3 italic text phrases in markdown syntax, i.e *italic text*."}, {"role": "assistant", "content": "I wear a *painted smile* for every passer-by,  \na *perfectly symmetrical* face that never cracks.  \nBehind me, the *crumbling bricks* sigh in secret.  \nWhat am I?"}], "input_metadata": {"row_id": "do-test-spicy-dead-television", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct", "temperature": 0.0, "max_tokens": 4096}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "ground_truth": "3", "evaluation_result": {"score": 1.0, "is_score_valid": true, "reason": "\u2705 Found 3 highlighted sections (required: 3)", "metrics": {}}, "created_at": "2025-08-09T10:35:28.043069", "eval_metadata": {"name": "test_markdown_highlighting_evaluation", "description": "\n    Evaluation function that checks if the model's response contains the required number of formatted sections.\n    ", "version": "v0.2.6-dev1.dev4+g67b03c6", "status": "finished", "run_id": "stop-choose-expect-major-pain", "num_runs": 1, "aggregation_method": "mean", "threshold_of_success": 0.5, "passed": true}, "pid": 43066}, {"messages": [{"role": "user", "content": "Write a template for a workshop on the importance of diversity in the workplace and highlight at least 3 sections with markdown, i.e. *highlighted section*."}, {"role": "assistant", "content": "# Workshop Template  \n**Title:** *Building Inclusive Excellence: The Business & Human Case for Workplace Diversity*  \n**Duration:** 3.5 hours (can be split into two 90-minute sessions)  \n**Audience:** Managers, HR professionals, team leads, ERG members, and any employee who influences culture  \n\n---\n\n## 1. Welcome & Objectives (15 min)\n- Ice-breaker: \u201cTwo Truths & a Lie\u2014Diversity Edition\u201d  \n- *Learning objectives*: By the end of this workshop, participants will be able to:  \n  1. Define diversity, equity, inclusion, and belonging (DEIB) in their own words.  \n  2. Articulate at least three evidence-based benefits of a diverse workforce.  \n  3. Identify one action they can take within 30 days to foster inclusion on their team.  \n\n---\n\n## 2. *The Data-Driven Case for Diversity* (25 min)\n- Interactive poll: \u201cWhat % performance lift do you think diverse teams achieve?\u201d  \n- Mini-lecture with live charts:  \n  - McKinsey 2023 report: Top-quartile companies for ethnic/gender diversity are **39 % more likely to outperform** on profitability.  \n  - Harvard study: *Inclusive teams make better business decisions 87 % of the time* and do so twice as fast.  \n- Small-group exercise: Match the statistic to the business outcome (revenue, innovation, retention).  \n\n---\n\n## 3. *Unconscious Bias & Micro-behaviors* (30 min)\n- 5-minute video: \u201cThe Resume with Two Names\u201d (identical CVs, different callbacks).  \n- Bias Busters card game: Participants draw scenario cards and practice interrupting bias in real time.  \n- *Key takeaway*: Bias is not a moral failing; it is a neurological shortcut that can be rewired through deliberate habits.  \n\n---\n\n## 4. *Inclusive Leadership Toolkit* (40 min)\n- **The 4 A\u2019s Framework**:  \n  1. **Awareness** \u2013 Daily 3-minute bias check-in.  \n  2. **Amplification** \u2013 Repeat & credit ideas from under-represented voices.  \n  3. **Access** \u2013 Rotate high-visibility assignments.  \n  4. **Accountability** \u2013 Track inclusion metrics on team dashboards.  \n- Role-play: \u201cThe Missed Promotion\u201d \u2013 diagnose where each \u2018A\u2019 broke down and redesign the conversation.  \n\n---\n\n## 5. Action Planning & Commitments (20 min)\n- Personal *Inclusion Commitment Card*: \u201cIn the next 30 days I will\u2026\u201d  \n- Pair-share for accountability.  \n- QR code to a digital resource hub (TED talks, reading list, policy templates).  \n\n---\n\n## 6. Closing & Evaluation (10 min)\n- One-word checkout: \u201cHow do you feel now?\u201d  \n- *Post-workshop survey* link (2 min) with prize draw for completion.  \n\n---\n\n## Materials Checklist\n- Slide deck (with alt-text on all images)  \n- Printed Commitment Cards & pens  \n- Bias Busters card decks (laminated)  \n- Timer & bell  \n- Snacks labeled for common dietary restrictions (gluten-free, vegan, halal)  \n\n---\n\n## Follow-Up Email Template (send within 24 hrs)\nSubject: Your Inclusion Toolkit + Next Steps  \nBody:  \nHi team,  \nThank you for leaning into today\u2019s conversation on *why diversity wins*. Attached are:  \n- Slide deck & resource links  \n- Your signed Inclusion Commitment Card (photo reminder)  \n- Calendar invite for the optional 30-day check-in circle  \n\nRemember: Inclusion is a practice, not a one-time training. Let\u2019s keep the momentum!"}], "input_metadata": {"row_id": "protect-other-major-green-war", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct", "temperature": 0.0, "max_tokens": 4096}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "ground_truth": "3", "evaluation_result": {"score": 1.0, "is_score_valid": true, "reason": "\u2705 Found 19 highlighted sections (required: 3)", "metrics": {}}, "created_at": "2025-08-09T10:35:28.043094", "eval_metadata": {"name": "test_markdown_highlighting_evaluation", "description": "\n    Evaluation function that checks if the model's response contains the required number of formatted sections.\n    ", "version": "v0.2.6-dev1.dev4+g67b03c6", "status": "finished", "run_id": "stop-choose-expect-major-pain", "num_runs": 1, "aggregation_method": "mean", "threshold_of_success": 0.5, "passed": true}, "pid": 43066}, {"messages": [{"role": "user", "content": "Write a cover letter to a local political party, asking to be their rally organizer. Make sure to highlight at least 3 sections in your answer in markdown format."}, {"role": "assistant", "content": "**[Your Name]**  \n[Street Address]  \n[City, State ZIP]  \n[Email Address] | [Phone Number] | [LinkedIn URL]  \n[Date]\n\n**[Local Party Chairperson\u2019s Name]**  \n[Local Party Name]  \n[Party Headquarters Address]  \n[City, State ZIP]\n\nDear **Chairperson [Last Name]**,\n\nI am writing to express my enthusiastic interest in serving as **Rally Organizer** for **[Local Party Name]**. As a lifelong resident of **[City/County]**, I have watched our community\u2019s political pulse beat strongest when neighbors gather in common purpose. I want to channel that energy into rallies that not only energize our base but also invite undecided voters to witness the power of our shared vision.\n\n### **Proven Event-Execution Expertise**\nDuring the 2022 mid-term cycle, I spearheaded a **three-county voter-registration caravan** that staged 14 pop-up events in 30 days.  \n- **Logistics**: Coordinated permits, portable stages, and ADA-compliant access for 2,000+ attendees per stop.  \n- **Volunteer Mobilization**: Recruited and trained 120 volunteers, creating a color-coded \u201cRally-in-a-Box\u201d toolkit now used by three neighboring county parties.  \n- **Real-Time Adaptability**: When a thunderstorm threatened our largest outdoor rally, I pivoted to an indoor gymnasium within 90 minutes\u2014live-streaming the switch to maintain momentum and ultimately exceeding projected turnout by 18 %.\n\n### **Data-Driven Rally Strategy**\nI treat every rally as a **micro-campaign laboratory**.  \n- **Pre-Event Analytics**: I overlay precinct-level voter-file data with heat-mapping tools to select venues within 0.7 miles of our highest \u201cpersuasion index\u201d households.  \n- **On-Site Engagement**: QR-coded wristbands linked to instant sign-up forms increased volunteer recruitment by 34 % compared to clipboard methods.  \n- **Post-Event ROI**: My post-rally surveys (average 42 % response rate) feed a dashboard that tracks pledge-to-action conversion, allowing us to refine messaging within 48 hours.\n\n### **Coalition-Building & Safety Leadership**\nUnderstanding that rallies must feel **safe and inclusive**, I have:  \n- Partnered with local **faith leaders, PTAs, and small-business associations** to co-sponsor events, expanding our demographic reach by 27 %.  \n- Completed **FEMA crowd-management certification** and maintain relationships with county emergency management to ensure rapid-response protocols.  \n- Implemented a **\u201cCommunity Marshals\u201d** program\u2014training 50 volunteer marshals in de-escalation techniques, cutting incident reports to zero across 22 events.\n\nI would welcome the opportunity to discuss how my blend of grassroots passion, logistical precision, and data fluency can help **[Local Party Name]** turn every rally into a springboard for victory in the upcoming cycle. Thank you for considering my application.\n\n**Respectfully,**\n\n[Your Signature (if submitting hard copy)]  \n[Your Printed Name]"}], "input_metadata": {"row_id": "collect-destroy-difficult-conference-hardware", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct", "temperature": 0.0, "max_tokens": 4096}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "ground_truth": "3", "evaluation_result": {"score": 1.0, "is_score_valid": true, "reason": "\u2705 Found 23 highlighted sections (required: 3)", "metrics": {}}, "created_at": "2025-08-09T10:35:28.043121", "eval_metadata": {"name": "test_markdown_highlighting_evaluation", "description": "\n    Evaluation function that checks if the model's response contains the required number of formatted sections.\n    ", "version": "v0.2.6-dev1.dev4+g67b03c6", "status": "finished", "run_id": "stop-choose-expect-major-pain", "num_runs": 1, "aggregation_method": "mean", "threshold_of_success": 0.5, "passed": true}, "pid": 43066}, {"messages": [{"role": "user", "content": "The opposite of youth is not age, but ...? Highlight at least 2 sections in your answer with markdown, i.e. *highlighted section*."}, {"role": "assistant", "content": "The opposite of youth is not age, but **stagnation**\u2014the moment when *curiosity closes its eyes* and the impulse to explore, question, and reinvent quietly fades.  \n\nWhere youth is a *perpetual state of becoming*, its true counter is the condition in which *growth is willingly surrendered* and the mind settles into fixed certainties."}], "input_metadata": {"row_id": "build-upgrade-green-white-reason", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct", "temperature": 0.0, "max_tokens": 4096}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "ground_truth": "2", "evaluation_result": {"score": 1.0, "is_score_valid": true, "reason": "\u2705 Found 4 highlighted sections (required: 2)", "metrics": {}}, "created_at": "2025-08-09T10:35:28.043147", "eval_metadata": {"name": "test_markdown_highlighting_evaluation", "description": "\n    Evaluation function that checks if the model's response contains the required number of formatted sections.\n    ", "version": "v0.2.6-dev1.dev4+g67b03c6", "status": "finished", "run_id": "stop-choose-expect-major-pain", "num_runs": 1, "aggregation_method": "mean", "threshold_of_success": 0.5, "passed": true}, "pid": 43066}, {"messages": [{"role": "user", "content": "Write a description of the following data in a weird style: The Golden Palace eatType restaurant; The Golden Palace food Indian; The Golden Palace area city centre. Use markdown to highlight at least 3 sections in your answer."}, {"role": "assistant", "content": "**The Golden Palace** is not a palace at all, but a **restaurant** that has disguised itself as a mirage of saffron and cardamom in the **city centre**. It whispers **Indian** recipes into the ears of pigeons, who then forget how to fly and instead waddle around the square, dizzy with cumin.  \n\nInside, the walls sweat **garam masala** and the tables grow tiny **basil plants** that recite Bollywood dialogues when no one is looking. The waiters glide like **shadow puppets**, balancing trays of **butter chicken** that glow like radioactive sunsets.  \n\nIf you order **naan**, it arrives folded into the shape of a **paper swan**, flapping once before dissolving into steam that smells of **monsoon rain**. The bill is presented on a **lotus leaf**, and if you stare at it long enough, the numbers rearrange themselves into a **limerick about turmeric**."}], "input_metadata": {"row_id": "need-hostile-dream-business-person", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct", "temperature": 0.0, "max_tokens": 4096}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "ground_truth": "3", "evaluation_result": {"score": 1.0, "is_score_valid": true, "reason": "\u2705 Found 13 highlighted sections (required: 3)", "metrics": {}}, "created_at": "2025-08-09T10:35:28.043177", "eval_metadata": {"name": "test_markdown_highlighting_evaluation", "description": "\n    Evaluation function that checks if the model's response contains the required number of formatted sections.\n    ", "version": "v0.2.6-dev1.dev4+g67b03c6", "status": "finished", "run_id": "stop-choose-expect-major-pain", "num_runs": 1, "aggregation_method": "mean", "threshold_of_success": 0.5, "passed": true}, "pid": 43066}, {"messages": [{"role": "user", "content": "Write a funny rap about a man who gets a call from an official saying that he is a long lost relative of the king of Nigeria. Use markdown to highlight at least one section of your answer, i.e. *highlighted section*."}, {"role": "assistant", "content": "\ud83c\udfa4 *Yo, it\u2019s the tale of Sir Scam-a-lot, straight outta suburbia!* \ud83c\udfa4  \n\nYo, my name is Dave, I sell insurance in Ohio,  \nGot a cubicle, a cat, and a 401(k) that\u2019s kinda fly-o.  \nOne Tuesday, phone rings\u2014unknown number, what the heck?  \nVoice says, \u201c*Prince Ogbonna the Third* just wrote you a fat check!\u201d  \n\nI\u2019m like, \u201cHold up, who dis? I\u2019m whiter than printer paper,  \nMy bloodline\u2019s mayo on Wonder Bread\u2014ain\u2019t no royal vapor!\u201d  \nDude on the line, accent silkier than lotion,  \nSays, \u201cCousin, you\u2019re 37th in line\u2014time to claim your ocean!\u201d  \n\n**[Hook]**  \n\ud83c\udfb6 *Long lost king of Nigeria, crown sittin\u2019 on my dome,*  \n*Got a palace full of leopards and a Wi-Fi that roams!*  \n*They need my SSN and a quick wire fee,*  \n*But I\u2019m \u2018bout to be big pimpin\u2019 in Abuja, just wait and see!* \ud83c\udfb6  \n\nHe says, \u201cJust send five grand for customs, then the jet lands,  \nYou\u2019ll be dripping in so much gold, even your sweat glands bling.\u201d  \nI\u2019m sweatin\u2019 like a rotisserie, visions of caviar dreams,  \nI Venmo \u201cPrinceNaija69\u201d like I\u2019m funding meme schemes.  \n\nNext day, email: \u201cOops, palace goat ate the transfer slip,  \nNeed ten more for goat surgery and a diamond-encrusted whip.\u201d  \nI hock my lawnmower, my Xbox, and Aunt Linda\u2019s porcelain goose,  \nWire cash so fast my bank\u2019s like, \u201cDave, you loose!\u201d  \n\n**[Bridge \u2013 Spoken Word]**  \nThen the *official royal seal* arrives\u2014  \nIt\u2019s a GIF of Shrek in a crown doing the Macarena.  \nI\u2019m starting to sense a pattern here\u2026  \n\n**[Final Hook \u2013 Regret Remix]**  \n\ud83c\udfb6 *Now I\u2019m the king of my mom\u2019s basement, throne\u2019s a beanbag chair,*  \n*Royal decree: never trust a prince who types in all caps\u2014*  \n*\u201cURGENT, COUSIN, SEND MORE FLAMINGOS!\u201d*  \n*Lesson learned: if the crown\u2019s too shiny, it\u2019s probably aluminum foil, homie!*"}], "input_metadata": {"row_id": "store-reduce-unknown-girl-hate", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct", "temperature": 0.0, "max_tokens": 4096}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "ground_truth": "1", "evaluation_result": {"score": 1.0, "is_score_valid": true, "reason": "\u2705 Found 14 highlighted sections (required: 1)", "metrics": {}}, "created_at": "2025-08-09T10:35:28.043206", "eval_metadata": {"name": "test_markdown_highlighting_evaluation", "description": "\n    Evaluation function that checks if the model's response contains the required number of formatted sections.\n    ", "version": "v0.2.6-dev1.dev4+g67b03c6", "status": "finished", "run_id": "stop-choose-expect-major-pain", "num_runs": 1, "aggregation_method": "mean", "threshold_of_success": 0.5, "passed": true}, "pid": 43066}, {"messages": [{"role": "user", "content": "Expand the riddle into a story with a funny tone:\n\nWhat can you catch but not throw?\nA cold\n\nUse * to highlight at least 2 sections in your text. For example: *this is a highlighted text section*."}, {"role": "assistant", "content": "Gary the Germ was the *smallest, greenest, and most obnoxiously proud* microbe in the entire Sniffle Kingdom. Every morning he practiced his grand entrance: sliding down banisters made of nasal hair, cannon-balling into unsuspecting tonsils, and yelling, \u201cACHOO\u2014did somebody order a cold?\u201d  \n\nOne Tuesday, Gary spotted his dream target: *Bartholomew \u201cBart\u201d Bigglesworth, a man who believed hand-washing was a government conspiracy*. Bart had just exited the subway, triumphantly licking a subway pole \u201cfor the vitamins.\u201d Gary rubbed his microscopic hands together. \u201cPerfect. I\u2019ll hitch a ride on that finger, ride the express elevator to the sinuses, and throw the *snottiest* house-warming party in history!\u201d  \n\nBut there was a catch\u2014literally. Gary could *catch* Bart, but he couldn\u2019t *throw* him anywhere. Every time Gary tried to catapult Bart into the next office cubicle, Bart just sneezed, wiped his nose on his sleeve, and kept typing spreadsheets. Gary screamed, \u201cThis is mutiny! I\u2019m supposed to be the one doing the tossing!\u201d  \n\nMeanwhile, Bart\u2019s immune system\u2014led by Captain Phlegm and the Mucus Marines\u2014launched *Operation Tissue Storm*. They surrounded Gary, armed with vitamin C grenades and zinc zappers. Gary attempted a dramatic escape by surfing a sneeze, but the tissue slapped him like a flyswatter on a summer day.  \n\nDefeated, Gary sat in the crumpled tissue, sulking. \u201cI caught the cold,\u201d he muttered, \u201cbut I can\u2019t throw it anywhere. Worst. Superpower. Ever.\u201d  \n\nFrom that day forward, Gary changed his business card to read: \u201cProfessional Cold Catcher\u2014No Returns, No Refunds, Absolutely No Throwing.\u201d"}], "input_metadata": {"row_id": "mean-take-left-high-family", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct", "temperature": 0.0, "max_tokens": 4096}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "ground_truth": "2", "evaluation_result": {"score": 1.0, "is_score_valid": true, "reason": "\u2705 Found 6 highlighted sections (required: 2)", "metrics": {}}, "created_at": "2025-08-09T10:35:28.043233", "eval_metadata": {"name": "test_markdown_highlighting_evaluation", "description": "\n    Evaluation function that checks if the model's response contains the required number of formatted sections.\n    ", "version": "v0.2.6-dev1.dev4+g67b03c6", "status": "finished", "run_id": "stop-choose-expect-major-pain", "num_runs": 1, "aggregation_method": "mean", "threshold_of_success": 0.5, "passed": true}, "pid": 43066}, {"messages": [{"role": "user", "content": "Write a rap about the renaissance. It should be noticeably different from raps about other historical eras, and have an interesting or weird tone. Highlight at least 3 sections in your answer in markdown format."}, {"role": "assistant", "content": "**[Intro \u2013 harpsichord glitch beat, stylus scratching parchment]**  \nYo, it\u2019s 14-something, but my mind\u2019s 3-D printed in gold leaf\u2014  \nI\u2019m the **Renaissance Rapper**, got a quill in one hand and a scalpel in the other, dissectin\u2019 beats like Vesalius dissectin\u2019 spleens.  \nLet\u2019s get *weird* with the rebirth, baby.\n\n---\n\n### **Section 1 \u2013 The Fresco Flex**  \nI drip ultramarine like I\u2019m sponsored by Afghanistan mines,  \nMichelangelo on the ceiling, I\u2019m reclining on divine lines.  \nAngels doing backflips off the Sistine, they\u2019re hype for my set,  \nWhile I tag \u201c*AMOR VINCIT OMNIA*\u201d in invisible ink\u2014wet.  \nMy flow\u2019s a trompe-l\u2019\u0153il, looks flat till you tilt your head\u2014  \nThen it\u2019s depth for days, like Brunelleschi\u2019s dome overhead.  \nI\u2019m not baroque, I\u2019m pre-baroque, I\u2019m the prototype shock,  \nGot patrons in fur-lined sleeves stuffing florins in my sock.  \n**I paint in egg tempera so the colors never fade,**  \n**But the yolk smells funky\u2014call it *odeur de old crusade*.**\n\n---\n\n### **Section 2 \u2013 The Printing-Press Poltergeist**  \nGutenberg ghost in the machine, clanking lead type like chains,  \nEvery page a polter-page, every serif a soul that remains.  \nI drop a folio, it\u2019s viral before \u201cviral\u201d was a cough,  \nMonks in scriptoriums rage-quitting, \u201cBro, that illuminated stuff\u2019s off!\u201d  \nI\u2019m the incunabula influencer, my colophon\u2019s a flex,  \nGot red-ink rubricators sliding into my DMs\u2014*next*.  \n**I rhyme in movable type so my bars can rearrange,**  \n**Yesterday I was Petrarch, today I\u2019m deranged.**  \nThe press spits paper like a hydra with hiccups,  \nEach sheet a protest flyer: \u201cDown with scholastic pick-ups!\u201d\n\n---\n\n### **Section 3 \u2013 The Anatomical Anthem (Parental Advisory: Corpses)**  \nNight shift with da Vinci, we\u2019re sketching by candle and moon,  \nCadavers on the table like, \u201cDo me a favor\u2014autotune my coccyx soon.\u201d  \nI rap in reverse Latin so only the dead understand,  \nWhile Leo reverse-engineers a bird-man from a frying pan.  \n**My beats are ventricles pumping four-four time,**  \n**Aorta bass drop\u2014call that circulatory grime.**  \nI sample the sound of a scalpel scraping periosteum,  \nLayer it over a lute loop\u2014*that\u2019s* my magnum opium.  \nGalen\u2019s in the corner, throwing shade like a Greek tragedy,  \nBut I\u2019m Harvey before Harvey, already rapping blood circulation majestically.\n\n---\n\n**[Outro \u2013 fading lute arpeggio, distant cannon fire from Constantinople]**  \nSo remember me when you\u2019re doom-scrolling on glass,  \nI was the era that turned corpses into class.  \nI\u2019m the **Renaissance Rapper**, my ghost still tags in chalk,  \nLeaving QR codes on cathedral walls\u2014scan it, you\u2019ll squawk.  \nRebirth never dies, it just pixelates and loops,  \nNow if you\u2019ll excuse me, I\u2019ve got a date with Lucretius in the soup."}], "input_metadata": {"row_id": "seem-clean-simple-work-member", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct", "temperature": 0.0, "max_tokens": 4096}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "ground_truth": "3", "evaluation_result": {"score": 1.0, "is_score_valid": true, "reason": "\u2705 Found 17 highlighted sections (required: 3)", "metrics": {}}, "created_at": "2025-08-09T10:35:28.043257", "eval_metadata": {"name": "test_markdown_highlighting_evaluation", "description": "\n    Evaluation function that checks if the model's response contains the required number of formatted sections.\n    ", "version": "v0.2.6-dev1.dev4+g67b03c6", "status": "finished", "run_id": "stop-choose-expect-major-pain", "num_runs": 1, "aggregation_method": "mean", "threshold_of_success": 0.5, "passed": true}, "pid": 43066}, {"messages": [{"role": "user", "content": "Summarize the history of Japan. Italicize at least 5 keywords in your response. To indicate a italic word, wrap it with asterisk, like *italic*"}, {"role": "assistant", "content": "*J\u014dmon* hunter-gatherers (14,000 BCE) gave way to *Yayoi* rice farmers (300 BCE) who forged bronze and iron.  \nThe *Yamato* court unified clans (3rd\u20137th c.), adopting *Shinto* and Chinese script.  \nPrince *Sh\u014dtoku*\u2019s 17-article constitution (604) and the Taika Reforms (645) centralized power.  \nHeian aristocrats (794\u20131185) produced *The Tale of Genji* while samurai rose in the provinces.  \nMinamoto no Yoritomo founded the *Kamakura* shogunate (1185), introducing warrior rule.  \nMongol invasions (1274, 1281) were repelled by \u201cdivine winds\u201d (*kamikaze*).  \nAshikaga *Muromachi* shogunate (1336\u20131573) saw Zen gardens and \u014cnin War chaos.  \nOda Nobunaga, Toyotomi Hideyoshi, and Tokugawa Ieyasu ended the *Sengoku* \u201cwarring states\u201d era.  \nTokugawa *Edo* shogunate (1603\u20131868) isolated Japan, fostering *ukiyo-e* and *kabuki*.  \nCommodore Perry\u2019s *Black Ships* (1853) forced the Meiji Restoration (1868), modernizing the empire.  \nVictories in the *Sino-Japanese* (1894\u201395) and *Russo-Japanese* (1904\u201305) wars expanded territory.  \nMilitarism led to *Pearl Harbor* (1941) and WWII defeat (1945); the U.S. occupation drafted a pacifist constitution.  \nPost-war *economic miracle* made Japan the world\u2019s second-largest economy until the 1990s *bubble*.  \nToday, Japan balances *constitutional monarchy* with cutting-edge technology and soft-power culture."}], "input_metadata": {"row_id": "receive-final-best-visible-night", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct", "temperature": 0.0, "max_tokens": 4096}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "ground_truth": "5", "evaluation_result": {"score": 1.0, "is_score_valid": true, "reason": "\u2705 Found 20 highlighted sections (required: 5)", "metrics": {}}, "created_at": "2025-08-09T10:35:28.043282", "eval_metadata": {"name": "test_markdown_highlighting_evaluation", "description": "\n    Evaluation function that checks if the model's response contains the required number of formatted sections.\n    ", "version": "v0.2.6-dev1.dev4+g67b03c6", "status": "finished", "run_id": "stop-choose-expect-major-pain", "num_runs": 1, "aggregation_method": "mean", "threshold_of_success": 0.5, "passed": true}, "pid": 43066}, {"messages": [{"role": "user", "content": "Write a funny Haiku about a Quaker named John who lives in the town of De Smet, South Dakota. Use the asterisk symbol, *, to highlight some words or phrases twice. Example: *This is a highlighted phrase*."}, {"role": "assistant", "content": "*John* in *De Smet*  \nQuaker hat *blows off*\u2014chases  \n*Silent* *scream*\u2014\u201cCome back!\u201d"}], "input_metadata": {"row_id": "record-live-better-guy-country", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct", "temperature": 0.0, "max_tokens": 4096}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "ground_truth": "2", "evaluation_result": {"score": 1.0, "is_score_valid": true, "reason": "\u2705 Found 5 highlighted sections (required: 2)", "metrics": {}}, "created_at": "2025-08-09T10:35:28.043309", "eval_metadata": {"name": "test_markdown_highlighting_evaluation", "description": "\n    Evaluation function that checks if the model's response contains the required number of formatted sections.\n    ", "version": "v0.2.6-dev1.dev4+g67b03c6", "status": "finished", "run_id": "stop-choose-expect-major-pain", "num_runs": 1, "aggregation_method": "mean", "threshold_of_success": 0.5, "passed": true}, "pid": 43066}, {"messages": [{"role": "user", "content": "Today, at the 54th Annual Grammy Awards, the Recording Academy honors the talent and creativity of the artists, musicians, and producers who are the creators of the best recordings of the past year. Please continue writing this text in a formal tone, using notations. Highlight some key parts in your response with \"*\", like *highlighted text*."}, {"role": "assistant", "content": "Today, at the 54th Annual Grammy Awards, the Recording Academy honors the talent and creativity of the artists, musicians, and producers who are the creators of the best recordings of the past year. In a ceremony marked by *unprecedented artistic excellence* and *meticulous craftsmanship*, the Academy recognizes those whose work has defined the sonic landscape of 2011.\n\nThe evening\u2019s proceedings underscore the Academy\u2019s enduring commitment to *artistic integrity* and *technical innovation*. Among the most celebrated achievements are:\n\n\u2022 *Album of the Year* \u2013 awarded to an opus that has redefined genre boundaries while achieving both critical acclaim and popular resonance.  \n\u2022 *Record of the Year* \u2013 bestowed upon a single track whose *sonic clarity*, *emotive depth*, and *production mastery* have set a new benchmark for the industry.  \n\u2022 *Best New Artist* \u2013 conferred upon an emerging voice whose *visionary artistry* signals the future direction of contemporary music.\n\nThroughout the telecast, special tributes illuminate the *indelible legacies* of icons whose influence transcends generations, while collaborative performances showcase the *synergistic power* of cross-genre partnerships. The Academy further acknowledges the *behind-the-scenes luminaries*\u2014engineers, mixers, and mastering specialists\u2014whose *invisible expertise* transforms raw inspiration into *audible transcendence*.\n\nIn closing, the 54th Annual Grammy Awards reaffirm the Recording Academy\u2019s solemn charge: to *safeguard the heritage*, *nurture the present*, and *inspire the future* of recorded music."}], "input_metadata": {"row_id": "talk-wonder-bad-dangerous-health", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct", "temperature": 0.0, "max_tokens": 4096}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "ground_truth": "1", "evaluation_result": {"score": 1.0, "is_score_valid": true, "reason": "\u2705 Found 19 highlighted sections (required: 1)", "metrics": {}}, "created_at": "2025-08-09T10:35:28.043332", "eval_metadata": {"name": "test_markdown_highlighting_evaluation", "description": "\n    Evaluation function that checks if the model's response contains the required number of formatted sections.\n    ", "version": "v0.2.6-dev1.dev4+g67b03c6", "status": "finished", "run_id": "stop-choose-expect-major-pain", "num_runs": 1, "aggregation_method": "mean", "threshold_of_success": 0.5, "passed": true}, "pid": 43066}, {"messages": [{"role": "user", "content": "Write a blog post about interesting facts about the Dutch language. Italicize at least 2 sections in your answer with markdown, i.e. *italic text*."}, {"role": "assistant", "content": "# 10 Surprising Facts About the Dutch Language That Will Make You Say \u201cWauw!\u201d\n\nIf you think Dutch is just \u201cGerman with a sore throat,\u201d think again. From tongue-twisting consonant clusters to words that sound like they were invented by a stand-up comedian, Dutch is a linguistic playground. Below are ten facts that prove the language is far more colorful than its reputation for windmills and wooden shoes.\n\n---\n\n### 1. *The longest word in Dutch is 53 letters long*  \nMeet **\u201ckindercarnavalsoptochtvoorbereidingswerkzaamhedenplan\u201d**\u2014a single noun describing the preparation work for a children\u2019s carnival parade. Dutch loves to glue words together, so once you master the basics you can create your own kilometer-long compounds on the fly.\n\n---\n\n### 2. Dutch has three genders, but only two articles  \nGrammatically, nouns are masculine, feminine, or neuter. In everyday speech, however, masculine and feminine collapsed into a single \u201ccommon\u201d gender, leaving *\u201cde\u201d* for common and *\u201chet\u201d* for neuter. The result? A never-ending guessing game for learners.\n\n---\n\n### 3. *The Dutch alphabet ends with \u201cij,\u201d not \u201cz\u201d*  \nSchoolchildren recite the ABC and finish with \u201c\u2026x, y, ij, z.\u201d The digraph *ij* is so common (think *hij* = he, *mijn* = my) that it\u2019s treated almost like a separate letter. On old typewriters, *ij* even had its own key.\n\n---\n\n### 4. Dutch is the closest major language to English  \nFrisian is technically closer, but among languages with tens of millions of speakers, Dutch wins. Compare:  \n- English *\u201cbread\u201d* \u2192 Dutch *\u201cbrood\u201d*  \n- English *\u201cgreen\u201d* \u2192 Dutch *\u201cgroen\u201d*  \nShared roots mean you can read simple Dutch signs long before you\u2019ve cracked a single grammar book.\n\n---\n\n### 5. *Dutch gave New York its first words*  \nBefore it was New York, it was New Amsterdam. That\u2019s why we say *\u201ccookie\u201d* (from Dutch *koekje*) instead of *\u201cbiscuit,\u201d* and *\u201cboss\u201d* (from *baas*) instead of *\u201cmaster.\u201d* Wall Street? It once ran alongside the *\u201cwal\u201d* (rampart) of the city wall.\n\n---\n\n### 6. The word *\u201cgezellig\u201d* has no exact English translation*  \nRoughly it means \u201ccozy,\u201d \u201cconvivial,\u201d or \u201cthe warm feeling of being with friends,\u201d but none of those capture the full vibe. A caf\u00e9 can be *gezellig*, a conversation can be *gezellig*, even a spreadsheet can be *gezellig* if the team is laughing.\n\n---\n\n### 7. Dutch spelling changed overnight in 1996  \nThe *\u201cGroene Boekje\u201d* (Green Booklet) reforms simplified thousands of words. *\u201cpotatos\u201d* became *\u201caardappelen\u201d* and *\u201ccat\u2019s\u201d* became *\u201ckatten.\u201d* Overnight, every dictionary, street sign, and Scrabble board in the country was technically out of date.\n\n---\n\n### 8. *Dutch is spoken on six continents*  \nBesides the Netherlands and Belgium, it\u2019s an official language in Suriname, Aruba, Cura\u00e7ao, and Sint Maarten. Thanks to emigration, you\u2019ll also find active Dutch-speaking communities in Canada, Australia, Argentina\u2014even South Africa, where Afrikaans is a daughter language.\n\n---\n\n### 9. The Dutch love diminutives  \nAdd *-je* to almost any noun and you instantly shrink it: *\u201chuis\u201d* (house) \u2192 *\u201chuisje\u201d* (little house). It\u2019s not just about size; diminutives add affection. A *\u201cbiertje\u201d* isn\u2019t a small beer\u2014it\u2019s an invitation to relax.\n\n---\n\n### 10. *Dutch is the language of the world\u2019s oldest multinational*  \nThe Vereenigde Oostindische Compagnie (VOC) was founded in 1602 and issued the planet\u2019s first publicly traded stock. Its ledgers, contracts, and ship logs were all written in Dutch, making the language the original tongue of global capitalism.\n\n---\n\nReady to dive in? Start with *hallo* and *dank je wel*, and you\u2019ll already be halfway to *gezellig*."}], "input_metadata": {"row_id": "meet-hold-solid-video-moment", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct", "temperature": 0.0, "max_tokens": 4096}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "ground_truth": "2", "evaluation_result": {"score": 1.0, "is_score_valid": true, "reason": "\u2705 Found 39 highlighted sections (required: 2)", "metrics": {}}, "created_at": "2025-08-09T10:35:28.043356", "eval_metadata": {"name": "test_markdown_highlighting_evaluation", "description": "\n    Evaluation function that checks if the model's response contains the required number of formatted sections.\n    ", "version": "v0.2.6-dev1.dev4+g67b03c6", "status": "finished", "run_id": "stop-choose-expect-major-pain", "num_runs": 1, "aggregation_method": "mean", "threshold_of_success": 0.5, "passed": true}, "pid": 43066}, {"messages": [{"role": "user", "content": "Gideon is a farmer who has a surplus of crops from his farm this year. What might he do with that surplus? Highlight at least one section of your answer in markdown, i.e *highlighted section*."}, {"role": "assistant", "content": "Gideon has several good options for his surplus:\n\n1. **Sell it at the local farmers\u2019 market or to a regional wholesaler** to turn the extra produce into cash he can reinvest in seeds, equipment, or next season\u2019s labor.  \n2. *Donate a portion to the nearby food bank or community kitchen* so that families facing food insecurity can benefit from fresh, nutritious produce.  \n3. Preserve some of the surplus\u2014by canning tomatoes, drying herbs, or freezing berries\u2014so his own household has a steady supply of healthy food throughout the winter."}], "input_metadata": {"row_id": "understand-rational-hard-kind-project", "completion_params": {"model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct", "temperature": 0.0, "max_tokens": 4096}, "session_data": {"mode": "pointwise"}}, "rollout_status": {"status": "finished"}, "ground_truth": "1", "evaluation_result": {"score": 1.0, "is_score_valid": true, "reason": "\u2705 Found 2 highlighted sections (required: 1)", "metrics": {}}, "created_at": "2025-08-09T10:35:28.043383", "eval_metadata": {"name": "test_markdown_highlighting_evaluation", "description": "\n    Evaluation function that checks if the model's response contains the required number of formatted sections.\n    ", "version": "v0.2.6-dev1.dev4+g67b03c6", "status": "finished", "run_id": "stop-choose-expect-major-pain", "num_runs": 1, "aggregation_method": "mean", "threshold_of_success": 0.5, "passed": true}, "pid": 43066}]}
