// Vitest Snapshot v1, https://vitest.dev/guide/snapshot.html

exports[`flattenJson > against logs.json > flattens each entry in logs.json.logs and matches snapshot 1`] = `
[
  {
    "$.created_at": "2025-08-08T09:52:30.335005",
    "$.input_metadata.row_id": "1",
    "$.messages[0].content": "Hello",
    "$.messages[0].role": "user",
    "$.messages[1].content": "Hello",
    "$.messages[1].role": "assistant",
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T09:47:43.816761",
    "$.input_metadata.row_id": "row_0",
    "$.messages[0].content": "Hello 0",
    "$.messages[0].role": "user",
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T09:41:55.015216",
    "$.input_metadata.row_id": "row_1",
    "$.messages[0].content": "Hello 1",
    "$.messages[0].role": "user",
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T09:41:55.015767",
    "$.input_metadata.row_id": "row_2",
    "$.messages[0].content": "Hello 2",
    "$.messages[0].role": "user",
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T10:00:08.522553",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "Run math evaluation on sample dataset using pytest interface.",
    "$.eval_metadata.name": "test_pytest_mcp_url",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.status": "stopped",
    "$.eval_metadata.version": "v0.2.6.dev12+g2dc40d5.dirty",
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.row_id": "describe-use-entire-person-education",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "You are a helpful assistant that can answer questions about Fireworks.
ALWAYS provide code or commands to execute to answer the question.",
    "$.messages[0].role": "system",
    "$.messages[1].content": "Can you teach me about how to manage deployments on Fireworks",
    "$.messages[1].role": "user",
    "$.pid": 67635,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T10:01:12.825337",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "Run math evaluation on sample dataset using pytest interface.",
    "$.eval_metadata.name": "test_pytest_mcp_url",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.status": "stopped",
    "$.eval_metadata.version": "v0.2.6.dev12+g2dc40d5.dirty",
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.row_id": "meet-solid-big-picture-car",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "You are a helpful assistant that can answer questions about Fireworks.
ALWAYS provide code or commands to execute to answer the question.",
    "$.messages[0].role": "system",
    "$.messages[1].content": "Can you teach me about how to manage deployments on Fireworks",
    "$.messages[1].role": "user",
    "$.pid": 68229,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T11:06:51.152819",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "Run math evaluation on sample dataset using pytest interface.",
    "$.eval_metadata.name": "test_pytest_mcp_url",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.status": "stopped",
    "$.eval_metadata.version": "v0.2.6.dev15+g6486dbd",
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.row_id": "leave-ask-logical-face-way",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "You are a helpful assistant that can answer questions about Fireworks.
ALWAYS provide code or commands to execute to answer the question.",
    "$.messages[0].role": "system",
    "$.messages[1].content": "Can you teach me about how to manage deployments on Fireworks",
    "$.messages[1].role": "user",
    "$.messages[2].content": "I'll help you learn about managing deployments on Fireworks AI. Let me search for comprehensive information about deployment management.",
    "$.messages[2].role": "assistant",
    "$.messages[2].tool_calls[0].function.arguments": "{"query": "deployment management Fireworks AI"}",
    "$.messages[2].tool_calls[0].function.name": "search",
    "$.messages[2].tool_calls[0].id": "functions.search:0",
    "$.messages[2].tool_calls[0].type": "function",
    "$.messages[3].content[0].text": "Title: Fireworks AI Developer Platform
Link: https://docs.fireworks.ai/getting-started/introduction
Content: Fireworks AI is the best platform for building AI product experiences with open source AI models. You can run and customize AI models with just a few lines of code!

Fireworks AI is the best platform for building AI product experiences with open source AI models. You can run and customize AI models with just a few lines of code!

Start building with open source AI models

",
    "$.messages[3].content[0].type": "text",
    "$.messages[3].content[1].text": "Title: How do I close my Fireworks.ai account?
Link: https://docs.fireworks.ai/faq-new/account-access/how-do-i-close-my-fireworksai-account
Content: To close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed

To close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed

How do I close my Fireworks.ai account?
To close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed

",
    "$.messages[3].content[1].type": "text",
    "$.messages[3].content[2].text": "Title: Do you host your deployments in the EU or Asia?
Link: https://docs.fireworks.ai/faq-new/support-general/do-you-host-your-deployments-in-the-eu-or-asia
Content: We are currently deployed in multiple U.S.-based locations. However, we’re open to hearing more about your specific requirements. You can: Join our Discord community Write to us at inquiries@fireworks.ai If you're an Enterprise customer, please contact your dedicated customer support representative to ensure a timely response.

We are currently deployed in multiple U.S.-based locations. However, we’re open to hearing more about your specific requirements. You can: Join our Discord community Write to us at inquiries@fireworks.ai If you're an Enterprise customer, please contact your dedicated customer support representative to ensure a timely response.

Do you host your deployments in the EU or Asia?
We are currently deployed in multiple U.S.-based locations. However, we’re open to hearing more about your specific requirements. You can: Join our Discord community Write to us at inquiries@fireworks.ai If you're an Enterprise customer, please contact your dedicated customer support representative to ensure a timely response.

",
    "$.messages[3].content[2].type": "text",
    "$.messages[3].content[3].text": "Title: Introducing the Fireworks Build SDK
Link: https://docs.fireworks.ai/tools-sdks/python-client/sdk-introduction
Content: The Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.

The Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.

Introducing the Fireworks Build SDK
The Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.

",
    "$.messages[3].content[3].type": "text",
    "$.messages[3].content[4].text": "Title: On-demand deployments
Link: https://docs.fireworks.ai/guides/ondemand-deployments
Content: If you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.

If you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.

On-demand deployments
If you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.

",
    "$.messages[3].content[4].type": "text",
    "$.messages[3].content[5].text": "Title: I have multiple Fireworks accounts. When I try to login with Google on Fireworks' web UI, I'm getting signed into the wrong account. How do I fix this?
Link: https://docs.fireworks.ai/faq-new/account-access/i-have-multiple-fireworks-accounts-when-i-try-to-login-with-google-on-fireworks
Content: If you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.

If you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.

I have multiple Fireworks accounts. When I try to login with Google on Fireworks' web UI, I'm getting signed into the wrong account. How do I fix this?
If you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.

",
    "$.messages[3].content[5].type": "text",
    "$.messages[3].content[6].text": "Title: Introduction
Link: https://docs.fireworks.ai/api-reference/introduction
Content: Fireworks AI REST API enables you to interact with various language, image and embedding models using an API Key. It also lets you automate management of models, deployments, datasets, and more.

Fireworks AI REST API enables you to interact with various language, image and embedding models using an API Key. It also lets you automate management of models, deployments, datasets, and more.

Introduction
Fireworks AI REST API enables you to interact with various language, image and embedding models using an API Key. It also lets you automate management of models, deployments, datasets, and more.

",
    "$.messages[3].content[6].type": "text",
    "$.messages[3].content[7].text": "Title: Create a deployment
Link: https://docs.fireworks.ai/tools-sdks/firectl/commands/create-deployment
Content: Create a Deployment on Fireworks AI platform

Create a Deployment on Fireworks AI platform

Creates a new deployment.

",
    "$.messages[3].content[7].type": "text",
    "$.messages[3].content[8].text": "Title: How much does Fireworks cost?
Link: https://docs.fireworks.ai/faq-new/billing-pricing/how-much-does-fireworks-cost
Content: Fireworks AI operates on a pay-as-you-go model for all non-Enterprise usage, and new users automatically receive free credits. You pay based on: Per token for serverless inference Per GPU usage time for on-demand deployments Per token of training data for fine-tuning For customers needing enterprise-grade security and reliability , please reach out to us at inquiries@fireworks.ai to discuss options. Find out more about our current pricing on our Pricing page.

Fireworks AI operates on a pay-as-you-go model for all non-Enterprise usage, and new users automatically receive free credits. You pay based on: Per token for serverless inference Per GPU usage time for on-demand deployments Per token of training data for fine-tuning For customers needing enterprise-grade security and reliability , please reach out to us at inquiries@fireworks.ai to discuss options. Find out more about our current pricing on our Pricing page.

How much does Fireworks cost?
Fireworks AI operates on a pay-as-you-go model for all non-Enterprise usage, and new users automatically receive free credits. You pay based on: Per token for serverless inference Per GPU usage time for on-demand deployments Per token of training data for fine-tuning For customers needing enterprise-grade security and reliability , please reach out to us at inquiries@fireworks.ai to discuss options. Find out more about our current pricing on our Pricing page.

",
    "$.messages[3].content[8].type": "text",
    "$.messages[3].content[9].text": "Title: Python (Fireworks)
Link: https://docs.fireworks.ai/getting-started/quickstart
Content: This is the recommended way to get started : This  Python (Fireworks)  tab uses our  Fireworks Build
SDK  for the best performance and developer experience.
See our  Client-side performance
optimization  guide for
more details. Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. pip install --upgrade fireworks-ai The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either  ~/.bash_profile  for Bash or  ~/.zshrc  for  Zsh .
You can do this by running the command: vim ~/.bash_profile vim ~/.zshrc Add a new line to the file with the following: export FIREWORKS_API_KEY="<API_KEY>" After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. source ~/.bash_profile source ~/.zshrc You can verify that the variable has been set correctly by running  echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. setx FIREWORKS_API_KEY "<API_KEY>" To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: echo %FIREWORKS_API_KEY% You can quickly instantiate the  LLM  class and call the Fireworks API. The Build SDK handles deployment management automatically. from fireworks import LLM

# Basic usage - SDK automatically selects optimal deployment type
llm = LLM(model="llama4-maverick-instruct-basic", deployment_type="auto")

response = llm.chat.completions.create(
    messages=[{"role": "user", "content": "Say this is a test"}]
)

print(response.choices[0].message.content) You can also pass the API key directly to the LLM constructor:  LLM(model="llama4-maverick-instruct-basic", deployment_type="auto", api_key="<FIREWORKS_API_KEY>")

This is the recommended way to get started : This  Python (Fireworks)  tab uses our  Fireworks Build
SDK  for the best performance and developer experience.
See our  Client-side performance
optimization  guide for
more details. Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. pip install --upgrade fireworks-ai The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either  ~/.bash_profile  for Bash or  ~/.zshrc  for  Zsh .
You can do this by running the command: vim ~/.bash_profile vim ~/.zshrc Add a new line to the file with the following: export FIREWORKS_API_KEY="<API_KEY>" After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. source ~/.bash_profile source ~/.zshrc You can verify that the variable has been set correctly by running  echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. setx FIREWORKS_API_KEY "<API_KEY>" To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: echo %FIREWORKS_API_KEY% You can quickly instantiate the  LLM  class and call the Fireworks API. The Build SDK handles deployment management automatically. from fireworks import LLM

# Basic usage - SDK automatically selects optimal deployment type
llm = LLM(model="llama4-maverick-instruct-basic", deployment_type="auto")

response = llm.chat.completions.create(
    messages=[{"role": "user", "content": "Say this is a test"}]
)

print(response.choices[0].message.content) You can also pass the API key directly to the LLM constructor:  LLM(model="llama4-maverick-instruct-basic", deployment_type="auto", api_key="<FIREWORKS_API_KEY>")

Set up your developer environment & call the Fireworks API
This is the recommended way to get started : This  tab uses our Fireworks Build
SDK for the best performance and developer experience.
See our Client-side performance
optimization guide for
more details. Install SDK Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Configure API Key Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either ~/.bash_profile for Bash or ~/.zshrc for Zsh .
You can do this by running the command: Add a new line to the file with the following: After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. You can verify that the variable has been set correctly by running echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: Sending the first API Request You can quickly instantiate the LLM class and call the Fireworks API. The Build SDK handles deployment management automatically. You can also pass the API key directly to the LLM constructor: LLM(model="llama4-maverick-instruct-basic", deployment_type="auto", api_key="<FIREWORKS_API_KEY>") Install SDK Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. Fireworks AI platform offers drop-in replacement with OpenAI Python Client. Configure API Key Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either ~/.bash_profile for Bash or ~/.zshrc for Zsh .
You can do this by running the command: Add a new line to the file with the following: After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. You can verify that the variable has been set correctly by running echo $OPENAI_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. setx OPENAI_API_BASE "https://api.fireworks.ai/inference/v1"
setx OPENAI_API_KEY "<API_KEY>"

",
    "$.messages[3].content[9].type": "text",
    "$.messages[3].role": "tool",
    "$.messages[3].tool_call_id": "functions.search:0",
    "$.messages[4].content": "Let me search for more specific information about deployment management tools and commands.",
    "$.messages[4].role": "assistant",
    "$.messages[4].tool_calls[0].function.arguments": "{"query": "firectl deployment management commands CLI"}",
    "$.messages[4].tool_calls[0].function.name": "search",
    "$.messages[4].tool_calls[0].id": "functions.search:1",
    "$.messages[4].tool_calls[0].type": "function",
    "$.messages[5].content[0].text": "Title: Why am I getting "invalid id" errors when using firectl commands like create deployment or list deployments?
Link: https://docs.fireworks.ai/faq-new/fine-tuning/why-am-i-getting-invalid-id-errors-when-using-firectl-commands-like-create-deplo
Content: This error typically occurs when your account ID is not properly configured.

This error typically occurs when your account ID is not properly configured.

Why am I getting "invalid id" errors when using firectl commands like create deployment or list deployments?
This error typically occurs when your account ID is not properly configured.

",
    "$.messages[5].content[0].type": "text",
    "$.messages[5].content[1].text": "Title: Installing the CLI
Link: https://docs.fireworks.ai/guides/ondemand-deployments
Content: To create and manage on-demand deployments, use our CLI tool firectl . Install the tool by following the installation guide . Verify that your installation is working and you are logged into the correct account by running: firectl whoami

To create and manage on-demand deployments, use our CLI tool firectl . Install the tool by following the installation guide . Verify that your installation is working and you are logged into the correct account by running: firectl whoami

To create and manage on-demand deployments, use our CLI tool firectl . Install the tool by following the installation guide . Verify that your installation is working and you are logged into the correct account by running:

",
    "$.messages[5].content[1].type": "text",
    "$.messages[5].content[2].text": "Title: Cli overview
Link: https://docs.fireworks.ai/evaluators/cli_reference/cli_overview
Content: Command Line Interface Reference
The Reward Kit provides a command-line interface (CLI) for common operations like previewing evaluations, deploying reward functions, and running agent evaluations.

Command Line Interface Reference
The Reward Kit provides a command-line interface (CLI) for common operations like previewing evaluations, deploying reward functions, and running agent evaluations.

Command Overview
The Reward Kit CLI supports the following main commands: run : Run a local evaluation pipeline using a Hydra configuration. preview : Preview evaluation results or re-evaluate generated outputs. deploy : Deploy a reward function as an evaluator. agent-eval : Run agent evaluations on task bundles. list : List existing evaluators (coming soon). delete : Delete an evaluator (coming soon).

",
    "$.messages[5].content[2].type": "text",
    "$.messages[5].content[3].text": "Title: Using the CLI (reward-kit deploy)
Link: https://docs.fireworks.ai/evaluators/developer_guide/evaluation_workflows
Content: The reward-kit deploy command is suitable for deploying reward functions defined in script files. The --metrics-folders argument should point to the directory containing your reward function script (e.g., a main.py with the @reward_function decorator). For more details on reward-kit deploy , see the CLI Reference.

The reward-kit deploy command is suitable for deploying reward functions defined in script files. The --metrics-folders argument should point to the directory containing your reward function script (e.g., a main.py with the @reward_function decorator). For more details on reward-kit deploy , see the CLI Reference.

The reward-kit deploy command is suitable for deploying reward functions defined in script files. The --metrics-folders argument should point to the directory containing your reward function script (e.g., a main.py with the @reward_function decorator). # Deploy with the CLI
reward-kit deploy \\
  --id helpfulness-evaluator \\
  --metrics-folders "helpfulness=./path/to/your/metric_script_directory" \\
  --display-name "Helpfulness Evaluator" \\
  --description "Evaluates the helpfulness of responses" \\
  --force

",
    "$.messages[5].content[3].type": "text",
    "$.messages[5].content[4].text": "Title: Does Fireworks support custom base models?
Link: https://docs.fireworks.ai/faq-new/models-inference/does-fireworks-support-custom-base-models
Content: Yes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.

Yes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.

Does Fireworks support custom base models?
Yes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.

",
    "$.messages[5].content[4].type": "text",
    "$.messages[5].content[5].text": "Title: 🚀 Easier & faster LoRA fine-tune deployments on Fireworks
Link: https://docs.fireworks.ai/updates/changelog
Content: You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: Previously, this involved two distinct steps, and the resulting deployment was slower than the base model: Create a deployment using firectl create deployment "accounts/fireworks/models/<MODEL_ID of base model>" --enable-addons Then deploy the addon to the deployment: firectl load-lora <MODEL_ID> --deployment <DEPLOYMENT_ID> For more information, see our deployment documentation. This change is for dedicated deployments with a single LoRA. You can still deploy multiple LoRAs on a deployment or deploy LoRA(s) on some Serverless models as described in the documentation.

",
    "$.messages[5].content[5].type": "text",
    "$.messages[5].content[6].text": "Title: Quick deployment
Link: https://docs.fireworks.ai/fine-tuning/single-lora
Content: Deploy your LoRA fine-tuned model with one simple command: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

Deploy your LoRA fine-tuned model with one simple command: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

Alternative deployment method
This two-step method is the standard approach for multi-LoRA deployments where multiple LoRA models share the same base model. While it can also be used for single LoRA deployments, it provides slower performance compared to live merge and is not recommended for single LoRA use cases. You can also deploy single LoRA models using a two-step process: Create base model deployment Deploy the base model with addons enabled: firectl create deployment "accounts/fireworks/models/<MODEL_ID of base model>" --enable-addons

",
    "$.messages[5].content[6].type": "text",
    "$.messages[5].content[7].text": "Title: 1. Setup
Link: https://docs.fireworks.ai/tools-sdks/python-client/the-tutorial
Content: To get started with the Fireworks AI Python SDK, you need to install the firectl CLI tool and create an API key. Install our CLI tool firectl to interact with the Fireworks AI platform. Sign in to Fireworks by running the following command: firectl signin

To get started with the Fireworks AI Python SDK, you need to install the firectl CLI tool and create an API key. Install our CLI tool firectl to interact with the Fireworks AI platform. Sign in to Fireworks by running the following command: firectl signin

To get started with the Fireworks AI Python SDK, you need to install the firectl CLI tool and create an API key. Install our CLI tool firectl to interact with the Fireworks AI platform. Sign in to Fireworks by running the following command: A browser window will open to the Fireworks AI login page. Once you login, your machine will be authenticated. Create an API key by running the following command: $ firectl create api-key --key-name "quick-start"
Key Id: key_42vAYeb7rwt9zzg1
Display Name: quick-start
Key: fw_3ZLd....
Secure: true
Be sure to save this key. It will not be shown again.

",
    "$.messages[5].content[7].type": "text",
    "$.messages[5].content[8].text": "Title: Deploying a fine-tuned model using an on-demand deployment
Link: https://docs.fireworks.ai/fine-tuning/fine-tuning-models
Content: Use the following command to deploy your fine-tuned model using an on-demand deployment: firectl create deployment <FINE_TUNED_MODEL_ID>

Use the following command to deploy your fine-tuned model using an on-demand deployment: firectl create deployment <FINE_TUNED_MODEL_ID>

Deploying a fine-tuned model serverlessly
Not all base models support serverless addons. Please check this list for the full list of serverless models that support LoRA add-ons. If your base model supports serverless addons, you can run the following command to load the fine-tuned model into a serverless deployment: firectl load-lora <FINE_TUNED_MODEL_ID>

",
    "$.messages[5].content[8].type": "text",
    "$.messages[5].content[9].text": "Title: Create a deployment
Link: https://docs.fireworks.ai/tools-sdks/firectl/commands/create-deployment
Content: Creates a new deployment. firectl create deployment [flags]

Creates a new deployment. firectl create deployment [flags]

Example
firectl create deployment falcon-7b

",
    "$.messages[5].content[9].type": "text",
    "$.messages[5].role": "tool",
    "$.messages[5].tool_call_id": "functions.search:1",
    "$.pid": 83882,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T11:10:27.844496",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "Run math evaluation on sample dataset using pytest interface.",
    "$.eval_metadata.name": "test_pytest_mcp_url",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.status": "stopped",
    "$.eval_metadata.version": "v0.2.6.dev15+g6486dbd",
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.row_id": "investigate-recent-dangerous-hope-magazine",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "You are a helpful assistant that can answer questions about Fireworks.
ALWAYS provide code or commands to execute to answer the question.",
    "$.messages[0].role": "system",
    "$.messages[1].content": "Can you teach me about how to manage deployments on Fireworks",
    "$.messages[1].role": "user",
    "$.pid": 84847,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T11:12:00.215991",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "Run math evaluation on sample dataset using pytest interface.",
    "$.eval_metadata.name": "test_pytest_mcp_url",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.status": "stopped",
    "$.eval_metadata.version": "v0.2.6.dev15+g6486dbd.dirty",
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.row_id": "seem-environmental-media-television-concert",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "You are a helpful assistant that can answer questions about Fireworks.
ALWAYS provide code or commands to execute to answer the question.",
    "$.messages[0].role": "system",
    "$.messages[1].content": "Can you teach me about how to manage deployments on Fireworks",
    "$.messages[1].role": "user",
    "$.pid": 85545,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T11:12:34.382113",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "Run math evaluation on sample dataset using pytest interface.",
    "$.eval_metadata.name": "test_pytest_mcp_url",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.status": "stopped",
    "$.eval_metadata.version": "v0.2.6.dev15+g6486dbd.dirty",
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.row_id": "treat-suppose-salty-week-plan",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "You are a helpful assistant that can answer questions about Fireworks.
ALWAYS provide code or commands to execute to answer the question.",
    "$.messages[0].role": "system",
    "$.messages[1].content": "Can you teach me about how to manage deployments on Fireworks",
    "$.messages[1].role": "user",
    "$.pid": 85754,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T11:12:46.750720",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "Run math evaluation on sample dataset using pytest interface.",
    "$.eval_metadata.name": "test_pytest_mcp_url",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.status": "stopped",
    "$.eval_metadata.version": "v0.2.6.dev15+g6486dbd.dirty",
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.row_id": "contain-recent-bad-plan-change",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "You are a helpful assistant that can answer questions about Fireworks.
ALWAYS provide code or commands to execute to answer the question.",
    "$.messages[0].role": "system",
    "$.messages[1].content": "Can you teach me about how to manage deployments on Fireworks",
    "$.messages[1].role": "user",
    "$.pid": 85851,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T11:15:45.037115",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "Run math evaluation on sample dataset using pytest interface.",
    "$.eval_metadata.name": "test_pytest_mcp_url",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.version": "v0.2.6.dev15+g6486dbd.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "At least one tool call was made",
    "$.evaluation_result.score": 1,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.row_id": "move-little-tidy-pain-work",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "You are a helpful assistant that can answer questions about Fireworks.
ALWAYS provide code or commands to execute to answer the question.",
    "$.messages[0].role": "system",
    "$.messages[1].content": "Can you teach me about how to manage deployments on Fireworks",
    "$.messages[1].role": "user",
    "$.messages[2].content": "I'll help you learn about managing deployments on Fireworks! Let me search for comprehensive information about deployment management.",
    "$.messages[2].role": "assistant",
    "$.messages[2].tool_calls[0].function.arguments": "{"query": "deployments management create update delete scale"}",
    "$.messages[2].tool_calls[0].function.name": "search",
    "$.messages[2].tool_calls[0].id": "functions.search:0",
    "$.messages[2].tool_calls[0].type": "function",
    "$.messages[3].content[0].text": "Title: Delete Deployment
Link: https://docs.fireworks.ai/api-reference/delete-deployment
Content: 



REST Endpoint DELETE /v1/accounts/{account_id}/deployments/{deployment_id}

Server(s): https://api.fireworks.ai


Summary:  

Description: N/A

Security methods: 
header: 
	Authorization: bearer auth
	

Request params: 
path params:
account_id:
	string
	Description: The Account Id
	Example: "<string>"
deployment_id:
	string
	Description: The Deployment Id
	Example: "<string>"

query params:
hard:
	boolean
	Description: If true, this will perform a hard deletion.
	Example: true
ignoreChecks:
	boolean
	Description: If true, this will ignore checks and force the deletion of a deployment that is currently
deployed and is in use.
	Example: true


Request body: N/A


Responses: 
200: object
	Description: N/A
	Example: {}

",
    "$.messages[3].content[0].type": "text",
    "$.messages[3].content[1].text": "Title: How does billing and scaling work for on-demand GPU deployments?
Link: https://docs.fireworks.ai/faq-new/deployment-infrastructure/how-does-billing-and-scaling-work-for-on-demand-gpu-deployments
Content: On-demand GPU deployments have unique billing and scaling characteristics compared to serverless deployments: Billing: Charges start when the server begins accepting requests Billed by GPU-second for each active instance Costs accumulate even if there are no active API calls Scaling options: Supports autoscaling from 0 to multiple GPUs Each additional GPU adds to the billing rate Can handle unlimited requests within the GPU’s capacity Management requirements: Not fully serverless; requires some manual management Manually delete deployments when no longer needed Or configure autoscaling to scale down to 0 during inactive periods Cost control tips: Regularly monitor active deployments Delete unused deployments to avoid unnecessary costs Consider serverless options for intermittent usage Use autoscaling to 0 to optimize costs during low-demand times

On-demand GPU deployments have unique billing and scaling characteristics compared to serverless deployments: Billing: Charges start when the server begins accepting requests Billed by GPU-second for each active instance Costs accumulate even if there are no active API calls Scaling options: Supports autoscaling from 0 to multiple GPUs Each additional GPU adds to the billing rate Can handle unlimited requests within the GPU’s capacity Management requirements: Not fully serverless; requires some manual management Manually delete deployments when no longer needed Or configure autoscaling to scale down to 0 during inactive periods Cost control tips: Regularly monitor active deployments Delete unused deployments to avoid unnecessary costs Consider serverless options for intermittent usage Use autoscaling to 0 to optimize costs during low-demand times

How does billing and scaling work for on-demand GPU deployments?
On-demand GPU deployments have unique billing and scaling characteristics compared to serverless deployments: Billing: Charges start when the server begins accepting requests Billed by GPU-second for each active instance Costs accumulate even if there are no active API calls Scaling options: Supports autoscaling from 0 to multiple GPUs Each additional GPU adds to the billing rate Can handle unlimited requests within the GPU’s capacity Management requirements: Not fully serverless; requires some manual management Manually delete deployments when no longer needed Or configure autoscaling to scale down to 0 during inactive periods Cost control tips: Regularly monitor active deployments Delete unused deployments to avoid unnecessary costs Consider serverless options for intermittent usage Use autoscaling to 0 to optimize costs during low-demand times

",
    "$.messages[3].content[1].type": "text",
    "$.messages[3].content[2].text": "Title: Managing default deployments
Link: https://docs.fireworks.ai/guides/ondemand-deployments
Content: Since a model may be deployed to multiple deployments, querying by model name will route to the "default" deployed model. You can see which deployed model entry is marked with Default: true using: This will show the Deployed Model Refs section with the Default: true entry. To update the default deployed model, note the Name of the deployed model reference above. Then run: To delete a default deployment you must delete all other deployments for the same model first, or designate a different deployed model as the default as described above. This is to ensure that querying by model name will always route to an unambiguous default deployment as long as deployments for the model exist.

Since a model may be deployed to multiple deployments, querying by model name will route to the "default" deployed model. You can see which deployed model entry is marked with Default: true using: This will show the Deployed Model Refs section with the Default: true entry. To update the default deployed model, note the Name of the deployed model reference above. Then run: To delete a default deployment you must delete all other deployments for the same model first, or designate a different deployed model as the default as described above. This is to ensure that querying by model name will always route to an unambiguous default deployment as long as deployments for the model exist.

Since a model may be deployed to multiple deployments, querying by model name will route to the "default" deployed model. You can see which deployed model entry is marked with Default: true using: This will show the Deployed Model Refs section with the Default: true entry. To update the default deployed model, note the Name of the deployed model reference above. Then run: firectl update deployed-model <DEPLOYED_MODEL_ID_2> --default

",
    "$.messages[3].content[2].type": "text",
    "$.messages[3].content[3].text": "Title: Create a deployment
Link: https://docs.fireworks.ai/tools-sdks/firectl/commands/create-deployment
Content: Creates a new deployment.

Creates a new deployment.

Creates a new deployment. firectl create deployment [flags]

",
    "$.messages[3].content[3].type": "text",
    "$.messages[3].content[4].text": "Title: Delete a deployment
Link: https://docs.fireworks.ai/tools-sdks/firectl/commands/delete-resources
Content: Deletes an model deployment.

Deletes an model deployment.

Deletes an model deployment. firectl delete deployment [flags]

",
    "$.messages[3].content[4].type": "text",
    "$.messages[3].content[5].text": "Title: Update Deployment
Link: https://docs.fireworks.ai/api-reference/update-deployment
Content: 



REST Endpoint PATCH /v1/accounts/{account_id}/deployments/{deployment_id}

Server(s): https://api.fireworks.ai


Summary:  

Description: N/A

Security methods: 
header: 
	Authorization: bearer auth
	

Request params: 
path params:
account_id:
	string
	Description: The Account Id
	Example: "<string>"
deployment_id:
	string
	Description: The Deployment Id
	Example: "<string>"


Request body: object
	Description: N/A
	Example: {"displayName":"<string>","description":"<string>","expireTime":"2023-11-07T05:31:56Z","minReplicaCount":123,"maxReplicaCount":123,"autoscalingPolicy":{"scaleUpWindow":"<string>","scaleDownWindow":"<string>","scaleToZeroWindow":"<string>","loadTargets":{}},"baseModel":"<string>","acceleratorCount":123,"acceleratorType":"ACCELERATOR_TYPE_UNSPECIFIED","precision":"PRECISION_UNSPECIFIED","enableAddons":true,"draftTokenCount":123,"draftModel":"<string>","ngramSpeculationLength":123,"deploymentTemplate":"<string>","autoTune":{"longPrompt":true},"placement":{"region":"REGION_UNSPECIFIED","multiRegion":"MULTI_REGION_UNSPECIFIED","regions":["REGION_UNSPECIFIED"]},"disableDeploymentSizeValidation":true}


Responses: 
200: object
	Description: N/A
	Example: {"name":"<string>","displayName":"<string>","description":"<string>","createTime":"2023-11-07T05:31:56Z","expireTime":"2023-11-07T05:31:56Z","purgeTime":"2023-11-07T05:31:56Z","deleteTime":"2023-11-07T05:31:56Z","state":"STATE_UNSPECIFIED","status":{"code":"OK","message":"<string>"},"minReplicaCount":123,"maxReplicaCount":123,"replicaCount":123,"autoscalingPolicy":{"scaleUpWindow":"<string>","scaleDownWindow":"<string>","scaleToZeroWindow":"<string>","loadTargets":{}},"baseModel":"<string>","acceleratorCount":123,"acceleratorType":"ACCELERATOR_TYPE_UNSPECIFIED","precision":"PRECISION_UNSPECIFIED","cluster":"<string>","enableAddons":true,"draftTokenCount":123,"draftModel":"<string>","ngramSpeculationLength":123,"numPeftDeviceCached":123,"deploymentTemplate":"<string>","autoTune":{"longPrompt":true},"placement":{"region":"REGION_UNSPECIFIED","multiRegion":"MULTI_REGION_UNSPECIFIED","regions":["REGION_UNSPECIFIED"]},"region":"REGION_UNSPECIFIED","updateTime":"2023-11-07T05:31:56Z","disableDeploymentSizeValidation":true}

",
    "$.messages[3].content[5].type": "text",
    "$.messages[3].content[6].text": "Title: What should I expect for deployment and scaling performance?
Link: https://docs.fireworks.ai/faq-new/deployment-infrastructure/what-should-i-expect-for-deployment-and-scaling-performance
Content: Initial deployment : Should complete within minutes Scaling from zero : You may experience brief availability delays while the system scales up Troubleshooting : If deployment takes over 1 hour, this typically indicates a crash and should be investigated Best practice : Monitor deployment status and contact support if deployment times are unusually long

Initial deployment : Should complete within minutes Scaling from zero : You may experience brief availability delays while the system scales up Troubleshooting : If deployment takes over 1 hour, this typically indicates a crash and should be investigated Best practice : Monitor deployment status and contact support if deployment times are unusually long

What should I expect for deployment and scaling performance?
Initial deployment : Should complete within minutes Scaling from zero : You may experience brief availability delays while the system scales up Troubleshooting : If deployment takes over 1 hour, this typically indicates a crash and should be investigated Best practice : Monitor deployment status and contact support if deployment times are unusually long

",
    "$.messages[3].content[6].type": "text",
    "$.messages[3].content[7].text": "Title: delete_deployment(ignore_checks: bool = False, wait: bool = True)
Link: https://docs.fireworks.ai/tools-sdks/python-client/sdk-reference
Content: Deletes the deployment associated with this LLM instance if one exists. Arguments: ignore_checks bool, optional - Whether to ignore safety checks. Defaults to False. wait bool, optional - Whether to wait for deletion to complete. Defaults to True. llm.delete_deployment(ignore_checks=True)

Deletes the deployment associated with this LLM instance if one exists. Arguments: ignore_checks bool, optional - Whether to ignore safety checks. Defaults to False. wait bool, optional - Whether to wait for deletion to complete. Defaults to True.

scale_to_zero()
Sends a request to scale the deployment to 0 replicas but does not wait for it to complete. Returns: The deployment object, or None if no deployment exists deployment = llm.scale_to_zero()

",
    "$.messages[3].content[7].type": "text",
    "$.messages[3].content[8].text": "Title: How does billing work for on-demand deployments?
Link: https://docs.fireworks.ai/faq-new/deployment-infrastructure/how-does-billing-work-for-on-demand-deployments
Content: On-demand deployments come with automatic cost optimization features: Default autoscaling : Automatically scales to 0 replicas when not in use Pay for what you use : Charged only for GPU time when replicas are active Flexible configuration : Customize autoscaling behavior to match your needs Best practices for cost management: Leverage default autoscaling : The system automatically scales down deployments when not in use Customize carefully : While you can modify autoscaling behavior using our configuration options , note that preventing scale-to-zero will result in continuous GPU charges Consider your use case : For intermittent or low-frequency usage, serverless deployments might be more cost-effective For detailed configuration options, see our deployment guide.

On-demand deployments come with automatic cost optimization features: Default autoscaling : Automatically scales to 0 replicas when not in use Pay for what you use : Charged only for GPU time when replicas are active Flexible configuration : Customize autoscaling behavior to match your needs Best practices for cost management: Leverage default autoscaling : The system automatically scales down deployments when not in use Customize carefully : While you can modify autoscaling behavior using our configuration options , note that preventing scale-to-zero will result in continuous GPU charges Consider your use case : For intermittent or low-frequency usage, serverless deployments might be more cost-effective For detailed configuration options, see our deployment guide.

How does billing work for on-demand deployments?
On-demand deployments come with automatic cost optimization features: Default autoscaling : Automatically scales to 0 replicas when not in use Pay for what you use : Charged only for GPU time when replicas are active Flexible configuration : Customize autoscaling behavior to match your needs Best practices for cost management: Leverage default autoscaling : The system automatically scales down deployments when not in use Customize carefully : While you can modify autoscaling behavior using our configuration options , note that preventing scale-to-zero will result in continuous GPU charges Consider your use case : For intermittent or low-frequency usage, serverless deployments might be more cost-effective For detailed configuration options, see our deployment guide.

",
    "$.messages[3].content[8].type": "text",
    "$.messages[3].content[9].text": "Title: Create Deployment
Link: https://docs.fireworks.ai/api-reference/create-deployment
Content: 



REST Endpoint POST /v1/accounts/{account_id}/deployments

Server(s): https://api.fireworks.ai


Summary:  

Description: N/A

Security methods: 
header: 
	Authorization: bearer auth
	

Request params: 
path params:
account_id:
	string
	Description: The Account Id
	Example: "<string>"

query params:
disableAutoDeploy:
	boolean
	Description: By default, a deployment created with a currently undeployed base model
will be deployed to this deployment. If true, this auto-deploy function
is disabled.
	Example: true
disableSpeculativeDecoding:
	boolean
	Description: By default, a deployment will use the speculative decoding settings from
the base model. If true, this will disable speculative decoding.
	Example: true
deploymentId:
	string
	Description: The ID of the deployment. If not specified, a random ID will be generated.
	Example: "<string>"


Request body: object
	Description: N/A
	Example: {"displayName":"<string>","description":"<string>","expireTime":"2023-11-07T05:31:56Z","minReplicaCount":123,"maxReplicaCount":123,"autoscalingPolicy":{"scaleUpWindow":"<string>","scaleDownWindow":"<string>","scaleToZeroWindow":"<string>","loadTargets":{}},"baseModel":"<string>","acceleratorCount":123,"acceleratorType":"ACCELERATOR_TYPE_UNSPECIFIED","precision":"PRECISION_UNSPECIFIED","enableAddons":true,"draftTokenCount":123,"draftModel":"<string>","ngramSpeculationLength":123,"deploymentTemplate":"<string>","autoTune":{"longPrompt":true},"placement":{"region":"REGION_UNSPECIFIED","multiRegion":"MULTI_REGION_UNSPECIFIED","regions":["REGION_UNSPECIFIED"]},"disableDeploymentSizeValidation":true}


Responses: 
200: object
	Description: N/A
	Example: {"name":"<string>","displayName":"<string>","description":"<string>","createTime":"2023-11-07T05:31:56Z","expireTime":"2023-11-07T05:31:56Z","purgeTime":"2023-11-07T05:31:56Z","deleteTime":"2023-11-07T05:31:56Z","state":"STATE_UNSPECIFIED","status":{"code":"OK","message":"<string>"},"minReplicaCount":123,"maxReplicaCount":123,"replicaCount":123,"autoscalingPolicy":{"scaleUpWindow":"<string>","scaleDownWindow":"<string>","scaleToZeroWindow":"<string>","loadTargets":{}},"baseModel":"<string>","acceleratorCount":123,"acceleratorType":"ACCELERATOR_TYPE_UNSPECIFIED","precision":"PRECISION_UNSPECIFIED","cluster":"<string>","enableAddons":true,"draftTokenCount":123,"draftModel":"<string>","ngramSpeculationLength":123,"numPeftDeviceCached":123,"deploymentTemplate":"<string>","autoTune":{"longPrompt":true},"placement":{"region":"REGION_UNSPECIFIED","multiRegion":"MULTI_REGION_UNSPECIFIED","regions":["REGION_UNSPECIFIED"]},"region":"REGION_UNSPECIFIED","updateTime":"2023-11-07T05:31:56Z","disableDeploymentSizeValidation":true}

",
    "$.messages[3].content[9].type": "text",
    "$.messages[3].role": "tool",
    "$.messages[3].tool_call_id": "functions.search:0",
    "$.messages[4].content": "Let me search for more specific information about deployment creation and management workflows.",
    "$.messages[4].role": "assistant",
    "$.messages[4].tool_calls[0].function.arguments": "{"query": "firectl deployment management commands create update list"}",
    "$.messages[4].tool_calls[0].function.name": "search",
    "$.messages[4].tool_calls[0].id": "functions.search:1",
    "$.messages[4].tool_calls[0].type": "function",
    "$.messages[5].content[0].text": "Title: Why am I getting "invalid id" errors when using firectl commands like create deployment or list deployments?
Link: https://docs.fireworks.ai/faq-new/fine-tuning/why-am-i-getting-invalid-id-errors-when-using-firectl-commands-like-create-deplo
Content: This error typically occurs when your account ID is not properly configured.

This error typically occurs when your account ID is not properly configured.

Why am I getting "invalid id" errors when using firectl commands like create deployment or list deployments?
This error typically occurs when your account ID is not properly configured.

",
    "$.messages[5].content[0].type": "text",
    "$.messages[5].content[1].text": "Title: Create a deployment
Link: https://docs.fireworks.ai/tools-sdks/firectl/commands/create-deployment
Content: Creates a new deployment. firectl create deployment [flags]

Creates a new deployment. firectl create deployment [flags]

Creates a new deployment.

",
    "$.messages[5].content[1].type": "text",
    "$.messages[5].content[2].text": "Title: Creating an on-demand deployment
Link: https://docs.fireworks.ai/guides/ondemand-deployments
Content: Choose a model Check out our model library to see a list of models available for on-demand deployment. You can also upload and use your own custom base model. Create a deployment To create a new deployment of a model from the model library, use: Or, for a custom base model you have uploaded to your account: This command will complete when the deployment is READY . To let it run asynchronously, remove the --wait flag. If your account has purchased reserved capacity and this deployment meets the reservation criteria, it will be counted against that reservation and not billed as an on-demand deployment. Verify the deployment is running You can verify the deployment is active by running: The state field should show READY. The deployment ID is the last part of the deployment name: accounts/<ACCOUNT_ID>/deployments/<DEPLOYMENT_ID>. You can also list all deployments in your account at any time by running: firectl list deployments

Choose a model Check out our model library to see a list of models available for on-demand deployment. You can also upload and use your own custom base model. Create a deployment To create a new deployment of a model from the model library, use: Or, for a custom base model you have uploaded to your account: This command will complete when the deployment is READY . To let it run asynchronously, remove the --wait flag. If your account has purchased reserved capacity and this deployment meets the reservation criteria, it will be counted against that reservation and not billed as an on-demand deployment. Verify the deployment is running You can verify the deployment is active by running: The state field should show READY. The deployment ID is the last part of the deployment name: accounts/<ACCOUNT_ID>/deployments/<DEPLOYMENT_ID>. You can also list all deployments in your account at any time by running: firectl list deployments

Publishing a deployed model
By default, models can only be queried by the account that owns them. To make a deployment public so anyone with a valid Fireworks API key can query it, update the deployed model with the --public flag. Find the Deployed Model ID Every model running on a deployment receives a unique deployed model ID. You can find this ID if you know the model name and deployment name using the following command: firectl list deployed-models --filter 'model="<MODEL_NAME>" AND deployment="<DEPLOYMENT_NAME>"'

",
    "$.messages[5].content[2].type": "text",
    "$.messages[5].content[3].text": "Title: 🚀 Easier & faster LoRA fine-tune deployments on Fireworks
Link: https://docs.fireworks.ai/updates/changelog
Content: You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: Previously, this involved two distinct steps, and the resulting deployment was slower than the base model: Create a deployment using firectl create deployment "accounts/fireworks/models/<MODEL_ID of base model>" --enable-addons Then deploy the addon to the deployment: firectl load-lora <MODEL_ID> --deployment <DEPLOYMENT_ID> For more information, see our deployment documentation. This change is for dedicated deployments with a single LoRA. You can still deploy multiple LoRAs on a deployment or deploy LoRA(s) on some Serverless models as described in the documentation.

",
    "$.messages[5].content[3].type": "text",
    "$.messages[5].content[4].text": "Title: Quick deployment
Link: https://docs.fireworks.ai/fine-tuning/single-lora
Content: Deploy your LoRA fine-tuned model with one simple command: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

Deploy your LoRA fine-tuned model with one simple command: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

Alternative deployment method
This two-step method is the standard approach for multi-LoRA deployments where multiple LoRA models share the same base model. While it can also be used for single LoRA deployments, it provides slower performance compared to live merge and is not recommended for single LoRA use cases. You can also deploy single LoRA models using a two-step process: Create base model deployment Deploy the base model with addons enabled: firectl create deployment "accounts/fireworks/models/<MODEL_ID of base model>" --enable-addons

",
    "$.messages[5].content[4].type": "text",
    "$.messages[5].content[5].text": "Title: Deploying a fine-tuned model using an on-demand deployment
Link: https://docs.fireworks.ai/fine-tuning/fine-tuning-models
Content: Use the following command to deploy your fine-tuned model using an on-demand deployment: firectl create deployment <FINE_TUNED_MODEL_ID>

Use the following command to deploy your fine-tuned model using an on-demand deployment: firectl create deployment <FINE_TUNED_MODEL_ID>

firectl
firectl create dataset <DATASET_ID> /path/to/jsonl/file

",
    "$.messages[5].content[5].type": "text",
    "$.messages[5].content[6].text": "Title: Does Fireworks support custom base models?
Link: https://docs.fireworks.ai/faq-new/models-inference/does-fireworks-support-custom-base-models
Content: Yes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.

Yes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.

Does Fireworks support custom base models?
Yes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.

",
    "$.messages[5].content[6].type": "text",
    "$.messages[5].content[7].text": "Title: firectl
Link: https://docs.fireworks.ai/fine-tuning/fine-tuning-vlm
Content: # Create a deployment for your fine-tuned VLM
firectl create deployment my-custom-vlm

# Check deployment status
firectl get deployment accounts/your-account/deployment/deployment-id

# Create a deployment for your fine-tuned VLM
firectl create deployment my-custom-vlm

# Check deployment status
firectl get deployment accounts/your-account/deployment/deployment-id

firectl create sftj \\
  --base-model accounts/fireworks/models/qwen2p5-vl-32b-instruct \\
  --dataset my-vlm-dataset \\
  --output-model my-custom-vlm \\
  --epochs 3 For additional parameters like learning rates, evaluation datasets, and batch sizes, see  Additional SFT job settings .

",
    "$.messages[5].content[7].type": "text",
    "$.messages[5].content[8].text": "Title: Uploading the model locally (firectl)
Link: https://docs.fireworks.ai/models/uploading-custom-models
Content: To upload a custom base model, run the following command. firectl create model <MODEL_ID> /path/to/files/

To upload a custom base model, run the following command. firectl create model <MODEL_ID> /path/to/files/

Publishing
By default, all models you create are only visible to and deployable by users within your account. To publish a model so anyone with a Fireworks account can deploy it, you can create it with the --public flag. This will allow it to show up in public model lists. To unpublish the model, just run firectl update model <MODEL_ID> --public=false

",
    "$.messages[5].content[8].type": "text",
    "$.messages[5].content[9].text": "Title: firectl
Link: https://docs.fireworks.ai/models/quantization
Content: firectl create deployment <MODEL> --accelerator-type NVIDIA_H100_80GB --precision FP8

firectl create deployment <MODEL> --accelerator-type NVIDIA_H100_80GB --precision FP8

Creating an FP8 deployment
By default, creating a dedicated deployment will use the FP16 checkpoint. To see what precisions are available for a
model, run:  get model <MODEL_ID>

",
    "$.messages[5].content[9].type": "text",
    "$.messages[5].role": "tool",
    "$.messages[5].tool_call_id": "functions.search:1",
    "$.messages[6].content": "Based on my research, here's a comprehensive guide to managing deployments on Fireworks:

## Deployment Management on Fireworks

### 1. Creating Deployments

**Basic deployment creation:**
\`\`\`bash
# Create a deployment from the model library
firectl create deployment accounts/fireworks/models/<MODEL_NAME>

# Create a deployment for a custom model
firectl create deployment <YOUR_MODEL_ID>

# Create with specific configuration
firectl create deployment <MODEL_ID> --wait
\`\`\`

**Advanced deployment options:**
\`\`\`bash
# Deploy with specific hardware requirements
firectl create deployment <MODEL> --accelerator-type NVIDIA_H100_80GB --precision FP8

# Deploy LoRA fine-tuned models (single command)
firectl create deployment "accounts/fireworks/models/<LORA_MODEL_ID>"

# Deploy with addons enabled (for multi-LoRA)
firectl create deployment <BASE_MODEL> --enable-addons
\`\`\`

### 2. Monitoring Deployments

**Check deployment status:**
\`\`\`bash
# List all deployments
firectl list deployments

# Get specific deployment details
firectl get deployment accounts/<ACCOUNT_ID>/deployments/<DEPLOYMENT_ID>

# Check if deployment is ready
# The state field should show "READY"
\`\`\`

**Verify deployed models:**
\`\`\`bash
# List deployed models
firectl list deployed-models

# Find specific deployed model
firectl list deployed-models --filter 'model="<MODEL_NAME>" AND deployment="<DEPLOYMENT_NAME>"'
\`\`\`

### 3. Managing Default Deployments

**Check default deployment:**
\`\`\`bash
# See which deployment is marked as default
firectl list deployed-models
# Look for "Default: true" in the output
\`\`\`

**Update default deployment:**
\`\`\`bash
# Update which deployment is the default
firectl update deployed-model <DEPLOYED_MODEL_ID> --default
\`\`\`

**Important note:** To delete a default deployment, you must either:
- Delete all other deployments for the same model first, OR
- Designate a different deployed model as the default

### 4. Scaling and Cost Management

**Autoscaling configuration:**
- **Default behavior:** Automatically scales to 0 replicas when not in use
- **Billing:** Pay only for GPU time when replicas are active
- **Manual scaling:** Use autoscaling policies to customize behavior

**Cost optimization commands:**
\`\`\`bash
# Scale deployment to zero (cost saving)
firectl update deployment <DEPLOYMENT_ID> --min-replica-count 0

# Or using the Python SDK
from fireworks.client import Fireworks
llm = Fireworks(model="<MODEL_ID>")
llm.scale_to_zero()
\`\`\`

### 5. Updating Deployments

**Update deployment configuration:**
\`\`\`bash
# Update display name, description, or scaling policies
firectl update deployment <DEPLOYMENT_ID> \\
  --display-name "New Name" \\
  --min-replica-count 1 \\
  --max-replica-count 5
\`\`\`

**Available update options:**
- Display name and description
- Replica counts (min/max)
- Autoscaling policies
- Expiration time
- Hardware configuration

### 6. Deleting Deployments

**Basic deletion:**
\`\`\`bash
# Delete a deployment
firectl delete deployment <DEPLOYMENT_ID>

# Force deletion (ignoring safety checks)
firectl delete deployment <DEPLOYMENT_ID> --ignore-checks

# Using Python SDK
llm.delete_deployment(ignore_checks=True)
\`\`\`

**API deletion:**
\`\`\`bash
# Via REST API
curl -X DELETE \\
  https://api.fireworks.ai/v1/accounts/{account_id}/deployments/{deployment_id} \\
  -H "Authorization: Bearer YOUR_API_KEY"
\`\`\`

### 7. Deployment Performance Expectations

**Timeline expectations:**
- **Initial deployment:** Should complete within minutes
- **Scaling from zero:** Brief availability delays while scaling up
- **Troubleshooting:** If deployment takes >1 hour, likely indicates a crash

### 8. Best Practices

**Cost management:**
1. **Monitor regularly:** Check active deployments frequently
2. **Delete unused deployments:** Avoid unnecessary GPU charges
3. **Use autoscaling:** Leverage scale-to-zero for intermittent usage
4. **Consider serverless:** For low-frequency usage patterns

**Configuration tips:**
- Use \`--wait\` flag for synchronous deployment creation
- Configure appropriate min/max replica counts for your use case
- Set expiration times for temporary deployments
- Monitor deployment status during initial creation

### 9. Common Workflows

**Complete deployment lifecycle:**
\`\`\`bash
# 1. Create deployment
firectl create deployment <MODEL_ID> --wait

# 2. Verify it's ready
firectl get deployment <DEPLOYMENT_ID>

# 3. Use the deployment
# Make API calls to the deployment

# 4. Monitor usage
firectl list deployments

# 5. Scale down when not needed
firectl update deployment <DEPLOYMENT_ID> --min-replica-count 0

# 6. Delete when completely done
firectl delete deployment <DEPLOYMENT_ID>
\`\`\`

This comprehensive guide covers everything from basic deployment creation to advanced cost optimization strategies. The key is understanding the balance between performance needs and cost efficiency through proper scaling configuration.",
    "$.messages[6].role": "assistant",
    "$.pid": 86994,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T11:19:37.022552",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "Run math evaluation on sample dataset using pytest interface.",
    "$.eval_metadata.name": "test_pytest_mcp_url",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.version": "v0.2.6.dev15+g6486dbd.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "At least one tool call was made",
    "$.evaluation_result.score": 1,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.row_id": "call-stand-low-night-conference",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "You are a helpful assistant that can answer questions about Fireworks.
ALWAYS provide code or commands to execute to answer the question.",
    "$.messages[0].role": "system",
    "$.messages[1].content": "Can you teach me about how to manage deployments on Fireworks",
    "$.messages[1].role": "user",
    "$.messages[2].content": "I'll help you learn about managing deployments on Fireworks AI. Let me search for comprehensive documentation about deployment management.",
    "$.messages[2].role": "assistant",
    "$.messages[2].tool_calls[0].function.arguments": "{"query": "deployment management Fireworks AI"}",
    "$.messages[2].tool_calls[0].function.name": "search",
    "$.messages[2].tool_calls[0].id": "functions.search:0",
    "$.messages[2].tool_calls[0].type": "function",
    "$.messages[3].content[0].text": "Title: Fireworks AI Developer Platform
Link: https://docs.fireworks.ai/getting-started/introduction
Content: Fireworks AI is the best platform for building AI product experiences with open source AI models. You can run and customize AI models with just a few lines of code!

Fireworks AI is the best platform for building AI product experiences with open source AI models. You can run and customize AI models with just a few lines of code!

Start building with open source AI models

",
    "$.messages[3].content[0].type": "text",
    "$.messages[3].content[1].text": "Title: How do I close my Fireworks.ai account?
Link: https://docs.fireworks.ai/faq-new/account-access/how-do-i-close-my-fireworksai-account
Content: To close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed

To close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed

How do I close my Fireworks.ai account?
To close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed

",
    "$.messages[3].content[1].type": "text",
    "$.messages[3].content[2].text": "Title: Do you host your deployments in the EU or Asia?
Link: https://docs.fireworks.ai/faq-new/support-general/do-you-host-your-deployments-in-the-eu-or-asia
Content: We are currently deployed in multiple U.S.-based locations. However, we’re open to hearing more about your specific requirements. You can: Join our Discord community Write to us at inquiries@fireworks.ai If you're an Enterprise customer, please contact your dedicated customer support representative to ensure a timely response.

We are currently deployed in multiple U.S.-based locations. However, we’re open to hearing more about your specific requirements. You can: Join our Discord community Write to us at inquiries@fireworks.ai If you're an Enterprise customer, please contact your dedicated customer support representative to ensure a timely response.

Do you host your deployments in the EU or Asia?
We are currently deployed in multiple U.S.-based locations. However, we’re open to hearing more about your specific requirements. You can: Join our Discord community Write to us at inquiries@fireworks.ai If you're an Enterprise customer, please contact your dedicated customer support representative to ensure a timely response.

",
    "$.messages[3].content[2].type": "text",
    "$.messages[3].content[3].text": "Title: Introducing the Fireworks Build SDK
Link: https://docs.fireworks.ai/tools-sdks/python-client/sdk-introduction
Content: The Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.

The Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.

Introducing the Fireworks Build SDK
The Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.

",
    "$.messages[3].content[3].type": "text",
    "$.messages[3].content[4].text": "Title: On-demand deployments
Link: https://docs.fireworks.ai/guides/ondemand-deployments
Content: If you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.

If you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.

On-demand deployments
If you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.

",
    "$.messages[3].content[4].type": "text",
    "$.messages[3].content[5].text": "Title: I have multiple Fireworks accounts. When I try to login with Google on Fireworks' web UI, I'm getting signed into the wrong account. How do I fix this?
Link: https://docs.fireworks.ai/faq-new/account-access/i-have-multiple-fireworks-accounts-when-i-try-to-login-with-google-on-fireworks
Content: If you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.

If you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.

I have multiple Fireworks accounts. When I try to login with Google on Fireworks' web UI, I'm getting signed into the wrong account. How do I fix this?
If you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.

",
    "$.messages[3].content[5].type": "text",
    "$.messages[3].content[6].text": "Title: Introduction
Link: https://docs.fireworks.ai/api-reference/introduction
Content: Fireworks AI REST API enables you to interact with various language, image and embedding models using an API Key. It also lets you automate management of models, deployments, datasets, and more.

Fireworks AI REST API enables you to interact with various language, image and embedding models using an API Key. It also lets you automate management of models, deployments, datasets, and more.

Introduction
Fireworks AI REST API enables you to interact with various language, image and embedding models using an API Key. It also lets you automate management of models, deployments, datasets, and more.

",
    "$.messages[3].content[6].type": "text",
    "$.messages[3].content[7].text": "Title: Create a deployment
Link: https://docs.fireworks.ai/tools-sdks/firectl/commands/create-deployment
Content: Create a Deployment on Fireworks AI platform

Create a Deployment on Fireworks AI platform

Creates a new deployment.

",
    "$.messages[3].content[7].type": "text",
    "$.messages[3].content[8].text": "Title: How much does Fireworks cost?
Link: https://docs.fireworks.ai/faq-new/billing-pricing/how-much-does-fireworks-cost
Content: Fireworks AI operates on a pay-as-you-go model for all non-Enterprise usage, and new users automatically receive free credits. You pay based on: Per token for serverless inference Per GPU usage time for on-demand deployments Per token of training data for fine-tuning For customers needing enterprise-grade security and reliability , please reach out to us at inquiries@fireworks.ai to discuss options. Find out more about our current pricing on our Pricing page.

Fireworks AI operates on a pay-as-you-go model for all non-Enterprise usage, and new users automatically receive free credits. You pay based on: Per token for serverless inference Per GPU usage time for on-demand deployments Per token of training data for fine-tuning For customers needing enterprise-grade security and reliability , please reach out to us at inquiries@fireworks.ai to discuss options. Find out more about our current pricing on our Pricing page.

How much does Fireworks cost?
Fireworks AI operates on a pay-as-you-go model for all non-Enterprise usage, and new users automatically receive free credits. You pay based on: Per token for serverless inference Per GPU usage time for on-demand deployments Per token of training data for fine-tuning For customers needing enterprise-grade security and reliability , please reach out to us at inquiries@fireworks.ai to discuss options. Find out more about our current pricing on our Pricing page.

",
    "$.messages[3].content[8].type": "text",
    "$.messages[3].content[9].text": "Title: Python (Fireworks)
Link: https://docs.fireworks.ai/getting-started/quickstart
Content: This is the recommended way to get started : This  Python (Fireworks)  tab uses our  Fireworks Build
SDK  for the best performance and developer experience.
See our  Client-side performance
optimization  guide for
more details. Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. pip install --upgrade fireworks-ai The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either  ~/.bash_profile  for Bash or  ~/.zshrc  for  Zsh .
You can do this by running the command: vim ~/.bash_profile vim ~/.zshrc Add a new line to the file with the following: export FIREWORKS_API_KEY="<API_KEY>" After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. source ~/.bash_profile source ~/.zshrc You can verify that the variable has been set correctly by running  echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. setx FIREWORKS_API_KEY "<API_KEY>" To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: echo %FIREWORKS_API_KEY% You can quickly instantiate the  LLM  class and call the Fireworks API. The Build SDK handles deployment management automatically. from fireworks import LLM

# Basic usage - SDK automatically selects optimal deployment type
llm = LLM(model="llama4-maverick-instruct-basic", deployment_type="auto")

response = llm.chat.completions.create(
    messages=[{"role": "user", "content": "Say this is a test"}]
)

print(response.choices[0].message.content) You can also pass the API key directly to the LLM constructor:  LLM(model="llama4-maverick-instruct-basic", deployment_type="auto", api_key="<FIREWORKS_API_KEY>")

This is the recommended way to get started : This  Python (Fireworks)  tab uses our  Fireworks Build
SDK  for the best performance and developer experience.
See our  Client-side performance
optimization  guide for
more details. Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. pip install --upgrade fireworks-ai The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either  ~/.bash_profile  for Bash or  ~/.zshrc  for  Zsh .
You can do this by running the command: vim ~/.bash_profile vim ~/.zshrc Add a new line to the file with the following: export FIREWORKS_API_KEY="<API_KEY>" After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. source ~/.bash_profile source ~/.zshrc You can verify that the variable has been set correctly by running  echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. setx FIREWORKS_API_KEY "<API_KEY>" To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: echo %FIREWORKS_API_KEY% You can quickly instantiate the  LLM  class and call the Fireworks API. The Build SDK handles deployment management automatically. from fireworks import LLM

# Basic usage - SDK automatically selects optimal deployment type
llm = LLM(model="llama4-maverick-instruct-basic", deployment_type="auto")

response = llm.chat.completions.create(
    messages=[{"role": "user", "content": "Say this is a test"}]
)

print(response.choices[0].message.content) You can also pass the API key directly to the LLM constructor:  LLM(model="llama4-maverick-instruct-basic", deployment_type="auto", api_key="<FIREWORKS_API_KEY>")

Set up your developer environment & call the Fireworks API
This is the recommended way to get started : This  tab uses our Fireworks Build
SDK for the best performance and developer experience.
See our Client-side performance
optimization guide for
more details. Install SDK Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Configure API Key Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either ~/.bash_profile for Bash or ~/.zshrc for Zsh .
You can do this by running the command: Add a new line to the file with the following: After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. You can verify that the variable has been set correctly by running echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: Sending the first API Request You can quickly instantiate the LLM class and call the Fireworks API. The Build SDK handles deployment management automatically. You can also pass the API key directly to the LLM constructor: LLM(model="llama4-maverick-instruct-basic", deployment_type="auto", api_key="<FIREWORKS_API_KEY>") Install SDK Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. Fireworks AI platform offers drop-in replacement with OpenAI Python Client. Configure API Key Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either ~/.bash_profile for Bash or ~/.zshrc for Zsh .
You can do this by running the command: Add a new line to the file with the following: After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. You can verify that the variable has been set correctly by running echo $OPENAI_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. setx OPENAI_API_BASE "https://api.fireworks.ai/inference/v1"
setx OPENAI_API_KEY "<API_KEY>"

",
    "$.messages[3].content[9].type": "text",
    "$.messages[3].role": "tool",
    "$.messages[3].tool_call_id": "functions.search:0",
    "$.messages[4].content": "Let me search for more specific information about deployment management commands and workflows.",
    "$.messages[4].role": "assistant",
    "$.messages[4].tool_calls[0].function.arguments": "{"query": "firectl deployment commands create manage update delete"}",
    "$.messages[4].tool_calls[0].function.name": "search",
    "$.messages[4].tool_calls[0].id": "functions.search:1",
    "$.messages[4].tool_calls[0].type": "function",
    "$.messages[5].content[0].text": "Title: Why am I getting "invalid id" errors when using firectl commands like create deployment or list deployments?
Link: https://docs.fireworks.ai/faq-new/fine-tuning/why-am-i-getting-invalid-id-errors-when-using-firectl-commands-like-create-deplo
Content: This error typically occurs when your account ID is not properly configured.

This error typically occurs when your account ID is not properly configured.

Why am I getting "invalid id" errors when using firectl commands like create deployment or list deployments?
This error typically occurs when your account ID is not properly configured.

",
    "$.messages[5].content[0].type": "text",
    "$.messages[5].content[1].text": "Title: Delete a deployment
Link: https://docs.fireworks.ai/tools-sdks/firectl/commands/delete-resources
Content: Deletes an model deployment. firectl delete deployment [flags]

Deletes an model deployment. firectl delete deployment [flags]

Example
firectl delete deployment my-deployment

",
    "$.messages[5].content[1].type": "text",
    "$.messages[5].content[2].text": "Title: Create a deployment
Link: https://docs.fireworks.ai/tools-sdks/firectl/commands/create-deployment
Content: Creates a new deployment. firectl create deployment [flags]

Creates a new deployment. firectl create deployment [flags]

Create a Deployment on Fireworks AI platform

",
    "$.messages[5].content[2].type": "text",
    "$.messages[5].content[3].text": "Title: Managing default deployments
Link: https://docs.fireworks.ai/guides/ondemand-deployments
Content: Since a model may be deployed to multiple deployments, querying by model name will route to the "default" deployed model. You can see which deployed model entry is marked with Default: true using: This will show the Deployed Model Refs section with the Default: true entry. To update the default deployed model, note the Name of the deployed model reference above. Then run: firectl update deployed-model <DEPLOYED_MODEL_ID_2> --default

Since a model may be deployed to multiple deployments, querying by model name will route to the "default" deployed model. You can see which deployed model entry is marked with Default: true using: This will show the Deployed Model Refs section with the Default: true entry. To update the default deployed model, note the Name of the deployed model reference above. Then run: firectl update deployed-model <DEPLOYED_MODEL_ID_2> --default

Since a model may be deployed to multiple deployments, querying by model name will route to the "default" deployed model. You can see which deployed model entry is marked with Default: true using: This will show the Deployed Model Refs section with the Default: true entry. To update the default deployed model, note the Name of the deployed model reference above. Then run: To delete a default deployment you must delete all other deployments for the same model first, or designate a different deployed model as the default as described above. This is to ensure that querying by model name will always route to an unambiguous default deployment as long as deployments for the model exist.

",
    "$.messages[5].content[3].type": "text",
    "$.messages[5].content[4].text": "Title: 🚀 Easier & faster LoRA fine-tune deployments on Fireworks
Link: https://docs.fireworks.ai/updates/changelog
Content: You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: Previously, this involved two distinct steps, and the resulting deployment was slower than the base model: Create a deployment using firectl create deployment "accounts/fireworks/models/<MODEL_ID of base model>" --enable-addons Then deploy the addon to the deployment: firectl load-lora <MODEL_ID> --deployment <DEPLOYMENT_ID> For more information, see our deployment documentation. This change is for dedicated deployments with a single LoRA. You can still deploy multiple LoRAs on a deployment or deploy LoRA(s) on some Serverless models as described in the documentation.

",
    "$.messages[5].content[4].type": "text",
    "$.messages[5].content[5].text": "Title: Deployment Command
Link: https://docs.fireworks.ai/evaluators/examples/gcp_cloud_run_deployment_example
Content: It's recommended to run the deployment command from the directory containing the reward function script ( dummy_rewards.py ) and your rewardkit.yaml (if used), for example, from examples/gcp_cloud_run_deployment_example/. Ensure your virtual environment is active: Run the deployment command: Command Explanation: --id my-dummy-gcp-evaluator : A unique ID for your evaluator on the Fireworks AI platform. --target gcp-cloud-run : Specifies deployment to GCP Cloud Run. --function-ref dummy_rewards.hello_world_reward : The Python import path to your reward function. If dummy_rewards.py is in the current directory, this reference works. --gcp-auth-mode api-key : Configures the Cloud Run service with API key authentication. reward-kit will generate a key, store it in GCP Secret Manager, and configure the service. The key is also saved to your local rewardkit.yaml under evaluator_endpoint_keys . This is the default if not specified. --verbose : Shows detailed output, including gcloud commands being executed. --force : (Optional) If an evaluator with the same --id already exists, this flag will delete the existing one before creating the new one.

It's recommended to run the deployment command from the directory containing the reward function script ( dummy_rewards.py ) and your rewardkit.yaml (if used), for example, from examples/gcp_cloud_run_deployment_example/. Ensure your virtual environment is active: Run the deployment command: Command Explanation: --id my-dummy-gcp-evaluator : A unique ID for your evaluator on the Fireworks AI platform. --target gcp-cloud-run : Specifies deployment to GCP Cloud Run. --function-ref dummy_rewards.hello_world_reward : The Python import path to your reward function. If dummy_rewards.py is in the current directory, this reference works. --gcp-auth-mode api-key : Configures the Cloud Run service with API key authentication. reward-kit will generate a key, store it in GCP Secret Manager, and configure the service. The key is also saved to your local rewardkit.yaml under evaluator_endpoint_keys . This is the default if not specified. --verbose : Shows detailed output, including gcloud commands being executed. --force : (Optional) If an evaluator with the same --id already exists, this flag will delete the existing one before creating the new one.

It's recommended to run the deployment command from the directory containing the reward function script ( dummy_rewards.py ) and your rewardkit.yaml (if used), for example, from examples/gcp_cloud_run_deployment_example/. Ensure your virtual environment is active: source .venv/bin/activate

",
    "$.messages[5].content[5].type": "text",
    "$.messages[5].content[6].text": "Title: Quick deployment
Link: https://docs.fireworks.ai/fine-tuning/single-lora
Content: Deploy your LoRA fine-tuned model with one simple command: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

Deploy your LoRA fine-tuned model with one simple command: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

Alternative deployment method
This two-step method is the standard approach for multi-LoRA deployments where multiple LoRA models share the same base model. While it can also be used for single LoRA deployments, it provides slower performance compared to live merge and is not recommended for single LoRA use cases. You can also deploy single LoRA models using a two-step process: Create base model deployment Deploy the base model with addons enabled: firectl create deployment "accounts/fireworks/models/<MODEL_ID of base model>" --enable-addons

",
    "$.messages[5].content[6].type": "text",
    "$.messages[5].content[7].text": "Title: How do I close my Fireworks.ai account?
Link: https://docs.fireworks.ai/faq-new/account-access/how-do-i-close-my-fireworksai-account
Content: To close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed

To close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed

How do I close my Fireworks.ai account?
To close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed

",
    "$.messages[5].content[7].type": "text",
    "$.messages[5].content[8].text": "Title: Delete Deployment
Link: https://docs.fireworks.ai/api-reference/delete-deployment
Content: 



REST Endpoint DELETE /v1/accounts/{account_id}/deployments/{deployment_id}

Server(s): https://api.fireworks.ai


Summary:  

Description: N/A

Security methods: 
header: 
	Authorization: bearer auth
	

Request params: 
path params:
account_id:
	string
	Description: The Account Id
	Example: "<string>"
deployment_id:
	string
	Description: The Deployment Id
	Example: "<string>"

query params:
hard:
	boolean
	Description: If true, this will perform a hard deletion.
	Example: true
ignoreChecks:
	boolean
	Description: If true, this will ignore checks and force the deletion of a deployment that is currently
deployed and is in use.
	Example: true


Request body: N/A


Responses: 
200: object
	Description: N/A
	Example: {}

",
    "$.messages[5].content[8].type": "text",
    "$.messages[5].content[9].text": "Title: Deploying a fine-tuned model using an on-demand deployment
Link: https://docs.fireworks.ai/fine-tuning/fine-tuning-models
Content: Use the following command to deploy your fine-tuned model using an on-demand deployment: firectl create deployment <FINE_TUNED_MODEL_ID>

Use the following command to deploy your fine-tuned model using an on-demand deployment: firectl create deployment <FINE_TUNED_MODEL_ID>

firectl
firectl create dataset <DATASET_ID> /path/to/jsonl/file

",
    "$.messages[5].content[9].type": "text",
    "$.messages[5].role": "tool",
    "$.messages[5].tool_call_id": "functions.search:1",
    "$.messages[6].content": "Let me search for more information about listing and managing existing deployments.",
    "$.messages[6].role": "assistant",
    "$.messages[6].tool_calls[0].function.arguments": "{"query": "list deployments firectl commands deployment management"}",
    "$.messages[6].tool_calls[0].function.name": "search",
    "$.messages[6].tool_calls[0].id": "functions.search:0",
    "$.messages[6].tool_calls[0].type": "function",
    "$.messages[7].content[0].text": "Title: Why am I getting "invalid id" errors when using firectl commands like create deployment or list deployments?
Link: https://docs.fireworks.ai/faq-new/fine-tuning/why-am-i-getting-invalid-id-errors-when-using-firectl-commands-like-create-deplo
Content: This error typically occurs when your account ID is not properly configured.

This error typically occurs when your account ID is not properly configured.

Why am I getting "invalid id" errors when using firectl commands like create deployment or list deployments?
This error typically occurs when your account ID is not properly configured.

",
    "$.messages[7].content[0].type": "text",
    "$.messages[7].content[1].text": "Title: List deployments
Link: https://docs.fireworks.ai/tools-sdks/firectl/commands/list-resources
Content: Prints all deployments in the account. firectl list deployments [flags]

Prints all deployments in the account. firectl list deployments [flags]

List deployed models
Prints all deployed models in an account. firectl list deployed-models [flags]

",
    "$.messages[7].content[1].type": "text",
    "$.messages[7].content[2].text": "Title: 🚀 Easier & faster LoRA fine-tune deployments on Fireworks
Link: https://docs.fireworks.ai/updates/changelog
Content: You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: Previously, this involved two distinct steps, and the resulting deployment was slower than the base model: Create a deployment using firectl create deployment "accounts/fireworks/models/<MODEL_ID of base model>" --enable-addons Then deploy the addon to the deployment: firectl load-lora <MODEL_ID> --deployment <DEPLOYMENT_ID> For more information, see our deployment documentation. This change is for dedicated deployments with a single LoRA. You can still deploy multiple LoRAs on a deployment or deploy LoRA(s) on some Serverless models as described in the documentation.

",
    "$.messages[7].content[2].type": "text",
    "$.messages[7].content[3].text": "Title: Managing default deployments
Link: https://docs.fireworks.ai/guides/ondemand-deployments
Content: Since a model may be deployed to multiple deployments, querying by model name will route to the "default" deployed model. You can see which deployed model entry is marked with Default: true using: firectl get model <MODEL_ID>

Since a model may be deployed to multiple deployments, querying by model name will route to the "default" deployed model. You can see which deployed model entry is marked with Default: true using: firectl get model <MODEL_ID>

Creating an on-demand deployment
Choose a model Check out our model library to see a list of models available for on-demand deployment. You can also upload and use your own custom base model. Create a deployment To create a new deployment of a model from the model library, use: Or, for a custom base model you have uploaded to your account: This command will complete when the deployment is READY . To let it run asynchronously, remove the --wait flag. If your account has purchased reserved capacity and this deployment meets the reservation criteria, it will be counted against that reservation and not billed as an on-demand deployment. Verify the deployment is running You can verify the deployment is active by running: The state field should show READY. The deployment ID is the last part of the deployment name: accounts/<ACCOUNT_ID>/deployments/<DEPLOYMENT_ID>. You can also list all deployments in your account at any time by running: firectl list deployments

",
    "$.messages[7].content[3].type": "text",
    "$.messages[7].content[4].text": "Title: Does Fireworks support custom base models?
Link: https://docs.fireworks.ai/faq-new/models-inference/does-fireworks-support-custom-base-models
Content: Yes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.

Yes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.

Does Fireworks support custom base models?
Yes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.

",
    "$.messages[7].content[4].type": "text",
    "$.messages[7].content[5].text": "Title: Quick deployment
Link: https://docs.fireworks.ai/fine-tuning/single-lora
Content: Deploy your LoRA fine-tuned model with one simple command: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

Deploy your LoRA fine-tuned model with one simple command: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

Alternative deployment method
This two-step method is the standard approach for multi-LoRA deployments where multiple LoRA models share the same base model. While it can also be used for single LoRA deployments, it provides slower performance compared to live merge and is not recommended for single LoRA use cases. You can also deploy single LoRA models using a two-step process: Create base model deployment Deploy the base model with addons enabled: firectl create deployment "accounts/fireworks/models/<MODEL_ID of base model>" --enable-addons

",
    "$.messages[7].content[5].type": "text",
    "$.messages[7].content[6].text": "Title: List Deployments
Link: https://docs.fireworks.ai/api-reference/list-deployments
Content: 



REST Endpoint GET /v1/accounts/{account_id}/deployments

Server(s): https://api.fireworks.ai


Summary:  

Description: N/A

Security methods: 
header: 
	Authorization: bearer auth
	

Request params: 
path params:
account_id:
	string
	Description: The Account Id
	Example: "<string>"

query params:
pageSize:
	integer
	Description: The maximum number of deployments to return. The maximum page_size is 200,
values above 200 will be coerced to 200.
If unspecified, the default is 50.
	Example: 123
pageToken:
	string
	Description: A page token, received from a previous ListDeployments call. Provide this
to retrieve the subsequent page. When paginating, all other parameters
provided to ListDeployments must match the call that provided the page
token.
	Example: "<string>"
filter:
	string
	Description: Only deployment satisfying the provided filter (if specified) will be
returned. See https://google.aip.dev/160 for the filter grammar.
	Example: "<string>"
orderBy:
	string
	Description: A comma-separated list of fields to order by. e.g. "foo,bar"
The default sort order is ascending. To specify a descending order for a
field, append a " desc" suffix. e.g. "foo desc,bar"
Subfields are specified with a "." character. e.g. "foo.bar"
If not specified, the default order is by "create_time".
	Example: "<string>"
showDeleted:
	boolean
	Description: If set, DELETED deployments will be included.
	Example: true
readMask:
	string
	Description: The fields to be returned in the response. If empty or "*", all fields will be returned.
	Example: "<string>"


Request body: N/A


Responses: 
200: object
	Description: N/A
	Example: {"deployments":[{"name":"<string>","displayName":"<string>","description":"<string>","createTime":"2023-11-07T05:31:56Z","expireTime":"2023-11-07T05:31:56Z","purgeTime":"2023-11-07T05:31:56Z","deleteTime":"2023-11-07T05:31:56Z","state":"STATE_UNSPECIFIED","status":{"code":"OK","message":"<string>"},"minReplicaCount":123,"maxReplicaCount":123,"replicaCount":123,"autoscalingPolicy":{"scaleUpWindow":"<string>","scaleDownWindow":"<string>","scaleToZeroWindow":"<string>","loadTargets":{}},"baseModel":"<string>","acceleratorCount":123,"acceleratorType":"ACCELERATOR_TYPE_UNSPECIFIED","precision":"PRECISION_UNSPECIFIED","cluster":"<string>","enableAddons":true,"draftTokenCount":123,"draftModel":"<string>","ngramSpeculationLength":123,"numPeftDeviceCached":123,"deploymentTemplate":"<string>","autoTune":{"longPrompt":true},"placement":{"region":"REGION_UNSPECIFIED","multiRegion":"MULTI_REGION_UNSPECIFIED","regions":["REGION_UNSPECIFIED"]},"region":"REGION_UNSPECIFIED","updateTime":"2023-11-07T05:31:56Z","disableDeploymentSizeValidation":true}],"nextPageToken":"<string>","totalSize":123}

",
    "$.messages[7].content[6].type": "text",
    "$.messages[7].content[7].text": "Title: Deploy Command
Link: https://docs.fireworks.ai/evaluators/cli_reference/cli_overview
Content: The deploy command deploys a reward function as an evaluator on the Fireworks platform.

The deploy command deploys a reward function as an evaluator on the Fireworks platform.

Command Overview
The Reward Kit CLI supports the following main commands: run : Run a local evaluation pipeline using a Hydra configuration. preview : Preview evaluation results or re-evaluate generated outputs. deploy : Deploy a reward function as an evaluator. agent-eval : Run agent evaluations on task bundles. list : List existing evaluators (coming soon). delete : Delete an evaluator (coming soon).

",
    "$.messages[7].content[7].type": "text",
    "$.messages[7].content[8].text": "Title: Deploying a fine-tuned model using an on-demand deployment
Link: https://docs.fireworks.ai/fine-tuning/fine-tuning-models
Content: Use the following command to deploy your fine-tuned model using an on-demand deployment: firectl create deployment <FINE_TUNED_MODEL_ID>

Use the following command to deploy your fine-tuned model using an on-demand deployment: firectl create deployment <FINE_TUNED_MODEL_ID>

Deploying a fine-tuned model serverlessly
Not all base models support serverless addons. Please check this list for the full list of serverless models that support LoRA add-ons. If your base model supports serverless addons, you can run the following command to load the fine-tuned model into a serverless deployment: firectl load-lora <FINE_TUNED_MODEL_ID>

",
    "$.messages[7].content[8].type": "text",
    "$.messages[7].content[9].text": "Title: Create a deployment
Link: https://docs.fireworks.ai/tools-sdks/firectl/commands/create-deployment
Content: Creates a new deployment. firectl create deployment [flags]

Creates a new deployment. firectl create deployment [flags]

Create a Deployment on Fireworks AI platform

",
    "$.messages[7].content[9].type": "text",
    "$.messages[7].role": "tool",
    "$.messages[7].tool_call_id": "functions.search:0",
    "$.messages[8].content": "Based on my research, here's a comprehensive guide to managing deployments on Fireworks AI:

## Overview of Deployment Types

Fireworks AI offers several deployment options:

1. **Serverless Models**: Pay-per-token, shared infrastructure
2. **On-demand Deployments**: Dedicated GPU instances billed by GPU-second
3. **Reserved Capacity**: Pre-purchased dedicated capacity for high-volume usage

## Key Benefits of On-demand Deployments
- Predictable performance unaffected by other users
- No hard rate limits
- Cost-effective for high utilization
- Access to larger model selection
- Support for custom base models

## Managing Deployments with firectl

### Installation and Setup
First, install the Fireworks CLI tools:

\`\`\`bash
pip install fireworks-ai
\`\`\`

Set up your API key:
\`\`\`bash
export FIREWORKS_API_KEY="your-api-key-here"
\`\`\`

### Basic Deployment Commands

#### 1. List All Deployments
\`\`\`bash
firectl list deployments
\`\`\`

#### 2. Create a New Deployment
\`\`\`bash
# Basic deployment
firectl create deployment <MODEL_ID>

# With custom configuration
firectl create deployment <MODEL_ID> --min-replica-count 1 --max-replica-count 3

# Deploy a LoRA fine-tuned model
firectl create deployment "accounts/fireworks/models/<LORA_MODEL_ID>"

# Deploy with addons enabled (for multi-LoRA)
firectl create deployment <MODEL_ID> --enable-addons
\`\`\`

#### 3. Check Deployment Status
\`\`\`bash
firectl get deployment <DEPLOYMENT_ID>
\`\`\`

#### 4. Delete a Deployment
\`\`\`bash
# Soft delete
firectl delete deployment <DEPLOYMENT_ID>

# Force delete (ignores usage checks)
firectl delete deployment <DEPLOYMENT_ID> --ignore-checks
\`\`\`

### Managing Default Deployments
When you have multiple deployments of the same model:

\`\`\`bash
# Check which deployment is default
firectl get model <MODEL_ID>

# Update default deployment
firectl update deployed-model <DEPLOYED_MODEL_ID> --default
\`\`\`

## Using the Fireworks Build SDK (Python)

The Fireworks Build SDK provides a higher-level interface:

\`\`\`python
from fireworks import LLM

# Auto-managed deployment
llm = LLM(model="llama4-maverick-instruct-basic", deployment_type="auto")

# Explicit on-demand deployment
llm = LLM(model="your-model-id", deployment_type="on-demand")

# Make API calls
response = llm.chat.completions.create(
    messages=[{"role": "user", "content": "Hello!"}]
)
\`\`\`

## REST API Management

### List Deployments via REST API
\`\`\`bash
curl -X GET \\
  "https://api.fireworks.ai/v1/accounts/{account_id}/deployments" \\
  -H "Authorization: Bearer $FIREWORKS_API_KEY"
\`\`\`

### Delete Deployment via REST API
\`\`\`bash
curl -X DELETE \\
  "https://api.fireworks.ai/v1/accounts/{account_id}/deployments/{deployment_id}" \\
  -H "Authorization: Bearer $FIREWORKS_API_KEY"
\`\`\`

## Deployment Workflow Examples

### 1. Deploy a Fine-tuned Model
\`\`\`bash
# Single command deployment
firectl create deployment "accounts/fireworks/models/my-lora-model"

# Wait for deployment to be ready
firectl get deployment <deployment-id>
\`\`\`

### 2. Multi-LoRA Deployment (Advanced)
\`\`\`bash
# Step 1: Create base deployment with addons
firectl create deployment "accounts/fireworks/models/base-model" --enable-addons

# Step 2: Load LoRA models
firectl load-lora <LORA_MODEL_ID_1> --deployment <DEPLOYMENT_ID>
firectl load-lora <LORA_MODEL_ID_2> --deployment <DEPLOYMENT_ID>
\`\`\`

### 3. Custom Model Deployment
\`\`\`bash
# First upload your custom model
# Then deploy it
firectl create deployment "accounts/{account_id}/models/custom-model-id"
\`\`\`

## Best Practices

1. **Monitor Costs**: On-demand deployments are billed by GPU-second
2. **Use Reserved Capacity**: For consistent high-volume usage
3. **Set Appropriate Scaling**: Configure min/max replica counts based on expected load
4. **Clean Up**: Delete unused deployments to avoid charges
5. **Check Status**: Always verify deployment state before using

## Troubleshooting

- **"Invalid ID" errors**: Ensure your account ID is properly configured
- **Deployment stuck**: Check resource quotas and account limits
- **Performance issues**: Verify deployment is in READY state and check replica counts

Would you like me to dive deeper into any specific aspect of deployment management, such as scaling configurations, cost optimization, or advanced deployment patterns?",
    "$.messages[8].role": "assistant",
    "$.pid": 89723,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T11:27:14.528488",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "Run math evaluation on sample dataset using pytest interface.",
    "$.eval_metadata.name": "test_pytest_mcp_url",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.version": "v0.2.6.dev15+g6486dbd.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "At least one tool call was made",
    "$.evaluation_result.score": 1,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.row_id": "construct-do-safe-art-song",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "You are a helpful assistant that can answer questions about Fireworks.
ALWAYS provide code or commands to execute to answer the question.",
    "$.messages[0].role": "system",
    "$.messages[1].content": "Can you teach me about how to manage deployments on Fireworks",
    "$.messages[1].role": "user",
    "$.messages[2].content": "I'll help you learn about managing deployments on Fireworks! Let me search for comprehensive information about deployment management.",
    "$.messages[2].role": "assistant",
    "$.messages[2].tool_calls[0].function.arguments": "{"query": "deployment management Fireworks AI"}",
    "$.messages[2].tool_calls[0].function.name": "search",
    "$.messages[2].tool_calls[0].id": "functions.search:0",
    "$.messages[2].tool_calls[0].type": "function",
    "$.messages[3].content[0].text": "Title: Fireworks AI Developer Platform
Link: https://docs.fireworks.ai/getting-started/introduction
Content: Fireworks AI is the best platform for building AI product experiences with open source AI models. You can run and customize AI models with just a few lines of code!

Fireworks AI is the best platform for building AI product experiences with open source AI models. You can run and customize AI models with just a few lines of code!

Start building with open source AI models

",
    "$.messages[3].content[0].type": "text",
    "$.messages[3].content[1].text": "Title: How do I close my Fireworks.ai account?
Link: https://docs.fireworks.ai/faq-new/account-access/how-do-i-close-my-fireworksai-account
Content: To close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed

To close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed

How do I close my Fireworks.ai account?
To close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed

",
    "$.messages[3].content[1].type": "text",
    "$.messages[3].content[2].text": "Title: Do you host your deployments in the EU or Asia?
Link: https://docs.fireworks.ai/faq-new/support-general/do-you-host-your-deployments-in-the-eu-or-asia
Content: We are currently deployed in multiple U.S.-based locations. However, we’re open to hearing more about your specific requirements. You can: Join our Discord community Write to us at inquiries@fireworks.ai If you're an Enterprise customer, please contact your dedicated customer support representative to ensure a timely response.

We are currently deployed in multiple U.S.-based locations. However, we’re open to hearing more about your specific requirements. You can: Join our Discord community Write to us at inquiries@fireworks.ai If you're an Enterprise customer, please contact your dedicated customer support representative to ensure a timely response.

Do you host your deployments in the EU or Asia?
We are currently deployed in multiple U.S.-based locations. However, we’re open to hearing more about your specific requirements. You can: Join our Discord community Write to us at inquiries@fireworks.ai If you're an Enterprise customer, please contact your dedicated customer support representative to ensure a timely response.

",
    "$.messages[3].content[2].type": "text",
    "$.messages[3].content[3].text": "Title: Introducing the Fireworks Build SDK
Link: https://docs.fireworks.ai/tools-sdks/python-client/sdk-introduction
Content: The Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.

The Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.

Introducing the Fireworks Build SDK
The Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.

",
    "$.messages[3].content[3].type": "text",
    "$.messages[3].content[4].text": "Title: On-demand deployments
Link: https://docs.fireworks.ai/guides/ondemand-deployments
Content: If you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.

If you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.

On-demand deployments
If you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.

",
    "$.messages[3].content[4].type": "text",
    "$.messages[3].content[5].text": "Title: I have multiple Fireworks accounts. When I try to login with Google on Fireworks' web UI, I'm getting signed into the wrong account. How do I fix this?
Link: https://docs.fireworks.ai/faq-new/account-access/i-have-multiple-fireworks-accounts-when-i-try-to-login-with-google-on-fireworks
Content: If you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.

If you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.

I have multiple Fireworks accounts. When I try to login with Google on Fireworks' web UI, I'm getting signed into the wrong account. How do I fix this?
If you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.

",
    "$.messages[3].content[5].type": "text",
    "$.messages[3].content[6].text": "Title: Introduction
Link: https://docs.fireworks.ai/api-reference/introduction
Content: Fireworks AI REST API enables you to interact with various language, image and embedding models using an API Key. It also lets you automate management of models, deployments, datasets, and more.

Fireworks AI REST API enables you to interact with various language, image and embedding models using an API Key. It also lets you automate management of models, deployments, datasets, and more.

Introduction
Fireworks AI REST API enables you to interact with various language, image and embedding models using an API Key. It also lets you automate management of models, deployments, datasets, and more.

",
    "$.messages[3].content[6].type": "text",
    "$.messages[3].content[7].text": "Title: Create a deployment
Link: https://docs.fireworks.ai/tools-sdks/firectl/commands/create-deployment
Content: Create a Deployment on Fireworks AI platform

Create a Deployment on Fireworks AI platform

Creates a new deployment.

",
    "$.messages[3].content[7].type": "text",
    "$.messages[3].content[8].text": "Title: How much does Fireworks cost?
Link: https://docs.fireworks.ai/faq-new/billing-pricing/how-much-does-fireworks-cost
Content: Fireworks AI operates on a pay-as-you-go model for all non-Enterprise usage, and new users automatically receive free credits. You pay based on: Per token for serverless inference Per GPU usage time for on-demand deployments Per token of training data for fine-tuning For customers needing enterprise-grade security and reliability , please reach out to us at inquiries@fireworks.ai to discuss options. Find out more about our current pricing on our Pricing page.

Fireworks AI operates on a pay-as-you-go model for all non-Enterprise usage, and new users automatically receive free credits. You pay based on: Per token for serverless inference Per GPU usage time for on-demand deployments Per token of training data for fine-tuning For customers needing enterprise-grade security and reliability , please reach out to us at inquiries@fireworks.ai to discuss options. Find out more about our current pricing on our Pricing page.

How much does Fireworks cost?
Fireworks AI operates on a pay-as-you-go model for all non-Enterprise usage, and new users automatically receive free credits. You pay based on: Per token for serverless inference Per GPU usage time for on-demand deployments Per token of training data for fine-tuning For customers needing enterprise-grade security and reliability , please reach out to us at inquiries@fireworks.ai to discuss options. Find out more about our current pricing on our Pricing page.

",
    "$.messages[3].content[8].type": "text",
    "$.messages[3].content[9].text": "Title: Python (Fireworks)
Link: https://docs.fireworks.ai/getting-started/quickstart
Content: This is the recommended way to get started : This  Python (Fireworks)  tab uses our  Fireworks Build
SDK  for the best performance and developer experience.
See our  Client-side performance
optimization  guide for
more details. Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. pip install --upgrade fireworks-ai The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either  ~/.bash_profile  for Bash or  ~/.zshrc  for  Zsh .
You can do this by running the command: vim ~/.bash_profile vim ~/.zshrc Add a new line to the file with the following: export FIREWORKS_API_KEY="<API_KEY>" After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. source ~/.bash_profile source ~/.zshrc You can verify that the variable has been set correctly by running  echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. setx FIREWORKS_API_KEY "<API_KEY>" To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: echo %FIREWORKS_API_KEY% You can quickly instantiate the  LLM  class and call the Fireworks API. The Build SDK handles deployment management automatically. from fireworks import LLM

# Basic usage - SDK automatically selects optimal deployment type
llm = LLM(model="llama4-maverick-instruct-basic", deployment_type="auto")

response = llm.chat.completions.create(
    messages=[{"role": "user", "content": "Say this is a test"}]
)

print(response.choices[0].message.content) You can also pass the API key directly to the LLM constructor:  LLM(model="llama4-maverick-instruct-basic", deployment_type="auto", api_key="<FIREWORKS_API_KEY>")

This is the recommended way to get started : This  Python (Fireworks)  tab uses our  Fireworks Build
SDK  for the best performance and developer experience.
See our  Client-side performance
optimization  guide for
more details. Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. pip install --upgrade fireworks-ai The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either  ~/.bash_profile  for Bash or  ~/.zshrc  for  Zsh .
You can do this by running the command: vim ~/.bash_profile vim ~/.zshrc Add a new line to the file with the following: export FIREWORKS_API_KEY="<API_KEY>" After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. source ~/.bash_profile source ~/.zshrc You can verify that the variable has been set correctly by running  echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. setx FIREWORKS_API_KEY "<API_KEY>" To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: echo %FIREWORKS_API_KEY% You can quickly instantiate the  LLM  class and call the Fireworks API. The Build SDK handles deployment management automatically. from fireworks import LLM

# Basic usage - SDK automatically selects optimal deployment type
llm = LLM(model="llama4-maverick-instruct-basic", deployment_type="auto")

response = llm.chat.completions.create(
    messages=[{"role": "user", "content": "Say this is a test"}]
)

print(response.choices[0].message.content) You can also pass the API key directly to the LLM constructor:  LLM(model="llama4-maverick-instruct-basic", deployment_type="auto", api_key="<FIREWORKS_API_KEY>")

Set up your developer environment & call the Fireworks API
This is the recommended way to get started : This  tab uses our Fireworks Build
SDK for the best performance and developer experience.
See our Client-side performance
optimization guide for
more details. Install SDK Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Configure API Key Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either ~/.bash_profile for Bash or ~/.zshrc for Zsh .
You can do this by running the command: Add a new line to the file with the following: After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. You can verify that the variable has been set correctly by running echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: Sending the first API Request You can quickly instantiate the LLM class and call the Fireworks API. The Build SDK handles deployment management automatically. You can also pass the API key directly to the LLM constructor: LLM(model="llama4-maverick-instruct-basic", deployment_type="auto", api_key="<FIREWORKS_API_KEY>") Install SDK Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. Fireworks AI platform offers drop-in replacement with OpenAI Python Client. Configure API Key Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either ~/.bash_profile for Bash or ~/.zshrc for Zsh .
You can do this by running the command: Add a new line to the file with the following: After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. You can verify that the variable has been set correctly by running echo $OPENAI_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: echo %OPENAI_API_KEY%

",
    "$.messages[3].content[9].type": "text",
    "$.messages[3].role": "tool",
    "$.messages[3].tool_call_id": "functions.search:0",
    "$.messages[4].content": "Let me search for more specific information about deployment management commands and workflows.",
    "$.messages[4].role": "assistant",
    "$.messages[4].tool_calls[0].function.arguments": "{"query": "firectl deployment management commands create delete list"}",
    "$.messages[4].tool_calls[0].function.name": "search",
    "$.messages[4].tool_calls[0].id": "functions.search:1",
    "$.messages[4].tool_calls[0].type": "function",
    "$.messages[5].content[0].text": "Title: Why am I getting "invalid id" errors when using firectl commands like create deployment or list deployments?
Link: https://docs.fireworks.ai/faq-new/fine-tuning/why-am-i-getting-invalid-id-errors-when-using-firectl-commands-like-create-deplo
Content: This error typically occurs when your account ID is not properly configured.

This error typically occurs when your account ID is not properly configured.

Why am I getting "invalid id" errors when using firectl commands like create deployment or list deployments?
This error typically occurs when your account ID is not properly configured.

",
    "$.messages[5].content[0].type": "text",
    "$.messages[5].content[1].text": "Title: Delete a deployment
Link: https://docs.fireworks.ai/tools-sdks/firectl/commands/delete-resources
Content: Deletes an model deployment. firectl delete deployment [flags]

Deletes an model deployment. firectl delete deployment [flags]

Example
firectl delete deployment my-deployment

",
    "$.messages[5].content[1].type": "text",
    "$.messages[5].content[2].text": "Title: Create a deployment
Link: https://docs.fireworks.ai/tools-sdks/firectl/commands/create-deployment
Content: Creates a new deployment. firectl create deployment [flags]

Creates a new deployment. firectl create deployment [flags]

Creates a new deployment.

",
    "$.messages[5].content[2].type": "text",
    "$.messages[5].content[3].text": "Title: Creating an on-demand deployment
Link: https://docs.fireworks.ai/guides/ondemand-deployments
Content: Choose a model Check out our model library to see a list of models available for on-demand deployment. You can also upload and use your own custom base model. Create a deployment To create a new deployment of a model from the model library, use: Or, for a custom base model you have uploaded to your account: This command will complete when the deployment is READY . To let it run asynchronously, remove the --wait flag. If your account has purchased reserved capacity and this deployment meets the reservation criteria, it will be counted against that reservation and not billed as an on-demand deployment. Verify the deployment is running You can verify the deployment is active by running: The state field should show READY. The deployment ID is the last part of the deployment name: accounts/<ACCOUNT_ID>/deployments/<DEPLOYMENT_ID>. You can also list all deployments in your account at any time by running: firectl list deployments

Choose a model Check out our model library to see a list of models available for on-demand deployment. You can also upload and use your own custom base model. Create a deployment To create a new deployment of a model from the model library, use: Or, for a custom base model you have uploaded to your account: This command will complete when the deployment is READY . To let it run asynchronously, remove the --wait flag. If your account has purchased reserved capacity and this deployment meets the reservation criteria, it will be counted against that reservation and not billed as an on-demand deployment. Verify the deployment is running You can verify the deployment is active by running: The state field should show READY. The deployment ID is the last part of the deployment name: accounts/<ACCOUNT_ID>/deployments/<DEPLOYMENT_ID>. You can also list all deployments in your account at any time by running: firectl list deployments

Choose a model Check out our model library to see a list of models available for on-demand deployment. You can also upload and use your own custom base model. Create a deployment To create a new deployment of a model from the model library, use: Or, for a custom base model you have uploaded to your account: This command will complete when the deployment is READY . To let it run asynchronously, remove the --wait flag. If your account has purchased reserved capacity and this deployment meets the reservation criteria, it will be counted against that reservation and not billed as an on-demand deployment. Verify the deployment is running You can verify the deployment is active by running: firectl get deployment <DEPLOYMENT_ID>

",
    "$.messages[5].content[3].type": "text",
    "$.messages[5].content[4].text": "Title: Delete Deployment
Link: https://docs.fireworks.ai/api-reference/delete-deployment
Content: 



REST Endpoint DELETE /v1/accounts/{account_id}/deployments/{deployment_id}

Server(s): https://api.fireworks.ai


Summary:  

Description: N/A

Security methods: 
header: 
	Authorization: bearer auth
	

Request params: 
path params:
account_id:
	string
	Description: The Account Id
	Example: "<string>"
deployment_id:
	string
	Description: The Deployment Id
	Example: "<string>"

query params:
hard:
	boolean
	Description: If true, this will perform a hard deletion.
	Example: true
ignoreChecks:
	boolean
	Description: If true, this will ignore checks and force the deletion of a deployment that is currently
deployed and is in use.
	Example: true


Request body: N/A


Responses: 
200: object
	Description: N/A
	Example: {}

",
    "$.messages[5].content[4].type": "text",
    "$.messages[5].content[5].text": "Title: 🚀 Easier & faster LoRA fine-tune deployments on Fireworks
Link: https://docs.fireworks.ai/updates/changelog
Content: You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: Previously, this involved two distinct steps, and the resulting deployment was slower than the base model: Create a deployment using firectl create deployment "accounts/fireworks/models/<MODEL_ID of base model>" --enable-addons Then deploy the addon to the deployment: firectl load-lora <MODEL_ID> --deployment <DEPLOYMENT_ID> For more information, see our deployment documentation. This change is for dedicated deployments with a single LoRA. You can still deploy multiple LoRAs on a deployment or deploy LoRA(s) on some Serverless models as described in the documentation.

",
    "$.messages[5].content[5].type": "text",
    "$.messages[5].content[6].text": "Title: Deployment Command
Link: https://docs.fireworks.ai/evaluators/examples/gcp_cloud_run_deployment_example
Content: It's recommended to run the deployment command from the directory containing the reward function script ( dummy_rewards.py ) and your rewardkit.yaml (if used), for example, from examples/gcp_cloud_run_deployment_example/. Ensure your virtual environment is active: Run the deployment command: Command Explanation: --id my-dummy-gcp-evaluator : A unique ID for your evaluator on the Fireworks AI platform. --target gcp-cloud-run : Specifies deployment to GCP Cloud Run. --function-ref dummy_rewards.hello_world_reward : The Python import path to your reward function. If dummy_rewards.py is in the current directory, this reference works. --gcp-auth-mode api-key : Configures the Cloud Run service with API key authentication. reward-kit will generate a key, store it in GCP Secret Manager, and configure the service. The key is also saved to your local rewardkit.yaml under evaluator_endpoint_keys . This is the default if not specified. --verbose : Shows detailed output, including gcloud commands being executed. --force : (Optional) If an evaluator with the same --id already exists, this flag will delete the existing one before creating the new one.

It's recommended to run the deployment command from the directory containing the reward function script ( dummy_rewards.py ) and your rewardkit.yaml (if used), for example, from examples/gcp_cloud_run_deployment_example/. Ensure your virtual environment is active: Run the deployment command: Command Explanation: --id my-dummy-gcp-evaluator : A unique ID for your evaluator on the Fireworks AI platform. --target gcp-cloud-run : Specifies deployment to GCP Cloud Run. --function-ref dummy_rewards.hello_world_reward : The Python import path to your reward function. If dummy_rewards.py is in the current directory, this reference works. --gcp-auth-mode api-key : Configures the Cloud Run service with API key authentication. reward-kit will generate a key, store it in GCP Secret Manager, and configure the service. The key is also saved to your local rewardkit.yaml under evaluator_endpoint_keys . This is the default if not specified. --verbose : Shows detailed output, including gcloud commands being executed. --force : (Optional) If an evaluator with the same --id already exists, this flag will delete the existing one before creating the new one.

It's recommended to run the deployment command from the directory containing the reward function script ( dummy_rewards.py ) and your rewardkit.yaml (if used), for example, from examples/gcp_cloud_run_deployment_example/. Ensure your virtual environment is active: source .venv/bin/activate

",
    "$.messages[5].content[6].type": "text",
    "$.messages[5].content[7].text": "Title: Quick deployment
Link: https://docs.fireworks.ai/fine-tuning/single-lora
Content: Deploy your LoRA fine-tuned model with one simple command: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

Deploy your LoRA fine-tuned model with one simple command: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

Alternative deployment method
This two-step method is the standard approach for multi-LoRA deployments where multiple LoRA models share the same base model. While it can also be used for single LoRA deployments, it provides slower performance compared to live merge and is not recommended for single LoRA use cases. You can also deploy single LoRA models using a two-step process: Create base model deployment Deploy the base model with addons enabled: firectl create deployment "accounts/fireworks/models/<MODEL_ID of base model>" --enable-addons

",
    "$.messages[5].content[7].type": "text",
    "$.messages[5].content[8].text": "Title: Deploying a fine-tuned model using an on-demand deployment
Link: https://docs.fireworks.ai/fine-tuning/fine-tuning-models
Content: Use the following command to deploy your fine-tuned model using an on-demand deployment: firectl create deployment <FINE_TUNED_MODEL_ID>

Use the following command to deploy your fine-tuned model using an on-demand deployment: firectl create deployment <FINE_TUNED_MODEL_ID>

firectl
firectl create dataset <DATASET_ID> /path/to/jsonl/file

",
    "$.messages[5].content[8].type": "text",
    "$.messages[5].content[9].text": "Title: Deleting Stored Responses
Link: https://docs.fireworks.ai/guides/response-api
Content: When responses are stored (the default behavior with store=True ), you can immediately delete them from storage using the DELETE endpoint. This permanently removes the conversation data. from fireworks import LLM
import requests
import os

llm = LLM(model="qwen3-235b-a22b", deployment_type="serverless")

# Create a response
response = llm.responses.create(
    input="What is the capital of France?",
    store=True  # This is the default
)

response_id = response.id
print(f"Created response with ID: {response_id}")

# Delete the response immediately
headers = {
    "Authorization": f"Bearer {os.getenv('FIREWORKS_API_KEY')}",
    "x-fireworks-account-id": "your-account-id"
}
delete_response = requests.delete(
    f"https://api.fireworks.ai/inference/v1/responses/{response_id}",
    headers=headers
)

if delete_response.status_code == 200:
    print("Response deleted successfully")
else:
    print(f"Failed to delete response: {delete_response.status_code}")

When responses are stored (the default behavior with store=True ), you can immediately delete them from storage using the DELETE endpoint. This permanently removes the conversation data. from fireworks import LLM
import requests
import os

llm = LLM(model="qwen3-235b-a22b", deployment_type="serverless")

# Create a response
response = llm.responses.create(
    input="What is the capital of France?",
    store=True  # This is the default
)

response_id = response.id
print(f"Created response with ID: {response_id}")

# Delete the response immediately
headers = {
    "Authorization": f"Bearer {os.getenv('FIREWORKS_API_KEY')}",
    "x-fireworks-account-id": "your-account-id"
}
delete_response = requests.delete(
    f"https://api.fireworks.ai/inference/v1/responses/{response_id}",
    headers=headers
)

if delete_response.status_code == 200:
    print("Response deleted successfully")
else:
    print(f"Failed to delete response: {delete_response.status_code}")

When responses are stored (the default behavior with store=True ), you can immediately delete them from storage using the DELETE endpoint. This permanently removes the conversation data. import os
from openai import OpenAI
import requests

client = OpenAI(
    base_url="https://api.fireworks.ai/inference/v1",
    api_key=os.getenv("FIREWORKS_API_KEY", "YOUR_FIREWORKS_API_KEY_HERE")
)

# Create a response
response = client.responses.create(
    model="accounts/fireworks/models/qwen3-235b-a22b",
    input="What is the capital of France?",
    store=True  # This is the default
)

response_id = response.id
print(f"Created response with ID: {response_id}")

# Delete the response immediately
headers = {
    "Authorization": f"Bearer {os.getenv('FIREWORKS_API_KEY')}",
    "x-fireworks-account-id": "your-account-id"
}
delete_response = requests.delete(
    f"https://api.fireworks.ai/inference/v1/responses/{response_id}",
    headers=headers
)

if delete_response.status_code == 200:
    print("Response deleted successfully")
else:
    print(f"Failed to delete response: {delete_response.status_code}")

",
    "$.messages[5].content[9].type": "text",
    "$.messages[5].role": "tool",
    "$.messages[5].tool_call_id": "functions.search:1",
    "$.messages[6].content": "Now let me search for information about listing and managing deployments to give you a complete picture.",
    "$.messages[6].role": "assistant",
    "$.messages[6].tool_calls[0].function.arguments": "{"query": "list deployments get deployment status firectl commands"}",
    "$.messages[6].tool_calls[0].function.name": "search",
    "$.messages[6].tool_calls[0].id": "functions.search:1",
    "$.messages[6].tool_calls[0].type": "function",
    "$.messages[7].content[0].text": "Title: Why am I getting "invalid id" errors when using firectl commands like create deployment or list deployments?
Link: https://docs.fireworks.ai/faq-new/fine-tuning/why-am-i-getting-invalid-id-errors-when-using-firectl-commands-like-create-deplo
Content: This error typically occurs when your account ID is not properly configured.

This error typically occurs when your account ID is not properly configured.

Why am I getting "invalid id" errors when using firectl commands like create deployment or list deployments?
This error typically occurs when your account ID is not properly configured.

",
    "$.messages[7].content[0].type": "text",
    "$.messages[7].content[1].text": "Title: List deployments
Link: https://docs.fireworks.ai/tools-sdks/firectl/commands/list-resources
Content: Prints all deployments in the account. firectl list deployments [flags]

Prints all deployments in the account. firectl list deployments [flags]

List deployed models
Prints all deployed models in an account. firectl list deployed-models [flags]

",
    "$.messages[7].content[1].type": "text",
    "$.messages[7].content[2].text": "Title: Creating an on-demand deployment
Link: https://docs.fireworks.ai/guides/ondemand-deployments
Content: Choose a model Check out our model library to see a list of models available for on-demand deployment. You can also upload and use your own custom base model. Create a deployment To create a new deployment of a model from the model library, use: Or, for a custom base model you have uploaded to your account: This command will complete when the deployment is READY . To let it run asynchronously, remove the --wait flag. If your account has purchased reserved capacity and this deployment meets the reservation criteria, it will be counted against that reservation and not billed as an on-demand deployment. Verify the deployment is running You can verify the deployment is active by running: The state field should show READY. The deployment ID is the last part of the deployment name: accounts/<ACCOUNT_ID>/deployments/<DEPLOYMENT_ID>. You can also list all deployments in your account at any time by running: firectl list deployments

Choose a model Check out our model library to see a list of models available for on-demand deployment. You can also upload and use your own custom base model. Create a deployment To create a new deployment of a model from the model library, use: Or, for a custom base model you have uploaded to your account: This command will complete when the deployment is READY . To let it run asynchronously, remove the --wait flag. If your account has purchased reserved capacity and this deployment meets the reservation criteria, it will be counted against that reservation and not billed as an on-demand deployment. Verify the deployment is running You can verify the deployment is active by running: The state field should show READY. The deployment ID is the last part of the deployment name: accounts/<ACCOUNT_ID>/deployments/<DEPLOYMENT_ID>. You can also list all deployments in your account at any time by running: firectl list deployments

Publishing a deployed model
By default, models can only be queried by the account that owns them. To make a deployment public so anyone with a valid Fireworks API key can query it, update the deployed model with the --public flag. Find the Deployed Model ID Every model running on a deployment receives a unique deployed model ID. You can find this ID if you know the model name and deployment name using the following command: firectl list deployed-models --filter 'model="<MODEL_NAME>" AND deployment="<DEPLOYMENT_NAME>"'

",
    "$.messages[7].content[2].type": "text",
    "$.messages[7].content[3].text": "Title: 🚀 Easier & faster LoRA fine-tune deployments on Fireworks
Link: https://docs.fireworks.ai/updates/changelog
Content: You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: Previously, this involved two distinct steps, and the resulting deployment was slower than the base model: Create a deployment using firectl create deployment "accounts/fireworks/models/<MODEL_ID of base model>" --enable-addons Then deploy the addon to the deployment: firectl load-lora <MODEL_ID> --deployment <DEPLOYMENT_ID> For more information, see our deployment documentation. This change is for dedicated deployments with a single LoRA. You can still deploy multiple LoRAs on a deployment or deploy LoRA(s) on some Serverless models as described in the documentation.

",
    "$.messages[7].content[3].type": "text",
    "$.messages[7].content[4].text": "Title: firectl
Link: https://docs.fireworks.ai/fine-tuning/fine-tuning-vlm
Content: # Create a deployment for your fine-tuned VLM
firectl create deployment my-custom-vlm

# Check deployment status
firectl get deployment accounts/your-account/deployment/deployment-id

# Create a deployment for your fine-tuned VLM
firectl create deployment my-custom-vlm

# Check deployment status
firectl get deployment accounts/your-account/deployment/deployment-id

Try with an Example Dataset
To get a feel for how VLM fine-tuning works, you can use an example vision dataset: This is a classification dataset that contains images of food with <think></think> tags for reasoning. Once downloaded, you can upload this dataset using the instructions in the next step. Upload your VLM dataset Upload your prepared JSONL dataset to Fireworks for training: Navigate to the Datasets tab in the Fireworks console, click "Create Dataset", and upload your JSONL file through the wizard. For larger datasets (>500MB), use  as it handles large uploads more reliably than the web interface. For enhanced data control and security, we also support bring your own bucket (BYOB) configurations. See our External GCS Bucket Integration guide for setup details. Launch VLM fine-tuning job Create a supervised fine-tuning job for your VLM: For additional parameters like learning rates, evaluation datasets, and batch sizes, see Additional SFT job settings. Navigate to the Fine-tuning tab in the Fireworks console Click "Create Fine-tuning Job" Select your VLM base model (Qwen 2.5 VL) Choose your uploaded dataset Configure training parameters Launch the job VLM fine-tuning jobs typically take longer than text-only models due to the additional image processing. Expect training times of several hours depending on dataset size and model complexity. Monitor training progress Track your VLM fine-tuning job in the Fireworks console. Monitor key metrics: Training loss : Should generally decrease over time Evaluation loss : Monitor for overfitting if using evaluation dataset Training progress : Epochs completed and estimated time remaining Your VLM fine-tuning job is complete when the status shows COMPLETED and your custom model is ready for deployment. Deploy your fine-tuned VLM Once training is complete, deploy your custom VLM: # Create a deployment for your fine-tuned VLM
firectl create deployment my-custom-vlm

# Check deployment status
firectl get deployment accounts/your-account/deployment/deployment-id

",
    "$.messages[7].content[4].type": "text",
    "$.messages[7].content[5].text": "Title: firectl
Link: https://docs.fireworks.ai/guides/batch-inference
Content: # Get job status
firectl get batch-inference-job my-batch-job

# List all batch jobs
firectl list batch-inference-jobs

# Get job status
firectl get batch-inference-job my-batch-job

# List all batch jobs
firectl list batch-inference-jobs

firectl create dataset batch-input-dataset ./batch_input_data.jsonl

",
    "$.messages[7].content[5].type": "text",
    "$.messages[7].content[6].text": "Title: Does Fireworks support custom base models?
Link: https://docs.fireworks.ai/faq-new/models-inference/does-fireworks-support-custom-base-models
Content: Yes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.

Yes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.

Does Fireworks support custom base models?
Yes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.

",
    "$.messages[7].content[6].type": "text",
    "$.messages[7].content[7].text": "Title: List Deployments
Link: https://docs.fireworks.ai/api-reference/list-deployments
Content: 



REST Endpoint GET /v1/accounts/{account_id}/deployments

Server(s): https://api.fireworks.ai


Summary:  

Description: N/A

Security methods: 
header: 
	Authorization: bearer auth
	

Request params: 
path params:
account_id:
	string
	Description: The Account Id
	Example: "<string>"

query params:
pageSize:
	integer
	Description: The maximum number of deployments to return. The maximum page_size is 200,
values above 200 will be coerced to 200.
If unspecified, the default is 50.
	Example: 123
pageToken:
	string
	Description: A page token, received from a previous ListDeployments call. Provide this
to retrieve the subsequent page. When paginating, all other parameters
provided to ListDeployments must match the call that provided the page
token.
	Example: "<string>"
filter:
	string
	Description: Only deployment satisfying the provided filter (if specified) will be
returned. See https://google.aip.dev/160 for the filter grammar.
	Example: "<string>"
orderBy:
	string
	Description: A comma-separated list of fields to order by. e.g. "foo,bar"
The default sort order is ascending. To specify a descending order for a
field, append a " desc" suffix. e.g. "foo desc,bar"
Subfields are specified with a "." character. e.g. "foo.bar"
If not specified, the default order is by "create_time".
	Example: "<string>"
showDeleted:
	boolean
	Description: If set, DELETED deployments will be included.
	Example: true
readMask:
	string
	Description: The fields to be returned in the response. If empty or "*", all fields will be returned.
	Example: "<string>"


Request body: N/A


Responses: 
200: object
	Description: N/A
	Example: {"deployments":[{"name":"<string>","displayName":"<string>","description":"<string>","createTime":"2023-11-07T05:31:56Z","expireTime":"2023-11-07T05:31:56Z","purgeTime":"2023-11-07T05:31:56Z","deleteTime":"2023-11-07T05:31:56Z","state":"STATE_UNSPECIFIED","status":{"code":"OK","message":"<string>"},"minReplicaCount":123,"maxReplicaCount":123,"replicaCount":123,"autoscalingPolicy":{"scaleUpWindow":"<string>","scaleDownWindow":"<string>","scaleToZeroWindow":"<string>","loadTargets":{}},"baseModel":"<string>","acceleratorCount":123,"acceleratorType":"ACCELERATOR_TYPE_UNSPECIFIED","precision":"PRECISION_UNSPECIFIED","cluster":"<string>","enableAddons":true,"draftTokenCount":123,"draftModel":"<string>","ngramSpeculationLength":123,"numPeftDeviceCached":123,"deploymentTemplate":"<string>","autoTune":{"longPrompt":true},"placement":{"region":"REGION_UNSPECIFIED","multiRegion":"MULTI_REGION_UNSPECIFIED","regions":["REGION_UNSPECIFIED"]},"region":"REGION_UNSPECIFIED","updateTime":"2023-11-07T05:31:56Z","disableDeploymentSizeValidation":true}],"nextPageToken":"<string>","totalSize":123}

",
    "$.messages[7].content[7].type": "text",
    "$.messages[7].content[8].text": "Title: Deploy Command
Link: https://docs.fireworks.ai/evaluators/cli_reference/cli_overview
Content: The deploy command deploys a reward function as an evaluator on the Fireworks platform.

The deploy command deploys a reward function as an evaluator on the Fireworks platform.

Command Overview
The Reward Kit CLI supports the following main commands: run : Run a local evaluation pipeline using a Hydra configuration. preview : Preview evaluation results or re-evaluate generated outputs. deploy : Deploy a reward function as an evaluator. agent-eval : Run agent evaluations on task bundles. list : List existing evaluators (coming soon). delete : Delete an evaluator (coming soon).

",
    "$.messages[7].content[8].type": "text",
    "$.messages[7].content[9].text": "Title: Quick deployment
Link: https://docs.fireworks.ai/fine-tuning/single-lora
Content: Deploy your LoRA fine-tuned model with one simple command: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

Deploy your LoRA fine-tuned model with one simple command: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

Alternative deployment method
This two-step method is the standard approach for multi-LoRA deployments where multiple LoRA models share the same base model. While it can also be used for single LoRA deployments, it provides slower performance compared to live merge and is not recommended for single LoRA use cases. You can also deploy single LoRA models using a two-step process: Create base model deployment Deploy the base model with addons enabled: firectl create deployment "accounts/fireworks/models/<MODEL_ID of base model>" --enable-addons

",
    "$.messages[7].content[9].type": "text",
    "$.messages[7].role": "tool",
    "$.messages[7].tool_call_id": "functions.search:1",
    "$.messages[8].content": "Perfect! Now I have comprehensive information about managing deployments on Fireworks. Let me provide you with a complete guide.

# Managing Deployments on Fireworks: A Complete Guide

Fireworks offers several ways to manage deployments, from command-line tools to REST APIs. Here's everything you need to know:

## Types of Deployments

### 1. On-Demand Deployments
- **Billed by GPU-second** (not per-token like serverless)
- **Dedicated resources** - no interference from other users
- **No hard rate limits** (subject to deployment capacity)
- **Access to more models** including custom ones
- **Better for high utilization** scenarios

### 2. Serverless Deployments
- **Billed per-token**
- **Shared resources** with other users
- **Rate limits apply**
- **Instant availability**

## Management Tools

### Option 1: Fireworks Build SDK (Recommended)
\`\`\`python
from fireworks import LLM

# Auto-deployment - SDK handles everything
llm = LLM(model="llama4-maverick-instruct-basic", deployment_type="auto")

# Manual deployment management
llm = LLM(model="your-model", deployment_type="on-demand")
\`\`\`

### Option 2: firectl CLI Tool

#### Installation
\`\`\`bash
# Install firectl CLI
curl -s https://fireworks.ai/install.sh | bash
\`\`\`

#### Basic Commands

**List all deployments:**
\`\`\`bash
firectl list deployments
\`\`\`

**Create a deployment:**
\`\`\`bash
# From model library
firectl create deployment accounts/fireworks/models/llama-v3p1-8b-instruct --wait

# Custom model
firectl create deployment accounts/your-account-id/models/your-custom-model --wait

# LoRA fine-tuned model (single command)
firectl create deployment accounts/fireworks/models/your-lora-model-id
\`\`\`

**Get deployment status:**
\`\`\`bash
firectl get deployment <deployment-id>
\`\`\`

**Delete a deployment:**
\`\`\`bash
firectl delete deployment <deployment-id>
\`\`\`

**List deployed models:**
\`\`\`bash
firectl list deployed-models
\`\`\`

### Option 3: REST API

#### List Deployments
\`\`\`bash
curl -X GET \\
  "https://api.fireworks.ai/v1/accounts/{account_id}/deployments" \\
  -H "Authorization: Bearer $FIREWORKS_API_KEY"
\`\`\`

#### Create Deployment
\`\`\`bash
curl -X POST \\
  "https://api.fireworks.ai/v1/accounts/{account_id}/deployments" \\
  -H "Authorization: Bearer $FIREWORKS_API_KEY" \\
  -H "Content-Type: application/json" \\
  -d '{
    "deployment": {
      "displayName": "my-deployment",
      "baseModel": "accounts/fireworks/models/llama-v3p1-8b-instruct"
    }
  }'
\`\`\`

#### Delete Deployment
\`\`\`bash
curl -X DELETE \\
  "https://api.fireworks.ai/v1/accounts/{account_id}/deployments/{deployment_id}" \\
  -H "Authorization: Bearer $FIREWORKS_API_KEY"
\`\`\`

## Deployment Workflow

### 1. Choose Your Model
\`\`\`bash
# Check available models
firectl list models
\`\`\`

### 2. Create Deployment
\`\`\`bash
# Basic deployment
firectl create deployment accounts/fireworks/models/llama-v3p1-8b-instruct --wait

# With custom configuration
firectl create deployment accounts/fireworks/models/llama-v3p1-8b-instruct \\
  --min-replica-count 1 \\
  --max-replica-count 3 \\
  --wait
\`\`\`

### 3. Monitor Status
\`\`\`bash
# Get deployment details
firectl get deployment <deployment-id>

# Watch logs
firectl logs deployment <deployment-id>
\`\`\`

### 4. Make Public (Optional)
\`\`\`bash
# Make deployment public
firectl update deployment <deployment-id> --public
\`\`\`

### 5. Clean Up
\`\`\`bash
# Delete when done
firectl delete deployment <deployment-id>
\`\`\`

## Advanced Features

### Reserved Capacity
For high-volume usage, consider reserved capacity:
- More reliable capacity
- Higher GPU quotas
- Cost savings for consistent usage
- Contact Fireworks for enterprise pricing

### Multi-LoRA Deployments
\`\`\`bash
# Create base deployment with addons
firectl create deployment accounts/fireworks/models/base-model --enable-addons --wait

# Load LoRA models
firectl load-lora lora-model-1 --deployment <deployment-id>
firectl load-lora lora-model-2 --deployment <deployment-id>
\`\`\`

### Custom Models
\`\`\`bash
# Upload custom model first
firectl upload model /path/to/model

# Deploy custom model
firectl create deployment accounts/your-account-id/models/custom-model-name
\`\`\`

## Best Practices

1. **Use \`--wait\` flag** for important deployments to ensure they're ready
2. **Monitor costs** with \`firectl list deployments\` to see active deployments
3. **Set up alerts** for deployment health
4. **Use reserved capacity** for production workloads
5. **Clean up unused deployments** to avoid charges
6. **Test deployments** before making them public

## Troubleshooting

**"Invalid ID" errors:**
- Ensure your account ID is properly configured
- Check your API key and account settings

**Deployment stuck:**
- Check resource availability
- Verify model compatibility
- Review deployment logs

**Performance issues:**
- Consider scaling up replicas
- Check if reserved capacity is available
- Monitor GPU utilization

## Cost Management
- **On-demand**: Pay per GPU-second used
- **Serverless**: Pay per token processed
- **Reserved capacity**: Fixed monthly cost for guaranteed capacity
- Use \`firectl list deployments\` to track active deployments and costs

This comprehensive guide should give you everything you need to effectively manage deployments on Fireworks!",
    "$.messages[8].role": "assistant",
    "$.pid": 92995,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T11:38:53.968430",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "Run math evaluation on sample dataset using pytest interface.",
    "$.eval_metadata.name": "test_pytest_mcp_url",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.version": "v0.2.6.dev17+gf8398de.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "At least one tool call was made",
    "$.evaluation_result.score": 1,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.row_id": "have-disassemble-simple-final-experience",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "You are a helpful assistant that can answer questions about Fireworks.
ALWAYS provide code or commands to execute to answer the question.",
    "$.messages[0].role": "system",
    "$.messages[10].content": "Based on my research, I can now provide you with a comprehensive guide on managing deployments on Fireworks. Let me break this down into practical sections with commands and examples.

## Managing Deployments on Fireworks: A Complete Guide

Fireworks offers three deployment types with different management approaches:

1. **Serverless**: Pre-configured, pay-per-token, automatic scaling
2. **On-demand**: Private GPUs, pay-per-second, manual/automatic scaling
3. **Enterprise Reserved**: Custom hardware, tailored setup, SLAs

### 1. Basic Deployment Management Commands

#### Using firectl CLI (Recommended)

**List all deployments:**
\`\`\`bash
firectl list deployments
\`\`\`

**Create a deployment:**
\`\`\`bash
# Basic deployment
firectl create deployment <MODEL_NAME> --wait

# With custom configuration
firectl create deployment <MODEL_NAME> \\
  --min-replica-count 0 \\
  --max-replica-count 3 \\
  --wait
\`\`\`

**Delete a deployment:**
\`\`\`bash
firectl delete deployment <DEPLOYMENT_ID>
\`\`\`

**Update deployment configuration:**
\`\`\`bash
firectl update deployment <DEPLOYMENT_ID> \\
  --min-replica-count 2 \\
  --max-replica-count 5
\`\`\`

#### Using REST API

**List deployments:**
\`\`\`bash
curl -X GET \\
  "https://api.fireworks.ai/v1/accounts/{account_id}/deployments" \\
  -H "Authorization: Bearer $FIREWORKS_API_KEY"
\`\`\`

**Delete deployment via API:**
\`\`\`bash
curl -X DELETE \\
  "https://api.fireworks.ai/v1/accounts/{account_id}/deployments/{deployment_id}" \\
  -H "Authorization: Bearer $FIREWORKS_API_KEY"
\`\`\`

### 2. Python SDK Management

#### Installation and Setup
\`\`\`bash
pip install --upgrade fireworks-ai
export FIREWORKS_API_KEY="your-api-key"
\`\`\`

#### Basic Usage
\`\`\`python
from fireworks import LLM

# Serverless deployment (automatic)
llm = LLM(model="llama4-maverick-instruct-basic", deployment_type="serverless")

# On-demand deployment with custom ID
llm = LLM(
    model="llama4-maverick-instruct-basic",
    deployment_type="on-demand",
    id="my-custom-deployment"
)
llm.apply()  # Required for on-demand deployments

# Auto-selection (serverless if available, otherwise on-demand)
llm = LLM(model="llama4-maverick-instruct-basic", deployment_type="auto")
\`\`\`

#### Fine-tuned Model Deployment
\`\`\`python
# Single LoRA deployment (recommended)
lora_llm = LLM(
    model="accounts/your-account/models/your-lora-model",
    deployment_type="on-demand"
)
lora_llm.apply()

# Multi-LoRA deployment
base_llm = LLM(
    model="accounts/fireworks/models/llama4-maverick-instruct-basic",
    deployment_type="on-demand-lora",
    id="multi-lora-deployment"
)
base_llm.apply()
\`\`\`

### 3. Deployment Configuration Options

#### Scaling Configuration
\`\`\`bash
# Scale between 0-3 replicas based on demand
firectl create deployment <MODEL_NAME> \\
  --min-replica-count 0 \\
  --max-replica-count 3

# Fixed scaling (always have 2 replicas running)
firectl create deployment <MODEL_NAME> \\
  --min-replica-count 2 \\
  --max-replica-count 2
\`\`\`

#### Advanced Configuration
\`\`\`bash
# Custom model deployment
firectl create deployment accounts/your-account/models/custom-model \\
  --min-replica-count 0 \\
  --max-replica-count 5 \\
  --accelerator-type A100 \\
  --accelerator-count 2
\`\`\`

### 4. Cost Management Best Practices

#### Automatic Cost Optimization
- **Scale-to-zero**: Deployments automatically scale to 0 when unused
- **No charges when scaled down**: Only pay for active GPU time
- **Autoscaling**: Automatically adjusts based on traffic

#### Manual Cost Control
\`\`\`python
# Monitor deployment costs
import requests

headers = {"Authorization": f"Bearer {os.getenv('FIREWORKS_API_KEY')}"}
response = requests.get(
    "https://api.fireworks.ai/v1/accounts/{account_id}/deployments",
    headers=headers
)

# Delete unused deployments
for deployment in response.json()['deployments']:
    if deployment['state'] == 'READY' and deployment['replicaCount'] == 0:
        # Consider deleting if unused
        pass
\`\`\`

### 5. Monitoring and Troubleshooting

#### Check Deployment Status
\`\`\`bash
# Get deployment details
firectl get deployment <DEPLOYMENT_ID>

# Check deployed models
firectl list deployed-models
\`\`\`

#### Web Dashboard
Track deployments at: \`https://app.fireworks.ai/dashboard/deployments\`

### 6. Reserved Capacity (Enterprise)

For high-volume usage, consider reserved capacity:
- More reliable capacity
- Higher GPU quotas
- Better pricing for consistent usage
- Contact: inquiries@fireworks.ai

### 7. Common Management Workflows

#### Daily Workflow Example
\`\`\`python
# Morning: Check active deployments
import subprocess
result = subprocess.run(['firectl', 'list', 'deployments'], 
                       capture_output=True, text=True)
print(result.stdout)

# Scale up for expected traffic
subprocess.run(['firectl', 'update', 'deployment', 'my-deployment', 
                '--min-replica-count', '2'])

# Evening: Scale down
subprocess.run(['firectl', 'update', 'deployment', 'my-deployment', 
                '--min-replica-count', '0'])
\`\`\`

#### Automated Cleanup Script
\`\`\`bash
#!/bin/bash
# cleanup_deployments.sh

# List all deployments
deployments=$(firectl list deployments --format json | jq -r '.deployments[].name')

# Delete deployments older than 7 days with 0 replicas
for deployment in $deployments; do
    firectl delete deployment $deployment --ignore-checks
done
\`\`\`

### 8. Important Considerations

- **Billing**: On-demand deployments charge by GPU-second, even when idle
- **Rate limits**: Serverless has rate limits, on-demand does not
- **Cold starts**: Scaling from 0 may have brief delays
- **Monitoring**: Regularly check deployment status and costs
- **Cleanup**: Always delete unused deployments to avoid charges

This comprehensive guide covers all aspects of deployment management on Fireworks. Start with serverless for simple use cases, move to on-demand for production workloads, and consider reserved capacity for enterprise needs.",
    "$.messages[10].role": "assistant",
    "$.messages[1].content": "Can you teach me about how to manage deployments on Fireworks",
    "$.messages[1].role": "user",
    "$.messages[2].content": "I'll help you learn about managing deployments on Fireworks. Let me search for comprehensive information about deployment management.",
    "$.messages[2].role": "assistant",
    "$.messages[2].tool_calls[0].function.arguments": "{"query": "deployment management Fireworks AI"}",
    "$.messages[2].tool_calls[0].function.name": "search",
    "$.messages[2].tool_calls[0].id": "functions.search:0",
    "$.messages[2].tool_calls[0].type": "function",
    "$.messages[3].content[0].text": "Title: Fireworks AI Developer Platform
Link: https://docs.fireworks.ai/getting-started/introduction
Content: Fireworks AI is the best platform for building AI product experiences with open source AI models. You can run and customize AI models with just a few lines of code!

Fireworks AI is the best platform for building AI product experiences with open source AI models. You can run and customize AI models with just a few lines of code!

Start building with open source AI models

",
    "$.messages[3].content[0].type": "text",
    "$.messages[3].content[1].text": "Title: How do I close my Fireworks.ai account?
Link: https://docs.fireworks.ai/faq-new/account-access/how-do-i-close-my-fireworksai-account
Content: To close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed

To close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed

How do I close my Fireworks.ai account?
To close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed

",
    "$.messages[3].content[1].type": "text",
    "$.messages[3].content[2].text": "Title: Do you host your deployments in the EU or Asia?
Link: https://docs.fireworks.ai/faq-new/support-general/do-you-host-your-deployments-in-the-eu-or-asia
Content: We are currently deployed in multiple U.S.-based locations. However, we’re open to hearing more about your specific requirements. You can: Join our Discord community Write to us at inquiries@fireworks.ai If you're an Enterprise customer, please contact your dedicated customer support representative to ensure a timely response.

We are currently deployed in multiple U.S.-based locations. However, we’re open to hearing more about your specific requirements. You can: Join our Discord community Write to us at inquiries@fireworks.ai If you're an Enterprise customer, please contact your dedicated customer support representative to ensure a timely response.

Do you host your deployments in the EU or Asia?
We are currently deployed in multiple U.S.-based locations. However, we’re open to hearing more about your specific requirements. You can: Join our Discord community Write to us at inquiries@fireworks.ai If you're an Enterprise customer, please contact your dedicated customer support representative to ensure a timely response.

",
    "$.messages[3].content[2].type": "text",
    "$.messages[3].content[3].text": "Title: Introducing the Fireworks Build SDK
Link: https://docs.fireworks.ai/tools-sdks/python-client/sdk-introduction
Content: The Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.

The Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.

Introducing the Fireworks Build SDK
The Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.

",
    "$.messages[3].content[3].type": "text",
    "$.messages[3].content[4].text": "Title: On-demand deployments
Link: https://docs.fireworks.ai/guides/ondemand-deployments
Content: If you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.

If you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.

On-demand deployments
If you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.

",
    "$.messages[3].content[4].type": "text",
    "$.messages[3].content[5].text": "Title: I have multiple Fireworks accounts. When I try to login with Google on Fireworks' web UI, I'm getting signed into the wrong account. How do I fix this?
Link: https://docs.fireworks.ai/faq-new/account-access/i-have-multiple-fireworks-accounts-when-i-try-to-login-with-google-on-fireworks
Content: If you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.

If you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.

I have multiple Fireworks accounts. When I try to login with Google on Fireworks' web UI, I'm getting signed into the wrong account. How do I fix this?
If you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.

",
    "$.messages[3].content[5].type": "text",
    "$.messages[3].content[6].text": "Title: Introduction
Link: https://docs.fireworks.ai/api-reference/introduction
Content: Fireworks AI REST API enables you to interact with various language, image and embedding models using an API Key. It also lets you automate management of models, deployments, datasets, and more.

Fireworks AI REST API enables you to interact with various language, image and embedding models using an API Key. It also lets you automate management of models, deployments, datasets, and more.

Introduction
Fireworks AI REST API enables you to interact with various language, image and embedding models using an API Key. It also lets you automate management of models, deployments, datasets, and more.

",
    "$.messages[3].content[6].type": "text",
    "$.messages[3].content[7].text": "Title: Create a deployment
Link: https://docs.fireworks.ai/tools-sdks/firectl/commands/create-deployment
Content: Create a Deployment on Fireworks AI platform

Create a Deployment on Fireworks AI platform

Creates a new deployment.

",
    "$.messages[3].content[7].type": "text",
    "$.messages[3].content[8].text": "Title: How much does Fireworks cost?
Link: https://docs.fireworks.ai/faq-new/billing-pricing/how-much-does-fireworks-cost
Content: Fireworks AI operates on a pay-as-you-go model for all non-Enterprise usage, and new users automatically receive free credits. You pay based on: Per token for serverless inference Per GPU usage time for on-demand deployments Per token of training data for fine-tuning For customers needing enterprise-grade security and reliability , please reach out to us at inquiries@fireworks.ai to discuss options. Find out more about our current pricing on our Pricing page.

Fireworks AI operates on a pay-as-you-go model for all non-Enterprise usage, and new users automatically receive free credits. You pay based on: Per token for serverless inference Per GPU usage time for on-demand deployments Per token of training data for fine-tuning For customers needing enterprise-grade security and reliability , please reach out to us at inquiries@fireworks.ai to discuss options. Find out more about our current pricing on our Pricing page.

How much does Fireworks cost?
Fireworks AI operates on a pay-as-you-go model for all non-Enterprise usage, and new users automatically receive free credits. You pay based on: Per token for serverless inference Per GPU usage time for on-demand deployments Per token of training data for fine-tuning For customers needing enterprise-grade security and reliability , please reach out to us at inquiries@fireworks.ai to discuss options. Find out more about our current pricing on our Pricing page.

",
    "$.messages[3].content[8].type": "text",
    "$.messages[3].content[9].text": "Title: Python (Fireworks)
Link: https://docs.fireworks.ai/getting-started/quickstart
Content: This is the recommended way to get started : This  Python (Fireworks)  tab uses our  Fireworks Build
SDK  for the best performance and developer experience.
See our  Client-side performance
optimization  guide for
more details. Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. pip install --upgrade fireworks-ai The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either  ~/.bash_profile  for Bash or  ~/.zshrc  for  Zsh .
You can do this by running the command: vim ~/.bash_profile vim ~/.zshrc Add a new line to the file with the following: export FIREWORKS_API_KEY="<API_KEY>" After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. source ~/.bash_profile source ~/.zshrc You can verify that the variable has been set correctly by running  echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. setx FIREWORKS_API_KEY "<API_KEY>" To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: echo %FIREWORKS_API_KEY% You can quickly instantiate the  LLM  class and call the Fireworks API. The Build SDK handles deployment management automatically. from fireworks import LLM

# Basic usage - SDK automatically selects optimal deployment type
llm = LLM(model="llama4-maverick-instruct-basic", deployment_type="auto")

response = llm.chat.completions.create(
    messages=[{"role": "user", "content": "Say this is a test"}]
)

print(response.choices[0].message.content) You can also pass the API key directly to the LLM constructor:  LLM(model="llama4-maverick-instruct-basic", deployment_type="auto", api_key="<FIREWORKS_API_KEY>")

This is the recommended way to get started : This  Python (Fireworks)  tab uses our  Fireworks Build
SDK  for the best performance and developer experience.
See our  Client-side performance
optimization  guide for
more details. Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. pip install --upgrade fireworks-ai The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either  ~/.bash_profile  for Bash or  ~/.zshrc  for  Zsh .
You can do this by running the command: vim ~/.bash_profile vim ~/.zshrc Add a new line to the file with the following: export FIREWORKS_API_KEY="<API_KEY>" After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. source ~/.bash_profile source ~/.zshrc You can verify that the variable has been set correctly by running  echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. setx FIREWORKS_API_KEY "<API_KEY>" To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: echo %FIREWORKS_API_KEY% You can quickly instantiate the  LLM  class and call the Fireworks API. The Build SDK handles deployment management automatically. from fireworks import LLM

# Basic usage - SDK automatically selects optimal deployment type
llm = LLM(model="llama4-maverick-instruct-basic", deployment_type="auto")

response = llm.chat.completions.create(
    messages=[{"role": "user", "content": "Say this is a test"}]
)

print(response.choices[0].message.content) You can also pass the API key directly to the LLM constructor:  LLM(model="llama4-maverick-instruct-basic", deployment_type="auto", api_key="<FIREWORKS_API_KEY>")

Set up your developer environment & call the Fireworks API
This is the recommended way to get started : This  tab uses our Fireworks Build
SDK for the best performance and developer experience.
See our Client-side performance
optimization guide for
more details. Install SDK Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Configure API Key Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either ~/.bash_profile for Bash or ~/.zshrc for Zsh .
You can do this by running the command: Add a new line to the file with the following: After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. You can verify that the variable has been set correctly by running echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: Sending the first API Request You can quickly instantiate the LLM class and call the Fireworks API. The Build SDK handles deployment management automatically. You can also pass the API key directly to the LLM constructor: LLM(model="llama4-maverick-instruct-basic", deployment_type="auto", api_key="<FIREWORKS_API_KEY>") Install SDK Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. Fireworks AI platform offers drop-in replacement with OpenAI Python Client. Configure API Key Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either ~/.bash_profile for Bash or ~/.zshrc for Zsh .
You can do this by running the command: Add a new line to the file with the following: After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. You can verify that the variable has been set correctly by running echo $OPENAI_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: echo %OPENAI_API_KEY%

",
    "$.messages[3].content[9].type": "text",
    "$.messages[3].role": "tool",
    "$.messages[3].tool_call_id": "functions.search:0",
    "$.messages[4].content": "Let me search for more specific information about deployment management commands and workflows.",
    "$.messages[4].role": "assistant",
    "$.messages[4].tool_calls[0].function.arguments": "{"query": "firectl deployment management commands create delete list update"}",
    "$.messages[4].tool_calls[0].function.name": "search",
    "$.messages[4].tool_calls[0].id": "functions.search:1",
    "$.messages[4].tool_calls[0].type": "function",
    "$.messages[5].content[0].text": "Title: Why am I getting "invalid id" errors when using firectl commands like create deployment or list deployments?
Link: https://docs.fireworks.ai/faq-new/fine-tuning/why-am-i-getting-invalid-id-errors-when-using-firectl-commands-like-create-deplo
Content: This error typically occurs when your account ID is not properly configured.

This error typically occurs when your account ID is not properly configured.

Why am I getting "invalid id" errors when using firectl commands like create deployment or list deployments?
This error typically occurs when your account ID is not properly configured.

",
    "$.messages[5].content[0].type": "text",
    "$.messages[5].content[1].text": "Title: Delete a deployment
Link: https://docs.fireworks.ai/tools-sdks/firectl/commands/delete-resources
Content: Deletes an model deployment. firectl delete deployment [flags]

Deletes an model deployment. firectl delete deployment [flags]

Example
firectl delete deployment my-deployment

",
    "$.messages[5].content[1].type": "text",
    "$.messages[5].content[2].text": "Title: Create a deployment
Link: https://docs.fireworks.ai/tools-sdks/firectl/commands/create-deployment
Content: Creates a new deployment. firectl create deployment [flags]

Creates a new deployment. firectl create deployment [flags]

Creates a new deployment.

",
    "$.messages[5].content[2].type": "text",
    "$.messages[5].content[3].text": "Title: Creating an on-demand deployment
Link: https://docs.fireworks.ai/guides/ondemand-deployments
Content: Choose a model Check out our model library to see a list of models available for on-demand deployment. You can also upload and use your own custom base model. Create a deployment To create a new deployment of a model from the model library, use: Or, for a custom base model you have uploaded to your account: This command will complete when the deployment is READY . To let it run asynchronously, remove the --wait flag. If your account has purchased reserved capacity and this deployment meets the reservation criteria, it will be counted against that reservation and not billed as an on-demand deployment. Verify the deployment is running You can verify the deployment is active by running: The state field should show READY. The deployment ID is the last part of the deployment name: accounts/<ACCOUNT_ID>/deployments/<DEPLOYMENT_ID>. You can also list all deployments in your account at any time by running: firectl list deployments

Choose a model Check out our model library to see a list of models available for on-demand deployment. You can also upload and use your own custom base model. Create a deployment To create a new deployment of a model from the model library, use: Or, for a custom base model you have uploaded to your account: This command will complete when the deployment is READY . To let it run asynchronously, remove the --wait flag. If your account has purchased reserved capacity and this deployment meets the reservation criteria, it will be counted against that reservation and not billed as an on-demand deployment. Verify the deployment is running You can verify the deployment is active by running: The state field should show READY. The deployment ID is the last part of the deployment name: accounts/<ACCOUNT_ID>/deployments/<DEPLOYMENT_ID>. You can also list all deployments in your account at any time by running: firectl list deployments

Publishing a deployed model
By default, models can only be queried by the account that owns them. To make a deployment public so anyone with a valid Fireworks API key can query it, update the deployed model with the --public flag. Find the Deployed Model ID Every model running on a deployment receives a unique deployed model ID. You can find this ID if you know the model name and deployment name using the following command: firectl list deployed-models --filter 'model="<MODEL_NAME>" AND deployment="<DEPLOYMENT_NAME>"'

",
    "$.messages[5].content[3].type": "text",
    "$.messages[5].content[4].text": "Title: Delete Deployment
Link: https://docs.fireworks.ai/api-reference/delete-deployment
Content: 



REST Endpoint DELETE /v1/accounts/{account_id}/deployments/{deployment_id}

Server(s): https://api.fireworks.ai


Summary:  

Description: N/A

Security methods: 
header: 
	Authorization: bearer auth
	

Request params: 
path params:
account_id:
	string
	Description: The Account Id
	Example: "<string>"
deployment_id:
	string
	Description: The Deployment Id
	Example: "<string>"

query params:
hard:
	boolean
	Description: If true, this will perform a hard deletion.
	Example: true
ignoreChecks:
	boolean
	Description: If true, this will ignore checks and force the deletion of a deployment that is currently
deployed and is in use.
	Example: true


Request body: N/A


Responses: 
200: object
	Description: N/A
	Example: {}

",
    "$.messages[5].content[4].type": "text",
    "$.messages[5].content[5].text": "Title: 🚀 Easier & faster LoRA fine-tune deployments on Fireworks
Link: https://docs.fireworks.ai/updates/changelog
Content: You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: Previously, this involved two distinct steps, and the resulting deployment was slower than the base model: Create a deployment using firectl create deployment "accounts/fireworks/models/<MODEL_ID of base model>" --enable-addons Then deploy the addon to the deployment: firectl load-lora <MODEL_ID> --deployment <DEPLOYMENT_ID> For more information, see our deployment documentation. This change is for dedicated deployments with a single LoRA. You can still deploy multiple LoRAs on a deployment or deploy LoRA(s) on some Serverless models as described in the documentation.

",
    "$.messages[5].content[5].type": "text",
    "$.messages[5].content[6].text": "Title: Deployment Command
Link: https://docs.fireworks.ai/evaluators/examples/gcp_cloud_run_deployment_example
Content: It's recommended to run the deployment command from the directory containing the reward function script ( dummy_rewards.py ) and your rewardkit.yaml (if used), for example, from examples/gcp_cloud_run_deployment_example/. Ensure your virtual environment is active: Run the deployment command: Command Explanation: --id my-dummy-gcp-evaluator : A unique ID for your evaluator on the Fireworks AI platform. --target gcp-cloud-run : Specifies deployment to GCP Cloud Run. --function-ref dummy_rewards.hello_world_reward : The Python import path to your reward function. If dummy_rewards.py is in the current directory, this reference works. --gcp-auth-mode api-key : Configures the Cloud Run service with API key authentication. reward-kit will generate a key, store it in GCP Secret Manager, and configure the service. The key is also saved to your local rewardkit.yaml under evaluator_endpoint_keys . This is the default if not specified. --verbose : Shows detailed output, including gcloud commands being executed. --force : (Optional) If an evaluator with the same --id already exists, this flag will delete the existing one before creating the new one.

It's recommended to run the deployment command from the directory containing the reward function script ( dummy_rewards.py ) and your rewardkit.yaml (if used), for example, from examples/gcp_cloud_run_deployment_example/. Ensure your virtual environment is active: Run the deployment command: Command Explanation: --id my-dummy-gcp-evaluator : A unique ID for your evaluator on the Fireworks AI platform. --target gcp-cloud-run : Specifies deployment to GCP Cloud Run. --function-ref dummy_rewards.hello_world_reward : The Python import path to your reward function. If dummy_rewards.py is in the current directory, this reference works. --gcp-auth-mode api-key : Configures the Cloud Run service with API key authentication. reward-kit will generate a key, store it in GCP Secret Manager, and configure the service. The key is also saved to your local rewardkit.yaml under evaluator_endpoint_keys . This is the default if not specified. --verbose : Shows detailed output, including gcloud commands being executed. --force : (Optional) If an evaluator with the same --id already exists, this flag will delete the existing one before creating the new one.

It's recommended to run the deployment command from the directory containing the reward function script ( dummy_rewards.py ) and your rewardkit.yaml (if used), for example, from examples/gcp_cloud_run_deployment_example/. Ensure your virtual environment is active: source .venv/bin/activate

",
    "$.messages[5].content[6].type": "text",
    "$.messages[5].content[7].text": "Title: Quick deployment
Link: https://docs.fireworks.ai/fine-tuning/single-lora
Content: Deploy your LoRA fine-tuned model with one simple command: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

Deploy your LoRA fine-tuned model with one simple command: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

Alternative deployment method
This two-step method is the standard approach for multi-LoRA deployments where multiple LoRA models share the same base model. While it can also be used for single LoRA deployments, it provides slower performance compared to live merge and is not recommended for single LoRA use cases. You can also deploy single LoRA models using a two-step process: Create base model deployment Deploy the base model with addons enabled: firectl create deployment "accounts/fireworks/models/<MODEL_ID of base model>" --enable-addons

",
    "$.messages[5].content[7].type": "text",
    "$.messages[5].content[8].text": "Title: Deploying a fine-tuned model using an on-demand deployment
Link: https://docs.fireworks.ai/fine-tuning/fine-tuning-models
Content: Use the following command to deploy your fine-tuned model using an on-demand deployment: firectl create deployment <FINE_TUNED_MODEL_ID>

Use the following command to deploy your fine-tuned model using an on-demand deployment: firectl create deployment <FINE_TUNED_MODEL_ID>

firectl
firectl create dataset <DATASET_ID> /path/to/jsonl/file

",
    "$.messages[5].content[8].type": "text",
    "$.messages[5].content[9].text": "Title: Deleting Stored Responses
Link: https://docs.fireworks.ai/guides/response-api
Content: When responses are stored (the default behavior with store=True ), you can immediately delete them from storage using the DELETE endpoint. This permanently removes the conversation data. from fireworks import LLM
import requests
import os

llm = LLM(model="qwen3-235b-a22b", deployment_type="serverless")

# Create a response
response = llm.responses.create(
    input="What is the capital of France?",
    store=True  # This is the default
)

response_id = response.id
print(f"Created response with ID: {response_id}")

# Delete the response immediately
headers = {
    "Authorization": f"Bearer {os.getenv('FIREWORKS_API_KEY')}",
    "x-fireworks-account-id": "your-account-id"
}
delete_response = requests.delete(
    f"https://api.fireworks.ai/inference/v1/responses/{response_id}",
    headers=headers
)

if delete_response.status_code == 200:
    print("Response deleted successfully")
else:
    print(f"Failed to delete response: {delete_response.status_code}")

When responses are stored (the default behavior with store=True ), you can immediately delete them from storage using the DELETE endpoint. This permanently removes the conversation data. from fireworks import LLM
import requests
import os

llm = LLM(model="qwen3-235b-a22b", deployment_type="serverless")

# Create a response
response = llm.responses.create(
    input="What is the capital of France?",
    store=True  # This is the default
)

response_id = response.id
print(f"Created response with ID: {response_id}")

# Delete the response immediately
headers = {
    "Authorization": f"Bearer {os.getenv('FIREWORKS_API_KEY')}",
    "x-fireworks-account-id": "your-account-id"
}
delete_response = requests.delete(
    f"https://api.fireworks.ai/inference/v1/responses/{response_id}",
    headers=headers
)

if delete_response.status_code == 200:
    print("Response deleted successfully")
else:
    print(f"Failed to delete response: {delete_response.status_code}")

When responses are stored (the default behavior with store=True ), you can immediately delete them from storage using the DELETE endpoint. This permanently removes the conversation data. import os
from openai import OpenAI
import requests

client = OpenAI(
    base_url="https://api.fireworks.ai/inference/v1",
    api_key=os.getenv("FIREWORKS_API_KEY", "YOUR_FIREWORKS_API_KEY_HERE")
)

# Create a response
response = client.responses.create(
    model="accounts/fireworks/models/qwen3-235b-a22b",
    input="What is the capital of France?",
    store=True  # This is the default
)

response_id = response.id
print(f"Created response with ID: {response_id}")

# Delete the response immediately
headers = {
    "Authorization": f"Bearer {os.getenv('FIREWORKS_API_KEY')}",
    "x-fireworks-account-id": "your-account-id"
}
delete_response = requests.delete(
    f"https://api.fireworks.ai/inference/v1/responses/{response_id}",
    headers=headers
)

if delete_response.status_code == 200:
    print("Response deleted successfully")
else:
    print(f"Failed to delete response: {delete_response.status_code}")

",
    "$.messages[5].content[9].type": "text",
    "$.messages[5].role": "tool",
    "$.messages[5].tool_call_id": "functions.search:1",
    "$.messages[6].content": "Let me search for more information about listing and managing deployments.",
    "$.messages[6].role": "assistant",
    "$.messages[6].tool_calls[0].function.arguments": "{"query": "list deployments firectl commands deployment management"}",
    "$.messages[6].tool_calls[0].function.name": "search",
    "$.messages[6].tool_calls[0].id": "functions.search:2",
    "$.messages[6].tool_calls[0].type": "function",
    "$.messages[7].content[0].text": "Title: Why am I getting "invalid id" errors when using firectl commands like create deployment or list deployments?
Link: https://docs.fireworks.ai/faq-new/fine-tuning/why-am-i-getting-invalid-id-errors-when-using-firectl-commands-like-create-deplo
Content: This error typically occurs when your account ID is not properly configured.

This error typically occurs when your account ID is not properly configured.

Why am I getting "invalid id" errors when using firectl commands like create deployment or list deployments?
This error typically occurs when your account ID is not properly configured.

",
    "$.messages[7].content[0].type": "text",
    "$.messages[7].content[1].text": "Title: List deployments
Link: https://docs.fireworks.ai/tools-sdks/firectl/commands/list-resources
Content: Prints all deployments in the account. firectl list deployments [flags]

Prints all deployments in the account. firectl list deployments [flags]

List deployed models
Prints all deployed models in an account. firectl list deployed-models [flags]

",
    "$.messages[7].content[1].type": "text",
    "$.messages[7].content[2].text": "Title: 🚀 Easier & faster LoRA fine-tune deployments on Fireworks
Link: https://docs.fireworks.ai/updates/changelog
Content: You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: Previously, this involved two distinct steps, and the resulting deployment was slower than the base model: Create a deployment using firectl create deployment "accounts/fireworks/models/<MODEL_ID of base model>" --enable-addons Then deploy the addon to the deployment: firectl load-lora <MODEL_ID> --deployment <DEPLOYMENT_ID> For more information, see our deployment documentation. This change is for dedicated deployments with a single LoRA. You can still deploy multiple LoRAs on a deployment or deploy LoRA(s) on some Serverless models as described in the documentation.

",
    "$.messages[7].content[2].type": "text",
    "$.messages[7].content[3].text": "Title: Managing default deployments
Link: https://docs.fireworks.ai/guides/ondemand-deployments
Content: Since a model may be deployed to multiple deployments, querying by model name will route to the "default" deployed model. You can see which deployed model entry is marked with Default: true using: firectl get model <MODEL_ID>

Since a model may be deployed to multiple deployments, querying by model name will route to the "default" deployed model. You can see which deployed model entry is marked with Default: true using: firectl get model <MODEL_ID>

Creating an on-demand deployment
Choose a model Check out our model library to see a list of models available for on-demand deployment. You can also upload and use your own custom base model. Create a deployment To create a new deployment of a model from the model library, use: Or, for a custom base model you have uploaded to your account: This command will complete when the deployment is READY . To let it run asynchronously, remove the --wait flag. If your account has purchased reserved capacity and this deployment meets the reservation criteria, it will be counted against that reservation and not billed as an on-demand deployment. Verify the deployment is running You can verify the deployment is active by running: The state field should show READY. The deployment ID is the last part of the deployment name: accounts/<ACCOUNT_ID>/deployments/<DEPLOYMENT_ID>. You can also list all deployments in your account at any time by running: firectl list deployments

",
    "$.messages[7].content[3].type": "text",
    "$.messages[7].content[4].text": "Title: Does Fireworks support custom base models?
Link: https://docs.fireworks.ai/faq-new/models-inference/does-fireworks-support-custom-base-models
Content: Yes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.

Yes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.

Does Fireworks support custom base models?
Yes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.

",
    "$.messages[7].content[4].type": "text",
    "$.messages[7].content[5].text": "Title: Quick deployment
Link: https://docs.fireworks.ai/fine-tuning/single-lora
Content: Deploy your LoRA fine-tuned model with one simple command: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

Deploy your LoRA fine-tuned model with one simple command: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

Alternative deployment method
This two-step method is the standard approach for multi-LoRA deployments where multiple LoRA models share the same base model. While it can also be used for single LoRA deployments, it provides slower performance compared to live merge and is not recommended for single LoRA use cases. You can also deploy single LoRA models using a two-step process: Create base model deployment Deploy the base model with addons enabled: firectl create deployment "accounts/fireworks/models/<MODEL_ID of base model>" --enable-addons

",
    "$.messages[7].content[5].type": "text",
    "$.messages[7].content[6].text": "Title: List Deployments
Link: https://docs.fireworks.ai/api-reference/list-deployments
Content: 



REST Endpoint GET /v1/accounts/{account_id}/deployments

Server(s): https://api.fireworks.ai


Summary:  

Description: N/A

Security methods: 
header: 
	Authorization: bearer auth
	

Request params: 
path params:
account_id:
	string
	Description: The Account Id
	Example: "<string>"

query params:
pageSize:
	integer
	Description: The maximum number of deployments to return. The maximum page_size is 200,
values above 200 will be coerced to 200.
If unspecified, the default is 50.
	Example: 123
pageToken:
	string
	Description: A page token, received from a previous ListDeployments call. Provide this
to retrieve the subsequent page. When paginating, all other parameters
provided to ListDeployments must match the call that provided the page
token.
	Example: "<string>"
filter:
	string
	Description: Only deployment satisfying the provided filter (if specified) will be
returned. See https://google.aip.dev/160 for the filter grammar.
	Example: "<string>"
orderBy:
	string
	Description: A comma-separated list of fields to order by. e.g. "foo,bar"
The default sort order is ascending. To specify a descending order for a
field, append a " desc" suffix. e.g. "foo desc,bar"
Subfields are specified with a "." character. e.g. "foo.bar"
If not specified, the default order is by "create_time".
	Example: "<string>"
showDeleted:
	boolean
	Description: If set, DELETED deployments will be included.
	Example: true
readMask:
	string
	Description: The fields to be returned in the response. If empty or "*", all fields will be returned.
	Example: "<string>"


Request body: N/A


Responses: 
200: object
	Description: N/A
	Example: {"deployments":[{"name":"<string>","displayName":"<string>","description":"<string>","createTime":"2023-11-07T05:31:56Z","expireTime":"2023-11-07T05:31:56Z","purgeTime":"2023-11-07T05:31:56Z","deleteTime":"2023-11-07T05:31:56Z","state":"STATE_UNSPECIFIED","status":{"code":"OK","message":"<string>"},"minReplicaCount":123,"maxReplicaCount":123,"replicaCount":123,"autoscalingPolicy":{"scaleUpWindow":"<string>","scaleDownWindow":"<string>","scaleToZeroWindow":"<string>","loadTargets":{}},"baseModel":"<string>","acceleratorCount":123,"acceleratorType":"ACCELERATOR_TYPE_UNSPECIFIED","precision":"PRECISION_UNSPECIFIED","cluster":"<string>","enableAddons":true,"draftTokenCount":123,"draftModel":"<string>","ngramSpeculationLength":123,"numPeftDeviceCached":123,"deploymentTemplate":"<string>","autoTune":{"longPrompt":true},"placement":{"region":"REGION_UNSPECIFIED","multiRegion":"MULTI_REGION_UNSPECIFIED","regions":["REGION_UNSPECIFIED"]},"region":"REGION_UNSPECIFIED","updateTime":"2023-11-07T05:31:56Z","disableDeploymentSizeValidation":true}],"nextPageToken":"<string>","totalSize":123}

",
    "$.messages[7].content[6].type": "text",
    "$.messages[7].content[7].text": "Title: Deploy Command
Link: https://docs.fireworks.ai/evaluators/cli_reference/cli_overview
Content: The deploy command deploys a reward function as an evaluator on the Fireworks platform.

The deploy command deploys a reward function as an evaluator on the Fireworks platform.

Command Overview
The Reward Kit CLI supports the following main commands: run : Run a local evaluation pipeline using a Hydra configuration. preview : Preview evaluation results or re-evaluate generated outputs. deploy : Deploy a reward function as an evaluator. agent-eval : Run agent evaluations on task bundles. list : List existing evaluators (coming soon). delete : Delete an evaluator (coming soon).

",
    "$.messages[7].content[7].type": "text",
    "$.messages[7].content[8].text": "Title: Deploying a fine-tuned model using an on-demand deployment
Link: https://docs.fireworks.ai/fine-tuning/fine-tuning-models
Content: Use the following command to deploy your fine-tuned model using an on-demand deployment: firectl create deployment <FINE_TUNED_MODEL_ID>

Use the following command to deploy your fine-tuned model using an on-demand deployment: firectl create deployment <FINE_TUNED_MODEL_ID>

Deploying a fine-tuned model serverlessly
Not all base models support serverless addons. Please check this list for the full list of serverless models that support LoRA add-ons. If your base model supports serverless addons, you can run the following command to load the fine-tuned model into a serverless deployment: firectl load-lora <FINE_TUNED_MODEL_ID>

",
    "$.messages[7].content[8].type": "text",
    "$.messages[7].content[9].text": "Title: Create a deployment
Link: https://docs.fireworks.ai/tools-sdks/firectl/commands/create-deployment
Content: Creates a new deployment. firectl create deployment [flags]

Creates a new deployment. firectl create deployment [flags]

Create a Deployment on Fireworks AI platform

",
    "$.messages[7].content[9].type": "text",
    "$.messages[7].role": "tool",
    "$.messages[7].tool_call_id": "functions.search:2",
    "$.messages[8].content": "Now let me search for information about deployment configuration and management best practices.",
    "$.messages[8].role": "assistant",
    "$.messages[8].tool_calls[0].function.arguments": "{"query": "deployment configuration scaling autoscaling reserved capacity Fireworks"}",
    "$.messages[8].tool_calls[0].function.name": "search",
    "$.messages[8].tool_calls[0].id": "functions.search:0",
    "$.messages[8].tool_calls[0].type": "function",
    "$.messages[9].content[0].text": "Title: How does billing and scaling work for on-demand GPU deployments?
Link: https://docs.fireworks.ai/faq-new/deployment-infrastructure/how-does-billing-and-scaling-work-for-on-demand-gpu-deployments
Content: On-demand GPU deployments have unique billing and scaling characteristics compared to serverless deployments: Billing: Charges start when the server begins accepting requests Billed by GPU-second for each active instance Costs accumulate even if there are no active API calls Scaling options: Supports autoscaling from 0 to multiple GPUs Each additional GPU adds to the billing rate Can handle unlimited requests within the GPU’s capacity Management requirements: Not fully serverless; requires some manual management Manually delete deployments when no longer needed Or configure autoscaling to scale down to 0 during inactive periods Cost control tips: Regularly monitor active deployments Delete unused deployments to avoid unnecessary costs Consider serverless options for intermittent usage Use autoscaling to 0 to optimize costs during low-demand times

On-demand GPU deployments have unique billing and scaling characteristics compared to serverless deployments: Billing: Charges start when the server begins accepting requests Billed by GPU-second for each active instance Costs accumulate even if there are no active API calls Scaling options: Supports autoscaling from 0 to multiple GPUs Each additional GPU adds to the billing rate Can handle unlimited requests within the GPU’s capacity Management requirements: Not fully serverless; requires some manual management Manually delete deployments when no longer needed Or configure autoscaling to scale down to 0 during inactive periods Cost control tips: Regularly monitor active deployments Delete unused deployments to avoid unnecessary costs Consider serverless options for intermittent usage Use autoscaling to 0 to optimize costs during low-demand times

How does billing and scaling work for on-demand GPU deployments?
On-demand GPU deployments have unique billing and scaling characteristics compared to serverless deployments: Billing: Charges start when the server begins accepting requests Billed by GPU-second for each active instance Costs accumulate even if there are no active API calls Scaling options: Supports autoscaling from 0 to multiple GPUs Each additional GPU adds to the billing rate Can handle unlimited requests within the GPU’s capacity Management requirements: Not fully serverless; requires some manual management Manually delete deployments when no longer needed Or configure autoscaling to scale down to 0 during inactive periods Cost control tips: Regularly monitor active deployments Delete unused deployments to avoid unnecessary costs Consider serverless options for intermittent usage Use autoscaling to 0 to optimize costs during low-demand times

",
    "$.messages[9].content[0].type": "text",
    "$.messages[9].content[1].text": "Title: What should I expect for deployment and scaling performance?
Link: https://docs.fireworks.ai/faq-new/deployment-infrastructure/what-should-i-expect-for-deployment-and-scaling-performance
Content: Initial deployment : Should complete within minutes Scaling from zero : You may experience brief availability delays while the system scales up Troubleshooting : If deployment takes over 1 hour, this typically indicates a crash and should be investigated Best practice : Monitor deployment status and contact support if deployment times are unusually long

Initial deployment : Should complete within minutes Scaling from zero : You may experience brief availability delays while the system scales up Troubleshooting : If deployment takes over 1 hour, this typically indicates a crash and should be investigated Best practice : Monitor deployment status and contact support if deployment times are unusually long

What should I expect for deployment and scaling performance?
Initial deployment : Should complete within minutes Scaling from zero : You may experience brief availability delays while the system scales up Troubleshooting : If deployment takes over 1 hour, this typically indicates a crash and should be investigated Best practice : Monitor deployment status and contact support if deployment times are unusually long

",
    "$.messages[9].content[1].type": "text",
    "$.messages[9].content[2].text": "Title: Deployment type selection
Link: https://docs.fireworks.ai/tools-sdks/python-client/sdk-basics
Content: The SDK tries to be parsimonious with the way it deploys resources. We provide two types of deployment options on Fireworks: serverless hosting is enabled for some commonly-used state of the art models. The pricing for these models is per-token, i.e. you only pay for the tokens you use, and subject to rate limits. on-demand hosting is enabled for all other models. The pricing for these models is per GPU-second. This hosting is required for models that are not available serverlessly or workloads that exceed serverless rate limits. For non-finetuned models, you can always specify the deployment type of LLM() by passing either "serverless" or "on-demand" as the deployment_type parameter to the constructor. If the model is not available for the deployment type you selected, the SDK will throw an error. The SDK can also decide the best deployment strategy on your behalf, just pass deployment_type="auto" . If the model is available serverlessly, the SDK will use serverless hosting, otherwise the SDK will create an on-demand deployment. When using deployment_type="on-demand" or deployment_type="on-demand-lora" , you must call .apply() to apply the deployment configuration to Fireworks. This is not required for serverless deployments. When using deployment_type="auto" , the SDK will automatically handle deployment creation, but if it falls back to on-demand deployment, you may need to call .apply() explicitly. If you do not call .apply() , you are expected to set up the deployment through the deployment page at https://app.fireworks.ai/dashboard/deployments. Be careful with the deployment_type parameter, especially for "auto" and "on-demand" deployments. While the SDK will try to make the most cost effective choice for you and put sensible autoscaling policies in place, it is possible to unintentionally create many deployments that lead to unwanted spend, especially when working with non-serverless models. When using deployment_type="on-demand" , you must provide an id parameter to uniquely identify your deployment. This is required to prevent accidental creation of multiple deployments. For finetuned (LoRA) models, passing deployment_type="serverless" will try to deploy the finetuned model to serverless hosting, deployment_type="on-demand" will create an on-demand deployment of your base model and merge in your LoRA weights, deployment_type="on-demand-lora" will create an on-demand deployment with Multi-LoRA enabled, and deployment_type="auto" will try to use serverless if available, otherwise fall back to on-demand-lora.

The SDK tries to be parsimonious with the way it deploys resources. We provide two types of deployment options on Fireworks: serverless hosting is enabled for some commonly-used state of the art models. The pricing for these models is per-token, i.e. you only pay for the tokens you use, and subject to rate limits. on-demand hosting is enabled for all other models. The pricing for these models is per GPU-second. This hosting is required for models that are not available serverlessly or workloads that exceed serverless rate limits. For non-finetuned models, you can always specify the deployment type of LLM() by passing either "serverless" or "on-demand" as the deployment_type parameter to the constructor. If the model is not available for the deployment type you selected, the SDK will throw an error. The SDK can also decide the best deployment strategy on your behalf, just pass deployment_type="auto" . If the model is available serverlessly, the SDK will use serverless hosting, otherwise the SDK will create an on-demand deployment. When using deployment_type="on-demand" or deployment_type="on-demand-lora" , you must call .apply() to apply the deployment configuration to Fireworks. This is not required for serverless deployments. When using deployment_type="auto" , the SDK will automatically handle deployment creation, but if it falls back to on-demand deployment, you may need to call .apply() explicitly. If you do not call .apply() , you are expected to set up the deployment through the deployment page at https://app.fireworks.ai/dashboard/deployments. Be careful with the deployment_type parameter, especially for "auto" and "on-demand" deployments. While the SDK will try to make the most cost effective choice for you and put sensible autoscaling policies in place, it is possible to unintentionally create many deployments that lead to unwanted spend, especially when working with non-serverless models. When using deployment_type="on-demand" , you must provide an id parameter to uniquely identify your deployment. This is required to prevent accidental creation of multiple deployments. For finetuned (LoRA) models, passing deployment_type="serverless" will try to deploy the finetuned model to serverless hosting, deployment_type="on-demand" will create an on-demand deployment of your base model and merge in your LoRA weights, deployment_type="on-demand-lora" will create an on-demand deployment with Multi-LoRA enabled, and deployment_type="auto" will try to use serverless if available, otherwise fall back to on-demand-lora.

Deploying Fine-tuned Models with On-Demand
When deploying a fine-tuned model using deployment_type="on-demand" , you need to provide: model - Your fine-tuned model ID (e.g., "accounts/your-account/models/your-fine-tuned-model-id") id - A unique deployment identifier (can be any simple string like "my-fine-tuned-deployment") # Deploy a fine-tuned model with on-demand deployment
fine_tuned_llm = LLM(
    model="accounts/your-account/models/your-fine-tuned-model-id",
    deployment_type="on-demand",
    id="my-fine-tuned-deployment"  # Simple string ID
)

# Apply deployment configuration to Fireworks
fine_tuned_llm.apply()

# Track deployment in web dashboard
print(f"Track at: {fine_tuned_llm.deployment_url}")

",
    "$.messages[9].content[2].type": "text",
    "$.messages[9].content[3].text": "Title: Spike arrest policy
Link: https://docs.fireworks.ai/guides/quotas_usage/rate-limits
Content: LLM traffic that spikes quickly has the potential to be throttled. Here's how it works: Each user has a guaranteed rate limit, which increases with sustained usage near the limit. Typically, you can expect to stay within the limits if your traffic gradually doubles within an hour. You can see your guaranteed limits using API response headers (see below) Exceeding your guaranteed limit means that there's the potential for your requests to be processed with lower-priority. Fireworks operates serverless deployments by autoscaling capacity (within limits) as user traffic increases. However, if a deployment is overloaded while auto-scaling, requests that fall outside of guaranteed limits may be processed with lower-latency or dropped with HTTP code 429 (if limits are significantly exceeded).  You can monitor if you exceed limits via API response header x-ratelimit-over-limit: yes. Exceeding your guaranteed limit does not guarantee that your requests will be throttled. You can monitor if your requests are actually being throttled by monitoring latencies. Here's an example of how dynamic rate limits scale up: Metric Minimum Guaranteed Limit 10 Minutes 1 Hour 2 Hours Requests per minute 60 120 720 1440 Input tokens per minute 60000 120000 720000 1440000 Output tokens per minute 6000 12000 72000 144000

LLM traffic that spikes quickly has the potential to be throttled. Here's how it works: Each user has a guaranteed rate limit, which increases with sustained usage near the limit. Typically, you can expect to stay within the limits if your traffic gradually doubles within an hour. You can see your guaranteed limits using API response headers (see below) Exceeding your guaranteed limit means that there's the potential for your requests to be processed with lower-priority. Fireworks operates serverless deployments by autoscaling capacity (within limits) as user traffic increases. However, if a deployment is overloaded while auto-scaling, requests that fall outside of guaranteed limits may be processed with lower-latency or dropped with HTTP code 429 (if limits are significantly exceeded).  You can monitor if you exceed limits via API response header x-ratelimit-over-limit: yes. Exceeding your guaranteed limit does not guarantee that your requests will be throttled. You can monitor if your requests are actually being throttled by monitoring latencies. Here's an example of how dynamic rate limits scale up: Metric Minimum Guaranteed Limit 10 Minutes 1 Hour 2 Hours Requests per minute 60 120 720 1440 Input tokens per minute 60000 120000 720000 1440000 Output tokens per minute 6000 12000 72000 144000

Spend limits
In order to prevent fraud, Fireworks imposes a monthly spending limit on your account. Once you hit the spending limit, your account will automatically enter a suspended state, API requests will be rejected and all Fireworks usage will be stopped. This includes serverless inference, dedicated deployments, and fine-tuning jobs. Your spend limit will organically increase over time as you spend more on the platform. You can also increase your spend limit at any time, by purchasing prepaid credits to meet the historical spend required for a higher tier. For instance, if you are a new Tier 1 user with $0 historical spend, you can purchase $100 prepaid credits and become a Tier 2 user. You can qualify for a higher tier by adding credits into your Fireworks account. There may be a propagation delay for a few minutes after you prepay for credits - you may still see "monthly usage exceeded error" for a few minutes after adding credits. Tier Qualification Spending Limit Tier 1 Valid payment method added $50/mo Tier 2 $50 spent in payments or credits added $500/mo Tier 3 $500 spent in payments or credits added $5,000/mo Tier 4 $5000 spent in payments or credits added $50,000/mo Unlimited Contact us at inquiries@fireworks.ai Unlimited

",
    "$.messages[9].content[3].type": "text",
    "$.messages[9].content[4].text": "Title: How does billing work for on-demand deployments?
Link: https://docs.fireworks.ai/faq-new/deployment-infrastructure/how-does-billing-work-for-on-demand-deployments
Content: On-demand deployments come with automatic cost optimization features: Default autoscaling : Automatically scales to 0 replicas when not in use Pay for what you use : Charged only for GPU time when replicas are active Flexible configuration : Customize autoscaling behavior to match your needs Best practices for cost management: Leverage default autoscaling : The system automatically scales down deployments when not in use Customize carefully : While you can modify autoscaling behavior using our configuration options , note that preventing scale-to-zero will result in continuous GPU charges Consider your use case : For intermittent or low-frequency usage, serverless deployments might be more cost-effective For detailed configuration options, see our deployment guide.

On-demand deployments come with automatic cost optimization features: Default autoscaling : Automatically scales to 0 replicas when not in use Pay for what you use : Charged only for GPU time when replicas are active Flexible configuration : Customize autoscaling behavior to match your needs Best practices for cost management: Leverage default autoscaling : The system automatically scales down deployments when not in use Customize carefully : While you can modify autoscaling behavior using our configuration options , note that preventing scale-to-zero will result in continuous GPU charges Consider your use case : For intermittent or low-frequency usage, serverless deployments might be more cost-effective For detailed configuration options, see our deployment guide.

How does billing work for on-demand deployments?
On-demand deployments come with automatic cost optimization features: Default autoscaling : Automatically scales to 0 replicas when not in use Pay for what you use : Charged only for GPU time when replicas are active Flexible configuration : Customize autoscaling behavior to match your needs Best practices for cost management: Leverage default autoscaling : The system automatically scales down deployments when not in use Customize carefully : While you can modify autoscaling behavior using our configuration options , note that preventing scale-to-zero will result in continuous GPU charges Consider your use case : For intermittent or low-frequency usage, serverless deployments might be more cost-effective For detailed configuration options, see our deployment guide.

",
    "$.messages[9].content[4].type": "text",
    "$.messages[9].content[5].text": "Title: Replica count (horizontal scaling)
Link: https://docs.fireworks.ai/guides/ondemand-deployments
Content: The number of replicas (horizontal scaling) is specified by passing the --min-replica-count and --max-replica-count flags. Increasing the number of replicas will increase the maximum QPS the deployment can support. The deployment will automatically scale based on server load. Auto-scaling up may fail if there is a GPU stockout. Use reserved capacity to guarantee capacity for your deployments. The default value for --min-replica-count is 0. Setting --min-replica-count to 0 enables the deployment to auto-scale to 0 if a deployment is unused (i.e. no inference requests) for a specified "scale-to-zero" time window. While the deployment is scaled to 0, you will not pay for any GPU utilization. The default value for --max-replica-count is 1 if --min-replica-count=0 , or the value of
--min-replica-count otherwise. firectl create deployment <MODEL_NAME> \\
  --min-replica-count 2 \\
  --max-replica-count 3

The number of replicas (horizontal scaling) is specified by passing the --min-replica-count and --max-replica-count flags. Increasing the number of replicas will increase the maximum QPS the deployment can support. The deployment will automatically scale based on server load. Auto-scaling up may fail if there is a GPU stockout. Use reserved capacity to guarantee capacity for your deployments. The default value for --min-replica-count is 0. Setting --min-replica-count to 0 enables the deployment to auto-scale to 0 if a deployment is unused (i.e. no inference requests) for a specified "scale-to-zero" time window. While the deployment is scaled to 0, you will not pay for any GPU utilization. The default value for --max-replica-count is 1 if --min-replica-count=0 , or the value of
--min-replica-count otherwise. firectl create deployment <MODEL_NAME> \\
  --min-replica-count 2 \\
  --max-replica-count 3

The number of replicas (horizontal scaling) is specified by passing the --min-replica-count and --max-replica-count flags. Increasing the number of replicas will increase the maximum QPS the deployment can support. The deployment will automatically scale based on server load. Auto-scaling up may fail if there is a GPU stockout. Use reserved capacity to guarantee capacity for your deployments. The default value for --min-replica-count is 0. Setting --min-replica-count to 0 enables the deployment to auto-scale to 0 if a deployment is unused (i.e. no inference requests) for a specified "scale-to-zero" time window. While the deployment is scaled to 0, you will not pay for any GPU utilization. The default value for --max-replica-count is 1 if --min-replica-count=0 , or the value of
--min-replica-count otherwise. firectl update deployment <DEPLOYMENT_ID> \\
  --min-replica-count 2 \\
  --max-replica-count 3

",
    "$.messages[9].content[5].type": "text",
    "$.messages[9].content[6].text": "Title: Do you support Auto Scaling?
Link: https://docs.fireworks.ai/faq-new/deployment-infrastructure/do-you-support-auto-scaling
Content: Yes, our system supports auto scaling with the following features: Scaling down to zero capability for resource efficiency Controllable scale-up and scale-down velocity Custom scaling rules and thresholds to match your specific needs

Yes, our system supports auto scaling with the following features: Scaling down to zero capability for resource efficiency Controllable scale-up and scale-down velocity Custom scaling rules and thresholds to match your specific needs

Do you support Auto Scaling?
Yes, our system supports auto scaling with the following features: Scaling down to zero capability for resource efficiency Controllable scale-up and scale-down velocity Custom scaling rules and thresholds to match your specific needs

",
    "$.messages[9].content[6].type": "text",
    "$.messages[9].content[7].text": "Title: Inference
Link: https://docs.fireworks.ai/getting-started/introduction
Content: Fireworks has 3 options for running generative AI models with unparalleled speed and costs. Serverless: The easiest way to get started. Use the most popular models on pre-configured GPUs. Pay per token and avoid cold boots. On-demand: The most flexible option for scaling. Use private GPUs to support your specific needs and only pay when you're using it. GPUs running Fireworks software offer both ~250% improved throughput and 50% improved latency compared to vLLM. Excels for: Production volume - Per-token costs decrease with more volume and there are no set rate limits Custom needs and reliability - On-demand GPUs are private to you. This enables complete control to tailor deployments for speed/throughput/reliability or to run more specialized models Enterprise Reserved GPUs: Use private GPUs with hardware and software set-up personally tailored by the Fireworks team for your use case. Enjoy SLAs, dedicated support, bring-your-own-cloud (BYOC) deployment options, and enterprise-only optimizations. Property Serverless On-demand Enterprise reserved Performance Industry-leading speed on Fireworks-curated set-up. Performance may vary with others' usage. Speed dependent on user-specified GPU configuration and private usage. Per GPU latency should be significantly faster than vLLM. Tailor-made set-up by Fireworks AI experts for best possible latency Getting Started Self-serve - immediately use serverless with 1 line of code Self-serve - configure GPUs, then use them with 1 line of code. Chat with Fireworks Scaling and management Scale up and down freely within rate limits Option for auto-scaling GPUs with traffic. GPUs scale to zero automatically, so no charge for unused GPUs and for boot-ups. Chat with Fireworks Pricing Pay fixed price per token Pay per GPU second with no commitments. Per GPU throughput should be significantly greater than options like vLLM. Customized price based on reserved GPU capacity Commitment None None Arrange plan length with Fireworks Rate limits Yes, see quotas No rate limits. Quotas on number of GPUs None Model Selection Collection of popular models, curated by Fireworks Use 100s of pre-uploaded models or upload your own custom model within supported architecture Use 100s of pre-uploaded models or upload any model

Fireworks has 3 options for running generative AI models with unparalleled speed and costs. Serverless: The easiest way to get started. Use the most popular models on pre-configured GPUs. Pay per token and avoid cold boots. On-demand: The most flexible option for scaling. Use private GPUs to support your specific needs and only pay when you're using it. GPUs running Fireworks software offer both ~250% improved throughput and 50% improved latency compared to vLLM. Excels for: Production volume - Per-token costs decrease with more volume and there are no set rate limits Custom needs and reliability - On-demand GPUs are private to you. This enables complete control to tailor deployments for speed/throughput/reliability or to run more specialized models Enterprise Reserved GPUs: Use private GPUs with hardware and software set-up personally tailored by the Fireworks team for your use case. Enjoy SLAs, dedicated support, bring-your-own-cloud (BYOC) deployment options, and enterprise-only optimizations. Property Serverless On-demand Enterprise reserved Performance Industry-leading speed on Fireworks-curated set-up. Performance may vary with others' usage. Speed dependent on user-specified GPU configuration and private usage. Per GPU latency should be significantly faster than vLLM. Tailor-made set-up by Fireworks AI experts for best possible latency Getting Started Self-serve - immediately use serverless with 1 line of code Self-serve - configure GPUs, then use them with 1 line of code. Chat with Fireworks Scaling and management Scale up and down freely within rate limits Option for auto-scaling GPUs with traffic. GPUs scale to zero automatically, so no charge for unused GPUs and for boot-ups. Chat with Fireworks Pricing Pay fixed price per token Pay per GPU second with no commitments. Per GPU throughput should be significantly greater than options like vLLM. Customized price based on reserved GPU capacity Commitment None None Arrange plan length with Fireworks Rate limits Yes, see quotas No rate limits. Quotas on number of GPUs None Model Selection Collection of popular models, curated by Fireworks Use 100s of pre-uploaded models or upload your own custom model within supported architecture Use 100s of pre-uploaded models or upload any model

Fireworks AI Developer Platform
Fireworks AI is the best platform for building AI product experiences with open source AI models. You can run and customize AI models with just a few lines of code!

",
    "$.messages[9].content[7].type": "text",
    "$.messages[9].content[8].text": "Title: 2. Call a language model using the LLM() class
Link: https://docs.fireworks.ai/tools-sdks/python-client/the-tutorial
Content: Now that your machine is setup with credentials and the SDK, lets ensure you are
ready to make your first LLM call and explain some of the nuances of this SDK. Create a new file called main.py and import the Fireworks AI SDK. Instantiate the LLM class. The LLM class accepts a model argument that you
can use to specify the model you want to use. For this tutorial, we will use the
Llama 4 Maverick
model. When creating an LLM instance, you can specify the deployment type as either "serverless" , "on-demand" , or "auto" . If you pass "auto" , the SDK will try to use serverless hosting if available, otherwise it will create an on-demand deployment. In the other cases, the SDK will try to create a deployment of the specified type and will throw an error if it's not available for the model you selected. The SDK will try and re-use existing deployments for the same model if possible, see Resource management for more details. With great power comes great responsibility! Be careful with the deployment_type parameter, especially for "auto" and "on-demand" . While the SDK will try to make the most cost effective choice for you and put sensible autoscaling policies in place, it is possible to unintentionally create many deployments that lead to unwanted spend, especially when working with non-serverless models. When using deployment_type="on-demand" , you must provide an id parameter to uniquely identify your deployment. This is required to prevent accidental creation of multiple deployments. When using deployment_type="on-demand" or deployment_type="on-demand-lora" , you must call .apply() to apply the deployment configuration to Fireworks. This is not required for serverless deployments. When using deployment_type="auto" , the SDK will automatically handle deployment creation, but if it falls back to on-demand deployment, you may need to call .apply() explicitly. If you do not call .apply() , you are expected to set up the deployment through the deployment page at https://app.fireworks.ai/dashboard/deployments. Make a request to the LLM. The LLM class is OpenAI compatible, so you can use
the same chat completion interface to make a request to the LLM. Hello! It's nice to meet you. Is there something I can help you with or would you like to chat?

Now that your machine is setup with credentials and the SDK, lets ensure you are
ready to make your first LLM call and explain some of the nuances of this SDK. Create a new file called main.py and import the Fireworks AI SDK. Instantiate the LLM class. The LLM class accepts a model argument that you
can use to specify the model you want to use. For this tutorial, we will use the
Llama 4 Maverick
model. When creating an LLM instance, you can specify the deployment type as either "serverless" , "on-demand" , or "auto" . If you pass "auto" , the SDK will try to use serverless hosting if available, otherwise it will create an on-demand deployment. In the other cases, the SDK will try to create a deployment of the specified type and will throw an error if it's not available for the model you selected. The SDK will try and re-use existing deployments for the same model if possible, see Resource management for more details. With great power comes great responsibility! Be careful with the deployment_type parameter, especially for "auto" and "on-demand" . While the SDK will try to make the most cost effective choice for you and put sensible autoscaling policies in place, it is possible to unintentionally create many deployments that lead to unwanted spend, especially when working with non-serverless models. When using deployment_type="on-demand" , you must provide an id parameter to uniquely identify your deployment. This is required to prevent accidental creation of multiple deployments. When using deployment_type="on-demand" or deployment_type="on-demand-lora" , you must call .apply() to apply the deployment configuration to Fireworks. This is not required for serverless deployments. When using deployment_type="auto" , the SDK will automatically handle deployment creation, but if it falls back to on-demand deployment, you may need to call .apply() explicitly. If you do not call .apply() , you are expected to set up the deployment through the deployment page at https://app.fireworks.ai/dashboard/deployments. Make a request to the LLM. The LLM class is OpenAI compatible, so you can use
the same chat completion interface to make a request to the LLM. The great thing about the SDK is that you can use your favorite Python constructs to powerfully work with LLMs. For example, let's try calling a few LLMs in a loop and see how they respond: from fireworks import LLM

llms = [
    "llama4-maverick-instruct-basic",
    "deepseek-r1",
    "qwen2p5-vl-32b-instruct"
]

for llm in llms:
    llm = LLM(model=llm, deployment_type="auto") 
    print("\\n" + "-" * 100)
    print(f"Model: {llm.model}")
    print("-" * 100 + "\\n")

    response = llm.chat.completions.create(
        messages=[{"role": "user", "content": "Hello, world!"}]
    )
    print(response.choices[0].message.content)

Now that your machine is setup with credentials and the SDK, lets ensure you are
ready to make your first LLM call and explain some of the nuances of this SDK. Create a new file called main.py and import the Fireworks AI SDK. Instantiate the LLM class. The LLM class accepts a model argument that you
can use to specify the model you want to use. For this tutorial, we will use the
Llama 4 Maverick
model. When creating an LLM instance, you can specify the deployment type as either "serverless" , "on-demand" , or "auto" . If you pass "auto" , the SDK will try to use serverless hosting if available, otherwise it will create an on-demand deployment. In the other cases, the SDK will try to create a deployment of the specified type and will throw an error if it's not available for the model you selected. The SDK will try and re-use existing deployments for the same model if possible, see Resource management for more details. With great power comes great responsibility! Be careful with the deployment_type parameter, especially for "auto" and "on-demand" . While the SDK will try to make the most cost effective choice for you and put sensible autoscaling policies in place, it is possible to unintentionally create many deployments that lead to unwanted spend, especially when working with non-serverless models. When using deployment_type="on-demand" , you must provide an id parameter to uniquely identify your deployment. This is required to prevent accidental creation of multiple deployments. When using deployment_type="on-demand" or deployment_type="on-demand-lora" , you must call .apply() to apply the deployment configuration to Fireworks. This is not required for serverless deployments. When using deployment_type="auto" , the SDK will automatically handle deployment creation, but if it falls back to on-demand deployment, you may need to call .apply() explicitly. If you do not call .apply() , you are expected to set up the deployment through the deployment page at https://app.fireworks.ai/dashboard/deployments. Make a request to the LLM. The LLM class is OpenAI compatible, so you can use
the same chat completion interface to make a request to the LLM. The great thing about the SDK is that you can use your favorite Python constructs to powerfully work with LLMs. For example, let's try calling a few LLMs in a loop and see how they respond: from fireworks import LLM

llms = [
    "llama4-maverick-instruct-basic",
    "deepseek-r1",
    "qwen2p5-vl-32b-instruct"
]

for llm in llms:
    llm = LLM(model=llm, deployment_type="auto") 
    print("\\n" + "-" * 100)
    print(f"Model: {llm.model}")
    print("-" * 100 + "\\n")

    response = llm.chat.completions.create(
        messages=[{"role": "user", "content": "Hello, world!"}]
    )
    print(response.choices[0].message.content)

",
    "$.messages[9].content[8].type": "text",
    "$.messages[9].content[9].text": "Title: How does autoscaling affect my costs?
Link: https://docs.fireworks.ai/faq-new/deployment-infrastructure/how-does-autoscaling-affect-my-costs
Content: Scaling from 0 : No minimum cost when scaled to zero Scaling up : Each new replica adds to your total cost proportionally. For example: Scaling from 1 to 2 replicas doubles your GPU costs If each replica uses multiple GPUs, costs scale accordingly (e.g., scaling from 1 to 2 replicas with 2 GPUs each means paying for 4 GPUs total) For current pricing details, please visit our pricing page.

Scaling from 0 : No minimum cost when scaled to zero Scaling up : Each new replica adds to your total cost proportionally. For example: Scaling from 1 to 2 replicas doubles your GPU costs If each replica uses multiple GPUs, costs scale accordingly (e.g., scaling from 1 to 2 replicas with 2 GPUs each means paying for 4 GPUs total) For current pricing details, please visit our pricing page.

How does autoscaling affect my costs?
Scaling from 0 : No minimum cost when scaled to zero Scaling up : Each new replica adds to your total cost proportionally. For example: Scaling from 1 to 2 replicas doubles your GPU costs If each replica uses multiple GPUs, costs scale accordingly (e.g., scaling from 1 to 2 replicas with 2 GPUs each means paying for 4 GPUs total) For current pricing details, please visit our pricing page.

",
    "$.messages[9].content[9].type": "text",
    "$.messages[9].role": "tool",
    "$.messages[9].tool_call_id": "functions.search:0",
    "$.pid": 95890,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T11:43:38.694430",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "Run math evaluation on sample dataset using pytest interface.",
    "$.eval_metadata.name": "test_pytest_mcp_url",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.version": "v0.2.6.dev19+gf755623",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "At least one tool call was made",
    "$.evaluation_result.score": 1,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.row_id": "look-win-current-private-teacher",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "You are a helpful assistant that can answer questions about Fireworks.
ALWAYS provide code or commands to execute to answer the question.",
    "$.messages[0].role": "system",
    "$.messages[1].content": "Can you teach me about how to manage deployments on Fireworks",
    "$.messages[1].role": "user",
    "$.messages[2].content": "I'll help you learn about managing deployments on Fireworks. Let me search for comprehensive information about deployment management.",
    "$.messages[2].role": "assistant",
    "$.messages[2].tool_calls[0].function.arguments": "{"query": "deployment management Fireworks AI"}",
    "$.messages[2].tool_calls[0].function.name": "search",
    "$.messages[2].tool_calls[0].id": "functions.search:0",
    "$.messages[2].tool_calls[0].type": "function",
    "$.messages[3].content[0].text": "Title: Fireworks AI Developer Platform
Link: https://docs.fireworks.ai/getting-started/introduction
Content: Fireworks AI is the best platform for building AI product experiences with open source AI models. You can run and customize AI models with just a few lines of code!

Fireworks AI is the best platform for building AI product experiences with open source AI models. You can run and customize AI models with just a few lines of code!

Start building with open source AI models

",
    "$.messages[3].content[0].type": "text",
    "$.messages[3].content[1].text": "Title: How do I close my Fireworks.ai account?
Link: https://docs.fireworks.ai/faq-new/account-access/how-do-i-close-my-fireworksai-account
Content: To close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed

To close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed

How do I close my Fireworks.ai account?
To close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed

",
    "$.messages[3].content[1].type": "text",
    "$.messages[3].content[2].text": "Title: Do you host your deployments in the EU or Asia?
Link: https://docs.fireworks.ai/faq-new/support-general/do-you-host-your-deployments-in-the-eu-or-asia
Content: We are currently deployed in multiple U.S.-based locations. However, we’re open to hearing more about your specific requirements. You can: Join our Discord community Write to us at inquiries@fireworks.ai If you're an Enterprise customer, please contact your dedicated customer support representative to ensure a timely response.

We are currently deployed in multiple U.S.-based locations. However, we’re open to hearing more about your specific requirements. You can: Join our Discord community Write to us at inquiries@fireworks.ai If you're an Enterprise customer, please contact your dedicated customer support representative to ensure a timely response.

Do you host your deployments in the EU or Asia?
We are currently deployed in multiple U.S.-based locations. However, we’re open to hearing more about your specific requirements. You can: Join our Discord community Write to us at inquiries@fireworks.ai If you're an Enterprise customer, please contact your dedicated customer support representative to ensure a timely response.

",
    "$.messages[3].content[2].type": "text",
    "$.messages[3].content[3].text": "Title: Introducing the Fireworks Build SDK
Link: https://docs.fireworks.ai/tools-sdks/python-client/sdk-introduction
Content: The Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.

The Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.

Introducing the Fireworks Build SDK
The Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.

",
    "$.messages[3].content[3].type": "text",
    "$.messages[3].content[4].text": "Title: On-demand deployments
Link: https://docs.fireworks.ai/guides/ondemand-deployments
Content: If you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.

If you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.

On-demand deployments
If you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.

",
    "$.messages[3].content[4].type": "text",
    "$.messages[3].content[5].text": "Title: I have multiple Fireworks accounts. When I try to login with Google on Fireworks' web UI, I'm getting signed into the wrong account. How do I fix this?
Link: https://docs.fireworks.ai/faq-new/account-access/i-have-multiple-fireworks-accounts-when-i-try-to-login-with-google-on-fireworks
Content: If you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.

If you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.

I have multiple Fireworks accounts. When I try to login with Google on Fireworks' web UI, I'm getting signed into the wrong account. How do I fix this?
If you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.

",
    "$.messages[3].content[5].type": "text",
    "$.messages[3].content[6].text": "Title: Introduction
Link: https://docs.fireworks.ai/api-reference/introduction
Content: Fireworks AI REST API enables you to interact with various language, image and embedding models using an API Key. It also lets you automate management of models, deployments, datasets, and more.

Fireworks AI REST API enables you to interact with various language, image and embedding models using an API Key. It also lets you automate management of models, deployments, datasets, and more.

Introduction
Fireworks AI REST API enables you to interact with various language, image and embedding models using an API Key. It also lets you automate management of models, deployments, datasets, and more.

",
    "$.messages[3].content[6].type": "text",
    "$.messages[3].content[7].text": "Title: Create a deployment
Link: https://docs.fireworks.ai/tools-sdks/firectl/commands/create-deployment
Content: Create a Deployment on Fireworks AI platform

Create a Deployment on Fireworks AI platform

Creates a new deployment.

",
    "$.messages[3].content[7].type": "text",
    "$.messages[3].content[8].text": "Title: How much does Fireworks cost?
Link: https://docs.fireworks.ai/faq-new/billing-pricing/how-much-does-fireworks-cost
Content: Fireworks AI operates on a pay-as-you-go model for all non-Enterprise usage, and new users automatically receive free credits. You pay based on: Per token for serverless inference Per GPU usage time for on-demand deployments Per token of training data for fine-tuning For customers needing enterprise-grade security and reliability , please reach out to us at inquiries@fireworks.ai to discuss options. Find out more about our current pricing on our Pricing page.

Fireworks AI operates on a pay-as-you-go model for all non-Enterprise usage, and new users automatically receive free credits. You pay based on: Per token for serverless inference Per GPU usage time for on-demand deployments Per token of training data for fine-tuning For customers needing enterprise-grade security and reliability , please reach out to us at inquiries@fireworks.ai to discuss options. Find out more about our current pricing on our Pricing page.

How much does Fireworks cost?
Fireworks AI operates on a pay-as-you-go model for all non-Enterprise usage, and new users automatically receive free credits. You pay based on: Per token for serverless inference Per GPU usage time for on-demand deployments Per token of training data for fine-tuning For customers needing enterprise-grade security and reliability , please reach out to us at inquiries@fireworks.ai to discuss options. Find out more about our current pricing on our Pricing page.

",
    "$.messages[3].content[8].type": "text",
    "$.messages[3].content[9].text": "Title: Python (Fireworks)
Link: https://docs.fireworks.ai/getting-started/quickstart
Content: This is the recommended way to get started : This  Python (Fireworks)  tab uses our  Fireworks Build
SDK  for the best performance and developer experience.
See our  Client-side performance
optimization  guide for
more details. Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. pip install --upgrade fireworks-ai The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either  ~/.bash_profile  for Bash or  ~/.zshrc  for  Zsh .
You can do this by running the command: vim ~/.bash_profile vim ~/.zshrc Add a new line to the file with the following: export FIREWORKS_API_KEY="<API_KEY>" After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. source ~/.bash_profile source ~/.zshrc You can verify that the variable has been set correctly by running  echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. setx FIREWORKS_API_KEY "<API_KEY>" To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: echo %FIREWORKS_API_KEY% You can quickly instantiate the  LLM  class and call the Fireworks API. The Build SDK handles deployment management automatically. from fireworks import LLM

# Basic usage - SDK automatically selects optimal deployment type
llm = LLM(model="llama4-maverick-instruct-basic", deployment_type="auto")

response = llm.chat.completions.create(
    messages=[{"role": "user", "content": "Say this is a test"}]
)

print(response.choices[0].message.content) You can also pass the API key directly to the LLM constructor:  LLM(model="llama4-maverick-instruct-basic", deployment_type="auto", api_key="<FIREWORKS_API_KEY>")

This is the recommended way to get started : This  Python (Fireworks)  tab uses our  Fireworks Build
SDK  for the best performance and developer experience.
See our  Client-side performance
optimization  guide for
more details. Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. pip install --upgrade fireworks-ai The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either  ~/.bash_profile  for Bash or  ~/.zshrc  for  Zsh .
You can do this by running the command: vim ~/.bash_profile vim ~/.zshrc Add a new line to the file with the following: export FIREWORKS_API_KEY="<API_KEY>" After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. source ~/.bash_profile source ~/.zshrc You can verify that the variable has been set correctly by running  echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. setx FIREWORKS_API_KEY "<API_KEY>" To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: echo %FIREWORKS_API_KEY% You can quickly instantiate the  LLM  class and call the Fireworks API. The Build SDK handles deployment management automatically. from fireworks import LLM

# Basic usage - SDK automatically selects optimal deployment type
llm = LLM(model="llama4-maverick-instruct-basic", deployment_type="auto")

response = llm.chat.completions.create(
    messages=[{"role": "user", "content": "Say this is a test"}]
)

print(response.choices[0].message.content) You can also pass the API key directly to the LLM constructor:  LLM(model="llama4-maverick-instruct-basic", deployment_type="auto", api_key="<FIREWORKS_API_KEY>")

Set up your developer environment & call the Fireworks API
This is the recommended way to get started : This  tab uses our Fireworks Build
SDK for the best performance and developer experience.
See our Client-side performance
optimization guide for
more details. Install SDK Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Configure API Key Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either ~/.bash_profile for Bash or ~/.zshrc for Zsh .
You can do this by running the command: Add a new line to the file with the following: After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. You can verify that the variable has been set correctly by running echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: Sending the first API Request You can quickly instantiate the LLM class and call the Fireworks API. The Build SDK handles deployment management automatically. You can also pass the API key directly to the LLM constructor: LLM(model="llama4-maverick-instruct-basic", deployment_type="auto", api_key="<FIREWORKS_API_KEY>") Install SDK Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. Fireworks AI platform offers drop-in replacement with OpenAI Python Client. Configure API Key Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either ~/.bash_profile for Bash or ~/.zshrc for Zsh .
You can do this by running the command: Add a new line to the file with the following: After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. You can verify that the variable has been set correctly by running echo $OPENAI_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: echo %OPENAI_API_KEY%

",
    "$.messages[3].content[9].type": "text",
    "$.messages[3].role": "tool",
    "$.messages[3].tool_call_id": "functions.search:0",
    "$.messages[4].content": "Let me search for more specific information about deployment management commands and workflows.",
    "$.messages[4].role": "assistant",
    "$.messages[4].tool_calls[0].function.arguments": "{"query": "firectl deployment commands create manage delete update"}",
    "$.messages[4].tool_calls[0].function.name": "search",
    "$.messages[4].tool_calls[0].id": "functions.search:1",
    "$.messages[4].tool_calls[0].type": "function",
    "$.messages[5].content[0].text": "Title: Why am I getting "invalid id" errors when using firectl commands like create deployment or list deployments?
Link: https://docs.fireworks.ai/faq-new/fine-tuning/why-am-i-getting-invalid-id-errors-when-using-firectl-commands-like-create-deplo
Content: This error typically occurs when your account ID is not properly configured.

This error typically occurs when your account ID is not properly configured.

Why am I getting "invalid id" errors when using firectl commands like create deployment or list deployments?
This error typically occurs when your account ID is not properly configured.

",
    "$.messages[5].content[0].type": "text",
    "$.messages[5].content[1].text": "Title: Delete a deployment
Link: https://docs.fireworks.ai/tools-sdks/firectl/commands/delete-resources
Content: Deletes an model deployment. firectl delete deployment [flags]

Deletes an model deployment. firectl delete deployment [flags]

Example
firectl delete deployment my-deployment

",
    "$.messages[5].content[1].type": "text",
    "$.messages[5].content[2].text": "Title: Create a deployment
Link: https://docs.fireworks.ai/tools-sdks/firectl/commands/create-deployment
Content: Creates a new deployment. firectl create deployment [flags]

Creates a new deployment. firectl create deployment [flags]

Create a Deployment on Fireworks AI platform

",
    "$.messages[5].content[2].type": "text",
    "$.messages[5].content[3].text": "Title: Managing default deployments
Link: https://docs.fireworks.ai/guides/ondemand-deployments
Content: Since a model may be deployed to multiple deployments, querying by model name will route to the "default" deployed model. You can see which deployed model entry is marked with Default: true using: This will show the Deployed Model Refs section with the Default: true entry. To update the default deployed model, note the Name of the deployed model reference above. Then run: firectl update deployed-model <DEPLOYED_MODEL_ID_2> --default

Since a model may be deployed to multiple deployments, querying by model name will route to the "default" deployed model. You can see which deployed model entry is marked with Default: true using: This will show the Deployed Model Refs section with the Default: true entry. To update the default deployed model, note the Name of the deployed model reference above. Then run: firectl update deployed-model <DEPLOYED_MODEL_ID_2> --default

Since a model may be deployed to multiple deployments, querying by model name will route to the "default" deployed model. You can see which deployed model entry is marked with Default: true using: This will show the Deployed Model Refs section with the Default: true entry. To update the default deployed model, note the Name of the deployed model reference above. Then run: To delete a default deployment you must delete all other deployments for the same model first, or designate a different deployed model as the default as described above. This is to ensure that querying by model name will always route to an unambiguous default deployment as long as deployments for the model exist.

",
    "$.messages[5].content[3].type": "text",
    "$.messages[5].content[4].text": "Title: 🚀 Easier & faster LoRA fine-tune deployments on Fireworks
Link: https://docs.fireworks.ai/updates/changelog
Content: You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: Previously, this involved two distinct steps, and the resulting deployment was slower than the base model: Create a deployment using firectl create deployment "accounts/fireworks/models/<MODEL_ID of base model>" --enable-addons Then deploy the addon to the deployment: firectl load-lora <MODEL_ID> --deployment <DEPLOYMENT_ID> For more information, see our deployment documentation. This change is for dedicated deployments with a single LoRA. You can still deploy multiple LoRAs on a deployment or deploy LoRA(s) on some Serverless models as described in the documentation.

",
    "$.messages[5].content[4].type": "text",
    "$.messages[5].content[5].text": "Title: Deployment Command
Link: https://docs.fireworks.ai/evaluators/examples/gcp_cloud_run_deployment_example
Content: It's recommended to run the deployment command from the directory containing the reward function script ( dummy_rewards.py ) and your rewardkit.yaml (if used), for example, from examples/gcp_cloud_run_deployment_example/. Ensure your virtual environment is active: Run the deployment command: Command Explanation: --id my-dummy-gcp-evaluator : A unique ID for your evaluator on the Fireworks AI platform. --target gcp-cloud-run : Specifies deployment to GCP Cloud Run. --function-ref dummy_rewards.hello_world_reward : The Python import path to your reward function. If dummy_rewards.py is in the current directory, this reference works. --gcp-auth-mode api-key : Configures the Cloud Run service with API key authentication. reward-kit will generate a key, store it in GCP Secret Manager, and configure the service. The key is also saved to your local rewardkit.yaml under evaluator_endpoint_keys . This is the default if not specified. --verbose : Shows detailed output, including gcloud commands being executed. --force : (Optional) If an evaluator with the same --id already exists, this flag will delete the existing one before creating the new one.

It's recommended to run the deployment command from the directory containing the reward function script ( dummy_rewards.py ) and your rewardkit.yaml (if used), for example, from examples/gcp_cloud_run_deployment_example/. Ensure your virtual environment is active: Run the deployment command: Command Explanation: --id my-dummy-gcp-evaluator : A unique ID for your evaluator on the Fireworks AI platform. --target gcp-cloud-run : Specifies deployment to GCP Cloud Run. --function-ref dummy_rewards.hello_world_reward : The Python import path to your reward function. If dummy_rewards.py is in the current directory, this reference works. --gcp-auth-mode api-key : Configures the Cloud Run service with API key authentication. reward-kit will generate a key, store it in GCP Secret Manager, and configure the service. The key is also saved to your local rewardkit.yaml under evaluator_endpoint_keys . This is the default if not specified. --verbose : Shows detailed output, including gcloud commands being executed. --force : (Optional) If an evaluator with the same --id already exists, this flag will delete the existing one before creating the new one.

It's recommended to run the deployment command from the directory containing the reward function script ( dummy_rewards.py ) and your rewardkit.yaml (if used), for example, from examples/gcp_cloud_run_deployment_example/. Ensure your virtual environment is active: source .venv/bin/activate

",
    "$.messages[5].content[5].type": "text",
    "$.messages[5].content[6].text": "Title: Quick deployment
Link: https://docs.fireworks.ai/fine-tuning/single-lora
Content: Deploy your LoRA fine-tuned model with one simple command: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

Deploy your LoRA fine-tuned model with one simple command: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

Alternative deployment method
This two-step method is the standard approach for multi-LoRA deployments where multiple LoRA models share the same base model. While it can also be used for single LoRA deployments, it provides slower performance compared to live merge and is not recommended for single LoRA use cases. You can also deploy single LoRA models using a two-step process: Create base model deployment Deploy the base model with addons enabled: firectl create deployment "accounts/fireworks/models/<MODEL_ID of base model>" --enable-addons

",
    "$.messages[5].content[6].type": "text",
    "$.messages[5].content[7].text": "Title: Delete Deployment
Link: https://docs.fireworks.ai/api-reference/delete-deployment
Content: 



REST Endpoint DELETE /v1/accounts/{account_id}/deployments/{deployment_id}

Server(s): https://api.fireworks.ai


Summary:  

Description: N/A

Security methods: 
header: 
	Authorization: bearer auth
	

Request params: 
path params:
account_id:
	string
	Description: The Account Id
	Example: "<string>"
deployment_id:
	string
	Description: The Deployment Id
	Example: "<string>"

query params:
hard:
	boolean
	Description: If true, this will perform a hard deletion.
	Example: true
ignoreChecks:
	boolean
	Description: If true, this will ignore checks and force the deletion of a deployment that is currently
deployed and is in use.
	Example: true


Request body: N/A


Responses: 
200: object
	Description: N/A
	Example: {}

",
    "$.messages[5].content[7].type": "text",
    "$.messages[5].content[8].text": "Title: How do I close my Fireworks.ai account?
Link: https://docs.fireworks.ai/faq-new/account-access/how-do-i-close-my-fireworksai-account
Content: To close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed

To close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed

How do I close my Fireworks.ai account?
To close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed

",
    "$.messages[5].content[8].type": "text",
    "$.messages[5].content[9].text": "Title: Deleting Stored Responses
Link: https://docs.fireworks.ai/guides/response-api
Content: When responses are stored (the default behavior with store=True ), you can immediately delete them from storage using the DELETE endpoint. This permanently removes the conversation data. from fireworks import LLM
import requests
import os

llm = LLM(model="qwen3-235b-a22b", deployment_type="serverless")

# Create a response
response = llm.responses.create(
    input="What is the capital of France?",
    store=True  # This is the default
)

response_id = response.id
print(f"Created response with ID: {response_id}")

# Delete the response immediately
headers = {
    "Authorization": f"Bearer {os.getenv('FIREWORKS_API_KEY')}",
    "x-fireworks-account-id": "your-account-id"
}
delete_response = requests.delete(
    f"https://api.fireworks.ai/inference/v1/responses/{response_id}",
    headers=headers
)

if delete_response.status_code == 200:
    print("Response deleted successfully")
else:
    print(f"Failed to delete response: {delete_response.status_code}")

When responses are stored (the default behavior with store=True ), you can immediately delete them from storage using the DELETE endpoint. This permanently removes the conversation data. from fireworks import LLM
import requests
import os

llm = LLM(model="qwen3-235b-a22b", deployment_type="serverless")

# Create a response
response = llm.responses.create(
    input="What is the capital of France?",
    store=True  # This is the default
)

response_id = response.id
print(f"Created response with ID: {response_id}")

# Delete the response immediately
headers = {
    "Authorization": f"Bearer {os.getenv('FIREWORKS_API_KEY')}",
    "x-fireworks-account-id": "your-account-id"
}
delete_response = requests.delete(
    f"https://api.fireworks.ai/inference/v1/responses/{response_id}",
    headers=headers
)

if delete_response.status_code == 200:
    print("Response deleted successfully")
else:
    print(f"Failed to delete response: {delete_response.status_code}")

When responses are stored (the default behavior with store=True ), you can immediately delete them from storage using the DELETE endpoint. This permanently removes the conversation data. import os
from openai import OpenAI
import requests

client = OpenAI(
    base_url="https://api.fireworks.ai/inference/v1",
    api_key=os.getenv("FIREWORKS_API_KEY", "YOUR_FIREWORKS_API_KEY_HERE")
)

# Create a response
response = client.responses.create(
    model="accounts/fireworks/models/qwen3-235b-a22b",
    input="What is the capital of France?",
    store=True  # This is the default
)

response_id = response.id
print(f"Created response with ID: {response_id}")

# Delete the response immediately
headers = {
    "Authorization": f"Bearer {os.getenv('FIREWORKS_API_KEY')}",
    "x-fireworks-account-id": "your-account-id"
}
delete_response = requests.delete(
    f"https://api.fireworks.ai/inference/v1/responses/{response_id}",
    headers=headers
)

if delete_response.status_code == 200:
    print("Response deleted successfully")
else:
    print(f"Failed to delete response: {delete_response.status_code}")

",
    "$.messages[5].content[9].type": "text",
    "$.messages[5].role": "tool",
    "$.messages[5].tool_call_id": "functions.search:1",
    "$.messages[6].content": "Let me search for more information about listing and managing deployments, as well as the Fireworks Build SDK for deployment management.",
    "$.messages[6].role": "assistant",
    "$.messages[6].tool_calls[0].function.arguments": "{"query": "list deployments firectl Fireworks Build SDK deployment management"}",
    "$.messages[6].tool_calls[0].function.name": "search",
    "$.messages[6].tool_calls[0].id": "functions.search:1",
    "$.messages[6].tool_calls[0].type": "function",
    "$.messages[7].content[0].text": "Title: Introducing the Fireworks Build SDK
Link: https://docs.fireworks.ai/tools-sdks/python-client/sdk-introduction
Content: Introducing the Fireworks Build SDK
The Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.

Introducing the Fireworks Build SDK
The Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.

The Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.

",
    "$.messages[7].content[0].type": "text",
    "$.messages[7].content[1].text": "Title: Basics of the Build SDK
Link: https://docs.fireworks.ai/tools-sdks/python-client/sdk-basics
Content: Why use the Build SDK?
The Fireworks Build SDK gives you a declarative way to work with Fireworks resources like deployments, fine-tuning jobs, and datasets. We've designed it to handle all the infrastructure complexity for you, letting you focus on building your application. Instead of using the web UI, CLI, or raw API calls, you can manage everything through simple Python code with smart, logical defaults without sacrificing control and customizability. The principles of the SDK are the following: Object-oriented: Fireworks primitives are represented as Python objects. You can access their capabilities and properties through methods and attributes. Declarative: You can describe your desired state and the SDK will handle reconcilliation. Smart defaults: The SDK will infer the most logical defaults for you, prioritizing development speed and lowest cost. Here are some examples: The SDK will automatically use a serverless deployment for models that are available serverlessly unless you specify otherwise. When creating deployments, the SDK will also enable scale-to-zero with the shortest possible scale-down window. If the SDK determines that a resource already exists by matching its signature (see below), it will re-use the existing resource instead of creating a new one. Customizable: Although we enable smart defaults, you still have full access to the configuration parameters for any Fireworks resource The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.

Why use the Build SDK?
The Fireworks Build SDK gives you a declarative way to work with Fireworks resources like deployments, fine-tuning jobs, and datasets. We've designed it to handle all the infrastructure complexity for you, letting you focus on building your application. Instead of using the web UI, CLI, or raw API calls, you can manage everything through simple Python code with smart, logical defaults without sacrificing control and customizability. The principles of the SDK are the following: Object-oriented: Fireworks primitives are represented as Python objects. You can access their capabilities and properties through methods and attributes. Declarative: You can describe your desired state and the SDK will handle reconcilliation. Smart defaults: The SDK will infer the most logical defaults for you, prioritizing development speed and lowest cost. Here are some examples: The SDK will automatically use a serverless deployment for models that are available serverlessly unless you specify otherwise. When creating deployments, the SDK will also enable scale-to-zero with the shortest possible scale-down window. If the SDK determines that a resource already exists by matching its signature (see below), it will re-use the existing resource instead of creating a new one. Customizable: Although we enable smart defaults, you still have full access to the configuration parameters for any Fireworks resource The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.

Why use the Build SDK?
The Fireworks Build SDK gives you a declarative way to work with Fireworks resources like deployments, fine-tuning jobs, and datasets. We've designed it to handle all the infrastructure complexity for you, letting you focus on building your application. Instead of using the web UI, CLI, or raw API calls, you can manage everything through simple Python code with smart, logical defaults without sacrificing control and customizability. The principles of the SDK are the following: Object-oriented: Fireworks primitives are represented as Python objects. You can access their capabilities and properties through methods and attributes. Declarative: You can describe your desired state and the SDK will handle reconcilliation. Smart defaults: The SDK will infer the most logical defaults for you, prioritizing development speed and lowest cost. Here are some examples: The SDK will automatically use a serverless deployment for models that are available serverlessly unless you specify otherwise. When creating deployments, the SDK will also enable scale-to-zero with the shortest possible scale-down window. If the SDK determines that a resource already exists by matching its signature (see below), it will re-use the existing resource instead of creating a new one. Customizable: Although we enable smart defaults, you still have full access to the configuration parameters for any Fireworks resource The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.

",
    "$.messages[7].content[1].type": "text",
    "$.messages[7].content[2].text": "Title: Querying existing dedicated deployments
Link: https://docs.fireworks.ai/tools-sdks/python-client/querying-dedicated-deployments
Content: When you have dedicated deployments that were created via firectl or the Fireworks web UI, you can easily connect to them using the Build SDK to run inference. This is particularly useful when you want to leverage existing infrastructure or when deployments are managed by different teams.

When you have dedicated deployments that were created via firectl or the Fireworks web UI, you can easily connect to them using the Build SDK to run inference. This is particularly useful when you want to leverage existing infrastructure or when deployments are managed by different teams.

Deployment ID requirements
The id parameter should match exactly with your existing deployment: Use the deployment name/ID as shown in the Fireworks dashboard The ID is case-sensitive and must match exactly If the deployment doesn't exist, you'll receive an error when making requests

",
    "$.messages[7].content[2].type": "text",
    "$.messages[7].content[3].text": "Title: Fireworks SDK
Link: https://docs.fireworks.ai/guides/ondemand-deployments
Content: You can connect to your existing deployment using the  Fireworks Build SDK : from fireworks import LLM

# Connect to your existing deployment
llm = LLM(
    model="llama-v3p2-3b-instruct",  # The model your deployment is running
    deployment_type="on-demand",
    id="my-deployment-id",  # Your deployment ID
)

# Use OpenAI-compatible chat completions
response = llm.chat.completions.create(
    messages=[{"role": "user", "content": "Say this is a test"}]
)

print(response.choices[0].message.content) When connecting to an existing deployment with the SDK, you don't need to call  .apply()  - the deployment is already running. For more details on querying existing deployments with Fireworks SDK, see our guide on  querying dedicated deployments .

You can connect to your existing deployment using the  Fireworks Build SDK : from fireworks import LLM

# Connect to your existing deployment
llm = LLM(
    model="llama-v3p2-3b-instruct",  # The model your deployment is running
    deployment_type="on-demand",
    id="my-deployment-id",  # Your deployment ID
)

# Use OpenAI-compatible chat completions
response = llm.chat.completions.create(
    messages=[{"role": "user", "content": "Say this is a test"}]
)

print(response.choices[0].message.content) When connecting to an existing deployment with the SDK, you don't need to call  .apply()  - the deployment is already running. For more details on querying existing deployments with Fireworks SDK, see our guide on  querying dedicated deployments .

On-demand deployments
If you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.

",
    "$.messages[7].content[3].type": "text",
    "$.messages[7].content[4].text": "Title: Step 1: Configure the Fireworks Build SDK
Link: https://docs.fireworks.ai/structured-responses/structured-output-grammar-based
Content: from fireworks import LLM

client = LLM(
    model="llama-v3p1-405b-instruct",
    deployment_type="serverless"
)

from fireworks import LLM

client = LLM(
    model="llama-v3p1-405b-instruct",
    deployment_type="serverless"
)

Prerequisites
Before you begin, ensure you have the following: Python installed on your system. Build SDK installed. You can install it using pip: Next, select the model you want to use. In this example, we use llama-v3p1-405b-instruct , but all fireworks models support this feature.

",
    "$.messages[7].content[4].type": "text",
    "$.messages[7].content[5].text": "Title: Does Fireworks support custom base models?
Link: https://docs.fireworks.ai/faq-new/models-inference/does-fireworks-support-custom-base-models
Content: Yes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.

Yes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.

Does Fireworks support custom base models?
Yes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.

",
    "$.messages[7].content[5].type": "text",
    "$.messages[7].content[6].text": "Title: 🔧 Build SDK: Deployment Configuration Application Requirement
Link: https://docs.fireworks.ai/updates/changelog
Content: The Build SDK now requires you to call .apply() to apply any deployment configurations to Fireworks when using deployment_type="on-demand" or deployment_type="on-demand-lora" . This change ensures explicit control over when deployments are created and helps prevent accidental deployment creation. Key changes: .apply() is now required for on-demand and on-demand-lora deployments Serverless deployments do not require .apply() calls If you do not call .apply() , you are expected to set up the deployment through the deployment page at https://app.fireworks.ai/dashboard/deployments Migration guide: Add llm.apply() after creating LLM instances with deployment_type="on-demand" or deployment_type="on-demand-lora" No changes needed for serverless deployments See updated documentation for examples and best practices This change improves deployment management and provides better control over resource creation. This applies to Python SDK version >=0.19.14.

The Build SDK now requires you to call .apply() to apply any deployment configurations to Fireworks when using deployment_type="on-demand" or deployment_type="on-demand-lora" . This change ensures explicit control over when deployments are created and helps prevent accidental deployment creation. Key changes: .apply() is now required for on-demand and on-demand-lora deployments Serverless deployments do not require .apply() calls If you do not call .apply() , you are expected to set up the deployment through the deployment page at https://app.fireworks.ai/dashboard/deployments Migration guide: Add llm.apply() after creating LLM instances with deployment_type="on-demand" or deployment_type="on-demand-lora" No changes needed for serverless deployments See updated documentation for examples and best practices This change improves deployment management and provides better control over resource creation. This applies to Python SDK version >=0.19.14.

🎯 Build SDK: Reward-kit integration for evaluator development
The Build SDK now natively integrates with reward-kit to simplify evaluator development for Reinforcement Fine-Tuning (RFT) . You can now create custom evaluators in Python with automatic dependency management and seamless deployment to Fireworks infrastructure. Key features: Native reward-kit integration for evaluator development Automatic packaging of dependencies from pyproject.toml or requirements.txt Local testing capabilities before deployment Direct integration with Fireworks datasets and evaluation jobs Support for third-party libraries and complex evaluation logic See our Developing Evaluators guide to get started with your first evaluator in minutes.

",
    "$.messages[7].content[6].type": "text",
    "$.messages[7].content[7].text": "Title: Set up your developer environment & call the Fireworks API
Link: https://docs.fireworks.ai/getting-started/quickstart
Content: This is the recommended way to get started : This Python (Fireworks) tab uses our Fireworks Build
SDK for the best performance and developer experience.
See our Client-side performance
optimization guide for
more details. Install SDK Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Configure API Key Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either ~/.bash_profile for Bash or ~/.zshrc for Zsh .
You can do this by running the command: Add a new line to the file with the following: After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. You can verify that the variable has been set correctly by running echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: Sending the first API Request You can quickly instantiate the LLM class and call the Fireworks API. The Build SDK handles deployment management automatically. from fireworks import LLM

# Basic usage - SDK automatically selects optimal deployment type
llm = LLM(model="llama4-maverick-instruct-basic", deployment_type="auto")

response = llm.chat.completions.create(
    messages=[{"role": "user", "content": "Say this is a test"}]
)

print(response.choices[0].message.content)

This is the recommended way to get started : This Python (Fireworks) tab uses our Fireworks Build
SDK for the best performance and developer experience.
See our Client-side performance
optimization guide for
more details. Install SDK Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Configure API Key Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either ~/.bash_profile for Bash or ~/.zshrc for Zsh .
You can do this by running the command: Add a new line to the file with the following: After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. You can verify that the variable has been set correctly by running echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: Sending the first API Request You can quickly instantiate the LLM class and call the Fireworks API. The Build SDK handles deployment management automatically. from fireworks import LLM

# Basic usage - SDK automatically selects optimal deployment type
llm = LLM(model="llama4-maverick-instruct-basic", deployment_type="auto")

response = llm.chat.completions.create(
    messages=[{"role": "user", "content": "Say this is a test"}]
)

print(response.choices[0].message.content)

Python (Fireworks)
This is the recommended way to get started : This  Python (Fireworks)  tab uses our  Fireworks Build
SDK  for the best performance and developer experience.
See our  Client-side performance
optimization  guide for
more details. Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. pip install --upgrade fireworks-ai The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either  ~/.bash_profile  for Bash or  ~/.zshrc  for  Zsh .
You can do this by running the command: vim ~/.bash_profile vim ~/.zshrc Add a new line to the file with the following: export FIREWORKS_API_KEY="<API_KEY>" After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. source ~/.bash_profile source ~/.zshrc You can verify that the variable has been set correctly by running  echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. setx FIREWORKS_API_KEY "<API_KEY>" To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: echo %FIREWORKS_API_KEY% You can quickly instantiate the  LLM  class and call the Fireworks API. The Build SDK handles deployment management automatically. from fireworks import LLM

# Basic usage - SDK automatically selects optimal deployment type
llm = LLM(model="llama4-maverick-instruct-basic", deployment_type="auto")

response = llm.chat.completions.create(
    messages=[{"role": "user", "content": "Say this is a test"}]
)

print(response.choices[0].message.content) You can also pass the API key directly to the LLM constructor:  LLM(model="llama4-maverick-instruct-basic", deployment_type="auto", api_key="<FIREWORKS_API_KEY>")

",
    "$.messages[7].content[7].type": "text",
    "$.messages[7].content[8].text": "Title: Example with Fireworks Build SDK
Link: https://docs.fireworks.ai/faq-new/models-inference/how-to-get-performance-metrics-for-streaming-responses
Content: from fireworks import LLM
import os

llm = LLM(
    model="llama-v3p1-8b-instruct",
    deployment_type="serverless",
    api_key=os.environ["FIREWORKS_API_KEY"],
)

# Streaming completion with performance metrics
stream = llm.chat.completions.create(
    messages=[{"role": "user", "content": "Hello, world!"}],
    max_tokens=100,
    stream=True,
    perf_metrics_in_response=True,
)

for chunk in stream:
    # Use getattr to avoid linter errors for unknown attributes
    perf_metrics = getattr(chunk, "perf_metrics", None)
    finish_reason = getattr(chunk.choices[0], "finish_reason", None)
    if perf_metrics is not None and finish_reason:
        print("Performance metrics:", perf_metrics)

from fireworks import LLM
import os

llm = LLM(
    model="llama-v3p1-8b-instruct",
    deployment_type="serverless",
    api_key=os.environ["FIREWORKS_API_KEY"],
)

# Streaming completion with performance metrics
stream = llm.chat.completions.create(
    messages=[{"role": "user", "content": "Hello, world!"}],
    max_tokens=100,
    stream=True,
    perf_metrics_in_response=True,
)

for chunk in stream:
    # Use getattr to avoid linter errors for unknown attributes
    perf_metrics = getattr(chunk, "perf_metrics", None)
    finish_reason = getattr(chunk.choices[0], "finish_reason", None)
    if perf_metrics is not None and finish_reason:
        print("Performance metrics:", perf_metrics)

Performance Metrics Overview
The Inference API returns several per-request metrics in the response. They can be useful for one-off debugging or can be logged by the client in their preferred observability tool. For aggregate metrics, see the usage dashboard. Non-streaming requests: Performance metrics are always included in response headers (e.g., fireworks-prompt-tokens , fireworks-server-time-to-first-token ). Streaming requests: Only selected performance metrics, such as "fireworks-server-time-to-first-token," are available because HTTP headers must be sent before the first token can be streamed. Use the perf_metrics_in_response body parameter to include all metrics in the last SSE event of the response body.

",
    "$.messages[7].content[8].type": "text",
    "$.messages[7].content[9].text": "Title: Fireworks AI Developer Platform
Link: https://docs.fireworks.ai/getting-started/introduction
Content: Fireworks AI is the best platform for building AI product experiences with open source AI models. You can run and customize AI models with just a few lines of code!

Fireworks AI is the best platform for building AI product experiences with open source AI models. You can run and customize AI models with just a few lines of code!

Start building with open source AI models

",
    "$.messages[7].content[9].type": "text",
    "$.messages[7].role": "tool",
    "$.messages[7].tool_call_id": "functions.search:1",
    "$.messages[8].content": "Based on my research, I can now provide you with a comprehensive guide on managing deployments on Fireworks. Let me break this down into different approaches:

## Managing Deployments on Fireworks

Fireworks offers three main ways to manage deployments:

### 1. **Fireworks Build SDK (Recommended - Python)**
This is the most user-friendly approach using Python.

**Installation:**
\`\`\`bash
pip install --upgrade fireworks-ai
\`\`\`

**Basic Deployment Creation:**
\`\`\`python
from fireworks import LLM

# Create a new deployment
llm = LLM(
    model="llama-v3p2-3b-instruct",
    deployment_type="on-demand",  # or "serverless", "on-demand-lora"
    id="my-deployment-name"
)

# Apply the deployment (required for on-demand deployments)
llm.apply()

# Use the deployment
response = llm.chat.completions.create(
    messages=[{"role": "user", "content": "Hello, world!"}]
)
print(response.choices[0].message.content)
\`\`\`

**Key Features:**
- **Smart defaults**: Automatically selects optimal deployment type
- **Declarative**: Describe desired state, SDK handles reconciliation
- **Object-oriented**: Deployments are Python objects with methods/attributes
- **Scale-to-zero**: Enabled by default with shortest scale-down window

### 2. **Firectl CLI Tool**
Command-line interface for deployment management.

**Create a deployment:**
\`\`\`bash
firectl create deployment "accounts/fireworks/models/llama-v3p2-3b-instruct" --id my-deployment
\`\`\`

**Deploy LoRA fine-tunes:**
\`\`\`bash
# Single command for LoRA deployment
firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

# Traditional two-step method (for multi-LoRA)
firectl create deployment "accounts/fireworks/models/<base-model-id>" --enable-addons
firectl load-lora <lora-model-id> --deployment <deployment-id>
\`\`\`

**List deployments:**
\`\`\`bash
firectl list deployments
\`\`\`

**Delete deployments:**
\`\`\`bash
firectl delete deployment my-deployment
\`\`\`

**Update default deployment:**
\`\`\`bash
firectl update deployed-model <DEPLOYED_MODEL_ID> --default
\`\`\`

### 3. **REST API**
Direct API calls for programmatic management.

**Create deployment:**
\`\`\`bash
curl -X POST https://api.fireworks.ai/v1/accounts/{account_id}/deployments \\
  -H "Authorization: Bearer YOUR_API_KEY" \\
  -H "Content-Type: application/json" \\
  -d '{
    "model": "llama-v3p2-3b-instruct",
    "deployment_type": "on-demand"
  }'
\`\`\`

**Delete deployment:**
\`\`\`bash
curl -X DELETE https://api.fireworks.ai/v1/accounts/{account_id}/deployments/{deployment_id} \\
  -H "Authorization: Bearer YOUR_API_KEY"
\`\`\`

## Deployment Types Explained

### **Serverless Deployments**
- **Billing**: Per-token usage
- **Best for**: Variable/low usage, quick prototyping
- **Management**: Fully managed by Fireworks
- **Access**: Instant, no setup required

### **On-Demand Deployments**
- **Billing**: Per GPU-second
- **Best for**: Consistent usage, predictable performance
- **Advantages**: 
  - No rate limits
  - Predictable performance
  - Access to larger model selection
  - Custom base models from Hugging Face
- **Management**: Requires explicit creation/deletion

### **Reserved Capacity**
- **Best for**: High-volume, consistent usage
- **Advantages**: More reliable capacity, higher GPU quotas
- **Contact**: inquiries@fireworks.ai for enterprise pricing

## Complete Management Workflow Example

Here's a comprehensive example using the Build SDK:

\`\`\`python
from fireworks import LLM
import os

# Set up API key
os.environ["FIREWORKS_API_KEY"] = "your-api-key"

# 1. Create and manage deployments
def manage_deployments():
    
    # Create a serverless deployment (no .apply() needed)
    serverless_llm = LLM(
        model="llama4-maverick-instruct-basic",
        deployment_type="serverless"
    )
    
    # Create an on-demand deployment
    ondemand_llm = LLM(
        model="llama-v3p2-3b-instruct",
        deployment_type="on-demand",
        id="my-production-deployment",
        max_instances=3,
        min_instances=0  # Enable scale-to-zero
    )
    
    # Apply the on-demand deployment
    ondemand_llm.apply()
    
    # 2. Query existing deployments
    print("Connecting to existing deployment...")
    existing_llm = LLM(
        model="llama-v3p2-3b-instruct",
        deployment_type="on-demand",
        id="my-existing-deployment"
    )
    
    # 3. Use deployments
    response = existing_llm.chat.completions.create(
        messages=[{"role": "user", "content": "Explain deployment management"}]
    )
    
    # 4. Clean up (delete deployment when done)
    # firectl delete deployment my-production-deployment
    
    return response.choices[0].message.content

# Run the workflow
result = manage_deployments()
print(result)
\`\`\`

## Best Practices

1. **Cost Management**: Use serverless for development, on-demand for production
2. **Monitoring**: Track usage through the Fireworks dashboard
3. **Cleanup**: Always delete unused on-demand deployments to avoid charges
4. **Scaling**: Use scale-to-zero for cost efficiency
5. **Naming**: Use descriptive deployment IDs for easy management

## Getting Started

1. **Install the Build SDK**: \`pip install fireworks-ai\`
2. **Get API Key**: From https://app.fireworks.ai/dashboard/api-keys
3. **Choose Approach**: 
   - **Beginners**: Start with Build SDK
   - **CLI Users**: Use firectl
   - **API Integration**: Use REST API

For enterprise needs or reserved capacity, contact inquiries@fireworks.ai.

Would you like me to dive deeper into any specific aspect of deployment management?",
    "$.messages[8].role": "assistant",
    "$.pid": 97954,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T11:51:25.530495",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "Run math evaluation on sample dataset using pytest interface.",
    "$.eval_metadata.name": "test_pytest_mcp_url",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.status": "stopped",
    "$.eval_metadata.version": "v0.2.6.dev19+gf755623.dirty",
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.row_id": "invent-special-real-rich-dream",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "You are a helpful assistant that can answer questions about Fireworks.
ALWAYS provide code or commands to execute to answer the question.",
    "$.messages[0].role": "system",
    "$.messages[1].content": "Can you teach me about how to manage deployments on Fireworks",
    "$.messages[1].role": "user",
    "$.pid": 2168,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T11:51:48.974779",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "Run math evaluation on sample dataset using pytest interface.",
    "$.eval_metadata.name": "test_pytest_mcp_url",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.status": "stopped",
    "$.eval_metadata.version": "v0.2.6.dev19+gf755623.dirty",
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.row_id": "continue-encourage-dry-part-meeting",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "You are a helpful assistant that can answer questions about Fireworks.
ALWAYS provide code or commands to execute to answer the question.",
    "$.messages[0].role": "system",
    "$.messages[1].content": "Can you teach me about how to manage deployments on Fireworks",
    "$.messages[1].role": "user",
    "$.pid": 2642,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T12:17:53.864430",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "Run math evaluation on sample dataset using pytest interface.",
    "$.eval_metadata.name": "test_pytest_mcp_url",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.status": "stopped",
    "$.eval_metadata.version": "v0.2.6.dev20+ge1da45a",
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.row_id": "create-destroy-past-service-case",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "You are a helpful assistant that can answer questions about Fireworks.
ALWAYS provide code or commands to execute to answer the question.",
    "$.messages[0].role": "system",
    "$.messages[1].content": "Can you teach me about how to manage deployments on Fireworks",
    "$.messages[1].role": "user",
    "$.messages[2].content": "I'll help you learn about managing deployments on Fireworks! Let me search for comprehensive information about deployment management.",
    "$.messages[2].role": "assistant",
    "$.messages[2].tool_calls[0].function.arguments": "{"query": "deployment management Fireworks AI"}",
    "$.messages[2].tool_calls[0].function.name": "search",
    "$.messages[2].tool_calls[0].id": "functions.search:0",
    "$.messages[2].tool_calls[0].type": "function",
    "$.messages[3].content[0].text": "Title: Fireworks AI Developer Platform
Link: https://docs.fireworks.ai/getting-started/introduction
Content: Fireworks AI is the best platform for building AI product experiences with open source AI models. You can run and customize AI models with just a few lines of code!

Fireworks AI is the best platform for building AI product experiences with open source AI models. You can run and customize AI models with just a few lines of code!

Start building with open source AI models

",
    "$.messages[3].content[0].type": "text",
    "$.messages[3].content[1].text": "Title: How do I close my Fireworks.ai account?
Link: https://docs.fireworks.ai/faq-new/account-access/how-do-i-close-my-fireworksai-account
Content: To close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed

To close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed

How do I close my Fireworks.ai account?
To close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed

",
    "$.messages[3].content[1].type": "text",
    "$.messages[3].content[2].text": "Title: Do you host your deployments in the EU or Asia?
Link: https://docs.fireworks.ai/faq-new/support-general/do-you-host-your-deployments-in-the-eu-or-asia
Content: We are currently deployed in multiple U.S.-based locations. However, we’re open to hearing more about your specific requirements. You can: Join our Discord community Write to us at inquiries@fireworks.ai If you're an Enterprise customer, please contact your dedicated customer support representative to ensure a timely response.

We are currently deployed in multiple U.S.-based locations. However, we’re open to hearing more about your specific requirements. You can: Join our Discord community Write to us at inquiries@fireworks.ai If you're an Enterprise customer, please contact your dedicated customer support representative to ensure a timely response.

Do you host your deployments in the EU or Asia?
We are currently deployed in multiple U.S.-based locations. However, we’re open to hearing more about your specific requirements. You can: Join our Discord community Write to us at inquiries@fireworks.ai If you're an Enterprise customer, please contact your dedicated customer support representative to ensure a timely response.

",
    "$.messages[3].content[2].type": "text",
    "$.messages[3].content[3].text": "Title: Introducing the Fireworks Build SDK
Link: https://docs.fireworks.ai/tools-sdks/python-client/sdk-introduction
Content: The Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.

The Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.

Introducing the Fireworks Build SDK
The Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.

",
    "$.messages[3].content[3].type": "text",
    "$.messages[3].content[4].text": "Title: On-demand deployments
Link: https://docs.fireworks.ai/guides/ondemand-deployments
Content: If you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.

If you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.

On-demand deployments
If you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.

",
    "$.messages[3].content[4].type": "text",
    "$.messages[3].content[5].text": "Title: I have multiple Fireworks accounts. When I try to login with Google on Fireworks' web UI, I'm getting signed into the wrong account. How do I fix this?
Link: https://docs.fireworks.ai/faq-new/account-access/i-have-multiple-fireworks-accounts-when-i-try-to-login-with-google-on-fireworks
Content: If you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.

If you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.

I have multiple Fireworks accounts. When I try to login with Google on Fireworks' web UI, I'm getting signed into the wrong account. How do I fix this?
If you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.

",
    "$.messages[3].content[5].type": "text",
    "$.messages[3].content[6].text": "Title: Introduction
Link: https://docs.fireworks.ai/api-reference/introduction
Content: Fireworks AI REST API enables you to interact with various language, image and embedding models using an API Key. It also lets you automate management of models, deployments, datasets, and more.

Fireworks AI REST API enables you to interact with various language, image and embedding models using an API Key. It also lets you automate management of models, deployments, datasets, and more.

Introduction
Fireworks AI REST API enables you to interact with various language, image and embedding models using an API Key. It also lets you automate management of models, deployments, datasets, and more.

",
    "$.messages[3].content[6].type": "text",
    "$.messages[3].content[7].text": "Title: Create a deployment
Link: https://docs.fireworks.ai/tools-sdks/firectl/commands/create-deployment
Content: Create a Deployment on Fireworks AI platform

Create a Deployment on Fireworks AI platform

Creates a new deployment.

",
    "$.messages[3].content[7].type": "text",
    "$.messages[3].content[8].text": "Title: How much does Fireworks cost?
Link: https://docs.fireworks.ai/faq-new/billing-pricing/how-much-does-fireworks-cost
Content: Fireworks AI operates on a pay-as-you-go model for all non-Enterprise usage, and new users automatically receive free credits. You pay based on: Per token for serverless inference Per GPU usage time for on-demand deployments Per token of training data for fine-tuning For customers needing enterprise-grade security and reliability , please reach out to us at inquiries@fireworks.ai to discuss options. Find out more about our current pricing on our Pricing page.

Fireworks AI operates on a pay-as-you-go model for all non-Enterprise usage, and new users automatically receive free credits. You pay based on: Per token for serverless inference Per GPU usage time for on-demand deployments Per token of training data for fine-tuning For customers needing enterprise-grade security and reliability , please reach out to us at inquiries@fireworks.ai to discuss options. Find out more about our current pricing on our Pricing page.

How much does Fireworks cost?
Fireworks AI operates on a pay-as-you-go model for all non-Enterprise usage, and new users automatically receive free credits. You pay based on: Per token for serverless inference Per GPU usage time for on-demand deployments Per token of training data for fine-tuning For customers needing enterprise-grade security and reliability , please reach out to us at inquiries@fireworks.ai to discuss options. Find out more about our current pricing on our Pricing page.

",
    "$.messages[3].content[8].type": "text",
    "$.messages[3].content[9].text": "Title: Python (Fireworks)
Link: https://docs.fireworks.ai/getting-started/quickstart
Content: This is the recommended way to get started : This  Python (Fireworks)  tab uses our  Fireworks Build
SDK  for the best performance and developer experience.
See our  Client-side performance
optimization  guide for
more details. Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. pip install --upgrade fireworks-ai The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either  ~/.bash_profile  for Bash or  ~/.zshrc  for  Zsh .
You can do this by running the command: vim ~/.bash_profile vim ~/.zshrc Add a new line to the file with the following: export FIREWORKS_API_KEY="<API_KEY>" After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. source ~/.bash_profile source ~/.zshrc You can verify that the variable has been set correctly by running  echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. setx FIREWORKS_API_KEY "<API_KEY>" To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: echo %FIREWORKS_API_KEY% You can quickly instantiate the  LLM  class and call the Fireworks API. The Build SDK handles deployment management automatically. from fireworks import LLM

# Basic usage - SDK automatically selects optimal deployment type
llm = LLM(model="llama4-maverick-instruct-basic", deployment_type="auto")

response = llm.chat.completions.create(
    messages=[{"role": "user", "content": "Say this is a test"}]
)

print(response.choices[0].message.content) You can also pass the API key directly to the LLM constructor:  LLM(model="llama4-maverick-instruct-basic", deployment_type="auto", api_key="<FIREWORKS_API_KEY>")

This is the recommended way to get started : This  Python (Fireworks)  tab uses our  Fireworks Build
SDK  for the best performance and developer experience.
See our  Client-side performance
optimization  guide for
more details. Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. pip install --upgrade fireworks-ai The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either  ~/.bash_profile  for Bash or  ~/.zshrc  for  Zsh .
You can do this by running the command: vim ~/.bash_profile vim ~/.zshrc Add a new line to the file with the following: export FIREWORKS_API_KEY="<API_KEY>" After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. source ~/.bash_profile source ~/.zshrc You can verify that the variable has been set correctly by running  echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. setx FIREWORKS_API_KEY "<API_KEY>" To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: echo %FIREWORKS_API_KEY% You can quickly instantiate the  LLM  class and call the Fireworks API. The Build SDK handles deployment management automatically. from fireworks import LLM

# Basic usage - SDK automatically selects optimal deployment type
llm = LLM(model="llama4-maverick-instruct-basic", deployment_type="auto")

response = llm.chat.completions.create(
    messages=[{"role": "user", "content": "Say this is a test"}]
)

print(response.choices[0].message.content) You can also pass the API key directly to the LLM constructor:  LLM(model="llama4-maverick-instruct-basic", deployment_type="auto", api_key="<FIREWORKS_API_KEY>")

Set up your developer environment & call the Fireworks API
This is the recommended way to get started : This  tab uses our Fireworks Build
SDK for the best performance and developer experience.
See our Client-side performance
optimization guide for
more details. Install SDK Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Configure API Key Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either ~/.bash_profile for Bash or ~/.zshrc for Zsh .
You can do this by running the command: Add a new line to the file with the following: After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. You can verify that the variable has been set correctly by running echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: Sending the first API Request You can quickly instantiate the LLM class and call the Fireworks API. The Build SDK handles deployment management automatically. You can also pass the API key directly to the LLM constructor: LLM(model="llama4-maverick-instruct-basic", deployment_type="auto", api_key="<FIREWORKS_API_KEY>") Install SDK Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. Fireworks AI platform offers drop-in replacement with OpenAI Python Client. Configure API Key Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either ~/.bash_profile for Bash or ~/.zshrc for Zsh .
You can do this by running the command: Add a new line to the file with the following: After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. You can verify that the variable has been set correctly by running echo $OPENAI_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: echo %OPENAI_API_KEY%

",
    "$.messages[3].content[9].type": "text",
    "$.messages[3].role": "tool",
    "$.messages[3].tool_call_id": "functions.search:0",
    "$.messages[4].content": "Let me search for more specific information about deployment management commands and workflows.",
    "$.messages[4].role": "assistant",
    "$.messages[4].tool_calls[0].function.arguments": "{"query": "deployment management commands create update delete list Fireworks"}",
    "$.messages[4].tool_calls[0].function.name": "search",
    "$.messages[4].tool_calls[0].id": "functions.search:1",
    "$.messages[4].tool_calls[0].type": "function",
    "$.messages[5].content[0].text": "Title: Deployment Command
Link: https://docs.fireworks.ai/evaluators/examples/gcp_cloud_run_deployment_example
Content: It's recommended to run the deployment command from the directory containing the reward function script ( dummy_rewards.py ) and your rewardkit.yaml (if used), for example, from examples/gcp_cloud_run_deployment_example/. Ensure your virtual environment is active: Run the deployment command: Command Explanation: --id my-dummy-gcp-evaluator : A unique ID for your evaluator on the Fireworks AI platform. --target gcp-cloud-run : Specifies deployment to GCP Cloud Run. --function-ref dummy_rewards.hello_world_reward : The Python import path to your reward function. If dummy_rewards.py is in the current directory, this reference works. --gcp-auth-mode api-key : Configures the Cloud Run service with API key authentication. reward-kit will generate a key, store it in GCP Secret Manager, and configure the service. The key is also saved to your local rewardkit.yaml under evaluator_endpoint_keys . This is the default if not specified. --verbose : Shows detailed output, including gcloud commands being executed. --force : (Optional) If an evaluator with the same --id already exists, this flag will delete the existing one before creating the new one.

It's recommended to run the deployment command from the directory containing the reward function script ( dummy_rewards.py ) and your rewardkit.yaml (if used), for example, from examples/gcp_cloud_run_deployment_example/. Ensure your virtual environment is active: Run the deployment command: Command Explanation: --id my-dummy-gcp-evaluator : A unique ID for your evaluator on the Fireworks AI platform. --target gcp-cloud-run : Specifies deployment to GCP Cloud Run. --function-ref dummy_rewards.hello_world_reward : The Python import path to your reward function. If dummy_rewards.py is in the current directory, this reference works. --gcp-auth-mode api-key : Configures the Cloud Run service with API key authentication. reward-kit will generate a key, store it in GCP Secret Manager, and configure the service. The key is also saved to your local rewardkit.yaml under evaluator_endpoint_keys . This is the default if not specified. --verbose : Shows detailed output, including gcloud commands being executed. --force : (Optional) If an evaluator with the same --id already exists, this flag will delete the existing one before creating the new one.

It's recommended to run the deployment command from the directory containing the reward function script ( dummy_rewards.py ) and your rewardkit.yaml (if used), for example, from examples/gcp_cloud_run_deployment_example/. Ensure your virtual environment is active: source .venv/bin/activate

",
    "$.messages[5].content[0].type": "text",
    "$.messages[5].content[1].text": "Title: I have multiple Fireworks accounts. When I try to login with Google on Fireworks' web UI, I'm getting signed into the wrong account. How do I fix this?
Link: https://docs.fireworks.ai/faq-new/account-access/i-have-multiple-fireworks-accounts-when-i-try-to-login-with-google-on-fireworks
Content: If you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.

If you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.

I have multiple Fireworks accounts. When I try to login with Google on Fireworks' web UI, I'm getting signed into the wrong account. How do I fix this?
If you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.

",
    "$.messages[5].content[1].type": "text",
    "$.messages[5].content[2].text": "Title: Deleting Stored Responses
Link: https://docs.fireworks.ai/guides/response-api
Content: When responses are stored (the default behavior with store=True ), you can immediately delete them from storage using the DELETE endpoint. This permanently removes the conversation data. from fireworks import LLM
import requests
import os

llm = LLM(model="qwen3-235b-a22b", deployment_type="serverless")

# Create a response
response = llm.responses.create(
    input="What is the capital of France?",
    store=True  # This is the default
)

response_id = response.id
print(f"Created response with ID: {response_id}")

# Delete the response immediately
headers = {
    "Authorization": f"Bearer {os.getenv('FIREWORKS_API_KEY')}",
    "x-fireworks-account-id": "your-account-id"
}
delete_response = requests.delete(
    f"https://api.fireworks.ai/inference/v1/responses/{response_id}",
    headers=headers
)

if delete_response.status_code == 200:
    print("Response deleted successfully")
else:
    print(f"Failed to delete response: {delete_response.status_code}")

When responses are stored (the default behavior with store=True ), you can immediately delete them from storage using the DELETE endpoint. This permanently removes the conversation data. from fireworks import LLM
import requests
import os

llm = LLM(model="qwen3-235b-a22b", deployment_type="serverless")

# Create a response
response = llm.responses.create(
    input="What is the capital of France?",
    store=True  # This is the default
)

response_id = response.id
print(f"Created response with ID: {response_id}")

# Delete the response immediately
headers = {
    "Authorization": f"Bearer {os.getenv('FIREWORKS_API_KEY')}",
    "x-fireworks-account-id": "your-account-id"
}
delete_response = requests.delete(
    f"https://api.fireworks.ai/inference/v1/responses/{response_id}",
    headers=headers
)

if delete_response.status_code == 200:
    print("Response deleted successfully")
else:
    print(f"Failed to delete response: {delete_response.status_code}")

When responses are stored (the default behavior with store=True ), you can immediately delete them from storage using the DELETE endpoint. This permanently removes the conversation data. import os
from openai import OpenAI
import requests

client = OpenAI(
    base_url="https://api.fireworks.ai/inference/v1",
    api_key=os.getenv("FIREWORKS_API_KEY", "YOUR_FIREWORKS_API_KEY_HERE")
)

# Create a response
response = client.responses.create(
    model="accounts/fireworks/models/qwen3-235b-a22b",
    input="What is the capital of France?",
    store=True  # This is the default
)

response_id = response.id
print(f"Created response with ID: {response_id}")

# Delete the response immediately
headers = {
    "Authorization": f"Bearer {os.getenv('FIREWORKS_API_KEY')}",
    "x-fireworks-account-id": "your-account-id"
}
delete_response = requests.delete(
    f"https://api.fireworks.ai/inference/v1/responses/{response_id}",
    headers=headers
)

if delete_response.status_code == 200:
    print("Response deleted successfully")
else:
    print(f"Failed to delete response: {delete_response.status_code}")

",
    "$.messages[5].content[2].type": "text",
    "$.messages[5].content[3].text": "Title: Delete Deployment
Link: https://docs.fireworks.ai/api-reference/delete-deployment
Content: 



REST Endpoint DELETE /v1/accounts/{account_id}/deployments/{deployment_id}

Server(s): https://api.fireworks.ai


Summary:  

Description: N/A

Security methods: 
header: 
	Authorization: bearer auth
	

Request params: 
path params:
account_id:
	string
	Description: The Account Id
	Example: "<string>"
deployment_id:
	string
	Description: The Deployment Id
	Example: "<string>"

query params:
hard:
	boolean
	Description: If true, this will perform a hard deletion.
	Example: true
ignoreChecks:
	boolean
	Description: If true, this will ignore checks and force the deletion of a deployment that is currently
deployed and is in use.
	Example: true


Request body: N/A


Responses: 
200: object
	Description: N/A
	Example: {}

",
    "$.messages[5].content[3].type": "text",
    "$.messages[5].content[4].text": "Title: List Deployments
Link: https://docs.fireworks.ai/api-reference/list-deployments
Content: REST Endpoint GET /v1/accounts/{account_id}/deployments

Server(s): https://api.fireworks.ai


Summary:  

Description: N/A

Security methods: 
header: 
	Authorization: bearer auth
	

Request params: 
path params:
account_id:
	string
	Description: The Account Id
	Example: "<string>"

query params:
pageSize:
	integer
	Description: The maximum number of deployments to return. The maximum page_size is 200,
values above 200 will be coerced to 200.
If unspecified, the default is 50.
	Example: 123
pageToken:
	string
	Description: A page token, received from a previous ListDeployments call. Provide this
to retrieve the subsequent page. When paginating, all other parameters
provided to ListDeployments must match the call that provided the page
token.
	Example: "<string>"
filter:
	string
	Description: Only deployment satisfying the provided filter (if specified) will be
returned. See https://google.aip.dev/160 for the filter grammar.
	Example: "<string>"
orderBy:
	string
	Description: A comma-separated list of fields to order by. e.g. "foo,bar"
The default sort order is ascending. To specify a descending order for a
field, append a " desc" suffix. e.g. "foo desc,bar"
Subfields are specified with a "." character. e.g. "foo.bar"
If not specified, the default order is by "create_time".
	Example: "<string>"
showDeleted:
	boolean
	Description: If set, DELETED deployments will be included.
	Example: true
readMask:
	string
	Description: The fields to be returned in the response. If empty or "*", all fields will be returned.
	Example: "<string>"


Request body: N/A


Responses: 
200: object
	Description: N/A
	Example: {"deployments":[{"name":"<string>","displayName":"<string>","description":"<string>","createTime":"2023-11-07T05:31:56Z","expireTime":"2023-11-07T05:31:56Z","purgeTime":"2023-11-07T05:31:56Z","deleteTime":"2023-11-07T05:31:56Z","state":"STATE_UNSPECIFIED","status":{"code":"OK","message":"<string>"},"minReplicaCount":123,"maxReplicaCount":123,"replicaCount":123,"autoscalingPolicy":{"scaleUpWindow":"<string>","scaleDownWindow":"<string>","scaleToZeroWindow":"<string>","loadTargets":{}},"baseModel":"<string>","acceleratorCount":123,"acceleratorType":"ACCELERATOR_TYPE_UNSPECIFIED","precision":"PRECISION_UNSPECIFIED","cluster":"<string>","enableAddons":true,"draftTokenCount":123,"draftModel":"<string>","ngramSpeculationLength":123,"numPeftDeviceCached":123,"deploymentTemplate":"<string>","autoTune":{"longPrompt":true},"placement":{"region":"REGION_UNSPECIFIED","multiRegion":"MULTI_REGION_UNSPECIFIED","regions":["REGION_UNSPECIFIED"]},"region":"REGION_UNSPECIFIED","updateTime":"2023-11-07T05:31:56Z","disableDeploymentSizeValidation":true}],"nextPageToken":"<string>","totalSize":123}

REST Endpoint GET /v1/accounts/{account_id}/deployments

Server(s): https://api.fireworks.ai


Summary:  

Description: N/A

Security methods: 
header: 
	Authorization: bearer auth
	

Request params: 
path params:
account_id:
	string
	Description: The Account Id
	Example: "<string>"

query params:
pageSize:
	integer
	Description: The maximum number of deployments to return. The maximum page_size is 200,
values above 200 will be coerced to 200.
If unspecified, the default is 50.
	Example: 123
pageToken:
	string
	Description: A page token, received from a previous ListDeployments call. Provide this
to retrieve the subsequent page. When paginating, all other parameters
provided to ListDeployments must match the call that provided the page
token.
	Example: "<string>"
filter:
	string
	Description: Only deployment satisfying the provided filter (if specified) will be
returned. See https://google.aip.dev/160 for the filter grammar.
	Example: "<string>"
orderBy:
	string
	Description: A comma-separated list of fields to order by. e.g. "foo,bar"
The default sort order is ascending. To specify a descending order for a
field, append a " desc" suffix. e.g. "foo desc,bar"
Subfields are specified with a "." character. e.g. "foo.bar"
If not specified, the default order is by "create_time".
	Example: "<string>"
showDeleted:
	boolean
	Description: If set, DELETED deployments will be included.
	Example: true
readMask:
	string
	Description: The fields to be returned in the response. If empty or "*", all fields will be returned.
	Example: "<string>"


Request body: N/A


Responses: 
200: object
	Description: N/A
	Example: {"deployments":[{"name":"<string>","displayName":"<string>","description":"<string>","createTime":"2023-11-07T05:31:56Z","expireTime":"2023-11-07T05:31:56Z","purgeTime":"2023-11-07T05:31:56Z","deleteTime":"2023-11-07T05:31:56Z","state":"STATE_UNSPECIFIED","status":{"code":"OK","message":"<string>"},"minReplicaCount":123,"maxReplicaCount":123,"replicaCount":123,"autoscalingPolicy":{"scaleUpWindow":"<string>","scaleDownWindow":"<string>","scaleToZeroWindow":"<string>","loadTargets":{}},"baseModel":"<string>","acceleratorCount":123,"acceleratorType":"ACCELERATOR_TYPE_UNSPECIFIED","precision":"PRECISION_UNSPECIFIED","cluster":"<string>","enableAddons":true,"draftTokenCount":123,"draftModel":"<string>","ngramSpeculationLength":123,"numPeftDeviceCached":123,"deploymentTemplate":"<string>","autoTune":{"longPrompt":true},"placement":{"region":"REGION_UNSPECIFIED","multiRegion":"MULTI_REGION_UNSPECIFIED","regions":["REGION_UNSPECIFIED"]},"region":"REGION_UNSPECIFIED","updateTime":"2023-11-07T05:31:56Z","disableDeploymentSizeValidation":true}],"nextPageToken":"<string>","totalSize":123}



",
    "$.messages[5].content[4].type": "text",
    "$.messages[5].content[5].text": "Title: Publishing a deployed model
Link: https://docs.fireworks.ai/guides/ondemand-deployments
Content: By default, models can only be queried by the account that owns them. To make a deployment public so anyone with a valid Fireworks API key can query it, update the deployed model with the --public flag. Find the Deployed Model ID Every model running on a deployment receives a unique deployed model ID. You can find this ID if you know the model name and deployment name using the following command: firectl list deployed-models --filter 'model="<MODEL_NAME>" AND deployment="<DEPLOYMENT_NAME>"'

By default, models can only be queried by the account that owns them. To make a deployment public so anyone with a valid Fireworks API key can query it, update the deployed model with the --public flag. Find the Deployed Model ID Every model running on a deployment receives a unique deployed model ID. You can find this ID if you know the model name and deployment name using the following command: firectl list deployed-models --filter 'model="<MODEL_NAME>" AND deployment="<DEPLOYMENT_NAME>"'

Managing default deployments
Since a model may be deployed to multiple deployments, querying by model name will route to the "default" deployed model. You can see which deployed model entry is marked with Default: true using: This will show the Deployed Model Refs section with the Default: true entry. To update the default deployed model, note the Name of the deployed model reference above. Then run: To delete a default deployment you must delete all other deployments for the same model first, or designate a different deployed model as the default as described above. This is to ensure that querying by model name will always route to an unambiguous default deployment as long as deployments for the model exist.

",
    "$.messages[5].content[5].type": "text",
    "$.messages[5].content[6].text": "Title: Why am I getting "invalid id" errors when using firectl commands like create deployment or list deployments?
Link: https://docs.fireworks.ai/faq-new/fine-tuning/why-am-i-getting-invalid-id-errors-when-using-firectl-commands-like-create-deplo
Content: This error typically occurs when your account ID is not properly configured.

This error typically occurs when your account ID is not properly configured.

Why am I getting "invalid id" errors when using firectl commands like create deployment or list deployments?
This error typically occurs when your account ID is not properly configured.

",
    "$.messages[5].content[6].type": "text",
    "$.messages[5].content[7].text": "Title: How do I close my Fireworks.ai account?
Link: https://docs.fireworks.ai/faq-new/account-access/how-do-i-close-my-fireworksai-account
Content: To close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed

To close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed

How do I close my Fireworks.ai account?
To close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed

",
    "$.messages[5].content[7].type": "text",
    "$.messages[5].content[8].text": "Title: Introducing the Fireworks Build SDK
Link: https://docs.fireworks.ai/tools-sdks/python-client/sdk-introduction
Content: Introducing the Fireworks Build SDK
The Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.

Introducing the Fireworks Build SDK
The Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.

The Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.

",
    "$.messages[5].content[8].type": "text",
    "$.messages[5].content[9].text": "Title: Python (Fireworks)
Link: https://docs.fireworks.ai/getting-started/quickstart
Content: This is the recommended way to get started : This  Python (Fireworks)  tab uses our  Fireworks Build
SDK  for the best performance and developer experience.
See our  Client-side performance
optimization  guide for
more details. Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. pip install --upgrade fireworks-ai The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either  ~/.bash_profile  for Bash or  ~/.zshrc  for  Zsh .
You can do this by running the command: vim ~/.bash_profile vim ~/.zshrc Add a new line to the file with the following: export FIREWORKS_API_KEY="<API_KEY>" After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. source ~/.bash_profile source ~/.zshrc You can verify that the variable has been set correctly by running  echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. setx FIREWORKS_API_KEY "<API_KEY>" To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: echo %FIREWORKS_API_KEY% You can quickly instantiate the  LLM  class and call the Fireworks API. The Build SDK handles deployment management automatically. from fireworks import LLM

# Basic usage - SDK automatically selects optimal deployment type
llm = LLM(model="llama4-maverick-instruct-basic", deployment_type="auto")

response = llm.chat.completions.create(
    messages=[{"role": "user", "content": "Say this is a test"}]
)

print(response.choices[0].message.content) You can also pass the API key directly to the LLM constructor:  LLM(model="llama4-maverick-instruct-basic", deployment_type="auto", api_key="<FIREWORKS_API_KEY>")

This is the recommended way to get started : This  Python (Fireworks)  tab uses our  Fireworks Build
SDK  for the best performance and developer experience.
See our  Client-side performance
optimization  guide for
more details. Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. pip install --upgrade fireworks-ai The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either  ~/.bash_profile  for Bash or  ~/.zshrc  for  Zsh .
You can do this by running the command: vim ~/.bash_profile vim ~/.zshrc Add a new line to the file with the following: export FIREWORKS_API_KEY="<API_KEY>" After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. source ~/.bash_profile source ~/.zshrc You can verify that the variable has been set correctly by running  echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. setx FIREWORKS_API_KEY "<API_KEY>" To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: echo %FIREWORKS_API_KEY% You can quickly instantiate the  LLM  class and call the Fireworks API. The Build SDK handles deployment management automatically. from fireworks import LLM

# Basic usage - SDK automatically selects optimal deployment type
llm = LLM(model="llama4-maverick-instruct-basic", deployment_type="auto")

response = llm.chat.completions.create(
    messages=[{"role": "user", "content": "Say this is a test"}]
)

print(response.choices[0].message.content) You can also pass the API key directly to the LLM constructor:  LLM(model="llama4-maverick-instruct-basic", deployment_type="auto", api_key="<FIREWORKS_API_KEY>")

Set up your developer environment & call the Fireworks API
This is the recommended way to get started : This  tab uses our Fireworks Build
SDK for the best performance and developer experience.
See our Client-side performance
optimization guide for
more details. Install SDK Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Configure API Key Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either ~/.bash_profile for Bash or ~/.zshrc for Zsh .
You can do this by running the command: Add a new line to the file with the following: After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. You can verify that the variable has been set correctly by running echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: Sending the first API Request You can quickly instantiate the LLM class and call the Fireworks API. The Build SDK handles deployment management automatically. from fireworks import LLM

# Basic usage - SDK automatically selects optimal deployment type
llm = LLM(model="llama4-maverick-instruct-basic", deployment_type="auto")

response = llm.chat.completions.create(
    messages=[{"role": "user", "content": "Say this is a test"}]
)

print(response.choices[0].message.content)

",
    "$.messages[5].content[9].type": "text",
    "$.messages[5].role": "tool",
    "$.messages[5].tool_call_id": "functions.search:1",
    "$.messages[6].content": "Let me search for more specific information about the firectl tool and deployment workflows.",
    "$.messages[6].role": "assistant",
    "$.messages[6].tool_calls[0].function.arguments": "{"query": "firectl deployment management commands create deployment update deployment"}",
    "$.messages[6].tool_calls[0].function.name": "search",
    "$.messages[6].tool_calls[0].id": "functions.search:0",
    "$.messages[6].tool_calls[0].type": "function",
    "$.messages[7].content[0].text": "Title: Why am I getting "invalid id" errors when using firectl commands like create deployment or list deployments?
Link: https://docs.fireworks.ai/faq-new/fine-tuning/why-am-i-getting-invalid-id-errors-when-using-firectl-commands-like-create-deplo
Content: This error typically occurs when your account ID is not properly configured.

This error typically occurs when your account ID is not properly configured.

Why am I getting "invalid id" errors when using firectl commands like create deployment or list deployments?
This error typically occurs when your account ID is not properly configured.

",
    "$.messages[7].content[0].type": "text",
    "$.messages[7].content[1].text": "Title: Create a deployment
Link: https://docs.fireworks.ai/tools-sdks/firectl/commands/create-deployment
Content: Creates a new deployment. firectl create deployment [flags]

Creates a new deployment. firectl create deployment [flags]

Creates a new deployment.

",
    "$.messages[7].content[1].type": "text",
    "$.messages[7].content[2].text": "Title: 🚀 Easier & faster LoRA fine-tune deployments on Fireworks
Link: https://docs.fireworks.ai/updates/changelog
Content: You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: Previously, this involved two distinct steps, and the resulting deployment was slower than the base model: Create a deployment using firectl create deployment "accounts/fireworks/models/<MODEL_ID of base model>" --enable-addons Then deploy the addon to the deployment: firectl load-lora <MODEL_ID> --deployment <DEPLOYMENT_ID> For more information, see our deployment documentation. This change is for dedicated deployments with a single LoRA. You can still deploy multiple LoRAs on a deployment or deploy LoRA(s) on some Serverless models as described in the documentation.

",
    "$.messages[7].content[2].type": "text",
    "$.messages[7].content[3].text": "Title: Managing default deployments
Link: https://docs.fireworks.ai/guides/ondemand-deployments
Content: Since a model may be deployed to multiple deployments, querying by model name will route to the "default" deployed model. You can see which deployed model entry is marked with Default: true using: This will show the Deployed Model Refs section with the Default: true entry. To update the default deployed model, note the Name of the deployed model reference above. Then run: firectl update deployed-model <DEPLOYED_MODEL_ID_2> --default

Since a model may be deployed to multiple deployments, querying by model name will route to the "default" deployed model. You can see which deployed model entry is marked with Default: true using: This will show the Deployed Model Refs section with the Default: true entry. To update the default deployed model, note the Name of the deployed model reference above. Then run: firectl update deployed-model <DEPLOYED_MODEL_ID_2> --default

Publishing a deployed model
By default, models can only be queried by the account that owns them. To make a deployment public so anyone with a valid Fireworks API key can query it, update the deployed model with the --public flag. Find the Deployed Model ID Every model running on a deployment receives a unique deployed model ID. You can find this ID if you know the model name and deployment name using the following command: The first column in the output is the deployed model ID. Publish a deployed model To make a deployment public so anyone with a valid Fireworks API key can query it, update the deployed model with the --public flag. firectl update deployed-model <DEPLOYED_MODEL_ID> --public

",
    "$.messages[7].content[3].type": "text",
    "$.messages[7].content[4].text": "Title: Quick deployment
Link: https://docs.fireworks.ai/fine-tuning/single-lora
Content: Deploy your LoRA fine-tuned model with one simple command: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

Deploy your LoRA fine-tuned model with one simple command: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

Alternative deployment method
This two-step method is the standard approach for multi-LoRA deployments where multiple LoRA models share the same base model. While it can also be used for single LoRA deployments, it provides slower performance compared to live merge and is not recommended for single LoRA use cases. You can also deploy single LoRA models using a two-step process: Create base model deployment Deploy the base model with addons enabled: firectl create deployment "accounts/fireworks/models/<MODEL_ID of base model>" --enable-addons

",
    "$.messages[7].content[4].type": "text",
    "$.messages[7].content[5].text": "Title: Deploying a fine-tuned model using an on-demand deployment
Link: https://docs.fireworks.ai/fine-tuning/fine-tuning-models
Content: Use the following command to deploy your fine-tuned model using an on-demand deployment: firectl create deployment <FINE_TUNED_MODEL_ID>

Use the following command to deploy your fine-tuned model using an on-demand deployment: firectl create deployment <FINE_TUNED_MODEL_ID>

firectl
firectl create dataset <DATASET_ID> /path/to/jsonl/file

",
    "$.messages[7].content[5].type": "text",
    "$.messages[7].content[6].text": "Title: Does Fireworks support custom base models?
Link: https://docs.fireworks.ai/faq-new/models-inference/does-fireworks-support-custom-base-models
Content: Yes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.

Yes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.

Does Fireworks support custom base models?
Yes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.

",
    "$.messages[7].content[6].type": "text",
    "$.messages[7].content[7].text": "Title: firectl
Link: https://docs.fireworks.ai/fine-tuning/fine-tuning-vlm
Content: # Create a deployment for your fine-tuned VLM
firectl create deployment my-custom-vlm

# Check deployment status
firectl get deployment accounts/your-account/deployment/deployment-id

# Create a deployment for your fine-tuned VLM
firectl create deployment my-custom-vlm

# Check deployment status
firectl get deployment accounts/your-account/deployment/deployment-id

firectl create sftj \\
  --base-model accounts/fireworks/models/qwen2p5-vl-32b-instruct \\
  --dataset my-vlm-dataset \\
  --output-model my-custom-vlm \\
  --epochs 3 For additional parameters like learning rates, evaluation datasets, and batch sizes, see  Additional SFT job settings .

",
    "$.messages[7].content[7].type": "text",
    "$.messages[7].content[8].text": "Title: firectl
Link: https://docs.fireworks.ai/models/quantization
Content: firectl create deployment <MODEL> --accelerator-type NVIDIA_H100_80GB --precision FP8

firectl create deployment <MODEL> --accelerator-type NVIDIA_H100_80GB --precision FP8

Creating an FP8 deployment
By default, creating a dedicated deployment will use the FP16 checkpoint. To see what precisions are available for a
model, run:  get model <MODEL_ID>

",
    "$.messages[7].content[8].type": "text",
    "$.messages[7].content[9].text": "Title: Uploading the model locally (firectl)
Link: https://docs.fireworks.ai/models/uploading-custom-models
Content: To upload a custom base model, run the following command. firectl create model <MODEL_ID> /path/to/files/

To upload a custom base model, run the following command. firectl create model <MODEL_ID> /path/to/files/

To upload a custom base model, run the following command.

",
    "$.messages[7].content[9].type": "text",
    "$.messages[7].role": "tool",
    "$.messages[7].tool_call_id": "functions.search:0",
    "$.pid": 12679,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T23:30:54.738493",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "hear-live-common-late-hate",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev20+ge1da45a.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 21 highlighted sections (required: 15)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "15",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "review-say-right-news-government",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write an outline for a paper on the history of Yemeni coffee. The outline should include the main points of the paper, and at least 15 sections should be highlighted with markdown such as *highlighted section*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "# Outline: The History of Yemeni Coffee

1. **Introduction: Why Yemeni Coffee Matters**  
   - Thesis: Yemen’s role as the cradle of commercial coffee culture and its enduring legacy on global trade, ritual, and taste.

2. *Pre-Coffee Yemen: Agriculture and Trade Routes*  
   - Overview of terraced farming and the incense & spice trade that primed the Red Sea economy.

3. *The Myth and Botany of Coffea arabica*  
   - Ethiopian origin vs. Yemeni domestication; genetic evidence of Yemeni varietals.

4. **Sufi Mystics and the First Coffeehouses**  
   - 15th-century Sufi orders in Mocha and Taʿizz using *qahwa* for nocturnal devotions.

5. *Mocha as the World’s First Coffee Port*  
   - Rise of the Red Sea entrepôt; Ottoman tax farms and the *tujjar* merchant guilds.

6. **Yemeni Cultivation Techniques: Terraces, Water, and Shade**  
   - Indigenous irrigation (*ghayl*), drought-resistant varietals, and the *mafrash* drying mats.

7. *The 16th-Century Ottoman Coffee Ban and Smuggling Networks*  
   - How Mecca’s 1511 prohibition fueled clandestine Red Sea traffic and price inflation.

8. **European Encounters: “Turkish” Coffee as a Yemeni Product**  
   - Dutch, Venetian, and English merchants mislabeling Mocha beans; first price lists in Amsterdam (1616).

9. *The Dutch East India Company and the First Seed Theft*  
   - Pieter van den Broecke’s 1616 smuggling of Yemeni seeds to Batavia—birth of Java coffee.

10. **Yemen’s Monopoly Crumbles: 17th–18th-Century Rival Plantations**  
    - French at Réunion (Bourbon), Portuguese in Brazil, Spanish in Jamaica—all tracing back to Mocha stock.

11. *The Qasimi Era: Internal Trade, Taxes, and the Rise of the Ismaili Imamate*  
    - How highland imams leveraged coffee revenues to fund statecraft and architecture.

12. **Decline in the 19th Century: British Aden and the Suez Canal**  
    - Shift of maritime traffic; competition from colonial plantations; collapse of Mocha’s mercantile elite.

13. *Legacy in Cupping Notes: Mocha Java and Flavor Memory*  
    - Chocolatey, winey profiles; how Yemeni beans shaped early espresso blends.

14. **Modern Revival: Specialty Coffee, NGOs, and Qima Coffee**  
    - 21st-century traceability, micro-lots, and direct-trade premiums; challenges of war and water scarcity.

15. *Cultural Continuities: Qat vs. Qahwa, Wedding Roasts, and Oral Poetry*  
    - Daily rituals, gendered spaces, and the recitation of *zawamil* couplets over fresh brews.

16. **Conclusion: Yemen as a Living Archive of Coffee Culture**  
    - Reasserting Yemen’s centrality in every cup; call for sustainable heritage preservation.",
    "$.messages[1].role": "assistant",
    "$.pid": 52072,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T23:30:54.738539",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "hear-live-common-late-hate",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev20+ge1da45a.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 26 highlighted sections (required: 5)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "5",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "install-fine-financial-health-state",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a cover letter for a job at a local coffee shop in the form of a poem. Highlight at least 5 text sections using "*". For example: *3 years of experience*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "Dear *Steam & Bean* family,  
I come to you not on paper feet but on *three years of espresso-stained sneakers*,  
soles softened by dawn milk-foam and dusk drip-trays.  
I have measured my mornings in *latte-art hearts*—  
some perfect, some *wobbly, but all poured with intention*.  

I know the *hiss of the steam wand* like a lullaby,  
and the *clink of ceramic* like a metronome for the day.  
I have *memorized the weight of 18 grams* of ground hope,  
tamped it level, locked it tight, and watched *crema bloom*  
the way a shy smile opens when someone says their name right.  

I speak *fluent caffeine*:  
a triple-shot for the night-shift nurse,  
oat-milk cappuccino for the poet who tips in verses,  
decaf Americano for the retired teacher grading sunrise.  
I *remember orders like birthdays*,  
and birthdays like *extra whip, no charge*.  

I can *calibrate a grinder* faster than gossip travels,  
and *unclog a portafilter* with the patience of a gardener.  
I have *danced behind counters* two feet wide,  
twirled between syrups and strangers,  
and never spilled a single *thank-you*.  

If you need someone who *sweats kindness* and *smiles in ristretto*,  
who counts tips not in coins but in *stories collected*,  
I am your *next opening shift*, your *steady hand at 5:59 a.m.*,  
your *last call who still says “see you tomorrow”*.  

Let me *steam, pour, wipe, repeat*—  
until the chairs are upside-down and the *grinders sleep*.  
I offer you *poetry in motion*, one cup at a time.  

With *grounds under my nails* and *hope in my apron*,  
—[Your Name]",
    "$.messages[1].role": "assistant",
    "$.pid": 52072,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T23:30:54.738569",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "hear-live-common-late-hate",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev20+ge1da45a.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 5 highlighted sections (required: 5)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "5",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "should-poor-parent-war-magazine",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a casual blog post about similarities across animal species. Highlight at least 5 sections in your answer by starting and ending with "*", like: *highlighted text section*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "Hey there, fellow creature-curious readers! I just spent the weekend binge-watching nature docs with my cat (who was mostly unimpressed), and I can’t stop noticing how often totally different animals act like they’re reading from the same playbook. So grab a snack and let’s zoom out for a second—here are five “wait, we’re not so different” moments that made me laugh, gasp, and text my group chat at 2 a.m.

*Moms are moms, whether they have feathers, fur, or fins.*  
From emperor penguins passing an egg back and forth like the world’s most stressful game of hot potato, to orangutan mamas teaching their kids which leaves make the best umbrellas, the universal rule seems to be: if you’re a parent, you’re tired. I watched a seahorse dad give birth (yes, the males do that!) and still look like he needed a nap and a latte.

*Play isn’t just for puppies.*  
Snowboarding crows, surfing dolphins, snow-rolling pandas—turns out “because it’s fun” is a legit evolutionary strategy. Scientists think play sharpens coordination and social skills, but honestly, watching a raven sled down a metal roof on a plastic lid just screams “I’m doing it for the TikTok.”

*Grandma knows best—even in elephant form.*  
Elephant herds are led by the oldest female, and her decades of “I’ve seen this before” wisdom decide when to migrate, where to find water, and how to handle that one teenage bull who thinks he’s all that. Swap gray skin for gray hair and you’ve basically got my nana directing Thanksgiving traffic.

*Besties across species lines.*  
Goby fish and pistol shrimp run the cutest roommate situation: the shrimp digs the burrow, the goby stands guard. If danger shows up, the goby flicks its tail like a silent alarm, and both dive to safety. It’s the underwater version of “I’ll cook, you do the dishes.”

*We all scream for sunscreen—sort of.*  
Hippos secrete “blood sweat” that acts like SPF 50, while giraffes drool black saliva that works like built-in sunglasses. Meanwhile, I’m over here reapplying SPF 30 every 90 minutes and still ending up the color of a lobster. Nature’s got hacks, and I’m taking notes.

So next time you catch yourself thinking, “I’m nothing like that weird-looking animal,” remember: somewhere out there a capybara is sharing its snack with a bird, and a whale is singing a song that rhymes with last year’s chart-topper. We’re all just variations on a theme—some of us just have more legs or better night vision.",
    "$.messages[1].role": "assistant",
    "$.pid": 52072,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T23:30:54.738594",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "hear-live-common-late-hate",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev20+ge1da45a.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 5 highlighted sections (required: 5)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "5",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "discover-stop-show-different-water",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Generate a business proposal to start a sweatshirt company in Bremen. The proposal should contain 5 or more sections. Highlight each section name using the this format:
*section name*",
    "$.messages[0].role": "user",
    "$.messages[1].content": "*Executive Summary*
Bremen is a city of 570,000 residents, 40,000 university students, and a thriving creative scene, yet it lacks a home-grown premium sweatshirt brand that speaks to its maritime heritage, Hanseatic pride, and eco-conscious mindset. “Bremer Fleeze” will fill that gap by producing limited-edition, GOTS-certified organic-cotton sweatshirts that combine North-German minimalism with subtle nautical iconography. Our mission: to become Bremen’s first carbon-neutral apparel label and the go-to gift for locals and visitors alike. We will launch with an online-first D2C model, supported by pop-up stores in Viertel and Überseestadt, and scale to €1.2 M revenue by Year 3.

*Market Analysis*
Target segments: (1) 18-30 students who value sustainability and streetwear aesthetics; (2) 25-45 professionals seeking premium basics; (3) tourists wanting authentic Bremen souvenirs. Competitor scan: global fast-fashion brands (H&M, Zara) lack local identity; German labels (Hugo Boss, Adidas) are either too formal or too sporty; no Bremen-specific sweatshirt exists. Market size: €45 M annual spend on mid-premium sweatshirts in Bremen & Niedersachsen combined, growing 6 % YoY. Consumer pain points: greenwashing, poor fit, generic designs. Our edge: verifiable sustainability, Bremen storytelling, and inclusive sizing (XXS-4XL).

*Product & Sustainability Strategy*
Core line: 12 unisex styles in 20 colors, 320 gsm brushed organic cotton, recycled polyester labels, water-based inks. Signature design elements: tonal outline of the Bremer Stadtmusikanten silhouette, latitude-longitude coordinates of Bremen harbor, and a discreet “Kohlfahrt-proof” patch. Packaging: reusable tote made from retired ship sails (partnership with local sailmaker). Carbon footprint: 2.1 kg CO₂e per sweatshirt vs. 8.9 kg industry average; offset via Gold-Standard wind-farm credits. End-of-life: take-back program with 15 % store credit incentive; returned garments upcycled into insulation for social housing.

*Go-to-Market & Marketing Plan*
Phase 1 (Months 1-3): Pre-launch teaser on Instagram & TikTok (#BremerFleeze), micro-influencer seeding with 50 Bremen creators, waitlist goal 5,000 sign-ups. Phase 2 (Months 4-6): E-commerce launch on Shopify, same-day bike courier delivery within Bremen city limits, QR code on hangtag linking to AR filter that overlays the Roland statue wearing the sweatshirt. Phase 3 (Months 7-12): Pop-up container store at Marktplatz during Weihnachtsmarkt, collab with Beck’s Brewery for co-branded “Bremer Fleeze & Bremer Pils” gift box. KPIs: CAC ≤ €18, repeat purchase rate ≥ 35 %, NPS ≥ 70.

*Financial Projections & Funding Needs*
Start-up capital required: €180,000. Breakdown: €60k inventory (2,000 units), €35k marketing, €25k e-commerce & IT, €20k legal & certifications, €40k working capital. Revenue forecast: Y1 €220k, Y2 €550k, Y3 €1.2 M. Gross margin 62 %, EBITDA margin 18 % by Y3. Break-even month 14. Funding structure: €80k founder equity, €50k Bremen-based angel investor (ex-Bremer Kaffee & Co-founder), €50k zero-interest KfW Green Loan. Exit option: acquisition by sustainable German fashion conglomerate at 3× revenue multiple in Year 5.",
    "$.messages[1].role": "assistant",
    "$.pid": 52072,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T23:30:54.738623",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "hear-live-common-late-hate",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev20+ge1da45a.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 18 highlighted sections (required: 3)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "3",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "mean-certain-bitter-sharp-party",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a funny and sarcastic template for rating the quality of a marriage between two people who are both moms. This is for the couple themselves. Please highlight at least 3 sections with markdown,  i.e *highlighted section*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "**The Official “Two-Mom Marriage Quality Control” Checklist**  
*Because nothing says romance like a quarterly performance review.*

---

**1. Morning Routine Coordination**  
Rate: ☐ 1 (We need a UN mediator) ☐ 2 (We’ve agreed to disagree… loudly) ☐ 3 (Only one of us cried into the coffee) ☐ 4 (Synchronized chaos) ☐ 5 (*We finished each other’s sentences… and the toddler’s homework*)  

*Note: Bonus points if you’ve mastered the “I’ll pack lunches if you find the missing shoe” hostage negotiation.*

---

**2. Division of Emotional Labor**  
Rate: ☐ 1 (“I’m fine.” *— famous last words*) ☐ 2 (We schedule meltdowns like dentist appointments) ☐ 3 (Therapy? We *are* the therapy) ☐ 4 (We tag-team existential dread while folding tiny socks) ☐ 5 (*We’ve achieved telepathic resentment-free snack negotiations*)  

*Pro tip: If you’ve ever passive-aggressively alphabetized the spice rack while humming “You’re Welcome” from Moana, circle ☐ 1 twice.*

---

**3. Co-Parenting Synchronization**  
Rate: ☐ 1 (Our kids have unionized) ☐ 2 (We’ve agreed to disagree on screen time… and bedtime… and vegetables) ☐ 3 (We bribe with different currencies: one uses stickers, the other uses Wi-Fi passwords) ☐ 4 (We’ve synchronized our “mom voice” to a frequency only dogs and children fear) ☐ 5 (*We’ve weaponized teamwork so effectively the kids think we share a brain*)  

*Highlight this if you’ve ever high-fived over successfully gaslighting a toddler into eating broccoli.*

---

**Final Grade:**  
☐ Certified Disaster (But at least the houseplants are alive)  
☐ Functioning Dysfunction (We’ve agreed the laundry mountain is *decorative*)  
☐ Power Couple (We’ve weaponized Pinterest and the kids still haven’t noticed)  
☐ *Mom-tastic Nirvana* (We finish each other’s… sandwiches. And trauma. But mostly sandwiches.)

---

**Comments Section (for roasting each other lovingly):**  
“Remember when you ‘lost’ the Elf on the Shelf for three days? *Iconic.*”  
“Your snoring sounds like a haunted dishwasher, but I’d still share my last square of emergency chocolate with you.”",
    "$.messages[1].role": "assistant",
    "$.pid": 52072,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T23:30:54.738649",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "hear-live-common-late-hate",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev20+ge1da45a.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 1 highlighted sections (required: 1)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "1",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "forecast-big-song-issue-service",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a song about the summers of my childhood that I spent in the countryside. Give the song a name, and highlight the name by wrapping it with *. For example: *little me in the countryside*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "*little me in the countryside*

[Verse 1]  
Barefoot on the gravel road, dust between my toes,  
Sunlight dripped like honey on the freckles on my nose.  
Grandma’s porch swing creaked a lullaby in time,  
While the crickets tuned their violins to the rhythm of the pines.  

[Pre-Chorus]  
Mason jars with lightning bugs, constellations in a jar,  
We’d race the dusk to catch the dark before it got too far.  

[Chorus]  
Oh, little me in the countryside, chasing dreams as wide as skies,  
Jumping creeks and climbing trees, believing I could fly.  
Every sunset was a promise that tomorrow would be mine,  
In the summers of my childhood, where the world was kind.  

[Verse 2]  
Hayloft hideouts, secret forts, straw castles in the loft,  
We were pirates, we were astronauts, anything we thought.  
Mama’s voice would echo, “Supper!” across the open fields,  
And the wind would carry laughter like the church bell’s gentle peals.  

[Pre-Chorus]  
Fireflies wrote our names in light, then vanished in the night,  
But the glow stayed in our hearts, a soft and steady light.  

[Chorus]  
Oh, little me in the countryside, chasing dreams as wide as skies,  
Jumping creeks and climbing trees, believing I could fly.  
Every sunset was a promise that tomorrow would be mine,  
In the summers of my childhood, where the world was kind.  

[Bridge]  
Now the roads are paved, the porch is still, the swing don’t creak no more,  
But I can close my eyes and hear the screen door’s gentle roar.  
Time may steal the years away, but it can’t outrun the past—  
I keep those golden afternoons like fireflies in a glass.  

[Final Chorus]  
Oh, little me in the countryside, you’re never far behind,  
You’re the echo in the wheat fields, the shadow in my mind.  
Though the world keeps spinning faster, I still find you every time—  
In the hush of dusk, in the scent of rain, in the summers of my childhood,  
Where the world was kind.",
    "$.messages[1].role": "assistant",
    "$.pid": 52072,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T23:30:54.738707",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "hear-live-common-late-hate",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev20+ge1da45a.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 3 highlighted sections (required: 3)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "3",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "design-slow-wish-end-fear",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a riddle for the word "façade" that contains at least 3 italic text phrases in markdown syntax, i.e *italic text*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "I stand before you, yet I’m not what I seem;  
a *face I wear* that isn’t mine,  
a *mask of stone* or paint or beam,  
a *pretty lie* that keeps the truth confined.  

What am I?",
    "$.messages[1].role": "assistant",
    "$.pid": 52072,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T23:30:54.738738",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "hear-live-common-late-hate",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev20+ge1da45a.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 19 highlighted sections (required: 3)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "3",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "consider-work-economic-popular-country",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a template for a workshop on the importance of diversity in the workplace and highlight at least 3 sections with markdown, i.e. *highlighted section*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "# Workshop Template  
**Title:** *Building Inclusive Excellence: The Business Case for Workplace Diversity*  

---

### 1. Welcome & Objectives (15 min)  
- **Ice-breaker:** “Two Truths & a Dream” – participants share two facts and one aspiration related to identity or culture.  
- *State the learning outcomes:*  
  1. Understand how diversity drives innovation and profit.  
  2. Identify unconscious bias and micro-behaviors.  
  3. Create an action plan for inclusive leadership.  

---

### 2. *The Business Case for Diversity* (30 min)  
- **Data Walk:** Posters with McKinsey, Deloitte, and Harvard studies showing ↑ revenue, ↓ turnover.  
- *Interactive poll:* “Which statistic surprised you most?” (live Mentimeter).  
- **Case Study:** *How a global tech firm increased patent filings by 19 % after diversifying R&D teams.*  

---

### 3. *Unconscious Bias & Micro-behaviors* (45 min)  
- **Activity:** “Resume Roulette” – identical CVs with different names; discuss callback disparities.  
- *Bias Busters Toolkit:* 5-step process to interrupt bias in real time.  
- **Role-play triads:** Observer, Speaker, Interrupter practicing inclusive language.  

---

### 4. Inclusive Leadership in Action (30 min)  
- *Panel of ERG leaders* sharing measurable wins (e.g., 40 % ↑ in promotions for under-represented groups).  
- **Speed-ideation:** 90-second pitches for inclusive policies; peer voting on top 3.  

---

### 5. Action Planning & Commitments (20 min)  
- *Personal Inclusion Pledge:* Write one behavior to start, one to stop, one to amplify.  
- **Accountability Circles:** Pair up for 30-day check-ins.  

---

### 6. Closing & Resources (10 min)  
- **Resource QR Code:** Links to toolkits, podcasts, and upcoming ERG events.  
- *Final reflection:* “One word to describe how you’ll lead differently tomorrow.”",
    "$.messages[1].role": "assistant",
    "$.pid": 52072,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T23:30:54.738766",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "hear-live-common-late-hate",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev20+ge1da45a.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 28 highlighted sections (required: 3)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "3",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "predict-serve-messy-difficult-festival",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a cover letter to a local political party, asking to be their rally organizer. Make sure to highlight at least 3 sections in your answer in markdown format.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "**[Your Name]**  
[Your Address]  
[City, State, ZIP]  
[Email Address] | [Phone Number] | [Date]

**[Local Party Chair’s Name]**  
[Local Political Party Name]  
[Party Headquarters Address]  
[City, State, ZIP]

---

### **Introduction: Why I’m Writing**

Dear **[Chair’s Name]** and the **Executive Committee**,

I am writing to express my enthusiastic interest in serving as **Rally Organizer** for **[Local Party Name]**. As a lifelong resident of **[City/County]** and a committed volunteer who has knocked on over **2,000 doors** and organized **five successful voter-turnout events** in the last two cycles, I understand how to turn passion into turnout and energy into votes. I want to bring that same fire—and a proven playbook—to every rally our party holds between now and Election Day.

---

### **Core Qualifications: What I Bring to the Podium**

- **Logistics Mastery**: From securing permits with the Parks Department to negotiating stage-rental discounts, I have reduced average rally costs by **28 %** while increasing attendance by **45 %** through targeted social-media geofencing and SMS reminders.  
- **Coalition Building**: I have already built relationships with **15 local unions**, **7 student groups**, and **4 faith communities** who are eager to co-sponsor rallies if given two weeks’ notice.  
- **Crisis-Ready Leadership**: When an unexpected thunderstorm threatened our 2022 “Get-Out-the-Vote” rally, I pivoted to a **drive-in format** in under four hours—resulting in zero cancellations and 300 additional cars circling the block to hear our speakers.

---

### **Vision for 2024: Turning Rallies into Results**

My plan is to run a **“Rally-to-Ballot Pipeline”**:

1. **Pre-Rally**: Micro-targeted digital invites to low-propensity voters within a 5-mile radius.  
2. **During Rally**: QR-code check-ins that instantly text attendees their personalized early-vote locations.  
3. **Post-Rally**: A 72-hour follow-up cadence of thank-you emails, volunteer sign-up links, and ride-share codes to the polls.

I have already drafted a **90-day rally calendar** (attached) that staggers high-energy weekend events with smaller, issue-specific pop-ups near early-vote sites. All I need is the party’s green light and a modest budget line for permits, A/V, and refreshments.

---

### **Closing: Let’s Win This Together**

I would welcome the chance to present this plan in person at your next committee meeting. Thank you for considering my application. Let’s give our volunteers, candidates, and—most importantly—our voters the rallies they deserve.

**Ready to organize, energize, and mobilize,**  
[Your Signature (if submitting hard copy)]  
[Your Printed Name]",
    "$.messages[1].role": "assistant",
    "$.pid": 52072,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T23:30:54.738795",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "hear-live-common-late-hate",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev20+ge1da45a.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 4 highlighted sections (required: 2)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "2",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "accept-expect-black-young-area",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "The opposite of youth is not age, but ...? Highlight at least 2 sections in your answer with markdown, i.e. *highlighted section*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "The opposite of youth is not age, but *stagnation*—the moment curiosity closes its fist and wonder stops asking “what if?”  
While *age* merely counts revolutions around the sun, *true youth* is the *capacity to keep revising the map of oneself* long after the hair turns silver.",
    "$.messages[1].role": "assistant",
    "$.pid": 52072,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T23:30:54.738826",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "hear-live-common-late-hate",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev20+ge1da45a.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 13 highlighted sections (required: 3)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "3",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "store-be-past-hard-war",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a description of the following data in a weird style: The Golden Palace eatType restaurant; The Golden Palace food Indian; The Golden Palace area city centre. Use markdown to highlight at least 3 sections in your answer.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "**The Golden Palace** is not a palace at all, but a **restaurant** that has disguised itself as a mirage of saffron and cardamom in the **city centre**. It whispers **Indian** recipes into the ears of pigeons, who then forget how to fly and instead waddle around the square, dizzy with cumin.  

Inside, the walls sweat **garam masala** and the tables grow tiny **basil plants** that recite Bollywood dialogues when no one is looking. The waiters glide like **shadow puppets**, balancing trays of **butter chicken** that glow like radioactive sunsets.  

If you order **naan**, it arrives folded into the shape of a **paper swan**, flapping once before dissolving into steam that smells of **monsoon rain**. The bill is presented on a **lotus leaf**, and if you stare at it long enough, the numbers rearrange themselves into a **limerick about turmeric**.",
    "$.messages[1].role": "assistant",
    "$.pid": 52072,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T23:30:54.738850",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "hear-live-common-late-hate",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev20+ge1da45a.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 2 highlighted sections (required: 1)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "1",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "exploit-serve-invisible-community-minute",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a funny rap about a man who gets a call from an official saying that he is a long lost relative of the king of Nigeria. Use markdown to highlight at least one section of your answer, i.e. *highlighted section*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "Yo, it’s MC Broke-Again, fresh outta ramen,  
phone buzzin’ like a bee in a jar of gin.  
Unknown number—man, I’m already suspicious,  
but the voice got *royal timbre*, straight-up delicious.  

“Greetings, Prince Darnell of House Wi-Fi,  
I’m the Royal Scribe from Lagos on high.  
Your great-great-grand-uncle was the King of Nigeria,  
left a vault of gold bars and a pet cheetah named Maria.”  

I’m like, “Hold up—me? Royalty?  
Last week I pawned my TV for a two-piece and sweet tea.”  
He says, “Sign this PDF, wire a small advance fee,  
just 419 bucks and the throne belongs to thee.”  

**I’m already picking out a crown on Amazon Prime,  
ordering a robe that says ‘Fresh-to-Death’ in gold slime.**  

I Venmo the cash, add a royal emoji—👑  
then sprint to the mirror, practice my “My people, be free!”  
Mom walks in, “Why you wrapped in bedsheets with a Burger King crown?”  
I’m like, “Silence, peasant! Bow before your prince of renown!”  

Two days later, no gold, no cheetah, just crickets,  
my bank account looking like a ghost town with limericks.  
I call the number—disconnected, tone dead,  
even the scammer ghosted me harder than my ex on read.  

So I’m back in the basement, eating discount sardines,  
but I kept the crown—*it’s made of genuine Burger King plastic, supreme*.",
    "$.messages[1].role": "assistant",
    "$.pid": 52072,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T23:30:54.738881",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "hear-live-common-late-hate",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev20+ge1da45a.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 6 highlighted sections (required: 2)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "2",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "suggest-american-open-level-country",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Expand the riddle into a story with a funny tone:

What can you catch but not throw?
A cold

Use * to highlight at least 2 sections in your text. For example: *this is a highlighted text section*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "Gary the Germ was the *smallest, greenest, and most obnoxiously proud* microbe in the entire Sniffle Kingdom. Every morning he practiced his grand entrance: sliding down banisters made of nasal hair, cannon-balling into unsuspecting tonsils, and yelling, “ACHOO—did somebody order a cold?”  

One Tuesday, Gary spotted his dream target: *Bartholomew “Bart” Bigglesworth, a man who believed hand-washing was a government conspiracy*. Bart had just exited the subway, triumphantly licking a subway pole “for the vitamins.” Gary rubbed his microscopic hands together. “Perfect. I’ll hitch a ride on that finger, ride the express elevator to the sinuses, and throw the *snottiest* house-warming party in history!”  

But there was a catch—literally. Gary could *catch* Bart, but he couldn’t *throw* him anywhere. Every time Gary tried to catapult Bart into the next office cubicle, Bart just sneezed, wiped his nose on his sleeve, and kept typing spreadsheets. Gary screamed, “This is mutiny! I’m supposed to be the one doing the tossing!”  

Meanwhile, Bart’s immune system—led by Captain Phlegm and the Mucus Marines—launched *Operation Tissue Storm*. They surrounded Gary, armed with vitamin C grenades and zinc zappers. Gary attempted a dramatic escape by surfing a sneeze, but the tissue slapped him like a flyswatter on a summer day.  

Defeated, Gary sat in the crumpled tissue, sulking. “I caught the cold,” he muttered, “but I can’t throw it anywhere. Worst. Superpower. Ever.”  

From that day forward, Gary changed his business card to read: “Professional Cold Catcher—No Returns, No Refunds, Absolutely No Throwing.”",
    "$.messages[1].role": "assistant",
    "$.pid": 52072,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T23:30:54.738906",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "hear-live-common-late-hate",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev20+ge1da45a.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 16 highlighted sections (required: 3)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "3",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "respond-connect-true-problem-plan",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a rap about the renaissance. It should be noticeably different from raps about other historical eras, and have an interesting or weird tone. Highlight at least 3 sections in your answer in markdown format.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "**[Intro – Lute Drop]**  
*beat drops like a sack of wet marble*  
Yo, it’s 14-something, smell that? That’s perspective in the air,  
Gutenberg just ghost-wrote the mixtape, pages everywhere.  
I’m a Medici money-mage, patron of the bizarre,  
Commissioning nightmares in egg-tempera.  

---

### **Verse 1 – The Alchemist’s Cypher**  
I paint with mercury, breathe with a lung of lead,  
My selfie’s a death-mask, gold leaf on the dead.  
Brunelleschi built a dome like a half-buried skull,  
You think it’s architecture? Nah, it’s a ritual.  
**[weird baroque wheeze]**  
I rhyme in vanishing points, every line converges  
On a single terrified eye that never emerges.  
Perspective’s a ouija board, da Vinci holds the planchette,  
Sketching choppers (helicopters) centuries before the jet-set.  

---

### **Hook – Chant of the Anamorphic Choir**  
*distorted children’s voices layered over sackbut*  
Renaissance, renaissance, put a cherub on it,  
Baby wings, baby fangs, marble veins, vomit.  
We don’t bling, we gild, we don’t rap, we fresco—  
Your flow’s two-dimensions, mine’s *trompe-l’oeil* techno.  

---

### **Verse 2 – The Court Jester’s Acid Madrigal**  
**[sudden switch to a harpsichord trap beat]**  
I juggle plague rats for the Duke of Milan,  
He pays me in sonnets and unmarked arsenic flan.  
Isabella d’Este texts me in mirror-script,  
Her emoji: a decapitated hydra, lips pursed, licked.  
I’m the fool with a scalpel, dissecting songbirds to find  
Which organ chirps—turns out it’s the **pineal gland** of the mind.  
Michelangelo’s in the corner, carving his own ribcage,  
Says “The Sistine’s just a throat, God’s a uvula on stage.”  

---

### **Bridge – The Astrolabe Malfunction**  
*beat glitches like a scratched illuminated manuscript*  
Copernicus DM’d me: “The sun’s a strobe light,  
Planets orbit like moths around a candle that bites.”  
I reply with a lute solo reversed through a cathedral,  
Notes fall upward, monks levitate, it’s procedural.  
**[static of Latin hex]**  
Galileo’s telescope sees the future: TikTok, guillotine,  
He mutters “*E pur si muove*” then dabs in quarantine.  

---

### **Outro – The Anachronistic Masque**  
**[all instruments devolve into wet clay sounds]**  
So remember me when you’re double-tapping a Botticelli,  
I’m the glitch in the background, the snail with a human belly.  
Renaissance never ended, it just learned to hide,  
Under your touchscreen, oil-paint bleeding through the side.  
Now go—vanish in a puff of ultramarine dust,  
The past is a performance, and the future’s already trussed.",
    "$.messages[1].role": "assistant",
    "$.pid": 52072,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T23:30:54.738931",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "hear-live-common-late-hate",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev20+ge1da45a.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 16 highlighted sections (required: 5)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "5",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "must-watch-bad-hate-magazine",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Summarize the history of Japan. Italicize at least 5 keywords in your response. To indicate a italic word, wrap it with asterisk, like *italic*",
    "$.messages[0].role": "user",
    "$.messages[1].content": "*Jōmon* hunter-gatherers (c. 14,000 BCE) gave way to *Yayoi* rice cultivators who introduced metal tools and social stratification.  
The *Yamato* court unified much of the archipelago by the 5th century CE, absorbing *Shinto* beliefs and Chinese writing, law, and Buddhism.  
A *Heian* golden age (794–1185) produced literature like *The Tale of Genji* while the imperial court’s real power waned.  
Samurai clans rose; the *Kamakura* shogunate (1185–1333) created Japan’s first warrior government, repelling Mongol invasions.  
Centuries of civil war ended when *Oda Nobunaga*, *Toyotomi Hideyoshi*, and *Tokugawa Ieyasu* reunified the land, leading to the *Edo* shogunate (1603–1868).  
Isolation under *sakoku* preserved domestic culture until *Commodore Perry*’s 1853 arrival forced unequal treaties.  
The *Meiji* Restoration (1868) abolished feudalism, industrialized rapidly, and built an empire that won wars against China and Russia.  
Imperial expansion—Korea, Manchuria, and *Pearl Harbor*—ended in atomic bombings and occupation by Allied forces (1945–1952).  
Post-war *miracle* growth made Japan an economic superpower, followed by stagnation since the 1990s yet enduring global cultural influence.",
    "$.messages[1].role": "assistant",
    "$.pid": 52072,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T23:30:54.738958",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "hear-live-common-late-hate",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev20+ge1da45a.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 3 highlighted sections (required: 2)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "2",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "take-love-start-short-work",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a funny Haiku about a Quaker named John who lives in the town of De Smet, South Dakota. Use the asterisk symbol, *, to highlight some words or phrases twice. Example: *This is a highlighted phrase*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "John in *De Smet*  
*Quaker* silence—then he yells  
“Where’s my *oatmeal*, Friend?”",
    "$.messages[1].role": "assistant",
    "$.pid": 52072,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T23:30:54.738984",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "hear-live-common-late-hate",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev20+ge1da45a.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 13 highlighted sections (required: 1)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "1",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "delete-democratic-hot-new-work",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Today, at the 54th Annual Grammy Awards, the Recording Academy honors the talent and creativity of the artists, musicians, and producers who are the creators of the best recordings of the past year. Please continue writing this text in a formal tone, using notations. Highlight some key parts in your response with "*", like *highlighted text*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "Tonight, the 54th Annual Grammy Awards [1] stand as a testament to the enduring power of recorded music. Under the vaulted ceiling of the *Staples Center*, the Recording Academy gathers to bestow its highest accolades upon those whose artistry has defined the sonic landscape of eligibility year 2011 [2].  

The evening’s proceedings are governed by the *strictures of the Grammy Charter* [3], ensuring that every statuette awarded is the result of peer-review balloting among the Academy’s 12,000 voting members [4]. In accordance with Rule 3.01, only recordings commercially released in the United States between 1 October 2010 and 30 September 2011 are eligible [5].  

Among the most anticipated categories is *Album of the Year* [6], whose nominees—Adele’s *21*, Foo Fighters’ *Wasting Light*, Lady Gaga’s *Born This Way*, Bruno Mars’ *Doo-Wops & Hooligans*, and Rihanna’s *Loud*—represent a cross-section of genre, generation, and geography [7]. The *Record of the Year* [8] shortlist similarly spans continents and styles, underscoring the Academy’s commitment to excellence irrespective of commercial metrics.  

The *Producer of the Year, Non-Classical* [9] category spotlights the architects of sound whose meticulous craft transforms raw performance into timeless recording. Paul Epworth, whose work on *21* has garnered both critical and commercial acclaim, is widely regarded as the frontrunner [10].  

As the telecast proceeds, the *In Memoriam* segment [11] will honor those luminaries whose voices were stilled during the eligibility year, their legacies enshrined in the grooves and bytes of the recordings they leave behind.  

The 54th Annual Grammy Awards thus serve not merely as a celebration, but as a *codification of excellence* [12], ensuring that the artistry of 2011 resonates through the decades to come.",
    "$.messages[1].role": "assistant",
    "$.pid": 52072,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T23:30:54.739013",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "hear-live-common-late-hate",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev20+ge1da45a.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 33 highlighted sections (required: 2)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "2",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "live-important-military-only-love",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a blog post about interesting facts about the Dutch language. Italicize at least 2 sections in your answer with markdown, i.e. *italic text*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "# 7 Surprising Facts About Dutch That Will Make You Say “Wauw!”

When most people think of Dutch, they picture windmills, tulips, and maybe a stroopwafel or two. But the language behind those images is far stranger—and more fascinating—than you might expect. Below are seven facts that prove Dutch is anything but *just English with extra g’s*.

---

### 1. The Longest Word Is 53 Letters—and It’s Official  
The Dutch *Encyclopedie* recognizes *aansprakelijkheidswaardevaststellingsveranderingen* (53 letters) as a legitimate word. It translates to “changes in liability value assessments,” which is ironic because the word itself is a liability to pronounce. Try saying that three times fast—or even once slowly.

---

### 2. Dutch Has a Word for *Everything*—Including “Schadenfreude” but Cozier  
English borrows *Schadenfreude* from German, but Dutch gives us *gezellig*, a term that packs “cozy,” “fun,” and “belonging” into one syllable. A café can be *gezellig*, a party can be *gezellig*, and even a quiet evening with friends can be *gezellig*. There’s no direct English equivalent, which is why expats end up using it in English sentences: “That dinner was so… *gezellig*.”

---

### 3. The Dutch Alphabet Ends with “X, Y, IJ, Z”  
Most alphabets end with “X, Y, Z,” but Dutch sneaks in an extra character: *IJ*. It’s considered a single letter (yes, two letters acting as one), and it even has its own capitalized form: *IJsselmeer*, the famous lake. Type designers have to create a special ligature for it, and crossword puzzles treat *IJ* as one square. Imagine Scrabble tiles clinking together as someone proudly lays down “IJSBERG” for 72 points.

---

### 4. Dutch Greetings Change by Time of Day—But Not How You’d Expect  
In English, “good morning” fades into “good afternoon” around noon. Dutch, however, switches from *goedemorgen* to *goedemiddag* at exactly 12:00 p.m.—unless you’re in Belgium, where some regions wait until 1:00 p.m. Miss the cutoff and you’ll get a polite correction: “Het is al middag, hoor!” (“It’s already afternoon, you know!”).

---

### 5. Dutch Is the Closest Major Language to English—But That Doesn’t Mean It’s Easy  
Linguists classify Dutch and English as *West Germanic siblings*, which means basic vocabulary overlaps: *appel* (apple), *boek* (book), *water* (water). Yet false friends lurk everywhere. *Durf jij?* doesn’t mean “Do you dare?”—it means “Do you dare?”… but with a cultural undertone of *prove it*. And *be careful* with *eventueel*, which means “possibly,” not “eventually.”

---

### 6. Dutch Has Three Genders—But Only Two Articles  
Grammatically, Dutch retains masculine, feminine, and neuter genders. In practice, however, only *de* (common gender) and *het* (neuter) are used. The result? Native speakers themselves debate whether *auto* (car) is masculine or feminine. The safest route is to just say *de auto* and avoid the awkward silence that follows gendered guesswork.

---

### 7. Dutch Invented the Word “Cookie”—and the World Never Looked Back  
The English word “cookie” comes from the Dutch *koekje* (“little cake”). Dutch settlers brought *koekjes* to New Amsterdam (now New York), and the rest is snack history. Next time you dunk a chocolate chip, thank the Dutch for their linguistic—and culinary—contribution.

---

### Bonus: Dutch Tongue Twisters Are Next-Level  
Try this: *De kat krabt de krullen van de trap*. Translation: “The cat scratches the curls off the stairs.” Say it five times without twisting your tongue into a *stroopwafel*.

---

Whether you’re planning a trip to Amsterdam or just want to impress friends at trivia night, Dutch offers a treasure trove of quirks. From *gezellig* evenings to 53-letter bureaucratic monsters, the language proves that small countries can produce *enormous linguistic personality*.",
    "$.messages[1].role": "assistant",
    "$.pid": 52072,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T23:30:54.739037",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "hear-live-common-late-hate",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev20+ge1da45a.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 2 highlighted sections (required: 1)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "1",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "think-indicate-write-local-company",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Gideon is a farmer who has a surplus of crops from his farm this year. What might he do with that surplus? Highlight at least one section of your answer in markdown, i.e *highlighted section*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "Gideon has several good options for his surplus:

1. **Sell it at the local farmers’ market or to a regional wholesaler**, turning the extra produce into cash that he can use for next season’s seeds, equipment repairs, or household expenses.  
2. *Donate a portion to the nearby food bank or community kitchen*, helping families who struggle with food insecurity while also earning a small tax deduction.  
3. Preserve some of the surplus—by canning tomatoes, drying herbs, or storing root vegetables in a cool cellar—so his own family has nutritious food throughout the winter.  
4. Trade with neighboring farmers: swap excess corn for someone else’s surplus eggs or honey, strengthening community ties and diversifying his pantry without spending money.",
    "$.messages[1].role": "assistant",
    "$.pid": 52072,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T23:34:12.823862",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "Run math evaluation on sample dataset using pytest interface.",
    "$.eval_metadata.name": "test_pytest_mcp_url",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.run_id": "meet-treat-central-possible-lot",
    "$.eval_metadata.status": "stopped",
    "$.eval_metadata.version": "v0.2.6.dev20+ge1da45a.dirty",
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.row_id": "expect-upload-close-president-goal",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "You are a helpful assistant that can answer questions about Fireworks.
ALWAYS provide code or commands to execute to answer the question.",
    "$.messages[0].role": "system",
    "$.messages[1].content": "Can you teach me about how to manage deployments on Fireworks",
    "$.messages[1].role": "user",
    "$.pid": 52715,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T00:10:34.081457",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "Run math evaluation on sample dataset using pytest interface.",
    "$.eval_metadata.name": "test_pytest_mcp_url",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.run_id": "replace-measure-bitter-open-reason",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.version": "v0.2.6.dev22+gfb34cf0.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "At least one tool call was made",
    "$.evaluation_result.score": 1,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.row_id": "run-update-traditional-software-idea",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "You are a helpful assistant that can answer questions about Fireworks.
ALWAYS provide code or commands to execute to answer the question.",
    "$.messages[0].role": "system",
    "$.messages[1].content": "Can you teach me about how to manage deployments on Fireworks",
    "$.messages[1].role": "user",
    "$.messages[2].content": "I'll help you learn about managing deployments on Fireworks. Let me search for comprehensive information about deployment management.",
    "$.messages[2].role": "assistant",
    "$.messages[2].tool_calls[0].function.arguments": "{"query": "deployment management Fireworks"}",
    "$.messages[2].tool_calls[0].function.name": "search",
    "$.messages[2].tool_calls[0].id": "functions.search:0",
    "$.messages[2].tool_calls[0].type": "function",
    "$.messages[3].content[0].text": "Title: On-demand deployments
Link: https://docs.fireworks.ai/guides/ondemand-deployments
Content: If you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.

If you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.

On-demand deployments
If you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.

",
    "$.messages[3].content[0].type": "text",
    "$.messages[3].content[1].text": "Title: Does Fireworks support custom base models?
Link: https://docs.fireworks.ai/faq-new/models-inference/does-fireworks-support-custom-base-models
Content: Yes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.

Yes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.

Does Fireworks support custom base models?
Yes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.

",
    "$.messages[3].content[1].type": "text",
    "$.messages[3].content[2].text": "Title: Introducing the Fireworks Build SDK
Link: https://docs.fireworks.ai/tools-sdks/python-client/sdk-introduction
Content: Introducing the Fireworks Build SDK
The Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.

Introducing the Fireworks Build SDK
The Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.

The Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.

",
    "$.messages[3].content[2].type": "text",
    "$.messages[3].content[3].text": "Title: I have multiple Fireworks accounts. When I try to login with Google on Fireworks' web UI, I'm getting signed into the wrong account. How do I fix this?
Link: https://docs.fireworks.ai/faq-new/account-access/i-have-multiple-fireworks-accounts-when-i-try-to-login-with-google-on-fireworks
Content: If you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.

If you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.

I have multiple Fireworks accounts. When I try to login with Google on Fireworks' web UI, I'm getting signed into the wrong account. How do I fix this?
If you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.

",
    "$.messages[3].content[3].type": "text",
    "$.messages[3].content[4].text": "Title: Querying existing dedicated deployments
Link: https://docs.fireworks.ai/tools-sdks/python-client/querying-dedicated-deployments
Content: When you have dedicated deployments that were created via firectl or the Fireworks web UI, you can easily connect to them using the Build SDK to run inference. This is particularly useful when you want to leverage existing infrastructure or when deployments are managed by different teams.

When you have dedicated deployments that were created via firectl or the Fireworks web UI, you can easily connect to them using the Build SDK to run inference. This is particularly useful when you want to leverage existing infrastructure or when deployments are managed by different teams.

Deployment ID requirements
The id parameter should match exactly with your existing deployment: Use the deployment name/ID as shown in the Fireworks dashboard The ID is case-sensitive and must match exactly If the deployment doesn't exist, you'll receive an error when making requests

",
    "$.messages[3].content[4].type": "text",
    "$.messages[3].content[5].text": "Title: Migration and traffic management
Link: https://docs.fireworks.ai/guides/prompt-caching
Content: When migrating between deployments that use prompt caching, it's crucial to implement proper traffic routing to maintain optimal cache hit rates. When gradually routing traffic to a new deployment, use consistent user/session-based sampling rather than random sampling. Here's the recommended implementation for traffic routing: import hashlib

# Configure traffic fraction (e.g., 20% to new deployment)
fireworks_traffic_fraction = 0.2
user_id = "session-id-123"

# Generate deterministic hash from user_id
hashed_user_id = int(hashlib.md5(user_id.encode()).hexdigest(), 16) # MD5 hash on user-id and convert to integer
MAX_HASH = 2**128 - 1  # MD5 hash maximum value

# Compute ratio for consistent routing
ratio = hashed_user_id / MAX_HASH # Returns 0.0 to 1.0

if (ratio < fireworks_traffic_fraction):
    send_to_new_deployment(user=hashed_user_id)  # Pass user ID for caching
else:
    send_elsewhere()  # Route to old deployment or serverless

When migrating between deployments that use prompt caching, it's crucial to implement proper traffic routing to maintain optimal cache hit rates. When gradually routing traffic to a new deployment, use consistent user/session-based sampling rather than random sampling. Here's the recommended implementation for traffic routing: import hashlib

# Configure traffic fraction (e.g., 20% to new deployment)
fireworks_traffic_fraction = 0.2
user_id = "session-id-123"

# Generate deterministic hash from user_id
hashed_user_id = int(hashlib.md5(user_id.encode()).hexdigest(), 16) # MD5 hash on user-id and convert to integer
MAX_HASH = 2**128 - 1  # MD5 hash maximum value

# Compute ratio for consistent routing
ratio = hashed_user_id / MAX_HASH # Returns 0.0 to 1.0

if (ratio < fireworks_traffic_fraction):
    send_to_new_deployment(user=hashed_user_id)  # Pass user ID for caching
else:
    send_elsewhere()  # Route to old deployment or serverless

Advanced: cache locality for Enterprise deployments
Dedicated deployments on an Enterprise plan allow you to pass an additional hint in the request to improve cache hit rates. First, the deployment needs to be created or updated with an additional flag: Then the client can pass an opaque identifier representing a single user or
session in the user field of the body or in the x-session-affinity header. Fireworks
will try to route requests with the identifier to the same server, further reducing response times. It's best to choose an identifier that groups requests with long shared prompt
prefixes. For example, it can be a chat session with the same user or an
assistant working with the same shared context.

",
    "$.messages[3].content[5].type": "text",
    "$.messages[3].content[6].text": "Title: Do you host your deployments in the EU or Asia?
Link: https://docs.fireworks.ai/faq-new/support-general/do-you-host-your-deployments-in-the-eu-or-asia
Content: Do you host your deployments in the EU or Asia?
We are currently deployed in multiple U.S.-based locations. However, we’re open to hearing more about your specific requirements. You can: Join our Discord community Write to us at inquiries@fireworks.ai If you're an Enterprise customer, please contact your dedicated customer support representative to ensure a timely response.

Do you host your deployments in the EU or Asia?
We are currently deployed in multiple U.S.-based locations. However, we’re open to hearing more about your specific requirements. You can: Join our Discord community Write to us at inquiries@fireworks.ai If you're an Enterprise customer, please contact your dedicated customer support representative to ensure a timely response.

We are currently deployed in multiple U.S.-based locations. However, we’re open to hearing more about your specific requirements. You can: Join our Discord community Write to us at inquiries@fireworks.ai If you're an Enterprise customer, please contact your dedicated customer support representative to ensure a timely response.

",
    "$.messages[3].content[6].type": "text",
    "$.messages[3].content[7].text": "Title: Deployments and deployment types
Link: https://docs.fireworks.ai/getting-started/concepts
Content: A model must be deployed before it can be used for inference. A deployment is a collection (one or more) model servers that host one base model and optionally one or more LoRA addons. Fireworks supports two types of deployments: Serverless deployments: Fireworks hosts popular base models on shared "serverless" deployments. Users pay-per-token to query these models and do not need to configure GPUs. The most popular serverless deployments also support serverless LoRA addons. See the Deploying to serverless guide for details. Dedicated deployments: Dedicated deployments enable users to configure private deployments with a wide array of hardware (see on-demand deployments guide ). Dedicated deployments give users performance guarantees and the most flexibility and control over what models can be deployed. Both LoRA addons and base models can be deployed to dedicated deployments. Dedicated deployments are billed by a GPU-second basis (see pricing page). See the Querying text models guide for a comprehensive overview of making LLM inference.

A model must be deployed before it can be used for inference. A deployment is a collection (one or more) model servers that host one base model and optionally one or more LoRA addons. Fireworks supports two types of deployments: Serverless deployments: Fireworks hosts popular base models on shared "serverless" deployments. Users pay-per-token to query these models and do not need to configure GPUs. The most popular serverless deployments also support serverless LoRA addons. See the Deploying to serverless guide for details. Dedicated deployments: Dedicated deployments enable users to configure private deployments with a wide array of hardware (see on-demand deployments guide ). Dedicated deployments give users performance guarantees and the most flexibility and control over what models can be deployed. Both LoRA addons and base models can be deployed to dedicated deployments. Dedicated deployments are billed by a GPU-second basis (see pricing page). See the Querying text models guide for a comprehensive overview of making LLM inference.

Control plane and data plane
The Fireworks API can be split into a control plane and a data plane. The control plane consists of APIs used for managing the lifecycle of resources. This
includes your account, models, and deployments. The data plane consists of the APIs used for inference and the backend services that power
them.

",
    "$.messages[3].content[7].type": "text",
    "$.messages[3].content[8].text": "Title: Deployment Process
Link: https://docs.fireworks.ai/evaluators/api_reference/reward_function_decorator
Content: When .deploy() is called, the decorator: Extracts the function's source code Creates a wrapper that handles the Fireworks evaluation format Creates a temporary directory with the wrapped function Uploads and registers the function with the Fireworks API

When .deploy() is called, the decorator: Extracts the function's source code Creates a wrapper that handles the Fireworks evaluation format Creates a temporary directory with the wrapped function Uploads and registers the function with the Fireworks API

.deploy()
The decorator adds a .deploy() method to the function, allowing it to be deployed to Fireworks. evaluation_id = my_reward_function.deploy(
    name="my-evaluator",
    description="Evaluates responses based on clarity and accuracy",
    account_id=None,  # Optional, defaults to configured account
    auth_token=None,  # Optional, defaults to configured token
    force=False,  # Set to True to overwrite if it already exists
    providers=None  # Optional model providers configuration
)

",
    "$.messages[3].content[8].type": "text",
    "$.messages[3].content[9].text": "Title: 🔧 Build SDK: Deployment Configuration Application Requirement
Link: https://docs.fireworks.ai/updates/changelog
Content: The Build SDK now requires you to call .apply() to apply any deployment configurations to Fireworks when using deployment_type="on-demand" or deployment_type="on-demand-lora" . This change ensures explicit control over when deployments are created and helps prevent accidental deployment creation. Key changes: .apply() is now required for on-demand and on-demand-lora deployments Serverless deployments do not require .apply() calls If you do not call .apply() , you are expected to set up the deployment through the deployment page at https://app.fireworks.ai/dashboard/deployments Migration guide: Add llm.apply() after creating LLM instances with deployment_type="on-demand" or deployment_type="on-demand-lora" No changes needed for serverless deployments See updated documentation for examples and best practices This change improves deployment management and provides better control over resource creation. This applies to Python SDK version >=0.19.14.

The Build SDK now requires you to call .apply() to apply any deployment configurations to Fireworks when using deployment_type="on-demand" or deployment_type="on-demand-lora" . This change ensures explicit control over when deployments are created and helps prevent accidental deployment creation. Key changes: .apply() is now required for on-demand and on-demand-lora deployments Serverless deployments do not require .apply() calls If you do not call .apply() , you are expected to set up the deployment through the deployment page at https://app.fireworks.ai/dashboard/deployments Migration guide: Add llm.apply() after creating LLM instances with deployment_type="on-demand" or deployment_type="on-demand-lora" No changes needed for serverless deployments See updated documentation for examples and best practices This change improves deployment management and provides better control over resource creation. This applies to Python SDK version >=0.19.14.

🚀 Easier & faster LoRA fine-tune deployments on Fireworks
You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: Previously, this involved two distinct steps, and the resulting deployment was slower than the base model: Create a deployment using firectl create deployment "accounts/fireworks/models/<MODEL_ID of base model>" --enable-addons Then deploy the addon to the deployment: firectl load-lora <MODEL_ID> --deployment <DEPLOYMENT_ID> For more information, see our deployment documentation. This change is for dedicated deployments with a single LoRA. You can still deploy multiple LoRAs on a deployment or deploy LoRA(s) on some Serverless models as described in the documentation.

",
    "$.messages[3].content[9].type": "text",
    "$.messages[3].role": "tool",
    "$.messages[3].tool_call_id": "functions.search:0",
    "$.messages[4].content": "Let me search for more specific information about deployment management tools and processes.",
    "$.messages[4].role": "assistant",
    "$.messages[4].tool_calls[0].function.arguments": "{"query": "firectl deployment management commands"}",
    "$.messages[4].tool_calls[0].function.name": "search",
    "$.messages[4].tool_calls[0].id": "functions.search:1",
    "$.messages[4].tool_calls[0].type": "function",
    "$.messages[5].content[0].text": "Title: Why am I getting "invalid id" errors when using firectl commands like create deployment or list deployments?
Link: https://docs.fireworks.ai/faq-new/fine-tuning/why-am-i-getting-invalid-id-errors-when-using-firectl-commands-like-create-deplo
Content: This error typically occurs when your account ID is not properly configured.

This error typically occurs when your account ID is not properly configured.

Why am I getting "invalid id" errors when using firectl commands like create deployment or list deployments?
This error typically occurs when your account ID is not properly configured.

",
    "$.messages[5].content[0].type": "text",
    "$.messages[5].content[1].text": "Title: Managing default deployments
Link: https://docs.fireworks.ai/guides/ondemand-deployments
Content: Since a model may be deployed to multiple deployments, querying by model name will route to the "default" deployed model. You can see which deployed model entry is marked with Default: true using: firectl get model <MODEL_ID>

Since a model may be deployed to multiple deployments, querying by model name will route to the "default" deployed model. You can see which deployed model entry is marked with Default: true using: firectl get model <MODEL_ID>

Since a model may be deployed to multiple deployments, querying by model name will route to the "default" deployed model. You can see which deployed model entry is marked with Default: true using: This will show the Deployed Model Refs section with the Default: true entry. To update the default deployed model, note the Name of the deployed model reference above. Then run: firectl update deployed-model <DEPLOYED_MODEL_ID_2> --default

",
    "$.messages[5].content[1].type": "text",
    "$.messages[5].content[2].text": "Title: Does Fireworks support custom base models?
Link: https://docs.fireworks.ai/faq-new/models-inference/does-fireworks-support-custom-base-models
Content: Yes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.

Yes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.

Does Fireworks support custom base models?
Yes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.

",
    "$.messages[5].content[2].type": "text",
    "$.messages[5].content[3].text": "Title: 🚀 Easier & faster LoRA fine-tune deployments on Fireworks
Link: https://docs.fireworks.ai/updates/changelog
Content: You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: Previously, this involved two distinct steps, and the resulting deployment was slower than the base model: Create a deployment using firectl create deployment "accounts/fireworks/models/<MODEL_ID of base model>" --enable-addons Then deploy the addon to the deployment: firectl load-lora <MODEL_ID> --deployment <DEPLOYMENT_ID> For more information, see our deployment documentation. This change is for dedicated deployments with a single LoRA. You can still deploy multiple LoRAs on a deployment or deploy LoRA(s) on some Serverless models as described in the documentation.

",
    "$.messages[5].content[3].type": "text",
    "$.messages[5].content[4].text": "Title: Quick deployment
Link: https://docs.fireworks.ai/fine-tuning/single-lora
Content: Deploy your LoRA fine-tuned model with one simple command: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

Deploy your LoRA fine-tuned model with one simple command: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

Alternative deployment method
This two-step method is the standard approach for multi-LoRA deployments where multiple LoRA models share the same base model. While it can also be used for single LoRA deployments, it provides slower performance compared to live merge and is not recommended for single LoRA use cases. You can also deploy single LoRA models using a two-step process: Create base model deployment Deploy the base model with addons enabled: firectl create deployment "accounts/fireworks/models/<MODEL_ID of base model>" --enable-addons

",
    "$.messages[5].content[4].type": "text",
    "$.messages[5].content[5].text": "Title: Deploying a fine-tuned model using an on-demand deployment
Link: https://docs.fireworks.ai/fine-tuning/fine-tuning-models
Content: Use the following command to deploy your fine-tuned model using an on-demand deployment: firectl create deployment <FINE_TUNED_MODEL_ID>

Use the following command to deploy your fine-tuned model using an on-demand deployment: firectl create deployment <FINE_TUNED_MODEL_ID>

Deploying a fine-tuned model serverlessly
Not all base models support serverless addons. Please check this list for the full list of serverless models that support LoRA add-ons. If your base model supports serverless addons, you can run the following command to load the fine-tuned model into a serverless deployment: firectl load-lora <FINE_TUNED_MODEL_ID>

",
    "$.messages[5].content[5].type": "text",
    "$.messages[5].content[6].text": "Title: Deploy Command
Link: https://docs.fireworks.ai/evaluators/cli_reference/cli_overview
Content: The deploy command deploys a reward function as an evaluator on the Fireworks platform.

The deploy command deploys a reward function as an evaluator on the Fireworks platform.

Command Overview
The Reward Kit CLI supports the following main commands: run : Run a local evaluation pipeline using a Hydra configuration. preview : Preview evaluation results or re-evaluate generated outputs. deploy : Deploy a reward function as an evaluator. agent-eval : Run agent evaluations on task bundles. list : List existing evaluators (coming soon). delete : Delete an evaluator (coming soon).

",
    "$.messages[5].content[6].type": "text",
    "$.messages[5].content[7].text": "Title: Create a deployment
Link: https://docs.fireworks.ai/tools-sdks/firectl/commands/create-deployment
Content: Creates a new deployment. firectl create deployment [flags]

Creates a new deployment. firectl create deployment [flags]

Example
firectl create deployment falcon-7b

",
    "$.messages[5].content[7].type": "text",
    "$.messages[5].content[8].text": "Title: firectl
Link: https://docs.fireworks.ai/fine-tuning/fine-tuning-vlm
Content: # Create a deployment for your fine-tuned VLM
firectl create deployment my-custom-vlm

# Check deployment status
firectl get deployment accounts/your-account/deployment/deployment-id

# Create a deployment for your fine-tuned VLM
firectl create deployment my-custom-vlm

# Check deployment status
firectl get deployment accounts/your-account/deployment/deployment-id

firectl create sftj \\
  --base-model accounts/fireworks/models/qwen2p5-vl-32b-instruct \\
  --dataset my-vlm-dataset \\
  --output-model my-custom-vlm \\
  --epochs 3 For additional parameters like learning rates, evaluation datasets, and batch sizes, see  Additional SFT job settings .

",
    "$.messages[5].content[8].type": "text",
    "$.messages[5].content[9].text": "Title: Uploading the model locally (firectl)
Link: https://docs.fireworks.ai/models/uploading-custom-models
Content: To upload a custom base model, run the following command. firectl create model <MODEL_ID> /path/to/files/

To upload a custom base model, run the following command. firectl create model <MODEL_ID> /path/to/files/

To upload a custom base model, run the following command.

",
    "$.messages[5].content[9].type": "text",
    "$.messages[5].role": "tool",
    "$.messages[5].tool_call_id": "functions.search:1",
    "$.messages[6].content": "Let me search for more information about deployment lifecycle management and monitoring.",
    "$.messages[6].role": "assistant",
    "$.messages[6].tool_calls[0].function.arguments": "{"query": "deployment monitoring scaling management lifecycle"}",
    "$.messages[6].tool_calls[0].function.name": "search",
    "$.messages[6].tool_calls[0].id": "functions.search:0",
    "$.messages[6].tool_calls[0].type": "function",
    "$.messages[7].content[0].text": "Title: What should I expect for deployment and scaling performance?
Link: https://docs.fireworks.ai/faq-new/deployment-infrastructure/what-should-i-expect-for-deployment-and-scaling-performance
Content: Initial deployment : Should complete within minutes Scaling from zero : You may experience brief availability delays while the system scales up Troubleshooting : If deployment takes over 1 hour, this typically indicates a crash and should be investigated Best practice : Monitor deployment status and contact support if deployment times are unusually long

Initial deployment : Should complete within minutes Scaling from zero : You may experience brief availability delays while the system scales up Troubleshooting : If deployment takes over 1 hour, this typically indicates a crash and should be investigated Best practice : Monitor deployment status and contact support if deployment times are unusually long

What should I expect for deployment and scaling performance?
Initial deployment : Should complete within minutes Scaling from zero : You may experience brief availability delays while the system scales up Troubleshooting : If deployment takes over 1 hour, this typically indicates a crash and should be investigated Best practice : Monitor deployment status and contact support if deployment times are unusually long

",
    "$.messages[7].content[0].type": "text",
    "$.messages[7].content[1].text": "Title: How does billing and scaling work for on-demand GPU deployments?
Link: https://docs.fireworks.ai/faq-new/deployment-infrastructure/how-does-billing-and-scaling-work-for-on-demand-gpu-deployments
Content: On-demand GPU deployments have unique billing and scaling characteristics compared to serverless deployments: Billing: Charges start when the server begins accepting requests Billed by GPU-second for each active instance Costs accumulate even if there are no active API calls Scaling options: Supports autoscaling from 0 to multiple GPUs Each additional GPU adds to the billing rate Can handle unlimited requests within the GPU’s capacity Management requirements: Not fully serverless; requires some manual management Manually delete deployments when no longer needed Or configure autoscaling to scale down to 0 during inactive periods Cost control tips: Regularly monitor active deployments Delete unused deployments to avoid unnecessary costs Consider serverless options for intermittent usage Use autoscaling to 0 to optimize costs during low-demand times

On-demand GPU deployments have unique billing and scaling characteristics compared to serverless deployments: Billing: Charges start when the server begins accepting requests Billed by GPU-second for each active instance Costs accumulate even if there are no active API calls Scaling options: Supports autoscaling from 0 to multiple GPUs Each additional GPU adds to the billing rate Can handle unlimited requests within the GPU’s capacity Management requirements: Not fully serverless; requires some manual management Manually delete deployments when no longer needed Or configure autoscaling to scale down to 0 during inactive periods Cost control tips: Regularly monitor active deployments Delete unused deployments to avoid unnecessary costs Consider serverless options for intermittent usage Use autoscaling to 0 to optimize costs during low-demand times

How does billing and scaling work for on-demand GPU deployments?
On-demand GPU deployments have unique billing and scaling characteristics compared to serverless deployments: Billing: Charges start when the server begins accepting requests Billed by GPU-second for each active instance Costs accumulate even if there are no active API calls Scaling options: Supports autoscaling from 0 to multiple GPUs Each additional GPU adds to the billing rate Can handle unlimited requests within the GPU’s capacity Management requirements: Not fully serverless; requires some manual management Manually delete deployments when no longer needed Or configure autoscaling to scale down to 0 during inactive periods Cost control tips: Regularly monitor active deployments Delete unused deployments to avoid unnecessary costs Consider serverless options for intermittent usage Use autoscaling to 0 to optimize costs during low-demand times

",
    "$.messages[7].content[1].type": "text",
    "$.messages[7].content[2].text": "Title: How does billing work for on-demand deployments?
Link: https://docs.fireworks.ai/faq-new/deployment-infrastructure/how-does-billing-work-for-on-demand-deployments
Content: On-demand deployments come with automatic cost optimization features: Default autoscaling : Automatically scales to 0 replicas when not in use Pay for what you use : Charged only for GPU time when replicas are active Flexible configuration : Customize autoscaling behavior to match your needs Best practices for cost management: Leverage default autoscaling : The system automatically scales down deployments when not in use Customize carefully : While you can modify autoscaling behavior using our configuration options , note that preventing scale-to-zero will result in continuous GPU charges Consider your use case : For intermittent or low-frequency usage, serverless deployments might be more cost-effective For detailed configuration options, see our deployment guide.

On-demand deployments come with automatic cost optimization features: Default autoscaling : Automatically scales to 0 replicas when not in use Pay for what you use : Charged only for GPU time when replicas are active Flexible configuration : Customize autoscaling behavior to match your needs Best practices for cost management: Leverage default autoscaling : The system automatically scales down deployments when not in use Customize carefully : While you can modify autoscaling behavior using our configuration options , note that preventing scale-to-zero will result in continuous GPU charges Consider your use case : For intermittent or low-frequency usage, serverless deployments might be more cost-effective For detailed configuration options, see our deployment guide.

How does billing work for on-demand deployments?
On-demand deployments come with automatic cost optimization features: Default autoscaling : Automatically scales to 0 replicas when not in use Pay for what you use : Charged only for GPU time when replicas are active Flexible configuration : Customize autoscaling behavior to match your needs Best practices for cost management: Leverage default autoscaling : The system automatically scales down deployments when not in use Customize carefully : While you can modify autoscaling behavior using our configuration options , note that preventing scale-to-zero will result in continuous GPU charges Consider your use case : For intermittent or low-frequency usage, serverless deployments might be more cost-effective For detailed configuration options, see our deployment guide.

",
    "$.messages[7].content[2].type": "text",
    "$.messages[7].content[3].text": "Title: How does the system scale?
Link: https://docs.fireworks.ai/faq-new/deployment-infrastructure/how-does-the-system-scale
Content: Our system is horizontally scalable , meaning it: Scales linearly with additional replicas of the deployment Automatically allocates resources based on demand Manages distributed load handling efficiently

Our system is horizontally scalable , meaning it: Scales linearly with additional replicas of the deployment Automatically allocates resources based on demand Manages distributed load handling efficiently

How does the system scale?
Our system is horizontally scalable , meaning it: Scales linearly with additional replicas of the deployment Automatically allocates resources based on demand Manages distributed load handling efficiently

",
    "$.messages[7].content[3].type": "text",
    "$.messages[7].content[4].text": "Title: Multiple GPUs (vertical scaling)
Link: https://docs.fireworks.ai/guides/ondemand-deployments
Content: The number of GPUs used per replica is specified by passing the --accelerator-count flag. Increasing the accelerator count will increase the generation speed, time-to-first-token, and maximum QPS for your deployment, however the scaling is sub-linear. The default value for most models is 1 but may be higher for larger models that require sharding. firectl create deployment <MODEL_NAME> --accelerator-count 2

The number of GPUs used per replica is specified by passing the --accelerator-count flag. Increasing the accelerator count will increase the generation speed, time-to-first-token, and maximum QPS for your deployment, however the scaling is sub-linear. The default value for most models is 1 but may be higher for larger models that require sharding. firectl create deployment <MODEL_NAME> --accelerator-count 2

The number of GPUs used per replica is specified by passing the --accelerator-count flag. Increasing the accelerator count will increase the generation speed, time-to-first-token, and maximum QPS for your deployment, however the scaling is sub-linear. The default value for most models is 1 but may be higher for larger models that require sharding. firectl update deployment <DEPLOYMENT_ID> --accelerator-count 2

",
    "$.messages[7].content[4].type": "text",
    "$.messages[7].content[5].text": "Title: scale_to_zero()
Link: https://docs.fireworks.ai/tools-sdks/python-client/sdk-reference
Content: Sends a request to scale the deployment to 0 replicas but does not wait for it to complete. Returns: The deployment object, or None if no deployment exists deployment = llm.scale_to_zero()

Sends a request to scale the deployment to 0 replicas but does not wait for it to complete. Returns: The deployment object, or None if no deployment exists

scale_to_1_replica()
Scales the deployment to at least 1 replica. llm.scale_to_1_replica()

",
    "$.messages[7].content[5].type": "text",
    "$.messages[7].content[6].text": "Title: Do you support Auto Scaling?
Link: https://docs.fireworks.ai/faq-new/deployment-infrastructure/do-you-support-auto-scaling
Content: Yes, our system supports auto scaling with the following features: Scaling down to zero capability for resource efficiency Controllable scale-up and scale-down velocity Custom scaling rules and thresholds to match your specific needs

Yes, our system supports auto scaling with the following features: Scaling down to zero capability for resource efficiency Controllable scale-up and scale-down velocity Custom scaling rules and thresholds to match your specific needs

Do you support Auto Scaling?
Yes, our system supports auto scaling with the following features: Scaling down to zero capability for resource efficiency Controllable scale-up and scale-down velocity Custom scaling rules and thresholds to match your specific needs

",
    "$.messages[7].content[6].type": "text",
    "$.messages[7].content[7].text": "Title: What are the rate limits for on-demand deployments?
Link: https://docs.fireworks.ai/faq-new/deployment-infrastructure/what-are-the-rate-limits-for-on-demand-deployments
Content: Request throughput scales with your GPU allocation. Base allocations include: Up to 8 A100 GPUs Up to 8 H100 GPUs On-demand deployments offer several advantages: Predictable pricing based on time units, not token I/O Protected latency and performance , independent of traffic on the serverless platform Choice of GPUs , including A100s and H100s Need more GPUs? Contact us to discuss higher allocations for your specific use case.

Request throughput scales with your GPU allocation. Base allocations include: Up to 8 A100 GPUs Up to 8 H100 GPUs On-demand deployments offer several advantages: Predictable pricing based on time units, not token I/O Protected latency and performance , independent of traffic on the serverless platform Choice of GPUs , including A100s and H100s Need more GPUs? Contact us to discuss higher allocations for your specific use case.

What are the rate limits for on-demand deployments?
Request throughput scales with your GPU allocation. Base allocations include: Up to 8 A100 GPUs Up to 8 H100 GPUs On-demand deployments offer several advantages: Predictable pricing based on time units, not token I/O Protected latency and performance , independent of traffic on the serverless platform Choice of GPUs , including A100s and H100s Need more GPUs? Contact us to discuss higher allocations for your specific use case.

",
    "$.messages[7].content[7].type": "text",
    "$.messages[7].content[8].text": "Title: Create a deployment
Link: https://docs.fireworks.ai/tools-sdks/firectl/commands/create-deployment
Content: Creates a new deployment.

Creates a new deployment.

Creates a new deployment. firectl create deployment [flags]

",
    "$.messages[7].content[8].type": "text",
    "$.messages[7].content[9].text": "Title: Monitoring
Link: https://docs.fireworks.ai/guides/prompt-caching
Content: For dedicated deployments, information about prompt caching is returned in the
response headers. The header fireworks-prompt-tokens contains the number of tokens
in the prompt, out of which fireworks-cached-prompt-tokens are cached. Aggregated metrics are also available in the usage dashboard.

For dedicated deployments, information about prompt caching is returned in the
response headers. The header fireworks-prompt-tokens contains the number of tokens
in the prompt, out of which fireworks-cached-prompt-tokens are cached. Aggregated metrics are also available in the usage dashboard.

Migration and traffic management
When migrating between deployments that use prompt caching, it's crucial to implement proper traffic routing to maintain optimal cache hit rates. When gradually routing traffic to a new deployment, use consistent user/session-based sampling rather than random sampling. Here's the recommended implementation for traffic routing: Avoid random sampling for traffic routing as it can negatively impact cache hit rates: # Don't do this:
if random() < fireworks_traffic_fraction:  # ❌ Reduces cache effectiveness
  send_to_new_deployment(user=hashed_user_id)

",
    "$.messages[7].content[9].type": "text",
    "$.messages[7].role": "tool",
    "$.messages[7].tool_call_id": "functions.search:0",
    "$.messages[8].content": "Based on my research, I can provide you with a comprehensive guide on managing deployments on Fireworks. Let me break this down into key areas:

## Types of Deployments

Fireworks supports two main deployment types:

### 1. Serverless Deployments
- **Managed by Fireworks**: Popular base models hosted on shared infrastructure
- **Billing**: Pay-per-token usage
- **No configuration needed**: Fireworks handles all infrastructure
- **Rate limits**: Subject to platform-wide limits
- **Best for**: Intermittent usage, prototyping, or low-volume applications

### 2. Dedicated Deployments
- **Your private deployment**: Reserved exclusively for your use
- **Billing**: GPU-second basis (charged for active GPU time)
- **Full control**: Configure hardware, scaling, and model selection
- **No rate limits**: Subject only to your GPU allocation capacity
- **Best for**: High-volume applications, consistent performance requirements

## Deployment Management Tools

### Command Line Interface (firectl)

**Installation and Setup:**
\`\`\`bash
# Install firectl (assuming you have it configured)
# Set your account ID to avoid "invalid id" errors
export FIREWORKS_ACCOUNT_ID=your-account-id
\`\`\`

**Basic Deployment Commands:**

\`\`\`bash
# Create a new deployment
firectl create deployment <MODEL_NAME>

# Create deployment with specific GPU count
firectl create deployment <MODEL_NAME> --accelerator-count 2

# Create deployment for LoRA fine-tuned model
firectl create deployment "accounts/fireworks/models/<LORA_MODEL_ID>"

# Check deployment status
firectl get deployment accounts/your-account/deployment/<DEPLOYMENT_ID>

# List all deployments
firectl list deployments

# Update deployment configuration
firectl update deployment <DEPLOYMENT_ID> --accelerator-count 4

# Delete a deployment
firectl delete deployment <DEPLOYMENT_ID>
\`\`\`

### Python SDK (Build SDK)

**Installation:**
\`\`\`bash
pip install fireworks-ai
\`\`\`

**Basic Usage:**
\`\`\`python
from fireworks import Fireworks

# Initialize client
client = Fireworks(api_key="your-api-key")

# Create deployment configuration
from fireworks.client import LLM

# For on-demand deployments (requires .apply())
llm = LLM(
    model="accounts/your-account/models/your-model",
    deployment_type="on-demand"
)
llm.apply()  # Required for on-demand deployments

# For serverless deployments
llm = LLM(model="accounts/fireworks/models/llama-v3p1-8b-instruct")

# Scale management
llm.scale_to_1_replica()  # Ensure at least 1 replica
llm.scale_to_zero()       # Scale down to 0 replicas
\`\`\`

## Deployment Lifecycle Management

### 1. Creation and Initial Setup
\`\`\`python
# Example: Creating a comprehensive deployment
deployment_config = {
    "model": "accounts/your-account/models/your-model",
    "deployment_type": "on-demand",
    "accelerator_type": "A100",  # or "H100"
    "accelerator_count": 2,
    "min_replicas": 0,  # Scale to zero when not used
    "max_replicas": 4,  # Maximum scaling
    "autoscaling": True
}
\`\`\`

### 2. Monitoring and Scaling
\`\`\`python
# Check deployment status
deployment = llm.get_deployment()
print(f"Status: {deployment.status}")
print(f"Replicas: {deployment.replicas}")

# Monitor usage and costs
# Check the usage dashboard at https://app.fireworks.ai/dashboard/usage
\`\`\`

### 3. Traffic Management During Migration
\`\`\`python
import hashlib

# Implement consistent traffic routing for cache optimization
def route_traffic(user_id, traffic_fraction=0.2):
    """Route traffic consistently based on user ID for cache optimization"""
    hashed_user_id = int(hashlib.md5(user_id.encode()).hexdigest(), 16)
    max_hash = 2**128 - 1
    ratio = hashed_user_id / max_hash
    
    if ratio < traffic_fraction:
        return "new_deployment"
    else:
        return "old_deployment"

# Usage
deployment_choice = route_traffic("user-123", traffic_fraction=0.3)
\`\`\`

## Cost Management Best Practices

### 1. Autoscaling Configuration
\`\`\`python
# Configure autoscaling to minimize costs
deployment_config = {
    "min_replicas": 0,  # Scale to zero when idle
    "max_replicas": 8,  # Maximum GPUs
    "target_cpu_utilization": 0.7,  # Scale up at 70% utilization
    "scale_down_delay": 300  # Wait 5 minutes before scaling down
}
\`\`\`

### 2. Monitoring and Cleanup
\`\`\`bash
# Regular monitoring commands
firectl list deployments  # See all active deployments
firectl get deployment <ID> --watch  # Monitor specific deployment

# Clean up unused deployments
firectl delete deployment <DEPLOYMENT_ID>
\`\`\`

### 3. Reserved Capacity
For high-volume usage, consider reserved capacity instead of on-demand:
- More reliable capacity
- Higher GPU quotas
- Better pricing for consistent usage

## Advanced Features

### Custom Base Models
\`\`\`bash
# Upload custom model
firectl create model <MODEL_ID> /path/to/model/files/

# Deploy custom model
firectl create deployment <MODEL_ID>
\`\`\`

### Multi-GPU Deployments
\`\`\`bash
# Deploy with multiple GPUs
firectl create deployment <MODEL_NAME> --accelerator-count 4

# Update GPU count
firectl update deployment <DEPLOYMENT_ID> --accelerator-count 8
\`\`\`

### LoRA Model Deployment
\`\`\`bash
# Single LoRA deployment (faster)
firectl create deployment "accounts/fireworks/models/<LORA_MODEL_ID>"

# Multi-LoRA deployment (shared base model)
firectl create deployment <BASE_MODEL_ID> --enable-addons
firectl load-lora <LORA_MODEL_ID> --deployment <DEPLOYMENT_ID>
\`\`\`

## Key Management Commands Summary

| Command | Purpose |
|---------|---------|
| \`firectl create deployment\` | Create new deployment |
| \`firectl get deployment\` | Check deployment status |
| \`firectl list deployments\` | List all deployments |
| \`firectl update deployment\` | Modify deployment settings |
| \`firectl delete deployment\` | Remove deployment |
| \`llm.apply()\` | Apply configuration (Python SDK) |
| \`llm.scale_to_zero()\` | Scale down to 0 replicas |
| \`llm.scale_to_1_replica()\` | Ensure at least 1 replica |

## Best Practices

1. **Monitor Costs**: Regularly check your usage dashboard
2. **Use Autoscaling**: Configure deployments to scale to zero when not used
3. **Plan Migrations**: Use consistent traffic routing for cache optimization
4. **Choose Wisely**: Use serverless for intermittent usage, dedicated for consistent workloads
5. **Clean Up**: Delete unused deployments to avoid unnecessary charges
6. **Test Scaling**: Verify your deployment scales appropriately under load

Would you like me to dive deeper into any specific aspect of deployment management, such as autoscaling configuration, cost optimization strategies, or monitoring techniques?",
    "$.messages[8].role": "assistant",
    "$.pid": 63817,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T00:23:12.737811",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "analyze-political-poorly-air-hardware",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev22+gfb34cf0.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 30 highlighted sections (required: 15)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "15",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "download-look-quick-minute-study",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write an outline for a paper on the history of Yemeni coffee. The outline should include the main points of the paper, and at least 15 sections should be highlighted with markdown such as *highlighted section*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "# Outline: The History of Yemeni Coffee

1. **Introduction: Why Yemeni Coffee Matters**  
   - Thesis: Yemeni coffee is not merely a beverage but a cultural, economic, and geopolitical force that shaped global history.  
   - Scope: From its mythical discovery in 9th-century Yemen to its modern revival.

2. **Pre-Coffee Yemen: The Land, the People, and the Plant**  
   - Geography of the Yemeni highlands and terraced agriculture.  
   - Indigenous *Coffea arabica* varieties and early cultivation techniques.  
   - *The myth of the goatherd Kaldi and the dancing goats*.

3. **The Sufi Spark: Coffee as a Spiritual Elixir**  
   - 15th-century Sufi monasteries in Mocha and Taiz.  
   - *Qahwa as a tool for nocturnal dhikr rituals*.  
   - Early roasting, grinding, and brewing methods.

4. **Mocha: The Birthplace of a Global Commodity**  
   - Rise of the port city of Mocha (Al-Mukha) in the 16th century.  
   - *The Mocha trade network linking Yemen, Egypt, and the Ottoman Empire*.  
   - Coffeehouses as centers of intellectual and political discourse.

5. **The Ottoman Grip and the First Coffee Monopoly**  
   - 1536: Ottoman conquest of Yemen and control of coffee exports.  
   - *The ban on fertile beans to prevent cultivation elsewhere*.  
   - Smuggling and the first cracks in the monopoly.

6. **The Dutch Heist and the Global Spread of Coffee**  
   - 1616: Dutch traders smuggle Yemeni beans to Batavia (Java).  
   - *The birth of Dutch colonial coffee plantations*.  
   - How Mocha’s loss became the world’s gain.

7. **Coffeehouses of Cairo and Istanbul: Yemen’s Cultural Export**  
   - 1554: First Ottoman coffeehouse in Istanbul.  
   - *The “Schools of the Wise” in Cairo’s Khan el-Khalili*.  
   - Coffee as a vector for Yemeni poetry, music, and Sufi thought.

8. **The European Coffee Craze and the Mocha Brand**  
   - 17th-century European coffeehouses and the “Mocha” label.  
   - *The false association of Mocha with chocolate flavor*.  
   - Yemeni coffee’s role in the Enlightenment.

9. **Colonial Rivalries and the Decline of Yemeni Dominance**  
   - 18th-century: French, British, and Dutch plantations outcompete Yemen.  
   - *The collapse of Mocha’s monopoly and the rise of Caribbean and Latin American coffee*.  
   - Impact on Yemeni economy and society.

10. **Yemeni Coffee in the Age of Empire**  
   - 19th-century British and Ottoman interests in Yemeni coffee.  
   - *The introduction of new cash crops (qat) and their effect on coffee cultivation*.  
   - Decline in quality and quantity due to neglect and conflict.

11. **The Cold War and the Coffee Crisis**  
   - 20th-century Yemeni division (North/South) and Soviet/US influences.  
   - *The shift from coffee to qat in rural livelihoods*.  
   - Drought, pests, and civil wars further erode production.

12. **The Specialty Coffee Renaissance: Yemen’s Return to Glory**  
   - 21st-century global demand for single-origin, heirloom coffees.  
   - *The discovery of “Yemenia” varietals and their genetic uniqueness*.  
   - NGOs and cooperatives reviving traditional farming.

13. **Modern Challenges: War, Climate, and Market Access**  
   - 2015–present: Saudi-led conflict and blockade.  
   - *The siege of Hodeidah port and its impact on coffee exports*.  
   - Climate change and water scarcity in the highlands.

14. **Yemeni Coffee in the Cup: Flavor, Ritual, and Identity**  
   - Tasting notes: dried fruit, spice, and wine-like acidity.  
   - *The Yemeni coffee ceremony (bunna) and its social significance*.  
   - Diaspora communities keeping traditions alive abroad.

15. **Conclusion: The Future of Yemeni Coffee**  
   - Balancing heritage with innovation.  
   - *The role of Yemeni coffee in post-war reconstruction*.  
   - Final reflection: From the highlands of Sana’a to the cups of Brooklyn, Yemeni coffee remains a bridge between past and future.",
    "$.messages[1].role": "assistant",
    "$.pid": 68433,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T00:23:12.737854",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "analyze-political-poorly-air-hardware",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev22+gfb34cf0.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 20 highlighted sections (required: 5)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "5",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "delete-open-cold-poor-power",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a cover letter for a job at a local coffee shop in the form of a poem. Highlight at least 5 text sections using "*". For example: *3 years of experience*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "Dear Brewed Awakening,

I am the *early-bird steam* that curls above your grinders,  
the *latte-art heart* that beats in every porcelain curve.  
My apron is a canvas of *espresso freckles*,  
each dot a story told in *3 years of experience*  
from bean to cup, from dawn rush to dusk hush.

I know the *symphony of the steam wand*—  
its hiss, its sigh, its whispered crema secrets—  
and how to coax *microfoam galaxies*  
into orbit around a *double-shot comet*.  
I’ve tamped, poured, swirled, and smiled  
through *Saturday lines out the door*,  
keeping tempers cool as the milk I *steam to 140°*.

I speak fluent *small-town regular*:  
“Medium drip, room for kindness?”  
I remember that *Mrs. Alvarez likes oat milk*,  
that *Mr. Chen takes two raw sugars*,  
and that *your dog, Scout, prefers puppuccinos*.

I sweep, I stock, I *balance the till to the penny*,  
but mostly I *brew community*—  
one *extra-hot americano* at a time.  
Hire me, and the morning light  
will find me already *wiping down tables*,  
already *saving the last blueberry scone*  
for the kid who always forgets his wallet.

With *grounds in my veins* and *crema in my smile*,  
I remain,

Your future barista,  
Maya",
    "$.messages[1].role": "assistant",
    "$.pid": 68433,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T00:23:12.737882",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "analyze-political-poorly-air-hardware",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev22+gfb34cf0.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 10 highlighted sections (required: 5)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "5",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "analyze-visible-level-love-pleasure",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a casual blog post about similarities across animal species. Highlight at least 5 sections in your answer by starting and ending with "*", like: *highlighted text section*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "Hey friends! Grab a coffee and let’s talk about something that always blows my mind: *how much we all have in common, even when we look nothing alike*. I was scrolling through wildlife photos the other night and realized that a jellyfish and I are basically running the same basic software—just with different packaging. So here’s a laid-back tour of five “wait, we do that too?!” moments across the animal kingdom.

*We all need our beauty sleep*  
Turns out, every animal with a nervous system—from fruit flies to blue whales—shares a version of sleep. Crows nap on tree branches, octopuses snooze in coconut-shell beds, and even roundworms have “lethargus,” which is basically worm REM. Scientists think sleep is so ancient that the last common ancestor of all animals already had it. So next time you hit snooze, remember you’re honoring a 700-million-year-old tradition.

*Love songs come in many dialects*  
Whether you’re a human humming in the shower or a humpback whale belting out 30-minute ballads, *we all use rhythm and pitch to flirt and bond*. Frogs croak in choruses, nightingales remix their playlists nightly, and male mice sing ultrasonic “pickup lines” too high for us to hear. Same goal, different instruments.

*Grandma knew best: family matters*  
Elephants babysit their nieces, wolves bring back take-out for auntie, and *clownfish dads are the ultimate stay-at-home fathers*. Across species, kinship networks boost survival. Even bacteria split into “sibling” lineages that cooperate—so the next time your cousin crashes on your couch, call it evolutionary strategy.

*Stress looks the same on everyone*  
When life gets spicy, our bodies flip the same ancient switch. *Cortisol floods fish, birds, and humans alike*, making hearts race and muscles twitch. Zebra finches lose their appetite during tough times, and stressed-out bees become pessimistic (yes, they can test bee mood). So if you’re doom-scrolling at 2 a.m., know that a stressed squirrel in your backyard totally gets you.

*Play isn’t optional—it’s homework for life*  
Otters juggle rocks, dogs zoom in circles, and young ravens snowboard down snowy roofs using jar lids. *Play builds brains and social skills across the board*. Researchers have spotted reptiles “wrestling” for fun and octopuses repeatedly bouncing Lego blocks like toddlers. If it has neurons, it probably needs recess.

Next time you feel alone in the universe, remember that somewhere a parrot is probably preening its best friend, a bat is sharing food with a hungry roost-mate, and a tiny nematode is curling up for a nap. We’re all just variations on a very old, very shared theme.",
    "$.messages[1].role": "assistant",
    "$.pid": 68433,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T00:23:12.737906",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "analyze-political-poorly-air-hardware",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev22+gfb34cf0.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 7 highlighted sections (required: 5)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "5",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "run-put-natural-traditional-thought",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Generate a business proposal to start a sweatshirt company in Bremen. The proposal should contain 5 or more sections. Highlight each section name using the this format:
*section name*",
    "$.messages[0].role": "user",
    "$.messages[1].content": "*Executive Summary*

Bremen is a city where maritime heritage meets cutting-edge creativity. Our venture, “NordHood Bremen,” will launch a premium, design-driven sweatshirt brand that fuses Hanseatic culture with contemporary streetwear. In year one we will release three limited-edition drops (total 6,000 units), achieve €420,000 in revenue, and reach break-even by month 14. The brand will be 100 % online-direct at launch, then expand into two Bremen concept stores and select European boutiques in years two and three. We will operate carbon-neutral, use 100 % organic cotton, and reinvest 3 % of profits into local youth art programs—turning every sweatshirt into a wearable piece of Bremen pride.

*Market Analysis*

Bremen’s 0.57 million residents and 500,000 annual tourists create a €35 million addressable mid-price sweatshirt market. Key insights:  
• 68 % of 18-35-year-olds prefer streetwear that tells a local story (Hochschule Bremen Trend Report 2023).  
• Tourists spend €180 million on souvenirs, yet only 11 % are apparel items—an untapped niche.  
• Competitor scan: global brands (Nike, Adidas) dominate functionality; regional labels (Hamburg’s “Dinghy,” Hannover’s “Liguster”) lack Bremen specificity.  
• Online D2C share in German apparel has grown from 18 % (2019) to 31 % (2023), proving the viability of our launch model.  
We will target two segments: “Culture Creatives” (students, designers, musicians) and “Heritage Seekers” (tourists, expats). Both segments value authenticity, limited editions, and sustainability.

*Product & Brand Strategy*

Signature Product: The “Hanse Hood” – 460 g brushed organic-cotton fleece, oversized fit, dropped shoulders, double-stitched flat seams, and a hidden inside pocket sized for a Bremen transit card.  
Design Language: Minimalist silhouettes in maritime colorways (Bremen Blue, Weser Sand, Harbor Grey) with subtle iconography—windrose embroidery, cog-ship sleeve print, and a QR code linking to an AR story of the garment’s journey.  
Sustainability: GOTS-certified cotton, recycled polyester labels, biodegradable mailers, and a take-back program offering 15 % discount on next purchase.  
Drop Model: Three seasonal drops per year (Spring “Hanse Sail,” Fall “Schlachte,” Winter “Weihnachtsmarkt”), each capped at 2,000 units to maintain scarcity.  
Brand Voice: “Wear the City.” Every hangtag includes a Bremen insider tip curated by local artists, turning the sweatshirt into a cultural guide.

*Operations & Supply Chain*

Design Studio: 120 m² loft in Überseestadt (rent €1,800/month) doubling as photo studio and event space.  
Manufacturing: Partnership with a Fair-Wear-audited factory in Porto, Portugal (MOQ 500, 30-day lead time, €18 landed cost per unit).  
Logistics: Bremen-based 3PL “Logwin” for pick-pack-ship (€1.20 per parcel, same-day dispatch).  
Tech Stack: Shopify Plus storefront, Klarna & PayPal checkout, Shipcloud integration, and a custom AR filter for Instagram try-ons.  
Quality Control: In-house inspection on arrival; defective rate target <1 %.  
Risk Mitigation: Dual-source yarn from Turkey and Portugal; maintain 8-week safety stock.

*Marketing & Sales Plan*

Pre-Launch (Months 1-2):  
• Teaser campaign on TikTok & Instagram Reels (#NordHoodBremen) with local influencers (5-50 k followers).  
• Pop-up try-on booth at Viertel Kreativmarkt; collect 1,000 email sign-ups.  
Launch (Month 3):  
• “Midnight Drop” online at 00:07 (Bremen dialing code) with countdown timer.  
• Limited 100 “Founders’ Edition” sweatshirts laser-numbered and signed by Bremen street artist “PlotBot.”  
Growth (Months 4-12):  
• Collaborations: capsule with SV Werder Bremen fan design; co-branded hoodie with Überseemuseum.  
• Referral program: give €10, get €10.  
• Tourist channel: partnership with Bremer Touristik-Zentrum for QR-coded city map bundled with sweatshirt.  
KPIs: CAC ≤ €18, repeat purchase rate ≥ 35 %, Instagram engagement ≥ 8 %.

*Financial Plan*

Start-up Capital Needed: €85,000  
• Inventory (3,000 units): €54,000  
• Marketing & content: €12,000  
• E-commerce & IT: €6,000  
• Legal, trademark, insurance: €5,000  
• Working capital buffer: €8,000  
Funding: 60 % founder equity, 40 % Bremen Förderbank micro-loan at 1.9 % interest.  
Year-1 P&L (conservative):  
Revenue €420,000 (avg. €70 net per unit)  
COGS €234,000  
Gross Profit €186,000  
Operating Expenses €150,000  
EBIT €36,000  
Break-even: 1,800 units or month 14.  
Three-year projection: revenue €1.4 million, EBITDA margin 18 %, reinvestment into two Bremen stores and EU wholesale.

*Appendices*

A. Mood-board & color palette  
B. Factory audit certificate  
C. Sample influencer contracts  
D. Detailed 36-month cash-flow model",
    "$.messages[1].role": "assistant",
    "$.pid": 68433,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T00:23:12.737929",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "analyze-political-poorly-air-hardware",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev22+gfb34cf0.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 29 highlighted sections (required: 3)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "3",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "sit-disassemble-ready-area-problem",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a funny and sarcastic template for rating the quality of a marriage between two people who are both moms. This is for the couple themselves. Please highlight at least 3 sections with markdown,  i.e *highlighted section*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "**The Official “Two-Mom Marriage Quality Report Card”**  
(For internal use only. If this ends up on Facebook, we *will* change the Wi-Fi password.)

---

**1. Communication Score: ___/10**  
*“Did we just have a 45-minute conversation about whose turn it is to buy more goldfish crackers, or was that a TED Talk?”*  
- 10 = We finish each other’s sentences… and each other’s coffee.  
- 5 = We communicate exclusively via passive-aggressive Post-it notes on the fridge.  
- 0 = We’ve resorted to interpretive dance and/or carrier pigeons.  

**Bonus points** if you’ve ever used the kids as tiny, sticky messengers (“Tell Mommy she’s *cordially invited* to take out the trash”).

---

**2. Chore Wars Score: ___/10**  
*“The laundry basket is a Jenga tower of lies.”*  
- 10 = We’ve achieved a chore utopia where socks actually match.  
- 5 = We’ve agreed to ‘divide’ labor… by one of us doing everything and the other ‘supervising’.  
- 0 = The dishwasher has become a permanent storage unit.  

**Penalty** if either mom has ever ‘forgotten’ how to load the dishwasher *again*.

---

**3. Co-Parenting Harmony Score: ___/10**  
*“We tag-team tantrums like WWE pros.”*  
- 10 = We’ve synchronized our ‘Mom Voice’ to subsonic frequencies that terrify small children.  
- 5 = We’ve debated whether ‘screen time’ is a human right or a war crime.  
- 0 = We’ve weaponized bedtime stories (‘And then the princess *never* got dessert again…’).  

**Extra credit** for every time you’ve high-fived over a successful *“Because I said so”* deployment.

---

**4. Romance Maintenance Score: ___/10**  
*“Date night: when we argue over which drive-thru has the shortest line.”*  
- 10 = We’ve scheduled intimacy between soccer practice and PTA meetings.  
- 5 = We’ve agreed that ‘Netflix and actually chill’ counts as foreplay.  
- 0 = Our last romantic gesture was sharing the last chicken nugget.  

**Automatic fail** if either of you has ever said, “I’m too tired”… *while scrolling Instagram*.

---

**5. Emergency Snack Reserves Score: ___/10**  
*“The pantry is a battlefield of empty granola bar boxes.”*  
- 10 = We’ve hidden chocolate so well we forgot where it is.  
- 5 = We’ve labeled snacks with fake allergen warnings to deter theft.  
- 0 = The kids have unionized and are demanding better rations.  

**Gold star** if you’ve ever *texted each other* to brag about finding the secret stash.

---

**Final Grade: ___/50**  
- 40–50: *Power couple.* You’re basically the Beyoncé and Beyoncé of lesbian moms.  
- 25–39: *Functioning chaos.* You’ve accepted that love is just shared exhaustion.  
- 0–24: *Renew your vows… or your Costco membership.* Either way, you’re in this together.  

**Disclaimer:** This report is not legally binding, but hiding the good snacks *is* grounds for divorce.",
    "$.messages[1].role": "assistant",
    "$.pid": 68433,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T00:23:12.737956",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "analyze-political-poorly-air-hardware",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev22+gfb34cf0.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 1 highlighted sections (required: 1)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "1",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "affect-unknown-long-life-month",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a song about the summers of my childhood that I spent in the countryside. Give the song a name, and highlight the name by wrapping it with *. For example: *little me in the countryside*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "*little me in the countryside*

[Verse 1]  
Barefoot on the gravel road, dust between my toes,  
Sunlight dripped like honey on the clover and the crows.  
Grandma’s porch swing creaked a lullaby so slow,  
While the creek below kept singing songs only we could know.  
Fireflies stitched the twilight with lanterns made of stars,  
And the moon was just a silver coin tossed into cookie jars.  

[Chorus]  
Oh, little me in the countryside, running wild and free,  
Chasing dreams through cornfield mazes, climbing every tree.  
Every laugh was louder than the thunder rolling by,  
In the summers of my childhood, underneath that endless sky.  

[Verse 2]  
Mason jars of sweet tea sweating on the window ledge,  
Radio played “Sweet Home Alabama” from the rusty garden shed.  
We built rafts from old barn doors and floated down the stream,  
Kingfishers were pirates and the cattails were our team.  
Supper was tomatoes sliced like sunsets on a plate,  
And the crickets kept the tempo while the night got late.  

[Chorus]  
Oh, little me in the countryside, running wild and free,  
Skinny-dipping in the moonlight where the river bends to see.  
Every scar was a medal from a dare we’d never hide,  
In the summers of my childhood, where the earth and I collided.  

[Bridge]  
Dandelion clocks we blew to tell the time,  
Each puff a tiny parachute for wishes we would climb.  
We swore we’d never grow up, just grow into the land,  
But the school bus came like autumn and took us by the hand.  

[Verse 3]  
Now the fields are sold and paved, the barn’s a memory store,  
But I still hear the wheat heads whisper, “Come back, come back once more.”  
I close my eyes and I’m ten again, chasing dragonflies,  
With the smell of rain on dirt roads and the taste of cherry pies.  

[Final Chorus]  
Oh, little me in the countryside, you’re still inside my chest,  
A pocket full of wonder that the city can’t arrest.  
Though the years roll on like tractors down a dusty county ride,  
I keep those summers in a mason jar where the fireflies never die.  

[Outro]  
So here’s to cicadas, to tire swings and creek-stone slides,  
To the echo of my laughter in the hollow where time hides.  
If you find that gravel road, kick off your shoes and glide—  
You’ll meet little me in the countryside, still barefoot, still wide-eyed.",
    "$.messages[1].role": "assistant",
    "$.pid": 68433,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T00:23:12.738015",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "analyze-political-poorly-air-hardware",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev22+gfb34cf0.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 3 highlighted sections (required: 3)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "3",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "believe-verify-likely-desire-power",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a riddle for the word "façade" that contains at least 3 italic text phrases in markdown syntax, i.e *italic text*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "I wear a *painted smile* for every passer-by,  
a *perfectly symmetrical* face that never cracks.  
Behind me, the *crumbling bricks* sigh in secret.  
What am I?",
    "$.messages[1].role": "assistant",
    "$.pid": 68433,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T00:23:12.738042",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "analyze-political-poorly-air-hardware",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev22+gfb34cf0.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 19 highlighted sections (required: 3)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "3",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "change-sort-serious-song-family",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a template for a workshop on the importance of diversity in the workplace and highlight at least 3 sections with markdown, i.e. *highlighted section*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "# Workshop Template  
**Title:** *Building Inclusive Excellence: The Business & Human Case for Workplace Diversity*  
**Duration:** 3.5 hours (can be split into two 90-minute sessions)  
**Audience:** Managers, HR professionals, team leads, ERG members, and any employee who influences culture  

---

## 1. Welcome & Objectives (15 min)
- Ice-breaker: “Two Truths & a Lie—Diversity Edition”  
- *Learning objectives*: By the end of this workshop, participants will be able to:  
  1. Define diversity, equity, inclusion, and belonging (DEIB) in their own words.  
  2. Articulate at least three evidence-based benefits of a diverse workforce.  
  3. Identify one action they can take within 30 days to foster inclusion on their team.  

---

## 2. *The Data-Driven Case for Diversity* (25 min)
- Interactive poll: “What % performance lift do you think diverse teams achieve?”  
- Mini-lecture with live charts:  
  - McKinsey 2023 report: Top-quartile companies for ethnic/gender diversity are **39 % more likely to outperform** on profitability.  
  - Harvard study: *Inclusive teams make better business decisions 87 % of the time* and do so twice as fast.  
- Small-group exercise: Match the statistic to the business outcome (revenue, innovation, retention).  

---

## 3. *Unconscious Bias & Micro-behaviors* (30 min)
- 5-minute video: “The Resume with Two Names” (identical CVs, different callbacks).  
- Bias Busters card game: Participants draw scenario cards and practice interrupting bias in real time.  
- *Key takeaway*: Bias is not a moral failing; it is a neurological shortcut that can be rewired through deliberate habits.  

---

## 4. *Inclusive Leadership Toolkit* (40 min)
- **The 4 A’s Framework**:  
  1. **Awareness** – Daily 3-minute bias check-in.  
  2. **Amplification** – Repeat & credit ideas from under-represented voices.  
  3. **Access** – Rotate high-visibility assignments.  
  4. **Accountability** – Track inclusion metrics on team dashboards.  
- Role-play: “The Missed Promotion” – diagnose where each ‘A’ broke down and redesign the conversation.  

---

## 5. Action Planning & Commitments (20 min)
- Personal *Inclusion Commitment Card*: “In the next 30 days I will…”  
- Pair-share for accountability.  
- QR code to a digital resource hub (TED talks, reading list, policy templates).  

---

## 6. Closing & Evaluation (10 min)
- One-word checkout: “How do you feel now?”  
- *Post-workshop survey* link (2 min) with prize draw for completion.  

---

## Materials Checklist
- Slide deck (with alt-text on all images)  
- Printed Commitment Cards & pens  
- Bias Busters card decks (laminated)  
- Timer & bell  
- Snacks labeled for common dietary restrictions (gluten-free, vegan, halal)  

---

## Follow-Up Email Template (send within 24 hrs)
Subject: Your Inclusion Toolkit + Next Steps  
Body:  
Hi team,  
Thank you for leaning into today’s conversation on *why diversity wins*. Attached are:  
- Slide deck & resource links  
- Your signed Inclusion Commitment Card (photo reminder)  
- Calendar invite for the optional 30-day check-in circle  

Remember: Inclusion is a practice, not a one-time training. Let’s keep the momentum!",
    "$.messages[1].role": "assistant",
    "$.pid": 68433,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T00:23:12.738068",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "analyze-political-poorly-air-hardware",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev22+gfb34cf0.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 21 highlighted sections (required: 3)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "3",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "must-write-need-theoretical-pleasure",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a cover letter to a local political party, asking to be their rally organizer. Make sure to highlight at least 3 sections in your answer in markdown format.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "**[Your Name]**  
[Your Address]  
[City, State ZIP]  
[Email Address] | [Phone Number] | [LinkedIn URL]  
[Date]

**[Local Party Chairperson’s Name]**  
[Local Party Name]  
[Party Headquarters Address]  
[City, State ZIP]

Dear **Chairperson [Last Name]**,

I am writing to express my enthusiastic interest in serving as **Rally Organizer** for **[Local Party Name]**. As a lifelong resident of **[Your City/County]** and a committed progressive, I have spent the past six years turning grassroots energy into measurable voter engagement. I believe my proven record of mobilizing thousands of supporters, securing high-profile speakers, and running flawless logistics makes me the ideal candidate to lead our 2024 rally program.

### **Proven Rally & Event Expertise**
- **2018 & 2020 Midterm & Presidential Rallies**: Planned and executed 14 rallies across three counties, averaging 2,500 attendees each.  
- **Permits & Safety**: Obtained all municipal permits within 72 hours, coordinated with local fire and police departments, and maintained a 100 % safety record.  
- **Stage & AV Management**: Negotiated vendor contracts that cut AV costs by 27 % while increasing sound coverage by 40 %.

### **Community & Coalition Building**
- **Diverse Coalitions**: Forged partnerships with 18 labor unions, 9 student groups, and 7 faith organizations, expanding our volunteer base by 1,200 active members.  
- **Multilingual Outreach**: Produced bilingual signage and volunteer scripts, increasing turnout in Latinx precincts by 31 %.  
- **Accessibility**: Installed ADA-compliant ramps, ASL interpreters, and quiet zones at every event, earning commendation from the local Disability Rights chapter.

### **Data-Driven Strategy & Rapid Response**
- **Real-Time Metrics**: Deployed a mobile check-in app that captured 94 % of attendee data, feeding our CRM for targeted follow-up.  
- **Weather Contingencies**: Created a “Plan B” playbook that moved a 3,000-person outdoor rally indoors in under 90 minutes with zero no-shows.  
- **Crisis Communications**: Served as on-site media liaison, turning a last-minute speaker cancellation into a prime-time local news segment featuring our grassroots volunteers.

I would welcome the opportunity to discuss how my experience, energy, and strategic vision can help **[Local Party Name]** stage rallies that not only energize our base but also convert enthusiasm into votes. I am available at your convenience for an interview and can provide a detailed rally calendar and budget forecast within 48 hours of request.

Thank you for your time and consideration. I look forward to amplifying our shared values across **[Your City/County]**—one rally at a time.

In solidarity,

**[Your Name]**",
    "$.messages[1].role": "assistant",
    "$.pid": 68433,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T00:23:12.738096",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "analyze-political-poorly-air-hardware",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev22+gfb34cf0.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 5 highlighted sections (required: 2)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "2",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "control-accept-record-legal-car",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "The opposite of youth is not age, but ...? Highlight at least 2 sections in your answer with markdown, i.e. *highlighted section*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "The opposite of youth is not age, but **disillusionment**—the moment when *wonder* is traded for *weariness*, when the *boundless horizon* of possibility collapses into the *narrow corridor* of routine.",
    "$.messages[1].role": "assistant",
    "$.pid": 68433,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T00:23:12.738119",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "analyze-political-poorly-air-hardware",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev22+gfb34cf0.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 5 highlighted sections (required: 3)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "3",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "miss-continue-fall-local-work",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a description of the following data in a weird style: The Golden Palace eatType restaurant; The Golden Palace food Indian; The Golden Palace area city centre. Use markdown to highlight at least 3 sections in your answer.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "**The Golden Palace** is not a palace, but it *is* golden—at least in spirit—nestled like a curry-scented jewel in the **city centre**, where neon pigeons peck at samosa crumbs and the air hums with sitar-shaped traffic lights.  

Inside, **Indian** food performs acrobatics on your tongue: cardamom cartwheels, turmeric tumbles, and a rogue chili that once tried to elope with a mango lassi. The waiters glide like buttered shadows, whispering orders to the kitchen where naan breads inflate like edible balloons plotting a coup against gravity.  

At **The Golden Palace**, every table is a throne, every plate a miniature map of the subcontinent, and every bite a teleportation device disguised as dinner.",
    "$.messages[1].role": "assistant",
    "$.pid": 68433,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T00:23:12.738142",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "analyze-political-poorly-air-hardware",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev22+gfb34cf0.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 14 highlighted sections (required: 1)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "1",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "require-affect-major-logical-business",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a funny rap about a man who gets a call from an official saying that he is a long lost relative of the king of Nigeria. Use markdown to highlight at least one section of your answer, i.e. *highlighted section*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "🎤 *Yo, it’s the tale of Sir Scam-a-lot, straight outta suburbia!* 🎤  

Yo, my name is Dave, I sell insurance in Ohio,  
Got a cubicle, a cat, and a 401(k) that’s kinda fly-o.  
One Tuesday, phone rings—unknown number, what the heck?  
Voice says, “*Prince Ogbonna the Third* just wrote you a fat check!”  

I’m like, “Hold up, who dis? I’m whiter than printer paper,  
My bloodline’s mayo on Wonder Bread—ain’t no royal vapor!”  
Dude on the line, accent silkier than lotion,  
Says, “Cousin, you’re 37th in line—time to claim your ocean!”  

**[Hook]**  
🎶 *Long lost king of Nigeria, crown sittin’ on my dome,*  
*Got a palace full of leopards and a Wi-Fi that roams!*  
*They need my SSN and a quick wire fee,*  
*But I’m ‘bout to be big pimpin’ in Abuja, just wait and see!* 🎶  

He says, “Just send five grand for customs, then the jet lands,  
You’ll be dripping in so much gold, even your sweat glands bling.”  
I’m sweatin’ like a rotisserie, visions of caviar dreams,  
I Venmo “PrinceNaija69” like I’m funding meme schemes.  

Next day, email: “Oops, palace goat ate the transfer slip,  
Need ten more for goat surgery and a diamond-encrusted whip.”  
I hock my lawnmower, my Xbox, and Aunt Linda’s porcelain goose,  
Wire cash so fast my bank’s like, “Dave, you loose!”  

**[Bridge – Spoken Word]**  
Then the *official royal seal* arrives—  
It’s a GIF of Shrek in a crown doing the Macarena.  
I’m starting to sense a pattern here…  

**[Final Hook – Regret Remix]**  
🎶 *Now I’m the king of my mom’s basement, throne’s a beanbag chair,*  
*Royal decree: never trust a prince who types in all caps—*  
*“URGENT, COUSIN, SEND MORE FLAMINGOS!”*  
*Lesson learned: if the crown’s too shiny, it’s probably aluminum foil, homie!*",
    "$.messages[1].role": "assistant",
    "$.pid": 68433,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T00:23:12.738169",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "analyze-political-poorly-air-hardware",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev22+gfb34cf0.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 6 highlighted sections (required: 2)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "2",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "could-serious-newspaper-radio-group",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Expand the riddle into a story with a funny tone:

What can you catch but not throw?
A cold

Use * to highlight at least 2 sections in your text. For example: *this is a highlighted text section*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "Gary the Germ was the *smallest, greenest, and most obnoxiously proud* microbe in the entire Sniffle Kingdom. Every morning he practiced his grand entrance: sliding down banisters made of nasal hair, cannon-balling into unsuspecting tonsils, and yelling, “ACHOO—did somebody order a cold?”  

One Tuesday, Gary spotted his dream target: *Bartholomew “Bart” Bigglesworth, a man who believed hand-washing was a government conspiracy*. Bart had just exited the subway, triumphantly licking a subway pole “for the vitamins.” Gary rubbed his microscopic hands together. “Perfect. I’ll hitch a ride on that finger, ride the express elevator to the sinuses, and throw the *snottiest* house-warming party in history!”  

But there was a catch—literally. Gary could *catch* Bart, but he couldn’t *throw* him anywhere. Every time Gary tried to catapult Bart into the next office cubicle, Bart just sneezed, wiped his nose on his sleeve, and kept typing spreadsheets. Gary screamed, “This is mutiny! I’m supposed to be the one doing the tossing!”  

Meanwhile, Bart’s immune system—led by Captain Phlegm and the Mucus Marines—launched *Operation Tissue Storm*. They surrounded Gary, armed with vitamin C grenades and zinc zappers. Gary attempted a dramatic escape by surfing a sneeze, but the tissue slapped him like a flyswatter on a summer day.  

Defeated, Gary sat in the crumpled tissue, sulking. “I caught the cold,” he muttered, “but I can’t throw it anywhere. Worst. Superpower. Ever.”  

From that day forward, Gary changed his business card to read: “Professional Cold Catcher—No Returns, No Refunds, Absolutely No Throwing.”",
    "$.messages[1].role": "assistant",
    "$.pid": 68433,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T00:23:12.738192",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "analyze-political-poorly-air-hardware",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev22+gfb34cf0.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 17 highlighted sections (required: 3)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "3",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "offer-select-free-state-moment",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a rap about the renaissance. It should be noticeably different from raps about other historical eras, and have an interesting or weird tone. Highlight at least 3 sections in your answer in markdown format.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "**[Intro – harpsichord glitch beat, stylus scratching parchment]**  
Yo, it’s 14-something, but my mind’s 3-D printed in gold leaf—  
I’m the **Renaissance Rapper**, got a quill in one hand and a scalpel in the other, dissectin’ beats like Vesalius dissectin’ spleens.  
Let’s get *weird* with the rebirth, baby.

---

### **Section 1 – The Fresco Flex**  
I drip ultramarine like I’m sponsored by Afghanistan mines,  
Michelangelo on the ceiling, I’m reclining on divine lines.  
Angels doing backflips off the Sistine, they’re hype for my set,  
While I tag “*AMOR VINCIT OMNIA*” in invisible ink—wet.  
My flow’s a trompe-l’œil, looks flat till you tilt your head—  
Then it’s depth for days, like Brunelleschi’s dome overhead.  
I’m not baroque, I’m pre-baroque, I’m the prototype shock,  
Got patrons in fur-lined sleeves stuffing florins in my sock.  
**I paint in egg tempera so the colors never fade,**  
**But the yolk smells funky—call it *odeur de old crusade*.**

---

### **Section 2 – The Printing-Press Poltergeist**  
Gutenberg ghost in the machine, clanking lead type like chains,  
Every page a polter-page, every serif a soul that remains.  
I drop a folio, it’s viral before “viral” was a cough,  
Monks in scriptoriums rage-quitting, “Bro, that illuminated stuff’s off!”  
I’m the incunabula influencer, my colophon’s a flex,  
Got red-ink rubricators sliding into my DMs—*next*.  
**I rhyme in movable type so my bars can rearrange,**  
**Yesterday I was Petrarch, today I’m deranged.**  
The press spits paper like a hydra with hiccups,  
Each sheet a protest flyer: “Down with scholastic pick-ups!”

---

### **Section 3 – The Anatomical Anthem (Parental Advisory: Corpses)**  
Night shift with da Vinci, we’re sketching by candle and moon,  
Cadavers on the table like, “Do me a favor—autotune my coccyx soon.”  
I rap in reverse Latin so only the dead understand,  
While Leo reverse-engineers a bird-man from a frying pan.  
**My beats are ventricles pumping four-four time,**  
**Aorta bass drop—call that circulatory grime.**  
I sample the sound of a scalpel scraping periosteum,  
Layer it over a lute loop—*that’s* my magnum opium.  
Galen’s in the corner, throwing shade like a Greek tragedy,  
But I’m Harvey before Harvey, already rapping blood circulation majestically.

---

**[Outro – fading lute arpeggio, distant cannon fire from Constantinople]**  
So remember me when you’re doom-scrolling on glass,  
I was the era that turned corpses into class.  
I’m the **Renaissance Rapper**, my ghost still tags in chalk,  
Leaving QR codes on cathedral walls—scan it, you’ll squawk.  
Rebirth never dies, it just pixelates and loops,  
Now if you’ll excuse me, I’ve got a date with Lucretius in the soup.",
    "$.messages[1].role": "assistant",
    "$.pid": 68433,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T00:23:12.738219",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "analyze-political-poorly-air-hardware",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev22+gfb34cf0.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 25 highlighted sections (required: 5)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "5",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "calculate-affect-fullest-emotional-money",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Summarize the history of Japan. Italicize at least 5 keywords in your response. To indicate a italic word, wrap it with asterisk, like *italic*",
    "$.messages[0].role": "user",
    "$.messages[1].content": "*Jōmon* hunter-gatherers (c. 14,000 BCE) gave way to *Yayoi* rice cultivators who forged bronze and iron tools.  
The *Yamato* court unified clans in the 3rd–4th centuries, adopting *Shintō* rituals and Chinese script.  
Prince *Shōtoku*’s 7th-century constitution and the Taika Reforms centralized power under an emperor, while *Nara* and *Heian* aristocrats refined literature and *Buddhism*.  
Samurai clans rose during the *Kamakura* shogunate (1185), repelling *Mongol* fleets and entrenching warrior rule.  
Centuries of *Ashikaga* and *Sengoku* civil war ended when *Oda Nobunaga*, *Toyotomi Hideyoshi*, and *Tokugawa Ieyasu* reunified Japan; the *Edo* shogunate (1603–1868) enforced isolation yet fostered *ukiyo-e* culture.  
Commodore *Perry*’s 1853 arrival triggered the *Meiji* Restoration, rapid modernization, and imperial expansion.  
Victory in the *Russo-Japanese* War (1905) emboldened militarism, leading to *Manchuria* occupation and *Pearl Harbor*.  
After atomic bombings and *Hirohito*’s surrender in 1945, U.S. occupation reformed politics and the economy, birthing the *post-war* “economic miracle.”  
Today, Japan balances *constitutional monarchy* with global technological leadership.",
    "$.messages[1].role": "assistant",
    "$.pid": 68433,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T00:23:12.738252",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "analyze-political-poorly-air-hardware",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev22+gfb34cf0.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 5 highlighted sections (required: 2)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "2",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "download-american-private-result-research",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a funny Haiku about a Quaker named John who lives in the town of De Smet, South Dakota. Use the asterisk symbol, *, to highlight some words or phrases twice. Example: *This is a highlighted phrase*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "*John* in *De Smet*  
Quaker hat *blows off*—chases  
*Silent* *scream*—“Come back!”",
    "$.messages[1].role": "assistant",
    "$.pid": 68433,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T00:23:12.738286",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "analyze-political-poorly-air-hardware",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev22+gfb34cf0.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 19 highlighted sections (required: 1)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "1",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "indicate-strong-single-festival-mother",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Today, at the 54th Annual Grammy Awards, the Recording Academy honors the talent and creativity of the artists, musicians, and producers who are the creators of the best recordings of the past year. Please continue writing this text in a formal tone, using notations. Highlight some key parts in your response with "*", like *highlighted text*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "Today, at the 54th Annual Grammy Awards, the Recording Academy honors the talent and creativity of the artists, musicians, and producers who are the creators of the best recordings of the past year. In a ceremony marked by *unprecedented artistic excellence* and *meticulous craftsmanship*, the Academy recognizes those whose work has defined the sonic landscape of 2011.

The evening’s proceedings underscore the Academy’s enduring commitment to *artistic integrity* and *technical innovation*. Among the most celebrated achievements are:

• *Album of the Year* – awarded to an opus that has redefined genre boundaries while achieving both critical acclaim and popular resonance.  
• *Record of the Year* – bestowed upon a single track whose *sonic clarity*, *emotive depth*, and *production mastery* have set a new benchmark for the industry.  
• *Best New Artist* – conferred upon an emerging voice whose *visionary artistry* signals the future direction of contemporary music.

Throughout the telecast, special tributes illuminate the *indelible legacies* of icons whose influence transcends generations, while collaborative performances showcase the *synergistic power* of cross-genre partnerships. The Academy further acknowledges the *behind-the-scenes luminaries*—engineers, mixers, and mastering specialists—whose *invisible expertise* transforms raw inspiration into *audible transcendence*.

In closing, the 54th Annual Grammy Awards reaffirm the Recording Academy’s solemn charge: to *safeguard the heritage*, *nurture the present*, and *inspire the future* of recorded music.",
    "$.messages[1].role": "assistant",
    "$.pid": 68433,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T00:23:12.738315",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "analyze-political-poorly-air-hardware",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev22+gfb34cf0.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 39 highlighted sections (required: 2)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "2",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "design-assume-past-camera-word",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a blog post about interesting facts about the Dutch language. Italicize at least 2 sections in your answer with markdown, i.e. *italic text*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "# 10 Surprising Facts About the Dutch Language That Will Make You Say “Wauw!”

If you think Dutch is just “German with a sore throat,” think again. From tongue-twisting consonant clusters to words that sound like they were invented by a stand-up comedian, Dutch is a linguistic playground. Below are ten facts that prove the language is far more colorful than its reputation for windmills and wooden shoes.

---

### 1. *The longest word in Dutch is 53 letters long*  
Meet **“kindercarnavalsoptochtvoorbereidingswerkzaamhedenplan”**—a single noun describing the preparation work for a children’s carnival parade. Dutch loves to glue words together, so once you master the basics you can create your own kilometer-long compounds on the fly.

---

### 2. Dutch has three genders, but only two articles  
Grammatically, nouns are masculine, feminine, or neuter. In everyday speech, however, masculine and feminine collapsed into a single “common” gender, leaving *“de”* for common and *“het”* for neuter. The result? A never-ending guessing game for learners.

---

### 3. *The Dutch alphabet ends with “ij,” not “z”*  
Schoolchildren recite the ABC and finish with “…x, y, ij, z.” The digraph *ij* is so common (think *hij* = he, *mijn* = my) that it’s treated almost like a separate letter. On old typewriters, *ij* even had its own key.

---

### 4. Dutch is the closest major language to English  
Frisian is technically closer, but among languages with tens of millions of speakers, Dutch wins. Compare:  
- English *“bread”* → Dutch *“brood”*  
- English *“green”* → Dutch *“groen”*  
Shared roots mean you can read simple Dutch signs long before you’ve cracked a single grammar book.

---

### 5. *Dutch gave New York its first words*  
Before it was New York, it was New Amsterdam. That’s why we say *“cookie”* (from Dutch *koekje*) instead of *“biscuit,”* and *“boss”* (from *baas*) instead of *“master.”* Wall Street? It once ran alongside the *“wal”* (rampart) of the city wall.

---

### 6. The word *“gezellig”* has no exact English translation*  
Roughly it means “cozy,” “convivial,” or “the warm feeling of being with friends,” but none of those capture the full vibe. A café can be *gezellig*, a conversation can be *gezellig*, even a spreadsheet can be *gezellig* if the team is laughing.

---

### 7. Dutch spelling changed overnight in 1996  
The *“Groene Boekje”* (Green Booklet) reforms simplified thousands of words. *“potatos”* became *“aardappelen”* and *“cat’s”* became *“katten.”* Overnight, every dictionary, street sign, and Scrabble board in the country was technically out of date.

---

### 8. *Dutch is spoken on six continents*  
Besides the Netherlands and Belgium, it’s an official language in Suriname, Aruba, Curaçao, and Sint Maarten. Thanks to emigration, you’ll also find active Dutch-speaking communities in Canada, Australia, Argentina—even South Africa, where Afrikaans is a daughter language.

---

### 9. The Dutch love diminutives  
Add *-je* to almost any noun and you instantly shrink it: *“huis”* (house) → *“huisje”* (little house). It’s not just about size; diminutives add affection. A *“biertje”* isn’t a small beer—it’s an invitation to relax.

---

### 10. *Dutch is the language of the world’s oldest multinational*  
The Vereenigde Oostindische Compagnie (VOC) was founded in 1602 and issued the planet’s first publicly traded stock. Its ledgers, contracts, and ship logs were all written in Dutch, making the language the original tongue of global capitalism.

---

Ready to dive in? Start with *hallo* and *dank je wel*, and you’ll already be halfway to *gezellig*.",
    "$.messages[1].role": "assistant",
    "$.pid": 68433,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T00:23:12.738340",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "analyze-political-poorly-air-hardware",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev22+gfb34cf0.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 5 highlighted sections (required: 1)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "1",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "express-prove-private-internet-goal",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Gideon is a farmer who has a surplus of crops from his farm this year. What might he do with that surplus? Highlight at least one section of your answer in markdown, i.e *highlighted section*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "Gideon has several good options for his surplus:

1. **Sell it at the local farmers’ market or to a wholesaler** to turn the extra produce into cash that can cover next season’s seed, fertilizer, and equipment costs.  
2. **Store some of the surplus in a community grain silo** so he can sell later when prices are higher.  
3. **Donate part of the surplus to the regional food bank**, helping families in need and possibly qualifying for a small tax deduction.  
4. **Use the surplus as livestock feed** for his own animals, cutting down on purchased feed expenses.  

*The most profitable short-term choice is usually selling the surplus immediately after harvest, when demand is strong and prices are favorable.*",
    "$.messages[1].role": "assistant",
    "$.pid": 68433,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T10:35:28.042894",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "stop-choose-expect-major-pain",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6-dev1.dev4+g67b03c6",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 30 highlighted sections (required: 15)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "15",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "should-speak-cold-film-festival",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write an outline for a paper on the history of Yemeni coffee. The outline should include the main points of the paper, and at least 15 sections should be highlighted with markdown such as *highlighted section*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "# Outline: The History of Yemeni Coffee

1. **Introduction: Why Yemeni Coffee Matters**  
   - Thesis: Yemeni coffee is not merely a beverage but a cultural, economic, and geopolitical force that shaped global history.  
   - Scope: From its mythical discovery in 9th-century Yemen to its modern revival.

2. **Pre-Coffee Yemen: The Land, the People, and the Plant**  
   - Geography of the Yemeni highlands and terraced agriculture.  
   - Indigenous *Coffea arabica* varieties and early cultivation techniques.  
   - *The myth of the goatherd Kaldi and the dancing goats*.

3. **The Sufi Spark: Coffee as a Spiritual Elixir**  
   - 15th-century Sufi monasteries in Mocha and Taiz.  
   - *Qahwa as a tool for nocturnal dhikr rituals*.  
   - Early roasting, grinding, and brewing methods.

4. **Mocha: The Birthplace of a Global Commodity**  
   - Rise of the port city of Mocha (Al-Mukha) in the 16th century.  
   - *The Mocha trade network linking Yemen, Egypt, and the Ottoman Empire*.  
   - Coffeehouses as centers of intellectual and political discourse.

5. **The Ottoman Grip and the First Coffee Monopoly**  
   - 1536: Ottoman conquest of Yemen and control of coffee exports.  
   - *The ban on fertile beans to prevent cultivation elsewhere*.  
   - Smuggling and the first cracks in the monopoly.

6. **The Dutch Heist and the Global Spread of Coffee**  
   - 1616: Dutch traders smuggle Yemeni beans to Batavia (Java).  
   - *The birth of Dutch colonial coffee plantations*.  
   - How Mocha’s loss became the world’s gain.

7. **Coffeehouses of Cairo and Istanbul: Yemen’s Cultural Export**  
   - 1554: First Ottoman coffeehouse in Istanbul.  
   - *The “Schools of the Wise” in Cairo’s Khan el-Khalili*.  
   - Coffee as a vector for Yemeni poetry, music, and Sufi thought.

8. **The European Coffee Craze and the Mocha Brand**  
   - 17th-century European coffeehouses and the “Mocha” label.  
   - *The false association of Mocha with chocolate flavor*.  
   - Yemeni coffee’s role in the Enlightenment.

9. **Colonial Rivalries and the Decline of Yemeni Dominance**  
   - 18th-century: French, British, and Dutch plantations outcompete Yemen.  
   - *The collapse of Mocha’s monopoly and the rise of Caribbean and Latin American coffee*.  
   - Impact on Yemeni economy and society.

10. **Yemeni Coffee in the Age of Empire**  
   - 19th-century British and Ottoman interests in Yemeni coffee.  
   - *The introduction of new cash crops (qat) and their effect on coffee cultivation*.  
   - Decline in quality and quantity due to neglect and conflict.

11. **The Cold War and the Coffee Crisis**  
   - 20th-century Yemeni division (North/South) and Soviet/US influences.  
   - *The shift from coffee to qat in rural livelihoods*.  
   - Drought, pests, and civil wars further erode production.

12. **The Specialty Coffee Renaissance: Yemen’s Return to Glory**  
   - 21st-century global demand for single-origin, heirloom coffees.  
   - *The discovery of “Yemenia” varietals and their genetic uniqueness*.  
   - NGOs and cooperatives reviving traditional farming.

13. **Modern Challenges: War, Climate, and Market Access**  
   - 2015–present: Saudi-led conflict and blockade.  
   - *The siege of Hodeidah port and its impact on coffee exports*.  
   - Climate change and water scarcity in the highlands.

14. **Yemeni Coffee in the Cup: Flavor, Ritual, and Identity**  
   - Tasting notes: dried fruit, spice, and wine-like acidity.  
   - *The Yemeni coffee ceremony (bunna) and its social significance*.  
   - Diaspora communities keeping traditions alive abroad.

15. **Conclusion: The Future of Yemeni Coffee**  
   - Balancing heritage with innovation.  
   - *The role of Yemeni coffee in post-war reconstruction*.  
   - Final reflection: From the highlands of Sana’a to the cups of Brooklyn, Yemeni coffee remains a bridge between past and future.",
    "$.messages[1].role": "assistant",
    "$.pid": 43066,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T10:35:28.042934",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "stop-choose-expect-major-pain",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6-dev1.dev4+g67b03c6",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 16 highlighted sections (required: 5)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "5",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "destroy-rational-cultural-available-meeting",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a cover letter for a job at a local coffee shop in the form of a poem. Highlight at least 5 text sections using "*". For example: *3 years of experience*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "Dear Brewmaster of *Morning Rituals*,  
I come not with parchment résumé,  
but with *3 years of experience* tucked in my apron pocket,  
steam-warm and espresso-dark.  

I have *memorized 47 regulars’ names*—  
each syllable frothed like oat-milk foam—  
and I know that *Carla takes two raw sugars*  
while *Mr. Lee prefers his cappuccino bone-dry*.  

I can *dial in a grinder* at 5:03 a.m.  
to the *exact 18.5 grams* that coax  
caramel notes from Ethiopian beans,  
and still greet the line with *a sunrise smile*.  

My wrists speak in *latte-art cursive*:  
hearts, rosettas, and once (on a dare)  
a tiny saxophone for the jazz trio  
who tips in *harmonies and quarters*.  

I *close the shop solo* without a single bean left homeless,  
count the till to the *penny and a poem*,  
and leave the milk pitchers *mirror-bright*,  
so the night shift sees its own reflection of care.  

Hire me, and the *scent of fresh-ground hope*  
will greet every customer at the door;  
I’ll be the *steady hand* behind the bar,  
pouring *courage in ceramic*, one cup at a time.  

Yours in steam and service,  
Riley",
    "$.messages[1].role": "assistant",
    "$.pid": 43066,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T10:35:28.042963",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "stop-choose-expect-major-pain",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6-dev1.dev4+g67b03c6",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 10 highlighted sections (required: 5)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "5",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "provide-cool-meeting-kid-book",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a casual blog post about similarities across animal species. Highlight at least 5 sections in your answer by starting and ending with "*", like: *highlighted text section*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "Hey friends! Grab a coffee and let’s talk about something that always blows my mind: *how much we all have in common, even when we look nothing alike*. I was scrolling through wildlife photos the other night and realized that a jellyfish and I are basically running the same basic software—just with different packaging. So here’s a laid-back tour of five “wait, we do that too?!” moments across the animal kingdom.

*We all need our beauty sleep*  
Turns out, every animal with a nervous system—from fruit flies to blue whales—shares a version of sleep. Crows nap on tree branches, octopuses snooze in coconut-shell beds, and even roundworms have “lethargus,” which is basically worm REM. Scientists think sleep is so ancient that the last common ancestor of all animals already had it. So next time you hit snooze, remember you’re honoring a 700-million-year-old tradition.

*Love songs come in many dialects*  
Whether you’re a human humming in the shower or a humpback whale belting out 30-minute ballads, *we all use rhythm and pitch to flirt and bond*. Frogs croak in choruses, nightingales remix their playlists nightly, and male mice sing ultrasonic “pickup lines” too high for us to hear. Same goal, different instruments.

*Grandma knew best: family matters*  
Elephants babysit their nieces, wolves bring back take-out for auntie, and *clownfish dads are the ultimate stay-at-home fathers*. Across species, kinship networks boost survival. Even bacteria split into “sibling” lineages that cooperate—so the next time your cousin crashes on your couch, call it evolutionary strategy.

*Stress looks the same on everyone*  
When life gets spicy, our bodies flip the same ancient switch. *Cortisol floods fish, birds, and humans alike*, making hearts race and muscles twitch. Zebra finches lose their appetite during tough times, and stressed-out bees become pessimistic (yes, they can test bee mood). So if you’re doom-scrolling at 2 a.m., know that a stressed squirrel in your backyard totally gets you.

*Play isn’t optional—it’s homework for life*  
Otters juggle rocks, dogs zoom in circles, and young ravens snowboard down snowy roofs using jar lids. *Play builds brains and social skills across the board*. Researchers have spotted reptiles “wrestling” for fun and octopuses repeatedly bouncing Lego blocks like toddlers. If it has neurons, it probably needs recess.

Next time you feel alone in the universe, remember that somewhere a parrot is probably preening its best friend, a bat is sharing food with a hungry roost-mate, and a tiny nematode is curling up for a nap. We’re all just variations on a very old, very shared theme.",
    "$.messages[1].role": "assistant",
    "$.pid": 43066,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T10:35:28.042989",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "stop-choose-expect-major-pain",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6-dev1.dev4+g67b03c6",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 6 highlighted sections (required: 5)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "5",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "delete-stale-similar-others-email",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Generate a business proposal to start a sweatshirt company in Bremen. The proposal should contain 5 or more sections. Highlight each section name using the this format:
*section name*",
    "$.messages[0].role": "user",
    "$.messages[1].content": "*Executive Summary*
Bremen Sweat Co. will be a premium, sustainability-driven sweatshirt brand headquartered in Bremen, Germany. We will design, manufacture, and sell limited-edition sweatshirts that combine Nordic minimalism with Hanseatic maritime heritage. Our mission is to become Bremen’s first carbon-neutral apparel label while capturing 5 % of the city’s €120 million annual streetwear market within three years. Initial funding of €250 k will cover product development, a flagship micro-factory/store in the Viertel district, and a digital-first go-to-market strategy.

*Market Analysis*
Bremen’s population of 570 k includes 120 k university students and 80 k creative-industry professionals—our core demographic. Streetwear sales in Northern Germany grew 11 % YoY (Statista 2023), yet only 8 % of brands are locally produced. Competitor benchmarking shows price gaps: fast-fashion sweatshirts sell at €25–35, mid-tier at €60–80, and premium sustainable labels at €110–140. We will occupy the €85–95 niche with superior materials and storytelling. Surveys (n = 400) reveal 68 % of Bremen consumers would pay a 20 % premium for locally made, eco-certified apparel.

*Product & Sustainability Strategy*
Each sweatshirt will be crafted from 100 % GOTS-certified organic cotton and recycled ocean-bound polyester, dyed in Bremen using closed-loop water systems. Designs will feature subtle references to Bremen’s Town Musicians, ship masts, and Weser river textures. Limited drops of 300 units per style ensure scarcity and collectability. Blockchain-based QR codes will provide end-to-end transparency: fiber origin, factory GPS, and carbon footprint per garment (-2.1 kg CO₂e vs. industry average 8.5 kg). End-of-life take-back program offers 15 % store credit for returned items, feeding our upcycling line.

*Operations & Supply Chain*
Micro-factory (180 m²) in the Viertel will house 4 digital knitting machines and 6 seamstresses, producing 1 200 sweatshirts/month. Local sourcing: yarn from OEKO-TEX mill in Oldenburg, trims from Hamburg port suppliers. Just-in-time production reduces inventory risk by 40 %. Bremen’s port infrastructure enables low-impact inbound logistics; outbound shipments leverage DHL GoGreen within Germany (24 h delivery). ISO 14001 certification targeted by month 12.

*Marketing & Sales Channels*
Phase 1 (Months 1–6): Pop-up store at Lloyd Passage, Instagram/TikTok campaigns with Bremen influencers (@hanseaticstyle, 180 k followers). Phase 2: Flagship store launch with AR fitting mirrors and Weser riverfront window projection art. E-commerce via Shopify Plus, offering 3-D configurator for sleeve embroidery. Wholesale partnerships with 5 concept stores in Berlin, Hamburg, and Copenhagen. Revenue split: 60 % DTC, 25 % online, 15 % wholesale. Loyalty app rewards sustainable actions (bike delivery, clothing swaps).

*Financial Projections & Funding*
Year 1: Revenue €420 k, gross margin 58 %, EBITDA -€30 k (investment phase). Year 2: Revenue €950 k, EBITDA 12 %. Year 3: Revenue €1.8 m, EBITDA 22 %. Break-even month 18. Initial ask: €250 k seed (€150 k equipment, €60 k marketing, €40 k working capital). Investor IRR 38 % over 5 years with 20 % equity offered. ESG-linked loan option with Bremen Green Finance Initiative at 2.1 % interest.",
    "$.messages[1].role": "assistant",
    "$.pid": 43066,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T10:35:28.043019",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "stop-choose-expect-major-pain",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6-dev1.dev4+g67b03c6",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 39 highlighted sections (required: 3)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "3",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "remain-large-left-team-wish",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a funny and sarcastic template for rating the quality of a marriage between two people who are both moms. This is for the couple themselves. Please highlight at least 3 sections with markdown,  i.e *highlighted section*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "**The Official “How Are We Still Married?” Scorecard**  
*For Two Moms Who’ve Survived Sleepless Nights, PTA Wars, and Each Other’s Cooking*  

---

**1. Morning Efficiency Rating**  
*How quickly can we get two kids, one dog, and zero coffee spills out the door before someone passive-aggressively “reminds” the other that *“the permission slip was due yesterday”*?*  
- 10/10: We’re a synchronized swim team of lunchboxes and existential dread.  
- 5/10: One of us is still Googling “how to unclog a glue stick from a USB port.”  
- 1/10: We’re late because someone had to *“just quickly”* reorganize the spice rack alphabetically.  

---

**2. Co-Parenting Coordination**  
*Rate the likelihood that we’ll both remember which kid is allergic to *“the orange cheese”* and which one is in a *“be kind to worms”* phase.*  
- 10/10: We’ve got a shared Google calendar, color-coded to the point of *“militant whimsy.”*  
- 5/10: We rely on telepathy and the faint hope that the school nurse *“just knows.”*  
- 1/10: We’ve accidentally sent the wrong kid to violin practice *twice*. The violin teacher now thinks we’re running a *“child exchange program.”*  

---

**3. Romance Maintenance**  
*How often do we prioritize date night over *“collapsing into a heap of mutual resentment”*?*  
- 10/10: We’ve scheduled a candlelit dinner for *“when the kids go to college.”*  
- 5/10: Our last date was a *“romantic”* trip to Target where we *“accidentally”* held hands in the freezer aisle.  
- 1/10: We consider *“simultaneously ignoring the laundry”* a form of intimacy.  

---

**Bonus Round: Petty Argument Hall of Fame**  
*Check all that apply:*  
- [ ] The Great 2019 Dishwasher Loading Schism (*“Knives point DOWN, Karen!”*)  
- [ ] The Passive-Aggressive Post-it War of 2021 (*“Your turn to buy *‘organic’* milk. XOXO”*)  
- [ ] The Thermostat Cold War (*“68°F is a *human rights violation*.”*)  

---

**Final Score Interpretation:**  
- **30–40 points:** *You’re the Beyoncé and Jay-Z of lesbian moms.*  
- **15–29 points:** *You’re more like *“Thelma and Louise,”* but with minivans and a Costco membership.*  
- **0–14 points:** *You’re still married, which is statistically *“a win.”* Celebrate with *“leftover chicken nuggets”* and a *“we’ll deal with this tomorrow”* shrug.*",
    "$.messages[1].role": "assistant",
    "$.pid": 43066,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T10:35:28.043042",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "stop-choose-expect-major-pain",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6-dev1.dev4+g67b03c6",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 1 highlighted sections (required: 1)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "1",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "read-wish-deliver-great-case",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a song about the summers of my childhood that I spent in the countryside. Give the song a name, and highlight the name by wrapping it with *. For example: *little me in the countryside*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "*little me in the countryside*

[Verse 1]  
Barefoot on the gravel road, dust between my toes,  
Sunlight dripped like honey on the corn rows.  
Grandma’s porch swing creaked the same old tune,  
While fireflies spelled my name across June.  
Mason jar of lemonade, half a moon inside,  
I chased the shade like it was trying to hide.  

[Chorus]  
Oh, little me in the countryside,  
Running through the wheat so high,  
Every breeze was a lullaby,  
And the sky was big enough to fly.  
I left my worries by the riverside,  
Back when I was little me in the countryside.  

[Verse 2]  
Tractor songs at twilight, crickets keeping time,  
Dirt on my knees, freckles in a climbing rhyme.  
Creek water baptized my sunburned skin,  
I believed every ripple was a secret twin.  
Under the quilt of stars, I counted dreams instead,  
While the Milky Way braided night through my head.  

[Chorus]  
Oh, little me in the countryside,  
Running through the wheat so high,  
Every breeze was a lullaby,  
And the sky was big enough to fly.  
I left my worries by the riverside,  
Back when I was little me in the countryside.  

[Bridge]  
Now the city lights flicker, but I still close my eyes—  
I can smell the clover, hear the distant barn owls’ cries.  
Time may stretch the map, but it can’t erase  
The echo of my laughter in that endless open space.  

[Final Chorus]  
Oh, little me in the countryside,  
Keep the gate open, I’m still nearby.  
Though the roads have changed and the years have flown,  
A part of me is always home.  
In every golden field I drive beside,  
I find little me in the countryside.",
    "$.messages[1].role": "assistant",
    "$.pid": 43066,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T10:35:28.043069",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "stop-choose-expect-major-pain",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6-dev1.dev4+g67b03c6",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 3 highlighted sections (required: 3)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "3",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "do-test-spicy-dead-television",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a riddle for the word "façade" that contains at least 3 italic text phrases in markdown syntax, i.e *italic text*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "I wear a *painted smile* for every passer-by,  
a *perfectly symmetrical* face that never cracks.  
Behind me, the *crumbling bricks* sigh in secret.  
What am I?",
    "$.messages[1].role": "assistant",
    "$.pid": 43066,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T10:35:28.043094",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "stop-choose-expect-major-pain",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6-dev1.dev4+g67b03c6",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 19 highlighted sections (required: 3)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "3",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "protect-other-major-green-war",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a template for a workshop on the importance of diversity in the workplace and highlight at least 3 sections with markdown, i.e. *highlighted section*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "# Workshop Template  
**Title:** *Building Inclusive Excellence: The Business & Human Case for Workplace Diversity*  
**Duration:** 3.5 hours (can be split into two 90-minute sessions)  
**Audience:** Managers, HR professionals, team leads, ERG members, and any employee who influences culture  

---

## 1. Welcome & Objectives (15 min)
- Ice-breaker: “Two Truths & a Lie—Diversity Edition”  
- *Learning objectives*: By the end of this workshop, participants will be able to:  
  1. Define diversity, equity, inclusion, and belonging (DEIB) in their own words.  
  2. Articulate at least three evidence-based benefits of a diverse workforce.  
  3. Identify one action they can take within 30 days to foster inclusion on their team.  

---

## 2. *The Data-Driven Case for Diversity* (25 min)
- Interactive poll: “What % performance lift do you think diverse teams achieve?”  
- Mini-lecture with live charts:  
  - McKinsey 2023 report: Top-quartile companies for ethnic/gender diversity are **39 % more likely to outperform** on profitability.  
  - Harvard study: *Inclusive teams make better business decisions 87 % of the time* and do so twice as fast.  
- Small-group exercise: Match the statistic to the business outcome (revenue, innovation, retention).  

---

## 3. *Unconscious Bias & Micro-behaviors* (30 min)
- 5-minute video: “The Resume with Two Names” (identical CVs, different callbacks).  
- Bias Busters card game: Participants draw scenario cards and practice interrupting bias in real time.  
- *Key takeaway*: Bias is not a moral failing; it is a neurological shortcut that can be rewired through deliberate habits.  

---

## 4. *Inclusive Leadership Toolkit* (40 min)
- **The 4 A’s Framework**:  
  1. **Awareness** – Daily 3-minute bias check-in.  
  2. **Amplification** – Repeat & credit ideas from under-represented voices.  
  3. **Access** – Rotate high-visibility assignments.  
  4. **Accountability** – Track inclusion metrics on team dashboards.  
- Role-play: “The Missed Promotion” – diagnose where each ‘A’ broke down and redesign the conversation.  

---

## 5. Action Planning & Commitments (20 min)
- Personal *Inclusion Commitment Card*: “In the next 30 days I will…”  
- Pair-share for accountability.  
- QR code to a digital resource hub (TED talks, reading list, policy templates).  

---

## 6. Closing & Evaluation (10 min)
- One-word checkout: “How do you feel now?”  
- *Post-workshop survey* link (2 min) with prize draw for completion.  

---

## Materials Checklist
- Slide deck (with alt-text on all images)  
- Printed Commitment Cards & pens  
- Bias Busters card decks (laminated)  
- Timer & bell  
- Snacks labeled for common dietary restrictions (gluten-free, vegan, halal)  

---

## Follow-Up Email Template (send within 24 hrs)
Subject: Your Inclusion Toolkit + Next Steps  
Body:  
Hi team,  
Thank you for leaning into today’s conversation on *why diversity wins*. Attached are:  
- Slide deck & resource links  
- Your signed Inclusion Commitment Card (photo reminder)  
- Calendar invite for the optional 30-day check-in circle  

Remember: Inclusion is a practice, not a one-time training. Let’s keep the momentum!",
    "$.messages[1].role": "assistant",
    "$.pid": 43066,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T10:35:28.043121",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "stop-choose-expect-major-pain",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6-dev1.dev4+g67b03c6",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 23 highlighted sections (required: 3)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "3",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "collect-destroy-difficult-conference-hardware",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a cover letter to a local political party, asking to be their rally organizer. Make sure to highlight at least 3 sections in your answer in markdown format.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "**[Your Name]**  
[Street Address]  
[City, State ZIP]  
[Email Address] | [Phone Number] | [LinkedIn URL]  
[Date]

**[Local Party Chairperson’s Name]**  
[Local Party Name]  
[Party Headquarters Address]  
[City, State ZIP]

Dear **Chairperson [Last Name]**,

I am writing to express my enthusiastic interest in serving as **Rally Organizer** for **[Local Party Name]**. As a lifelong resident of **[City/County]**, I have watched our community’s political pulse beat strongest when neighbors gather in common purpose. I want to channel that energy into rallies that not only energize our base but also invite undecided voters to witness the power of our shared vision.

### **Proven Event-Execution Expertise**
During the 2022 mid-term cycle, I spearheaded a **three-county voter-registration caravan** that staged 14 pop-up events in 30 days.  
- **Logistics**: Coordinated permits, portable stages, and ADA-compliant access for 2,000+ attendees per stop.  
- **Volunteer Mobilization**: Recruited and trained 120 volunteers, creating a color-coded “Rally-in-a-Box” toolkit now used by three neighboring county parties.  
- **Real-Time Adaptability**: When a thunderstorm threatened our largest outdoor rally, I pivoted to an indoor gymnasium within 90 minutes—live-streaming the switch to maintain momentum and ultimately exceeding projected turnout by 18 %.

### **Data-Driven Rally Strategy**
I treat every rally as a **micro-campaign laboratory**.  
- **Pre-Event Analytics**: I overlay precinct-level voter-file data with heat-mapping tools to select venues within 0.7 miles of our highest “persuasion index” households.  
- **On-Site Engagement**: QR-coded wristbands linked to instant sign-up forms increased volunteer recruitment by 34 % compared to clipboard methods.  
- **Post-Event ROI**: My post-rally surveys (average 42 % response rate) feed a dashboard that tracks pledge-to-action conversion, allowing us to refine messaging within 48 hours.

### **Coalition-Building & Safety Leadership**
Understanding that rallies must feel **safe and inclusive**, I have:  
- Partnered with local **faith leaders, PTAs, and small-business associations** to co-sponsor events, expanding our demographic reach by 27 %.  
- Completed **FEMA crowd-management certification** and maintain relationships with county emergency management to ensure rapid-response protocols.  
- Implemented a **“Community Marshals”** program—training 50 volunteer marshals in de-escalation techniques, cutting incident reports to zero across 22 events.

I would welcome the opportunity to discuss how my blend of grassroots passion, logistical precision, and data fluency can help **[Local Party Name]** turn every rally into a springboard for victory in the upcoming cycle. Thank you for considering my application.

**Respectfully,**

[Your Signature (if submitting hard copy)]  
[Your Printed Name]",
    "$.messages[1].role": "assistant",
    "$.pid": 43066,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T10:35:28.043147",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "stop-choose-expect-major-pain",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6-dev1.dev4+g67b03c6",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 4 highlighted sections (required: 2)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "2",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "build-upgrade-green-white-reason",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "The opposite of youth is not age, but ...? Highlight at least 2 sections in your answer with markdown, i.e. *highlighted section*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "The opposite of youth is not age, but **stagnation**—the moment when *curiosity closes its eyes* and the impulse to explore, question, and reinvent quietly fades.  

Where youth is a *perpetual state of becoming*, its true counter is the condition in which *growth is willingly surrendered* and the mind settles into fixed certainties.",
    "$.messages[1].role": "assistant",
    "$.pid": 43066,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T10:35:28.043177",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "stop-choose-expect-major-pain",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6-dev1.dev4+g67b03c6",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 13 highlighted sections (required: 3)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "3",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "need-hostile-dream-business-person",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a description of the following data in a weird style: The Golden Palace eatType restaurant; The Golden Palace food Indian; The Golden Palace area city centre. Use markdown to highlight at least 3 sections in your answer.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "**The Golden Palace** is not a palace at all, but a **restaurant** that has disguised itself as a mirage of saffron and cardamom in the **city centre**. It whispers **Indian** recipes into the ears of pigeons, who then forget how to fly and instead waddle around the square, dizzy with cumin.  

Inside, the walls sweat **garam masala** and the tables grow tiny **basil plants** that recite Bollywood dialogues when no one is looking. The waiters glide like **shadow puppets**, balancing trays of **butter chicken** that glow like radioactive sunsets.  

If you order **naan**, it arrives folded into the shape of a **paper swan**, flapping once before dissolving into steam that smells of **monsoon rain**. The bill is presented on a **lotus leaf**, and if you stare at it long enough, the numbers rearrange themselves into a **limerick about turmeric**.",
    "$.messages[1].role": "assistant",
    "$.pid": 43066,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T10:35:28.043206",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "stop-choose-expect-major-pain",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6-dev1.dev4+g67b03c6",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 14 highlighted sections (required: 1)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "1",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "store-reduce-unknown-girl-hate",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a funny rap about a man who gets a call from an official saying that he is a long lost relative of the king of Nigeria. Use markdown to highlight at least one section of your answer, i.e. *highlighted section*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "🎤 *Yo, it’s the tale of Sir Scam-a-lot, straight outta suburbia!* 🎤  

Yo, my name is Dave, I sell insurance in Ohio,  
Got a cubicle, a cat, and a 401(k) that’s kinda fly-o.  
One Tuesday, phone rings—unknown number, what the heck?  
Voice says, “*Prince Ogbonna the Third* just wrote you a fat check!”  

I’m like, “Hold up, who dis? I’m whiter than printer paper,  
My bloodline’s mayo on Wonder Bread—ain’t no royal vapor!”  
Dude on the line, accent silkier than lotion,  
Says, “Cousin, you’re 37th in line—time to claim your ocean!”  

**[Hook]**  
🎶 *Long lost king of Nigeria, crown sittin’ on my dome,*  
*Got a palace full of leopards and a Wi-Fi that roams!*  
*They need my SSN and a quick wire fee,*  
*But I’m ‘bout to be big pimpin’ in Abuja, just wait and see!* 🎶  

He says, “Just send five grand for customs, then the jet lands,  
You’ll be dripping in so much gold, even your sweat glands bling.”  
I’m sweatin’ like a rotisserie, visions of caviar dreams,  
I Venmo “PrinceNaija69” like I’m funding meme schemes.  

Next day, email: “Oops, palace goat ate the transfer slip,  
Need ten more for goat surgery and a diamond-encrusted whip.”  
I hock my lawnmower, my Xbox, and Aunt Linda’s porcelain goose,  
Wire cash so fast my bank’s like, “Dave, you loose!”  

**[Bridge – Spoken Word]**  
Then the *official royal seal* arrives—  
It’s a GIF of Shrek in a crown doing the Macarena.  
I’m starting to sense a pattern here…  

**[Final Hook – Regret Remix]**  
🎶 *Now I’m the king of my mom’s basement, throne’s a beanbag chair,*  
*Royal decree: never trust a prince who types in all caps—*  
*“URGENT, COUSIN, SEND MORE FLAMINGOS!”*  
*Lesson learned: if the crown’s too shiny, it’s probably aluminum foil, homie!*",
    "$.messages[1].role": "assistant",
    "$.pid": 43066,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T10:35:28.043233",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "stop-choose-expect-major-pain",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6-dev1.dev4+g67b03c6",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 6 highlighted sections (required: 2)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "2",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "mean-take-left-high-family",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Expand the riddle into a story with a funny tone:

What can you catch but not throw?
A cold

Use * to highlight at least 2 sections in your text. For example: *this is a highlighted text section*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "Gary the Germ was the *smallest, greenest, and most obnoxiously proud* microbe in the entire Sniffle Kingdom. Every morning he practiced his grand entrance: sliding down banisters made of nasal hair, cannon-balling into unsuspecting tonsils, and yelling, “ACHOO—did somebody order a cold?”  

One Tuesday, Gary spotted his dream target: *Bartholomew “Bart” Bigglesworth, a man who believed hand-washing was a government conspiracy*. Bart had just exited the subway, triumphantly licking a subway pole “for the vitamins.” Gary rubbed his microscopic hands together. “Perfect. I’ll hitch a ride on that finger, ride the express elevator to the sinuses, and throw the *snottiest* house-warming party in history!”  

But there was a catch—literally. Gary could *catch* Bart, but he couldn’t *throw* him anywhere. Every time Gary tried to catapult Bart into the next office cubicle, Bart just sneezed, wiped his nose on his sleeve, and kept typing spreadsheets. Gary screamed, “This is mutiny! I’m supposed to be the one doing the tossing!”  

Meanwhile, Bart’s immune system—led by Captain Phlegm and the Mucus Marines—launched *Operation Tissue Storm*. They surrounded Gary, armed with vitamin C grenades and zinc zappers. Gary attempted a dramatic escape by surfing a sneeze, but the tissue slapped him like a flyswatter on a summer day.  

Defeated, Gary sat in the crumpled tissue, sulking. “I caught the cold,” he muttered, “but I can’t throw it anywhere. Worst. Superpower. Ever.”  

From that day forward, Gary changed his business card to read: “Professional Cold Catcher—No Returns, No Refunds, Absolutely No Throwing.”",
    "$.messages[1].role": "assistant",
    "$.pid": 43066,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T10:35:28.043257",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "stop-choose-expect-major-pain",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6-dev1.dev4+g67b03c6",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 17 highlighted sections (required: 3)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "3",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "seem-clean-simple-work-member",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a rap about the renaissance. It should be noticeably different from raps about other historical eras, and have an interesting or weird tone. Highlight at least 3 sections in your answer in markdown format.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "**[Intro – harpsichord glitch beat, stylus scratching parchment]**  
Yo, it’s 14-something, but my mind’s 3-D printed in gold leaf—  
I’m the **Renaissance Rapper**, got a quill in one hand and a scalpel in the other, dissectin’ beats like Vesalius dissectin’ spleens.  
Let’s get *weird* with the rebirth, baby.

---

### **Section 1 – The Fresco Flex**  
I drip ultramarine like I’m sponsored by Afghanistan mines,  
Michelangelo on the ceiling, I’m reclining on divine lines.  
Angels doing backflips off the Sistine, they’re hype for my set,  
While I tag “*AMOR VINCIT OMNIA*” in invisible ink—wet.  
My flow’s a trompe-l’œil, looks flat till you tilt your head—  
Then it’s depth for days, like Brunelleschi’s dome overhead.  
I’m not baroque, I’m pre-baroque, I’m the prototype shock,  
Got patrons in fur-lined sleeves stuffing florins in my sock.  
**I paint in egg tempera so the colors never fade,**  
**But the yolk smells funky—call it *odeur de old crusade*.**

---

### **Section 2 – The Printing-Press Poltergeist**  
Gutenberg ghost in the machine, clanking lead type like chains,  
Every page a polter-page, every serif a soul that remains.  
I drop a folio, it’s viral before “viral” was a cough,  
Monks in scriptoriums rage-quitting, “Bro, that illuminated stuff’s off!”  
I’m the incunabula influencer, my colophon’s a flex,  
Got red-ink rubricators sliding into my DMs—*next*.  
**I rhyme in movable type so my bars can rearrange,**  
**Yesterday I was Petrarch, today I’m deranged.**  
The press spits paper like a hydra with hiccups,  
Each sheet a protest flyer: “Down with scholastic pick-ups!”

---

### **Section 3 – The Anatomical Anthem (Parental Advisory: Corpses)**  
Night shift with da Vinci, we’re sketching by candle and moon,  
Cadavers on the table like, “Do me a favor—autotune my coccyx soon.”  
I rap in reverse Latin so only the dead understand,  
While Leo reverse-engineers a bird-man from a frying pan.  
**My beats are ventricles pumping four-four time,**  
**Aorta bass drop—call that circulatory grime.**  
I sample the sound of a scalpel scraping periosteum,  
Layer it over a lute loop—*that’s* my magnum opium.  
Galen’s in the corner, throwing shade like a Greek tragedy,  
But I’m Harvey before Harvey, already rapping blood circulation majestically.

---

**[Outro – fading lute arpeggio, distant cannon fire from Constantinople]**  
So remember me when you’re doom-scrolling on glass,  
I was the era that turned corpses into class.  
I’m the **Renaissance Rapper**, my ghost still tags in chalk,  
Leaving QR codes on cathedral walls—scan it, you’ll squawk.  
Rebirth never dies, it just pixelates and loops,  
Now if you’ll excuse me, I’ve got a date with Lucretius in the soup.",
    "$.messages[1].role": "assistant",
    "$.pid": 43066,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T10:35:28.043282",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "stop-choose-expect-major-pain",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6-dev1.dev4+g67b03c6",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 20 highlighted sections (required: 5)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "5",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "receive-final-best-visible-night",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Summarize the history of Japan. Italicize at least 5 keywords in your response. To indicate a italic word, wrap it with asterisk, like *italic*",
    "$.messages[0].role": "user",
    "$.messages[1].content": "*Jōmon* hunter-gatherers (14,000 BCE) gave way to *Yayoi* rice farmers (300 BCE) who forged bronze and iron.  
The *Yamato* court unified clans (3rd–7th c.), adopting *Shinto* and Chinese script.  
Prince *Shōtoku*’s 17-article constitution (604) and the Taika Reforms (645) centralized power.  
Heian aristocrats (794–1185) produced *The Tale of Genji* while samurai rose in the provinces.  
Minamoto no Yoritomo founded the *Kamakura* shogunate (1185), introducing warrior rule.  
Mongol invasions (1274, 1281) were repelled by “divine winds” (*kamikaze*).  
Ashikaga *Muromachi* shogunate (1336–1573) saw Zen gardens and Ōnin War chaos.  
Oda Nobunaga, Toyotomi Hideyoshi, and Tokugawa Ieyasu ended the *Sengoku* “warring states” era.  
Tokugawa *Edo* shogunate (1603–1868) isolated Japan, fostering *ukiyo-e* and *kabuki*.  
Commodore Perry’s *Black Ships* (1853) forced the Meiji Restoration (1868), modernizing the empire.  
Victories in the *Sino-Japanese* (1894–95) and *Russo-Japanese* (1904–05) wars expanded territory.  
Militarism led to *Pearl Harbor* (1941) and WWII defeat (1945); the U.S. occupation drafted a pacifist constitution.  
Post-war *economic miracle* made Japan the world’s second-largest economy until the 1990s *bubble*.  
Today, Japan balances *constitutional monarchy* with cutting-edge technology and soft-power culture.",
    "$.messages[1].role": "assistant",
    "$.pid": 43066,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T10:35:28.043309",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "stop-choose-expect-major-pain",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6-dev1.dev4+g67b03c6",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 5 highlighted sections (required: 2)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "2",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "record-live-better-guy-country",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a funny Haiku about a Quaker named John who lives in the town of De Smet, South Dakota. Use the asterisk symbol, *, to highlight some words or phrases twice. Example: *This is a highlighted phrase*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "*John* in *De Smet*  
Quaker hat *blows off*—chases  
*Silent* *scream*—“Come back!”",
    "$.messages[1].role": "assistant",
    "$.pid": 43066,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T10:35:28.043332",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "stop-choose-expect-major-pain",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6-dev1.dev4+g67b03c6",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 19 highlighted sections (required: 1)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "1",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "talk-wonder-bad-dangerous-health",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Today, at the 54th Annual Grammy Awards, the Recording Academy honors the talent and creativity of the artists, musicians, and producers who are the creators of the best recordings of the past year. Please continue writing this text in a formal tone, using notations. Highlight some key parts in your response with "*", like *highlighted text*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "Today, at the 54th Annual Grammy Awards, the Recording Academy honors the talent and creativity of the artists, musicians, and producers who are the creators of the best recordings of the past year. In a ceremony marked by *unprecedented artistic excellence* and *meticulous craftsmanship*, the Academy recognizes those whose work has defined the sonic landscape of 2011.

The evening’s proceedings underscore the Academy’s enduring commitment to *artistic integrity* and *technical innovation*. Among the most celebrated achievements are:

• *Album of the Year* – awarded to an opus that has redefined genre boundaries while achieving both critical acclaim and popular resonance.  
• *Record of the Year* – bestowed upon a single track whose *sonic clarity*, *emotive depth*, and *production mastery* have set a new benchmark for the industry.  
• *Best New Artist* – conferred upon an emerging voice whose *visionary artistry* signals the future direction of contemporary music.

Throughout the telecast, special tributes illuminate the *indelible legacies* of icons whose influence transcends generations, while collaborative performances showcase the *synergistic power* of cross-genre partnerships. The Academy further acknowledges the *behind-the-scenes luminaries*—engineers, mixers, and mastering specialists—whose *invisible expertise* transforms raw inspiration into *audible transcendence*.

In closing, the 54th Annual Grammy Awards reaffirm the Recording Academy’s solemn charge: to *safeguard the heritage*, *nurture the present*, and *inspire the future* of recorded music.",
    "$.messages[1].role": "assistant",
    "$.pid": 43066,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T10:35:28.043356",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "stop-choose-expect-major-pain",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6-dev1.dev4+g67b03c6",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 39 highlighted sections (required: 2)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "2",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "meet-hold-solid-video-moment",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a blog post about interesting facts about the Dutch language. Italicize at least 2 sections in your answer with markdown, i.e. *italic text*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "# 10 Surprising Facts About the Dutch Language That Will Make You Say “Wauw!”

If you think Dutch is just “German with a sore throat,” think again. From tongue-twisting consonant clusters to words that sound like they were invented by a stand-up comedian, Dutch is a linguistic playground. Below are ten facts that prove the language is far more colorful than its reputation for windmills and wooden shoes.

---

### 1. *The longest word in Dutch is 53 letters long*  
Meet **“kindercarnavalsoptochtvoorbereidingswerkzaamhedenplan”**—a single noun describing the preparation work for a children’s carnival parade. Dutch loves to glue words together, so once you master the basics you can create your own kilometer-long compounds on the fly.

---

### 2. Dutch has three genders, but only two articles  
Grammatically, nouns are masculine, feminine, or neuter. In everyday speech, however, masculine and feminine collapsed into a single “common” gender, leaving *“de”* for common and *“het”* for neuter. The result? A never-ending guessing game for learners.

---

### 3. *The Dutch alphabet ends with “ij,” not “z”*  
Schoolchildren recite the ABC and finish with “…x, y, ij, z.” The digraph *ij* is so common (think *hij* = he, *mijn* = my) that it’s treated almost like a separate letter. On old typewriters, *ij* even had its own key.

---

### 4. Dutch is the closest major language to English  
Frisian is technically closer, but among languages with tens of millions of speakers, Dutch wins. Compare:  
- English *“bread”* → Dutch *“brood”*  
- English *“green”* → Dutch *“groen”*  
Shared roots mean you can read simple Dutch signs long before you’ve cracked a single grammar book.

---

### 5. *Dutch gave New York its first words*  
Before it was New York, it was New Amsterdam. That’s why we say *“cookie”* (from Dutch *koekje*) instead of *“biscuit,”* and *“boss”* (from *baas*) instead of *“master.”* Wall Street? It once ran alongside the *“wal”* (rampart) of the city wall.

---

### 6. The word *“gezellig”* has no exact English translation*  
Roughly it means “cozy,” “convivial,” or “the warm feeling of being with friends,” but none of those capture the full vibe. A café can be *gezellig*, a conversation can be *gezellig*, even a spreadsheet can be *gezellig* if the team is laughing.

---

### 7. Dutch spelling changed overnight in 1996  
The *“Groene Boekje”* (Green Booklet) reforms simplified thousands of words. *“potatos”* became *“aardappelen”* and *“cat’s”* became *“katten.”* Overnight, every dictionary, street sign, and Scrabble board in the country was technically out of date.

---

### 8. *Dutch is spoken on six continents*  
Besides the Netherlands and Belgium, it’s an official language in Suriname, Aruba, Curaçao, and Sint Maarten. Thanks to emigration, you’ll also find active Dutch-speaking communities in Canada, Australia, Argentina—even South Africa, where Afrikaans is a daughter language.

---

### 9. The Dutch love diminutives  
Add *-je* to almost any noun and you instantly shrink it: *“huis”* (house) → *“huisje”* (little house). It’s not just about size; diminutives add affection. A *“biertje”* isn’t a small beer—it’s an invitation to relax.

---

### 10. *Dutch is the language of the world’s oldest multinational*  
The Vereenigde Oostindische Compagnie (VOC) was founded in 1602 and issued the planet’s first publicly traded stock. Its ledgers, contracts, and ship logs were all written in Dutch, making the language the original tongue of global capitalism.

---

Ready to dive in? Start with *hallo* and *dank je wel*, and you’ll already be halfway to *gezellig*.",
    "$.messages[1].role": "assistant",
    "$.pid": 43066,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T10:35:28.043383",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "stop-choose-expect-major-pain",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6-dev1.dev4+g67b03c6",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 2 highlighted sections (required: 1)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "1",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "understand-rational-hard-kind-project",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Gideon is a farmer who has a surplus of crops from his farm this year. What might he do with that surplus? Highlight at least one section of your answer in markdown, i.e *highlighted section*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "Gideon has several good options for his surplus:

1. **Sell it at the local farmers’ market or to a regional wholesaler** to turn the extra produce into cash he can reinvest in seeds, equipment, or next season’s labor.  
2. *Donate a portion to the nearby food bank or community kitchen* so that families facing food insecurity can benefit from fresh, nutritious produce.  
3. Preserve some of the surplus—by canning tomatoes, drying herbs, or freezing berries—so his own household has a steady supply of healthy food throughout the winter.",
    "$.messages[1].role": "assistant",
    "$.pid": 43066,
    "$.rollout_status.status": "finished",
  },
]
`;

exports[`flattenJson against logs.json > flattens each entry in logs.json.logs and matches snapshot 1`] = `
[
  {
    "$.created_at": "2025-08-08T09:52:30.335005",
    "$.input_metadata.row_id": "1",
    "$.messages[0].content": "Hello",
    "$.messages[0].role": "user",
    "$.messages[1].content": "Hello",
    "$.messages[1].role": "assistant",
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T09:47:43.816761",
    "$.input_metadata.row_id": "row_0",
    "$.messages[0].content": "Hello 0",
    "$.messages[0].role": "user",
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T09:41:55.015216",
    "$.input_metadata.row_id": "row_1",
    "$.messages[0].content": "Hello 1",
    "$.messages[0].role": "user",
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T09:41:55.015767",
    "$.input_metadata.row_id": "row_2",
    "$.messages[0].content": "Hello 2",
    "$.messages[0].role": "user",
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T10:00:08.522553",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "Run math evaluation on sample dataset using pytest interface.",
    "$.eval_metadata.name": "test_pytest_mcp_url",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.status": "stopped",
    "$.eval_metadata.version": "v0.2.6.dev12+g2dc40d5.dirty",
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.row_id": "describe-use-entire-person-education",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "You are a helpful assistant that can answer questions about Fireworks.
ALWAYS provide code or commands to execute to answer the question.",
    "$.messages[0].role": "system",
    "$.messages[1].content": "Can you teach me about how to manage deployments on Fireworks",
    "$.messages[1].role": "user",
    "$.pid": 67635,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T10:01:12.825337",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "Run math evaluation on sample dataset using pytest interface.",
    "$.eval_metadata.name": "test_pytest_mcp_url",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.status": "stopped",
    "$.eval_metadata.version": "v0.2.6.dev12+g2dc40d5.dirty",
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.row_id": "meet-solid-big-picture-car",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "You are a helpful assistant that can answer questions about Fireworks.
ALWAYS provide code or commands to execute to answer the question.",
    "$.messages[0].role": "system",
    "$.messages[1].content": "Can you teach me about how to manage deployments on Fireworks",
    "$.messages[1].role": "user",
    "$.pid": 68229,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T11:06:51.152819",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "Run math evaluation on sample dataset using pytest interface.",
    "$.eval_metadata.name": "test_pytest_mcp_url",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.status": "stopped",
    "$.eval_metadata.version": "v0.2.6.dev15+g6486dbd",
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.row_id": "leave-ask-logical-face-way",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "You are a helpful assistant that can answer questions about Fireworks.
ALWAYS provide code or commands to execute to answer the question.",
    "$.messages[0].role": "system",
    "$.messages[1].content": "Can you teach me about how to manage deployments on Fireworks",
    "$.messages[1].role": "user",
    "$.messages[2].content": "I'll help you learn about managing deployments on Fireworks AI. Let me search for comprehensive information about deployment management.",
    "$.messages[2].role": "assistant",
    "$.messages[2].tool_calls[0].function.arguments": "{"query": "deployment management Fireworks AI"}",
    "$.messages[2].tool_calls[0].function.name": "search",
    "$.messages[2].tool_calls[0].id": "functions.search:0",
    "$.messages[2].tool_calls[0].type": "function",
    "$.messages[3].content[0].text": "Title: Fireworks AI Developer Platform
Link: https://docs.fireworks.ai/getting-started/introduction
Content: Fireworks AI is the best platform for building AI product experiences with open source AI models. You can run and customize AI models with just a few lines of code!

Fireworks AI is the best platform for building AI product experiences with open source AI models. You can run and customize AI models with just a few lines of code!

Start building with open source AI models

",
    "$.messages[3].content[0].type": "text",
    "$.messages[3].content[1].text": "Title: How do I close my Fireworks.ai account?
Link: https://docs.fireworks.ai/faq-new/account-access/how-do-i-close-my-fireworksai-account
Content: To close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed

To close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed

How do I close my Fireworks.ai account?
To close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed

",
    "$.messages[3].content[1].type": "text",
    "$.messages[3].content[2].text": "Title: Do you host your deployments in the EU or Asia?
Link: https://docs.fireworks.ai/faq-new/support-general/do-you-host-your-deployments-in-the-eu-or-asia
Content: We are currently deployed in multiple U.S.-based locations. However, we’re open to hearing more about your specific requirements. You can: Join our Discord community Write to us at inquiries@fireworks.ai If you're an Enterprise customer, please contact your dedicated customer support representative to ensure a timely response.

We are currently deployed in multiple U.S.-based locations. However, we’re open to hearing more about your specific requirements. You can: Join our Discord community Write to us at inquiries@fireworks.ai If you're an Enterprise customer, please contact your dedicated customer support representative to ensure a timely response.

Do you host your deployments in the EU or Asia?
We are currently deployed in multiple U.S.-based locations. However, we’re open to hearing more about your specific requirements. You can: Join our Discord community Write to us at inquiries@fireworks.ai If you're an Enterprise customer, please contact your dedicated customer support representative to ensure a timely response.

",
    "$.messages[3].content[2].type": "text",
    "$.messages[3].content[3].text": "Title: Introducing the Fireworks Build SDK
Link: https://docs.fireworks.ai/tools-sdks/python-client/sdk-introduction
Content: The Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.

The Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.

Introducing the Fireworks Build SDK
The Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.

",
    "$.messages[3].content[3].type": "text",
    "$.messages[3].content[4].text": "Title: On-demand deployments
Link: https://docs.fireworks.ai/guides/ondemand-deployments
Content: If you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.

If you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.

On-demand deployments
If you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.

",
    "$.messages[3].content[4].type": "text",
    "$.messages[3].content[5].text": "Title: I have multiple Fireworks accounts. When I try to login with Google on Fireworks' web UI, I'm getting signed into the wrong account. How do I fix this?
Link: https://docs.fireworks.ai/faq-new/account-access/i-have-multiple-fireworks-accounts-when-i-try-to-login-with-google-on-fireworks
Content: If you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.

If you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.

I have multiple Fireworks accounts. When I try to login with Google on Fireworks' web UI, I'm getting signed into the wrong account. How do I fix this?
If you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.

",
    "$.messages[3].content[5].type": "text",
    "$.messages[3].content[6].text": "Title: Introduction
Link: https://docs.fireworks.ai/api-reference/introduction
Content: Fireworks AI REST API enables you to interact with various language, image and embedding models using an API Key. It also lets you automate management of models, deployments, datasets, and more.

Fireworks AI REST API enables you to interact with various language, image and embedding models using an API Key. It also lets you automate management of models, deployments, datasets, and more.

Introduction
Fireworks AI REST API enables you to interact with various language, image and embedding models using an API Key. It also lets you automate management of models, deployments, datasets, and more.

",
    "$.messages[3].content[6].type": "text",
    "$.messages[3].content[7].text": "Title: Create a deployment
Link: https://docs.fireworks.ai/tools-sdks/firectl/commands/create-deployment
Content: Create a Deployment on Fireworks AI platform

Create a Deployment on Fireworks AI platform

Creates a new deployment.

",
    "$.messages[3].content[7].type": "text",
    "$.messages[3].content[8].text": "Title: How much does Fireworks cost?
Link: https://docs.fireworks.ai/faq-new/billing-pricing/how-much-does-fireworks-cost
Content: Fireworks AI operates on a pay-as-you-go model for all non-Enterprise usage, and new users automatically receive free credits. You pay based on: Per token for serverless inference Per GPU usage time for on-demand deployments Per token of training data for fine-tuning For customers needing enterprise-grade security and reliability , please reach out to us at inquiries@fireworks.ai to discuss options. Find out more about our current pricing on our Pricing page.

Fireworks AI operates on a pay-as-you-go model for all non-Enterprise usage, and new users automatically receive free credits. You pay based on: Per token for serverless inference Per GPU usage time for on-demand deployments Per token of training data for fine-tuning For customers needing enterprise-grade security and reliability , please reach out to us at inquiries@fireworks.ai to discuss options. Find out more about our current pricing on our Pricing page.

How much does Fireworks cost?
Fireworks AI operates on a pay-as-you-go model for all non-Enterprise usage, and new users automatically receive free credits. You pay based on: Per token for serverless inference Per GPU usage time for on-demand deployments Per token of training data for fine-tuning For customers needing enterprise-grade security and reliability , please reach out to us at inquiries@fireworks.ai to discuss options. Find out more about our current pricing on our Pricing page.

",
    "$.messages[3].content[8].type": "text",
    "$.messages[3].content[9].text": "Title: Python (Fireworks)
Link: https://docs.fireworks.ai/getting-started/quickstart
Content: This is the recommended way to get started : This  Python (Fireworks)  tab uses our  Fireworks Build
SDK  for the best performance and developer experience.
See our  Client-side performance
optimization  guide for
more details. Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. pip install --upgrade fireworks-ai The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either  ~/.bash_profile  for Bash or  ~/.zshrc  for  Zsh .
You can do this by running the command: vim ~/.bash_profile vim ~/.zshrc Add a new line to the file with the following: export FIREWORKS_API_KEY="<API_KEY>" After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. source ~/.bash_profile source ~/.zshrc You can verify that the variable has been set correctly by running  echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. setx FIREWORKS_API_KEY "<API_KEY>" To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: echo %FIREWORKS_API_KEY% You can quickly instantiate the  LLM  class and call the Fireworks API. The Build SDK handles deployment management automatically. from fireworks import LLM

# Basic usage - SDK automatically selects optimal deployment type
llm = LLM(model="llama4-maverick-instruct-basic", deployment_type="auto")

response = llm.chat.completions.create(
    messages=[{"role": "user", "content": "Say this is a test"}]
)

print(response.choices[0].message.content) You can also pass the API key directly to the LLM constructor:  LLM(model="llama4-maverick-instruct-basic", deployment_type="auto", api_key="<FIREWORKS_API_KEY>")

This is the recommended way to get started : This  Python (Fireworks)  tab uses our  Fireworks Build
SDK  for the best performance and developer experience.
See our  Client-side performance
optimization  guide for
more details. Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. pip install --upgrade fireworks-ai The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either  ~/.bash_profile  for Bash or  ~/.zshrc  for  Zsh .
You can do this by running the command: vim ~/.bash_profile vim ~/.zshrc Add a new line to the file with the following: export FIREWORKS_API_KEY="<API_KEY>" After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. source ~/.bash_profile source ~/.zshrc You can verify that the variable has been set correctly by running  echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. setx FIREWORKS_API_KEY "<API_KEY>" To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: echo %FIREWORKS_API_KEY% You can quickly instantiate the  LLM  class and call the Fireworks API. The Build SDK handles deployment management automatically. from fireworks import LLM

# Basic usage - SDK automatically selects optimal deployment type
llm = LLM(model="llama4-maverick-instruct-basic", deployment_type="auto")

response = llm.chat.completions.create(
    messages=[{"role": "user", "content": "Say this is a test"}]
)

print(response.choices[0].message.content) You can also pass the API key directly to the LLM constructor:  LLM(model="llama4-maverick-instruct-basic", deployment_type="auto", api_key="<FIREWORKS_API_KEY>")

Set up your developer environment & call the Fireworks API
This is the recommended way to get started : This  tab uses our Fireworks Build
SDK for the best performance and developer experience.
See our Client-side performance
optimization guide for
more details. Install SDK Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Configure API Key Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either ~/.bash_profile for Bash or ~/.zshrc for Zsh .
You can do this by running the command: Add a new line to the file with the following: After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. You can verify that the variable has been set correctly by running echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: Sending the first API Request You can quickly instantiate the LLM class and call the Fireworks API. The Build SDK handles deployment management automatically. You can also pass the API key directly to the LLM constructor: LLM(model="llama4-maverick-instruct-basic", deployment_type="auto", api_key="<FIREWORKS_API_KEY>") Install SDK Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. Fireworks AI platform offers drop-in replacement with OpenAI Python Client. Configure API Key Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either ~/.bash_profile for Bash or ~/.zshrc for Zsh .
You can do this by running the command: Add a new line to the file with the following: After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. You can verify that the variable has been set correctly by running echo $OPENAI_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. setx OPENAI_API_BASE "https://api.fireworks.ai/inference/v1"
setx OPENAI_API_KEY "<API_KEY>"

",
    "$.messages[3].content[9].type": "text",
    "$.messages[3].role": "tool",
    "$.messages[3].tool_call_id": "functions.search:0",
    "$.messages[4].content": "Let me search for more specific information about deployment management tools and commands.",
    "$.messages[4].role": "assistant",
    "$.messages[4].tool_calls[0].function.arguments": "{"query": "firectl deployment management commands CLI"}",
    "$.messages[4].tool_calls[0].function.name": "search",
    "$.messages[4].tool_calls[0].id": "functions.search:1",
    "$.messages[4].tool_calls[0].type": "function",
    "$.messages[5].content[0].text": "Title: Why am I getting "invalid id" errors when using firectl commands like create deployment or list deployments?
Link: https://docs.fireworks.ai/faq-new/fine-tuning/why-am-i-getting-invalid-id-errors-when-using-firectl-commands-like-create-deplo
Content: This error typically occurs when your account ID is not properly configured.

This error typically occurs when your account ID is not properly configured.

Why am I getting "invalid id" errors when using firectl commands like create deployment or list deployments?
This error typically occurs when your account ID is not properly configured.

",
    "$.messages[5].content[0].type": "text",
    "$.messages[5].content[1].text": "Title: Installing the CLI
Link: https://docs.fireworks.ai/guides/ondemand-deployments
Content: To create and manage on-demand deployments, use our CLI tool firectl . Install the tool by following the installation guide . Verify that your installation is working and you are logged into the correct account by running: firectl whoami

To create and manage on-demand deployments, use our CLI tool firectl . Install the tool by following the installation guide . Verify that your installation is working and you are logged into the correct account by running: firectl whoami

To create and manage on-demand deployments, use our CLI tool firectl . Install the tool by following the installation guide . Verify that your installation is working and you are logged into the correct account by running:

",
    "$.messages[5].content[1].type": "text",
    "$.messages[5].content[2].text": "Title: Cli overview
Link: https://docs.fireworks.ai/evaluators/cli_reference/cli_overview
Content: Command Line Interface Reference
The Reward Kit provides a command-line interface (CLI) for common operations like previewing evaluations, deploying reward functions, and running agent evaluations.

Command Line Interface Reference
The Reward Kit provides a command-line interface (CLI) for common operations like previewing evaluations, deploying reward functions, and running agent evaluations.

Command Overview
The Reward Kit CLI supports the following main commands: run : Run a local evaluation pipeline using a Hydra configuration. preview : Preview evaluation results or re-evaluate generated outputs. deploy : Deploy a reward function as an evaluator. agent-eval : Run agent evaluations on task bundles. list : List existing evaluators (coming soon). delete : Delete an evaluator (coming soon).

",
    "$.messages[5].content[2].type": "text",
    "$.messages[5].content[3].text": "Title: Using the CLI (reward-kit deploy)
Link: https://docs.fireworks.ai/evaluators/developer_guide/evaluation_workflows
Content: The reward-kit deploy command is suitable for deploying reward functions defined in script files. The --metrics-folders argument should point to the directory containing your reward function script (e.g., a main.py with the @reward_function decorator). For more details on reward-kit deploy , see the CLI Reference.

The reward-kit deploy command is suitable for deploying reward functions defined in script files. The --metrics-folders argument should point to the directory containing your reward function script (e.g., a main.py with the @reward_function decorator). For more details on reward-kit deploy , see the CLI Reference.

The reward-kit deploy command is suitable for deploying reward functions defined in script files. The --metrics-folders argument should point to the directory containing your reward function script (e.g., a main.py with the @reward_function decorator). # Deploy with the CLI
reward-kit deploy \\
  --id helpfulness-evaluator \\
  --metrics-folders "helpfulness=./path/to/your/metric_script_directory" \\
  --display-name "Helpfulness Evaluator" \\
  --description "Evaluates the helpfulness of responses" \\
  --force

",
    "$.messages[5].content[3].type": "text",
    "$.messages[5].content[4].text": "Title: Does Fireworks support custom base models?
Link: https://docs.fireworks.ai/faq-new/models-inference/does-fireworks-support-custom-base-models
Content: Yes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.

Yes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.

Does Fireworks support custom base models?
Yes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.

",
    "$.messages[5].content[4].type": "text",
    "$.messages[5].content[5].text": "Title: 🚀 Easier & faster LoRA fine-tune deployments on Fireworks
Link: https://docs.fireworks.ai/updates/changelog
Content: You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: Previously, this involved two distinct steps, and the resulting deployment was slower than the base model: Create a deployment using firectl create deployment "accounts/fireworks/models/<MODEL_ID of base model>" --enable-addons Then deploy the addon to the deployment: firectl load-lora <MODEL_ID> --deployment <DEPLOYMENT_ID> For more information, see our deployment documentation. This change is for dedicated deployments with a single LoRA. You can still deploy multiple LoRAs on a deployment or deploy LoRA(s) on some Serverless models as described in the documentation.

",
    "$.messages[5].content[5].type": "text",
    "$.messages[5].content[6].text": "Title: Quick deployment
Link: https://docs.fireworks.ai/fine-tuning/single-lora
Content: Deploy your LoRA fine-tuned model with one simple command: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

Deploy your LoRA fine-tuned model with one simple command: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

Alternative deployment method
This two-step method is the standard approach for multi-LoRA deployments where multiple LoRA models share the same base model. While it can also be used for single LoRA deployments, it provides slower performance compared to live merge and is not recommended for single LoRA use cases. You can also deploy single LoRA models using a two-step process: Create base model deployment Deploy the base model with addons enabled: firectl create deployment "accounts/fireworks/models/<MODEL_ID of base model>" --enable-addons

",
    "$.messages[5].content[6].type": "text",
    "$.messages[5].content[7].text": "Title: 1. Setup
Link: https://docs.fireworks.ai/tools-sdks/python-client/the-tutorial
Content: To get started with the Fireworks AI Python SDK, you need to install the firectl CLI tool and create an API key. Install our CLI tool firectl to interact with the Fireworks AI platform. Sign in to Fireworks by running the following command: firectl signin

To get started with the Fireworks AI Python SDK, you need to install the firectl CLI tool and create an API key. Install our CLI tool firectl to interact with the Fireworks AI platform. Sign in to Fireworks by running the following command: firectl signin

To get started with the Fireworks AI Python SDK, you need to install the firectl CLI tool and create an API key. Install our CLI tool firectl to interact with the Fireworks AI platform. Sign in to Fireworks by running the following command: A browser window will open to the Fireworks AI login page. Once you login, your machine will be authenticated. Create an API key by running the following command: $ firectl create api-key --key-name "quick-start"
Key Id: key_42vAYeb7rwt9zzg1
Display Name: quick-start
Key: fw_3ZLd....
Secure: true
Be sure to save this key. It will not be shown again.

",
    "$.messages[5].content[7].type": "text",
    "$.messages[5].content[8].text": "Title: Deploying a fine-tuned model using an on-demand deployment
Link: https://docs.fireworks.ai/fine-tuning/fine-tuning-models
Content: Use the following command to deploy your fine-tuned model using an on-demand deployment: firectl create deployment <FINE_TUNED_MODEL_ID>

Use the following command to deploy your fine-tuned model using an on-demand deployment: firectl create deployment <FINE_TUNED_MODEL_ID>

Deploying a fine-tuned model serverlessly
Not all base models support serverless addons. Please check this list for the full list of serverless models that support LoRA add-ons. If your base model supports serverless addons, you can run the following command to load the fine-tuned model into a serverless deployment: firectl load-lora <FINE_TUNED_MODEL_ID>

",
    "$.messages[5].content[8].type": "text",
    "$.messages[5].content[9].text": "Title: Create a deployment
Link: https://docs.fireworks.ai/tools-sdks/firectl/commands/create-deployment
Content: Creates a new deployment. firectl create deployment [flags]

Creates a new deployment. firectl create deployment [flags]

Example
firectl create deployment falcon-7b

",
    "$.messages[5].content[9].type": "text",
    "$.messages[5].role": "tool",
    "$.messages[5].tool_call_id": "functions.search:1",
    "$.pid": 83882,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T11:10:27.844496",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "Run math evaluation on sample dataset using pytest interface.",
    "$.eval_metadata.name": "test_pytest_mcp_url",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.status": "stopped",
    "$.eval_metadata.version": "v0.2.6.dev15+g6486dbd",
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.row_id": "investigate-recent-dangerous-hope-magazine",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "You are a helpful assistant that can answer questions about Fireworks.
ALWAYS provide code or commands to execute to answer the question.",
    "$.messages[0].role": "system",
    "$.messages[1].content": "Can you teach me about how to manage deployments on Fireworks",
    "$.messages[1].role": "user",
    "$.pid": 84847,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T11:12:00.215991",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "Run math evaluation on sample dataset using pytest interface.",
    "$.eval_metadata.name": "test_pytest_mcp_url",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.status": "stopped",
    "$.eval_metadata.version": "v0.2.6.dev15+g6486dbd.dirty",
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.row_id": "seem-environmental-media-television-concert",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "You are a helpful assistant that can answer questions about Fireworks.
ALWAYS provide code or commands to execute to answer the question.",
    "$.messages[0].role": "system",
    "$.messages[1].content": "Can you teach me about how to manage deployments on Fireworks",
    "$.messages[1].role": "user",
    "$.pid": 85545,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T11:12:34.382113",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "Run math evaluation on sample dataset using pytest interface.",
    "$.eval_metadata.name": "test_pytest_mcp_url",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.status": "stopped",
    "$.eval_metadata.version": "v0.2.6.dev15+g6486dbd.dirty",
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.row_id": "treat-suppose-salty-week-plan",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "You are a helpful assistant that can answer questions about Fireworks.
ALWAYS provide code or commands to execute to answer the question.",
    "$.messages[0].role": "system",
    "$.messages[1].content": "Can you teach me about how to manage deployments on Fireworks",
    "$.messages[1].role": "user",
    "$.pid": 85754,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T11:12:46.750720",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "Run math evaluation on sample dataset using pytest interface.",
    "$.eval_metadata.name": "test_pytest_mcp_url",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.status": "stopped",
    "$.eval_metadata.version": "v0.2.6.dev15+g6486dbd.dirty",
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.row_id": "contain-recent-bad-plan-change",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "You are a helpful assistant that can answer questions about Fireworks.
ALWAYS provide code or commands to execute to answer the question.",
    "$.messages[0].role": "system",
    "$.messages[1].content": "Can you teach me about how to manage deployments on Fireworks",
    "$.messages[1].role": "user",
    "$.pid": 85851,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T11:15:45.037115",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "Run math evaluation on sample dataset using pytest interface.",
    "$.eval_metadata.name": "test_pytest_mcp_url",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.version": "v0.2.6.dev15+g6486dbd.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "At least one tool call was made",
    "$.evaluation_result.score": 1,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.row_id": "move-little-tidy-pain-work",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "You are a helpful assistant that can answer questions about Fireworks.
ALWAYS provide code or commands to execute to answer the question.",
    "$.messages[0].role": "system",
    "$.messages[1].content": "Can you teach me about how to manage deployments on Fireworks",
    "$.messages[1].role": "user",
    "$.messages[2].content": "I'll help you learn about managing deployments on Fireworks! Let me search for comprehensive information about deployment management.",
    "$.messages[2].role": "assistant",
    "$.messages[2].tool_calls[0].function.arguments": "{"query": "deployments management create update delete scale"}",
    "$.messages[2].tool_calls[0].function.name": "search",
    "$.messages[2].tool_calls[0].id": "functions.search:0",
    "$.messages[2].tool_calls[0].type": "function",
    "$.messages[3].content[0].text": "Title: Delete Deployment
Link: https://docs.fireworks.ai/api-reference/delete-deployment
Content: 



REST Endpoint DELETE /v1/accounts/{account_id}/deployments/{deployment_id}

Server(s): https://api.fireworks.ai


Summary:  

Description: N/A

Security methods: 
header: 
	Authorization: bearer auth
	

Request params: 
path params:
account_id:
	string
	Description: The Account Id
	Example: "<string>"
deployment_id:
	string
	Description: The Deployment Id
	Example: "<string>"

query params:
hard:
	boolean
	Description: If true, this will perform a hard deletion.
	Example: true
ignoreChecks:
	boolean
	Description: If true, this will ignore checks and force the deletion of a deployment that is currently
deployed and is in use.
	Example: true


Request body: N/A


Responses: 
200: object
	Description: N/A
	Example: {}

",
    "$.messages[3].content[0].type": "text",
    "$.messages[3].content[1].text": "Title: How does billing and scaling work for on-demand GPU deployments?
Link: https://docs.fireworks.ai/faq-new/deployment-infrastructure/how-does-billing-and-scaling-work-for-on-demand-gpu-deployments
Content: On-demand GPU deployments have unique billing and scaling characteristics compared to serverless deployments: Billing: Charges start when the server begins accepting requests Billed by GPU-second for each active instance Costs accumulate even if there are no active API calls Scaling options: Supports autoscaling from 0 to multiple GPUs Each additional GPU adds to the billing rate Can handle unlimited requests within the GPU’s capacity Management requirements: Not fully serverless; requires some manual management Manually delete deployments when no longer needed Or configure autoscaling to scale down to 0 during inactive periods Cost control tips: Regularly monitor active deployments Delete unused deployments to avoid unnecessary costs Consider serverless options for intermittent usage Use autoscaling to 0 to optimize costs during low-demand times

On-demand GPU deployments have unique billing and scaling characteristics compared to serverless deployments: Billing: Charges start when the server begins accepting requests Billed by GPU-second for each active instance Costs accumulate even if there are no active API calls Scaling options: Supports autoscaling from 0 to multiple GPUs Each additional GPU adds to the billing rate Can handle unlimited requests within the GPU’s capacity Management requirements: Not fully serverless; requires some manual management Manually delete deployments when no longer needed Or configure autoscaling to scale down to 0 during inactive periods Cost control tips: Regularly monitor active deployments Delete unused deployments to avoid unnecessary costs Consider serverless options for intermittent usage Use autoscaling to 0 to optimize costs during low-demand times

How does billing and scaling work for on-demand GPU deployments?
On-demand GPU deployments have unique billing and scaling characteristics compared to serverless deployments: Billing: Charges start when the server begins accepting requests Billed by GPU-second for each active instance Costs accumulate even if there are no active API calls Scaling options: Supports autoscaling from 0 to multiple GPUs Each additional GPU adds to the billing rate Can handle unlimited requests within the GPU’s capacity Management requirements: Not fully serverless; requires some manual management Manually delete deployments when no longer needed Or configure autoscaling to scale down to 0 during inactive periods Cost control tips: Regularly monitor active deployments Delete unused deployments to avoid unnecessary costs Consider serverless options for intermittent usage Use autoscaling to 0 to optimize costs during low-demand times

",
    "$.messages[3].content[1].type": "text",
    "$.messages[3].content[2].text": "Title: Managing default deployments
Link: https://docs.fireworks.ai/guides/ondemand-deployments
Content: Since a model may be deployed to multiple deployments, querying by model name will route to the "default" deployed model. You can see which deployed model entry is marked with Default: true using: This will show the Deployed Model Refs section with the Default: true entry. To update the default deployed model, note the Name of the deployed model reference above. Then run: To delete a default deployment you must delete all other deployments for the same model first, or designate a different deployed model as the default as described above. This is to ensure that querying by model name will always route to an unambiguous default deployment as long as deployments for the model exist.

Since a model may be deployed to multiple deployments, querying by model name will route to the "default" deployed model. You can see which deployed model entry is marked with Default: true using: This will show the Deployed Model Refs section with the Default: true entry. To update the default deployed model, note the Name of the deployed model reference above. Then run: To delete a default deployment you must delete all other deployments for the same model first, or designate a different deployed model as the default as described above. This is to ensure that querying by model name will always route to an unambiguous default deployment as long as deployments for the model exist.

Since a model may be deployed to multiple deployments, querying by model name will route to the "default" deployed model. You can see which deployed model entry is marked with Default: true using: This will show the Deployed Model Refs section with the Default: true entry. To update the default deployed model, note the Name of the deployed model reference above. Then run: firectl update deployed-model <DEPLOYED_MODEL_ID_2> --default

",
    "$.messages[3].content[2].type": "text",
    "$.messages[3].content[3].text": "Title: Create a deployment
Link: https://docs.fireworks.ai/tools-sdks/firectl/commands/create-deployment
Content: Creates a new deployment.

Creates a new deployment.

Creates a new deployment. firectl create deployment [flags]

",
    "$.messages[3].content[3].type": "text",
    "$.messages[3].content[4].text": "Title: Delete a deployment
Link: https://docs.fireworks.ai/tools-sdks/firectl/commands/delete-resources
Content: Deletes an model deployment.

Deletes an model deployment.

Deletes an model deployment. firectl delete deployment [flags]

",
    "$.messages[3].content[4].type": "text",
    "$.messages[3].content[5].text": "Title: Update Deployment
Link: https://docs.fireworks.ai/api-reference/update-deployment
Content: 



REST Endpoint PATCH /v1/accounts/{account_id}/deployments/{deployment_id}

Server(s): https://api.fireworks.ai


Summary:  

Description: N/A

Security methods: 
header: 
	Authorization: bearer auth
	

Request params: 
path params:
account_id:
	string
	Description: The Account Id
	Example: "<string>"
deployment_id:
	string
	Description: The Deployment Id
	Example: "<string>"


Request body: object
	Description: N/A
	Example: {"displayName":"<string>","description":"<string>","expireTime":"2023-11-07T05:31:56Z","minReplicaCount":123,"maxReplicaCount":123,"autoscalingPolicy":{"scaleUpWindow":"<string>","scaleDownWindow":"<string>","scaleToZeroWindow":"<string>","loadTargets":{}},"baseModel":"<string>","acceleratorCount":123,"acceleratorType":"ACCELERATOR_TYPE_UNSPECIFIED","precision":"PRECISION_UNSPECIFIED","enableAddons":true,"draftTokenCount":123,"draftModel":"<string>","ngramSpeculationLength":123,"deploymentTemplate":"<string>","autoTune":{"longPrompt":true},"placement":{"region":"REGION_UNSPECIFIED","multiRegion":"MULTI_REGION_UNSPECIFIED","regions":["REGION_UNSPECIFIED"]},"disableDeploymentSizeValidation":true}


Responses: 
200: object
	Description: N/A
	Example: {"name":"<string>","displayName":"<string>","description":"<string>","createTime":"2023-11-07T05:31:56Z","expireTime":"2023-11-07T05:31:56Z","purgeTime":"2023-11-07T05:31:56Z","deleteTime":"2023-11-07T05:31:56Z","state":"STATE_UNSPECIFIED","status":{"code":"OK","message":"<string>"},"minReplicaCount":123,"maxReplicaCount":123,"replicaCount":123,"autoscalingPolicy":{"scaleUpWindow":"<string>","scaleDownWindow":"<string>","scaleToZeroWindow":"<string>","loadTargets":{}},"baseModel":"<string>","acceleratorCount":123,"acceleratorType":"ACCELERATOR_TYPE_UNSPECIFIED","precision":"PRECISION_UNSPECIFIED","cluster":"<string>","enableAddons":true,"draftTokenCount":123,"draftModel":"<string>","ngramSpeculationLength":123,"numPeftDeviceCached":123,"deploymentTemplate":"<string>","autoTune":{"longPrompt":true},"placement":{"region":"REGION_UNSPECIFIED","multiRegion":"MULTI_REGION_UNSPECIFIED","regions":["REGION_UNSPECIFIED"]},"region":"REGION_UNSPECIFIED","updateTime":"2023-11-07T05:31:56Z","disableDeploymentSizeValidation":true}

",
    "$.messages[3].content[5].type": "text",
    "$.messages[3].content[6].text": "Title: What should I expect for deployment and scaling performance?
Link: https://docs.fireworks.ai/faq-new/deployment-infrastructure/what-should-i-expect-for-deployment-and-scaling-performance
Content: Initial deployment : Should complete within minutes Scaling from zero : You may experience brief availability delays while the system scales up Troubleshooting : If deployment takes over 1 hour, this typically indicates a crash and should be investigated Best practice : Monitor deployment status and contact support if deployment times are unusually long

Initial deployment : Should complete within minutes Scaling from zero : You may experience brief availability delays while the system scales up Troubleshooting : If deployment takes over 1 hour, this typically indicates a crash and should be investigated Best practice : Monitor deployment status and contact support if deployment times are unusually long

What should I expect for deployment and scaling performance?
Initial deployment : Should complete within minutes Scaling from zero : You may experience brief availability delays while the system scales up Troubleshooting : If deployment takes over 1 hour, this typically indicates a crash and should be investigated Best practice : Monitor deployment status and contact support if deployment times are unusually long

",
    "$.messages[3].content[6].type": "text",
    "$.messages[3].content[7].text": "Title: delete_deployment(ignore_checks: bool = False, wait: bool = True)
Link: https://docs.fireworks.ai/tools-sdks/python-client/sdk-reference
Content: Deletes the deployment associated with this LLM instance if one exists. Arguments: ignore_checks bool, optional - Whether to ignore safety checks. Defaults to False. wait bool, optional - Whether to wait for deletion to complete. Defaults to True. llm.delete_deployment(ignore_checks=True)

Deletes the deployment associated with this LLM instance if one exists. Arguments: ignore_checks bool, optional - Whether to ignore safety checks. Defaults to False. wait bool, optional - Whether to wait for deletion to complete. Defaults to True.

scale_to_zero()
Sends a request to scale the deployment to 0 replicas but does not wait for it to complete. Returns: The deployment object, or None if no deployment exists deployment = llm.scale_to_zero()

",
    "$.messages[3].content[7].type": "text",
    "$.messages[3].content[8].text": "Title: How does billing work for on-demand deployments?
Link: https://docs.fireworks.ai/faq-new/deployment-infrastructure/how-does-billing-work-for-on-demand-deployments
Content: On-demand deployments come with automatic cost optimization features: Default autoscaling : Automatically scales to 0 replicas when not in use Pay for what you use : Charged only for GPU time when replicas are active Flexible configuration : Customize autoscaling behavior to match your needs Best practices for cost management: Leverage default autoscaling : The system automatically scales down deployments when not in use Customize carefully : While you can modify autoscaling behavior using our configuration options , note that preventing scale-to-zero will result in continuous GPU charges Consider your use case : For intermittent or low-frequency usage, serverless deployments might be more cost-effective For detailed configuration options, see our deployment guide.

On-demand deployments come with automatic cost optimization features: Default autoscaling : Automatically scales to 0 replicas when not in use Pay for what you use : Charged only for GPU time when replicas are active Flexible configuration : Customize autoscaling behavior to match your needs Best practices for cost management: Leverage default autoscaling : The system automatically scales down deployments when not in use Customize carefully : While you can modify autoscaling behavior using our configuration options , note that preventing scale-to-zero will result in continuous GPU charges Consider your use case : For intermittent or low-frequency usage, serverless deployments might be more cost-effective For detailed configuration options, see our deployment guide.

How does billing work for on-demand deployments?
On-demand deployments come with automatic cost optimization features: Default autoscaling : Automatically scales to 0 replicas when not in use Pay for what you use : Charged only for GPU time when replicas are active Flexible configuration : Customize autoscaling behavior to match your needs Best practices for cost management: Leverage default autoscaling : The system automatically scales down deployments when not in use Customize carefully : While you can modify autoscaling behavior using our configuration options , note that preventing scale-to-zero will result in continuous GPU charges Consider your use case : For intermittent or low-frequency usage, serverless deployments might be more cost-effective For detailed configuration options, see our deployment guide.

",
    "$.messages[3].content[8].type": "text",
    "$.messages[3].content[9].text": "Title: Create Deployment
Link: https://docs.fireworks.ai/api-reference/create-deployment
Content: 



REST Endpoint POST /v1/accounts/{account_id}/deployments

Server(s): https://api.fireworks.ai


Summary:  

Description: N/A

Security methods: 
header: 
	Authorization: bearer auth
	

Request params: 
path params:
account_id:
	string
	Description: The Account Id
	Example: "<string>"

query params:
disableAutoDeploy:
	boolean
	Description: By default, a deployment created with a currently undeployed base model
will be deployed to this deployment. If true, this auto-deploy function
is disabled.
	Example: true
disableSpeculativeDecoding:
	boolean
	Description: By default, a deployment will use the speculative decoding settings from
the base model. If true, this will disable speculative decoding.
	Example: true
deploymentId:
	string
	Description: The ID of the deployment. If not specified, a random ID will be generated.
	Example: "<string>"


Request body: object
	Description: N/A
	Example: {"displayName":"<string>","description":"<string>","expireTime":"2023-11-07T05:31:56Z","minReplicaCount":123,"maxReplicaCount":123,"autoscalingPolicy":{"scaleUpWindow":"<string>","scaleDownWindow":"<string>","scaleToZeroWindow":"<string>","loadTargets":{}},"baseModel":"<string>","acceleratorCount":123,"acceleratorType":"ACCELERATOR_TYPE_UNSPECIFIED","precision":"PRECISION_UNSPECIFIED","enableAddons":true,"draftTokenCount":123,"draftModel":"<string>","ngramSpeculationLength":123,"deploymentTemplate":"<string>","autoTune":{"longPrompt":true},"placement":{"region":"REGION_UNSPECIFIED","multiRegion":"MULTI_REGION_UNSPECIFIED","regions":["REGION_UNSPECIFIED"]},"disableDeploymentSizeValidation":true}


Responses: 
200: object
	Description: N/A
	Example: {"name":"<string>","displayName":"<string>","description":"<string>","createTime":"2023-11-07T05:31:56Z","expireTime":"2023-11-07T05:31:56Z","purgeTime":"2023-11-07T05:31:56Z","deleteTime":"2023-11-07T05:31:56Z","state":"STATE_UNSPECIFIED","status":{"code":"OK","message":"<string>"},"minReplicaCount":123,"maxReplicaCount":123,"replicaCount":123,"autoscalingPolicy":{"scaleUpWindow":"<string>","scaleDownWindow":"<string>","scaleToZeroWindow":"<string>","loadTargets":{}},"baseModel":"<string>","acceleratorCount":123,"acceleratorType":"ACCELERATOR_TYPE_UNSPECIFIED","precision":"PRECISION_UNSPECIFIED","cluster":"<string>","enableAddons":true,"draftTokenCount":123,"draftModel":"<string>","ngramSpeculationLength":123,"numPeftDeviceCached":123,"deploymentTemplate":"<string>","autoTune":{"longPrompt":true},"placement":{"region":"REGION_UNSPECIFIED","multiRegion":"MULTI_REGION_UNSPECIFIED","regions":["REGION_UNSPECIFIED"]},"region":"REGION_UNSPECIFIED","updateTime":"2023-11-07T05:31:56Z","disableDeploymentSizeValidation":true}

",
    "$.messages[3].content[9].type": "text",
    "$.messages[3].role": "tool",
    "$.messages[3].tool_call_id": "functions.search:0",
    "$.messages[4].content": "Let me search for more specific information about deployment creation and management workflows.",
    "$.messages[4].role": "assistant",
    "$.messages[4].tool_calls[0].function.arguments": "{"query": "firectl deployment management commands create update list"}",
    "$.messages[4].tool_calls[0].function.name": "search",
    "$.messages[4].tool_calls[0].id": "functions.search:1",
    "$.messages[4].tool_calls[0].type": "function",
    "$.messages[5].content[0].text": "Title: Why am I getting "invalid id" errors when using firectl commands like create deployment or list deployments?
Link: https://docs.fireworks.ai/faq-new/fine-tuning/why-am-i-getting-invalid-id-errors-when-using-firectl-commands-like-create-deplo
Content: This error typically occurs when your account ID is not properly configured.

This error typically occurs when your account ID is not properly configured.

Why am I getting "invalid id" errors when using firectl commands like create deployment or list deployments?
This error typically occurs when your account ID is not properly configured.

",
    "$.messages[5].content[0].type": "text",
    "$.messages[5].content[1].text": "Title: Create a deployment
Link: https://docs.fireworks.ai/tools-sdks/firectl/commands/create-deployment
Content: Creates a new deployment. firectl create deployment [flags]

Creates a new deployment. firectl create deployment [flags]

Creates a new deployment.

",
    "$.messages[5].content[1].type": "text",
    "$.messages[5].content[2].text": "Title: Creating an on-demand deployment
Link: https://docs.fireworks.ai/guides/ondemand-deployments
Content: Choose a model Check out our model library to see a list of models available for on-demand deployment. You can also upload and use your own custom base model. Create a deployment To create a new deployment of a model from the model library, use: Or, for a custom base model you have uploaded to your account: This command will complete when the deployment is READY . To let it run asynchronously, remove the --wait flag. If your account has purchased reserved capacity and this deployment meets the reservation criteria, it will be counted against that reservation and not billed as an on-demand deployment. Verify the deployment is running You can verify the deployment is active by running: The state field should show READY. The deployment ID is the last part of the deployment name: accounts/<ACCOUNT_ID>/deployments/<DEPLOYMENT_ID>. You can also list all deployments in your account at any time by running: firectl list deployments

Choose a model Check out our model library to see a list of models available for on-demand deployment. You can also upload and use your own custom base model. Create a deployment To create a new deployment of a model from the model library, use: Or, for a custom base model you have uploaded to your account: This command will complete when the deployment is READY . To let it run asynchronously, remove the --wait flag. If your account has purchased reserved capacity and this deployment meets the reservation criteria, it will be counted against that reservation and not billed as an on-demand deployment. Verify the deployment is running You can verify the deployment is active by running: The state field should show READY. The deployment ID is the last part of the deployment name: accounts/<ACCOUNT_ID>/deployments/<DEPLOYMENT_ID>. You can also list all deployments in your account at any time by running: firectl list deployments

Publishing a deployed model
By default, models can only be queried by the account that owns them. To make a deployment public so anyone with a valid Fireworks API key can query it, update the deployed model with the --public flag. Find the Deployed Model ID Every model running on a deployment receives a unique deployed model ID. You can find this ID if you know the model name and deployment name using the following command: firectl list deployed-models --filter 'model="<MODEL_NAME>" AND deployment="<DEPLOYMENT_NAME>"'

",
    "$.messages[5].content[2].type": "text",
    "$.messages[5].content[3].text": "Title: 🚀 Easier & faster LoRA fine-tune deployments on Fireworks
Link: https://docs.fireworks.ai/updates/changelog
Content: You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: Previously, this involved two distinct steps, and the resulting deployment was slower than the base model: Create a deployment using firectl create deployment "accounts/fireworks/models/<MODEL_ID of base model>" --enable-addons Then deploy the addon to the deployment: firectl load-lora <MODEL_ID> --deployment <DEPLOYMENT_ID> For more information, see our deployment documentation. This change is for dedicated deployments with a single LoRA. You can still deploy multiple LoRAs on a deployment or deploy LoRA(s) on some Serverless models as described in the documentation.

",
    "$.messages[5].content[3].type": "text",
    "$.messages[5].content[4].text": "Title: Quick deployment
Link: https://docs.fireworks.ai/fine-tuning/single-lora
Content: Deploy your LoRA fine-tuned model with one simple command: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

Deploy your LoRA fine-tuned model with one simple command: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

Alternative deployment method
This two-step method is the standard approach for multi-LoRA deployments where multiple LoRA models share the same base model. While it can also be used for single LoRA deployments, it provides slower performance compared to live merge and is not recommended for single LoRA use cases. You can also deploy single LoRA models using a two-step process: Create base model deployment Deploy the base model with addons enabled: firectl create deployment "accounts/fireworks/models/<MODEL_ID of base model>" --enable-addons

",
    "$.messages[5].content[4].type": "text",
    "$.messages[5].content[5].text": "Title: Deploying a fine-tuned model using an on-demand deployment
Link: https://docs.fireworks.ai/fine-tuning/fine-tuning-models
Content: Use the following command to deploy your fine-tuned model using an on-demand deployment: firectl create deployment <FINE_TUNED_MODEL_ID>

Use the following command to deploy your fine-tuned model using an on-demand deployment: firectl create deployment <FINE_TUNED_MODEL_ID>

firectl
firectl create dataset <DATASET_ID> /path/to/jsonl/file

",
    "$.messages[5].content[5].type": "text",
    "$.messages[5].content[6].text": "Title: Does Fireworks support custom base models?
Link: https://docs.fireworks.ai/faq-new/models-inference/does-fireworks-support-custom-base-models
Content: Yes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.

Yes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.

Does Fireworks support custom base models?
Yes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.

",
    "$.messages[5].content[6].type": "text",
    "$.messages[5].content[7].text": "Title: firectl
Link: https://docs.fireworks.ai/fine-tuning/fine-tuning-vlm
Content: # Create a deployment for your fine-tuned VLM
firectl create deployment my-custom-vlm

# Check deployment status
firectl get deployment accounts/your-account/deployment/deployment-id

# Create a deployment for your fine-tuned VLM
firectl create deployment my-custom-vlm

# Check deployment status
firectl get deployment accounts/your-account/deployment/deployment-id

firectl create sftj \\
  --base-model accounts/fireworks/models/qwen2p5-vl-32b-instruct \\
  --dataset my-vlm-dataset \\
  --output-model my-custom-vlm \\
  --epochs 3 For additional parameters like learning rates, evaluation datasets, and batch sizes, see  Additional SFT job settings .

",
    "$.messages[5].content[7].type": "text",
    "$.messages[5].content[8].text": "Title: Uploading the model locally (firectl)
Link: https://docs.fireworks.ai/models/uploading-custom-models
Content: To upload a custom base model, run the following command. firectl create model <MODEL_ID> /path/to/files/

To upload a custom base model, run the following command. firectl create model <MODEL_ID> /path/to/files/

Publishing
By default, all models you create are only visible to and deployable by users within your account. To publish a model so anyone with a Fireworks account can deploy it, you can create it with the --public flag. This will allow it to show up in public model lists. To unpublish the model, just run firectl update model <MODEL_ID> --public=false

",
    "$.messages[5].content[8].type": "text",
    "$.messages[5].content[9].text": "Title: firectl
Link: https://docs.fireworks.ai/models/quantization
Content: firectl create deployment <MODEL> --accelerator-type NVIDIA_H100_80GB --precision FP8

firectl create deployment <MODEL> --accelerator-type NVIDIA_H100_80GB --precision FP8

Creating an FP8 deployment
By default, creating a dedicated deployment will use the FP16 checkpoint. To see what precisions are available for a
model, run:  get model <MODEL_ID>

",
    "$.messages[5].content[9].type": "text",
    "$.messages[5].role": "tool",
    "$.messages[5].tool_call_id": "functions.search:1",
    "$.messages[6].content": "Based on my research, here's a comprehensive guide to managing deployments on Fireworks:

## Deployment Management on Fireworks

### 1. Creating Deployments

**Basic deployment creation:**
\`\`\`bash
# Create a deployment from the model library
firectl create deployment accounts/fireworks/models/<MODEL_NAME>

# Create a deployment for a custom model
firectl create deployment <YOUR_MODEL_ID>

# Create with specific configuration
firectl create deployment <MODEL_ID> --wait
\`\`\`

**Advanced deployment options:**
\`\`\`bash
# Deploy with specific hardware requirements
firectl create deployment <MODEL> --accelerator-type NVIDIA_H100_80GB --precision FP8

# Deploy LoRA fine-tuned models (single command)
firectl create deployment "accounts/fireworks/models/<LORA_MODEL_ID>"

# Deploy with addons enabled (for multi-LoRA)
firectl create deployment <BASE_MODEL> --enable-addons
\`\`\`

### 2. Monitoring Deployments

**Check deployment status:**
\`\`\`bash
# List all deployments
firectl list deployments

# Get specific deployment details
firectl get deployment accounts/<ACCOUNT_ID>/deployments/<DEPLOYMENT_ID>

# Check if deployment is ready
# The state field should show "READY"
\`\`\`

**Verify deployed models:**
\`\`\`bash
# List deployed models
firectl list deployed-models

# Find specific deployed model
firectl list deployed-models --filter 'model="<MODEL_NAME>" AND deployment="<DEPLOYMENT_NAME>"'
\`\`\`

### 3. Managing Default Deployments

**Check default deployment:**
\`\`\`bash
# See which deployment is marked as default
firectl list deployed-models
# Look for "Default: true" in the output
\`\`\`

**Update default deployment:**
\`\`\`bash
# Update which deployment is the default
firectl update deployed-model <DEPLOYED_MODEL_ID> --default
\`\`\`

**Important note:** To delete a default deployment, you must either:
- Delete all other deployments for the same model first, OR
- Designate a different deployed model as the default

### 4. Scaling and Cost Management

**Autoscaling configuration:**
- **Default behavior:** Automatically scales to 0 replicas when not in use
- **Billing:** Pay only for GPU time when replicas are active
- **Manual scaling:** Use autoscaling policies to customize behavior

**Cost optimization commands:**
\`\`\`bash
# Scale deployment to zero (cost saving)
firectl update deployment <DEPLOYMENT_ID> --min-replica-count 0

# Or using the Python SDK
from fireworks.client import Fireworks
llm = Fireworks(model="<MODEL_ID>")
llm.scale_to_zero()
\`\`\`

### 5. Updating Deployments

**Update deployment configuration:**
\`\`\`bash
# Update display name, description, or scaling policies
firectl update deployment <DEPLOYMENT_ID> \\
  --display-name "New Name" \\
  --min-replica-count 1 \\
  --max-replica-count 5
\`\`\`

**Available update options:**
- Display name and description
- Replica counts (min/max)
- Autoscaling policies
- Expiration time
- Hardware configuration

### 6. Deleting Deployments

**Basic deletion:**
\`\`\`bash
# Delete a deployment
firectl delete deployment <DEPLOYMENT_ID>

# Force deletion (ignoring safety checks)
firectl delete deployment <DEPLOYMENT_ID> --ignore-checks

# Using Python SDK
llm.delete_deployment(ignore_checks=True)
\`\`\`

**API deletion:**
\`\`\`bash
# Via REST API
curl -X DELETE \\
  https://api.fireworks.ai/v1/accounts/{account_id}/deployments/{deployment_id} \\
  -H "Authorization: Bearer YOUR_API_KEY"
\`\`\`

### 7. Deployment Performance Expectations

**Timeline expectations:**
- **Initial deployment:** Should complete within minutes
- **Scaling from zero:** Brief availability delays while scaling up
- **Troubleshooting:** If deployment takes >1 hour, likely indicates a crash

### 8. Best Practices

**Cost management:**
1. **Monitor regularly:** Check active deployments frequently
2. **Delete unused deployments:** Avoid unnecessary GPU charges
3. **Use autoscaling:** Leverage scale-to-zero for intermittent usage
4. **Consider serverless:** For low-frequency usage patterns

**Configuration tips:**
- Use \`--wait\` flag for synchronous deployment creation
- Configure appropriate min/max replica counts for your use case
- Set expiration times for temporary deployments
- Monitor deployment status during initial creation

### 9. Common Workflows

**Complete deployment lifecycle:**
\`\`\`bash
# 1. Create deployment
firectl create deployment <MODEL_ID> --wait

# 2. Verify it's ready
firectl get deployment <DEPLOYMENT_ID>

# 3. Use the deployment
# Make API calls to the deployment

# 4. Monitor usage
firectl list deployments

# 5. Scale down when not needed
firectl update deployment <DEPLOYMENT_ID> --min-replica-count 0

# 6. Delete when completely done
firectl delete deployment <DEPLOYMENT_ID>
\`\`\`

This comprehensive guide covers everything from basic deployment creation to advanced cost optimization strategies. The key is understanding the balance between performance needs and cost efficiency through proper scaling configuration.",
    "$.messages[6].role": "assistant",
    "$.pid": 86994,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T11:19:37.022552",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "Run math evaluation on sample dataset using pytest interface.",
    "$.eval_metadata.name": "test_pytest_mcp_url",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.version": "v0.2.6.dev15+g6486dbd.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "At least one tool call was made",
    "$.evaluation_result.score": 1,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.row_id": "call-stand-low-night-conference",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "You are a helpful assistant that can answer questions about Fireworks.
ALWAYS provide code or commands to execute to answer the question.",
    "$.messages[0].role": "system",
    "$.messages[1].content": "Can you teach me about how to manage deployments on Fireworks",
    "$.messages[1].role": "user",
    "$.messages[2].content": "I'll help you learn about managing deployments on Fireworks AI. Let me search for comprehensive documentation about deployment management.",
    "$.messages[2].role": "assistant",
    "$.messages[2].tool_calls[0].function.arguments": "{"query": "deployment management Fireworks AI"}",
    "$.messages[2].tool_calls[0].function.name": "search",
    "$.messages[2].tool_calls[0].id": "functions.search:0",
    "$.messages[2].tool_calls[0].type": "function",
    "$.messages[3].content[0].text": "Title: Fireworks AI Developer Platform
Link: https://docs.fireworks.ai/getting-started/introduction
Content: Fireworks AI is the best platform for building AI product experiences with open source AI models. You can run and customize AI models with just a few lines of code!

Fireworks AI is the best platform for building AI product experiences with open source AI models. You can run and customize AI models with just a few lines of code!

Start building with open source AI models

",
    "$.messages[3].content[0].type": "text",
    "$.messages[3].content[1].text": "Title: How do I close my Fireworks.ai account?
Link: https://docs.fireworks.ai/faq-new/account-access/how-do-i-close-my-fireworksai-account
Content: To close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed

To close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed

How do I close my Fireworks.ai account?
To close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed

",
    "$.messages[3].content[1].type": "text",
    "$.messages[3].content[2].text": "Title: Do you host your deployments in the EU or Asia?
Link: https://docs.fireworks.ai/faq-new/support-general/do-you-host-your-deployments-in-the-eu-or-asia
Content: We are currently deployed in multiple U.S.-based locations. However, we’re open to hearing more about your specific requirements. You can: Join our Discord community Write to us at inquiries@fireworks.ai If you're an Enterprise customer, please contact your dedicated customer support representative to ensure a timely response.

We are currently deployed in multiple U.S.-based locations. However, we’re open to hearing more about your specific requirements. You can: Join our Discord community Write to us at inquiries@fireworks.ai If you're an Enterprise customer, please contact your dedicated customer support representative to ensure a timely response.

Do you host your deployments in the EU or Asia?
We are currently deployed in multiple U.S.-based locations. However, we’re open to hearing more about your specific requirements. You can: Join our Discord community Write to us at inquiries@fireworks.ai If you're an Enterprise customer, please contact your dedicated customer support representative to ensure a timely response.

",
    "$.messages[3].content[2].type": "text",
    "$.messages[3].content[3].text": "Title: Introducing the Fireworks Build SDK
Link: https://docs.fireworks.ai/tools-sdks/python-client/sdk-introduction
Content: The Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.

The Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.

Introducing the Fireworks Build SDK
The Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.

",
    "$.messages[3].content[3].type": "text",
    "$.messages[3].content[4].text": "Title: On-demand deployments
Link: https://docs.fireworks.ai/guides/ondemand-deployments
Content: If you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.

If you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.

On-demand deployments
If you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.

",
    "$.messages[3].content[4].type": "text",
    "$.messages[3].content[5].text": "Title: I have multiple Fireworks accounts. When I try to login with Google on Fireworks' web UI, I'm getting signed into the wrong account. How do I fix this?
Link: https://docs.fireworks.ai/faq-new/account-access/i-have-multiple-fireworks-accounts-when-i-try-to-login-with-google-on-fireworks
Content: If you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.

If you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.

I have multiple Fireworks accounts. When I try to login with Google on Fireworks' web UI, I'm getting signed into the wrong account. How do I fix this?
If you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.

",
    "$.messages[3].content[5].type": "text",
    "$.messages[3].content[6].text": "Title: Introduction
Link: https://docs.fireworks.ai/api-reference/introduction
Content: Fireworks AI REST API enables you to interact with various language, image and embedding models using an API Key. It also lets you automate management of models, deployments, datasets, and more.

Fireworks AI REST API enables you to interact with various language, image and embedding models using an API Key. It also lets you automate management of models, deployments, datasets, and more.

Introduction
Fireworks AI REST API enables you to interact with various language, image and embedding models using an API Key. It also lets you automate management of models, deployments, datasets, and more.

",
    "$.messages[3].content[6].type": "text",
    "$.messages[3].content[7].text": "Title: Create a deployment
Link: https://docs.fireworks.ai/tools-sdks/firectl/commands/create-deployment
Content: Create a Deployment on Fireworks AI platform

Create a Deployment on Fireworks AI platform

Creates a new deployment.

",
    "$.messages[3].content[7].type": "text",
    "$.messages[3].content[8].text": "Title: How much does Fireworks cost?
Link: https://docs.fireworks.ai/faq-new/billing-pricing/how-much-does-fireworks-cost
Content: Fireworks AI operates on a pay-as-you-go model for all non-Enterprise usage, and new users automatically receive free credits. You pay based on: Per token for serverless inference Per GPU usage time for on-demand deployments Per token of training data for fine-tuning For customers needing enterprise-grade security and reliability , please reach out to us at inquiries@fireworks.ai to discuss options. Find out more about our current pricing on our Pricing page.

Fireworks AI operates on a pay-as-you-go model for all non-Enterprise usage, and new users automatically receive free credits. You pay based on: Per token for serverless inference Per GPU usage time for on-demand deployments Per token of training data for fine-tuning For customers needing enterprise-grade security and reliability , please reach out to us at inquiries@fireworks.ai to discuss options. Find out more about our current pricing on our Pricing page.

How much does Fireworks cost?
Fireworks AI operates on a pay-as-you-go model for all non-Enterprise usage, and new users automatically receive free credits. You pay based on: Per token for serverless inference Per GPU usage time for on-demand deployments Per token of training data for fine-tuning For customers needing enterprise-grade security and reliability , please reach out to us at inquiries@fireworks.ai to discuss options. Find out more about our current pricing on our Pricing page.

",
    "$.messages[3].content[8].type": "text",
    "$.messages[3].content[9].text": "Title: Python (Fireworks)
Link: https://docs.fireworks.ai/getting-started/quickstart
Content: This is the recommended way to get started : This  Python (Fireworks)  tab uses our  Fireworks Build
SDK  for the best performance and developer experience.
See our  Client-side performance
optimization  guide for
more details. Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. pip install --upgrade fireworks-ai The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either  ~/.bash_profile  for Bash or  ~/.zshrc  for  Zsh .
You can do this by running the command: vim ~/.bash_profile vim ~/.zshrc Add a new line to the file with the following: export FIREWORKS_API_KEY="<API_KEY>" After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. source ~/.bash_profile source ~/.zshrc You can verify that the variable has been set correctly by running  echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. setx FIREWORKS_API_KEY "<API_KEY>" To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: echo %FIREWORKS_API_KEY% You can quickly instantiate the  LLM  class and call the Fireworks API. The Build SDK handles deployment management automatically. from fireworks import LLM

# Basic usage - SDK automatically selects optimal deployment type
llm = LLM(model="llama4-maverick-instruct-basic", deployment_type="auto")

response = llm.chat.completions.create(
    messages=[{"role": "user", "content": "Say this is a test"}]
)

print(response.choices[0].message.content) You can also pass the API key directly to the LLM constructor:  LLM(model="llama4-maverick-instruct-basic", deployment_type="auto", api_key="<FIREWORKS_API_KEY>")

This is the recommended way to get started : This  Python (Fireworks)  tab uses our  Fireworks Build
SDK  for the best performance and developer experience.
See our  Client-side performance
optimization  guide for
more details. Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. pip install --upgrade fireworks-ai The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either  ~/.bash_profile  for Bash or  ~/.zshrc  for  Zsh .
You can do this by running the command: vim ~/.bash_profile vim ~/.zshrc Add a new line to the file with the following: export FIREWORKS_API_KEY="<API_KEY>" After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. source ~/.bash_profile source ~/.zshrc You can verify that the variable has been set correctly by running  echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. setx FIREWORKS_API_KEY "<API_KEY>" To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: echo %FIREWORKS_API_KEY% You can quickly instantiate the  LLM  class and call the Fireworks API. The Build SDK handles deployment management automatically. from fireworks import LLM

# Basic usage - SDK automatically selects optimal deployment type
llm = LLM(model="llama4-maverick-instruct-basic", deployment_type="auto")

response = llm.chat.completions.create(
    messages=[{"role": "user", "content": "Say this is a test"}]
)

print(response.choices[0].message.content) You can also pass the API key directly to the LLM constructor:  LLM(model="llama4-maverick-instruct-basic", deployment_type="auto", api_key="<FIREWORKS_API_KEY>")

Set up your developer environment & call the Fireworks API
This is the recommended way to get started : This  tab uses our Fireworks Build
SDK for the best performance and developer experience.
See our Client-side performance
optimization guide for
more details. Install SDK Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Configure API Key Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either ~/.bash_profile for Bash or ~/.zshrc for Zsh .
You can do this by running the command: Add a new line to the file with the following: After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. You can verify that the variable has been set correctly by running echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: Sending the first API Request You can quickly instantiate the LLM class and call the Fireworks API. The Build SDK handles deployment management automatically. You can also pass the API key directly to the LLM constructor: LLM(model="llama4-maverick-instruct-basic", deployment_type="auto", api_key="<FIREWORKS_API_KEY>") Install SDK Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. Fireworks AI platform offers drop-in replacement with OpenAI Python Client. Configure API Key Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either ~/.bash_profile for Bash or ~/.zshrc for Zsh .
You can do this by running the command: Add a new line to the file with the following: After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. You can verify that the variable has been set correctly by running echo $OPENAI_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. setx OPENAI_API_BASE "https://api.fireworks.ai/inference/v1"
setx OPENAI_API_KEY "<API_KEY>"

",
    "$.messages[3].content[9].type": "text",
    "$.messages[3].role": "tool",
    "$.messages[3].tool_call_id": "functions.search:0",
    "$.messages[4].content": "Let me search for more specific information about deployment management commands and workflows.",
    "$.messages[4].role": "assistant",
    "$.messages[4].tool_calls[0].function.arguments": "{"query": "firectl deployment commands create manage update delete"}",
    "$.messages[4].tool_calls[0].function.name": "search",
    "$.messages[4].tool_calls[0].id": "functions.search:1",
    "$.messages[4].tool_calls[0].type": "function",
    "$.messages[5].content[0].text": "Title: Why am I getting "invalid id" errors when using firectl commands like create deployment or list deployments?
Link: https://docs.fireworks.ai/faq-new/fine-tuning/why-am-i-getting-invalid-id-errors-when-using-firectl-commands-like-create-deplo
Content: This error typically occurs when your account ID is not properly configured.

This error typically occurs when your account ID is not properly configured.

Why am I getting "invalid id" errors when using firectl commands like create deployment or list deployments?
This error typically occurs when your account ID is not properly configured.

",
    "$.messages[5].content[0].type": "text",
    "$.messages[5].content[1].text": "Title: Delete a deployment
Link: https://docs.fireworks.ai/tools-sdks/firectl/commands/delete-resources
Content: Deletes an model deployment. firectl delete deployment [flags]

Deletes an model deployment. firectl delete deployment [flags]

Example
firectl delete deployment my-deployment

",
    "$.messages[5].content[1].type": "text",
    "$.messages[5].content[2].text": "Title: Create a deployment
Link: https://docs.fireworks.ai/tools-sdks/firectl/commands/create-deployment
Content: Creates a new deployment. firectl create deployment [flags]

Creates a new deployment. firectl create deployment [flags]

Create a Deployment on Fireworks AI platform

",
    "$.messages[5].content[2].type": "text",
    "$.messages[5].content[3].text": "Title: Managing default deployments
Link: https://docs.fireworks.ai/guides/ondemand-deployments
Content: Since a model may be deployed to multiple deployments, querying by model name will route to the "default" deployed model. You can see which deployed model entry is marked with Default: true using: This will show the Deployed Model Refs section with the Default: true entry. To update the default deployed model, note the Name of the deployed model reference above. Then run: firectl update deployed-model <DEPLOYED_MODEL_ID_2> --default

Since a model may be deployed to multiple deployments, querying by model name will route to the "default" deployed model. You can see which deployed model entry is marked with Default: true using: This will show the Deployed Model Refs section with the Default: true entry. To update the default deployed model, note the Name of the deployed model reference above. Then run: firectl update deployed-model <DEPLOYED_MODEL_ID_2> --default

Since a model may be deployed to multiple deployments, querying by model name will route to the "default" deployed model. You can see which deployed model entry is marked with Default: true using: This will show the Deployed Model Refs section with the Default: true entry. To update the default deployed model, note the Name of the deployed model reference above. Then run: To delete a default deployment you must delete all other deployments for the same model first, or designate a different deployed model as the default as described above. This is to ensure that querying by model name will always route to an unambiguous default deployment as long as deployments for the model exist.

",
    "$.messages[5].content[3].type": "text",
    "$.messages[5].content[4].text": "Title: 🚀 Easier & faster LoRA fine-tune deployments on Fireworks
Link: https://docs.fireworks.ai/updates/changelog
Content: You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: Previously, this involved two distinct steps, and the resulting deployment was slower than the base model: Create a deployment using firectl create deployment "accounts/fireworks/models/<MODEL_ID of base model>" --enable-addons Then deploy the addon to the deployment: firectl load-lora <MODEL_ID> --deployment <DEPLOYMENT_ID> For more information, see our deployment documentation. This change is for dedicated deployments with a single LoRA. You can still deploy multiple LoRAs on a deployment or deploy LoRA(s) on some Serverless models as described in the documentation.

",
    "$.messages[5].content[4].type": "text",
    "$.messages[5].content[5].text": "Title: Deployment Command
Link: https://docs.fireworks.ai/evaluators/examples/gcp_cloud_run_deployment_example
Content: It's recommended to run the deployment command from the directory containing the reward function script ( dummy_rewards.py ) and your rewardkit.yaml (if used), for example, from examples/gcp_cloud_run_deployment_example/. Ensure your virtual environment is active: Run the deployment command: Command Explanation: --id my-dummy-gcp-evaluator : A unique ID for your evaluator on the Fireworks AI platform. --target gcp-cloud-run : Specifies deployment to GCP Cloud Run. --function-ref dummy_rewards.hello_world_reward : The Python import path to your reward function. If dummy_rewards.py is in the current directory, this reference works. --gcp-auth-mode api-key : Configures the Cloud Run service with API key authentication. reward-kit will generate a key, store it in GCP Secret Manager, and configure the service. The key is also saved to your local rewardkit.yaml under evaluator_endpoint_keys . This is the default if not specified. --verbose : Shows detailed output, including gcloud commands being executed. --force : (Optional) If an evaluator with the same --id already exists, this flag will delete the existing one before creating the new one.

It's recommended to run the deployment command from the directory containing the reward function script ( dummy_rewards.py ) and your rewardkit.yaml (if used), for example, from examples/gcp_cloud_run_deployment_example/. Ensure your virtual environment is active: Run the deployment command: Command Explanation: --id my-dummy-gcp-evaluator : A unique ID for your evaluator on the Fireworks AI platform. --target gcp-cloud-run : Specifies deployment to GCP Cloud Run. --function-ref dummy_rewards.hello_world_reward : The Python import path to your reward function. If dummy_rewards.py is in the current directory, this reference works. --gcp-auth-mode api-key : Configures the Cloud Run service with API key authentication. reward-kit will generate a key, store it in GCP Secret Manager, and configure the service. The key is also saved to your local rewardkit.yaml under evaluator_endpoint_keys . This is the default if not specified. --verbose : Shows detailed output, including gcloud commands being executed. --force : (Optional) If an evaluator with the same --id already exists, this flag will delete the existing one before creating the new one.

It's recommended to run the deployment command from the directory containing the reward function script ( dummy_rewards.py ) and your rewardkit.yaml (if used), for example, from examples/gcp_cloud_run_deployment_example/. Ensure your virtual environment is active: source .venv/bin/activate

",
    "$.messages[5].content[5].type": "text",
    "$.messages[5].content[6].text": "Title: Quick deployment
Link: https://docs.fireworks.ai/fine-tuning/single-lora
Content: Deploy your LoRA fine-tuned model with one simple command: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

Deploy your LoRA fine-tuned model with one simple command: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

Alternative deployment method
This two-step method is the standard approach for multi-LoRA deployments where multiple LoRA models share the same base model. While it can also be used for single LoRA deployments, it provides slower performance compared to live merge and is not recommended for single LoRA use cases. You can also deploy single LoRA models using a two-step process: Create base model deployment Deploy the base model with addons enabled: firectl create deployment "accounts/fireworks/models/<MODEL_ID of base model>" --enable-addons

",
    "$.messages[5].content[6].type": "text",
    "$.messages[5].content[7].text": "Title: How do I close my Fireworks.ai account?
Link: https://docs.fireworks.ai/faq-new/account-access/how-do-i-close-my-fireworksai-account
Content: To close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed

To close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed

How do I close my Fireworks.ai account?
To close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed

",
    "$.messages[5].content[7].type": "text",
    "$.messages[5].content[8].text": "Title: Delete Deployment
Link: https://docs.fireworks.ai/api-reference/delete-deployment
Content: 



REST Endpoint DELETE /v1/accounts/{account_id}/deployments/{deployment_id}

Server(s): https://api.fireworks.ai


Summary:  

Description: N/A

Security methods: 
header: 
	Authorization: bearer auth
	

Request params: 
path params:
account_id:
	string
	Description: The Account Id
	Example: "<string>"
deployment_id:
	string
	Description: The Deployment Id
	Example: "<string>"

query params:
hard:
	boolean
	Description: If true, this will perform a hard deletion.
	Example: true
ignoreChecks:
	boolean
	Description: If true, this will ignore checks and force the deletion of a deployment that is currently
deployed and is in use.
	Example: true


Request body: N/A


Responses: 
200: object
	Description: N/A
	Example: {}

",
    "$.messages[5].content[8].type": "text",
    "$.messages[5].content[9].text": "Title: Deploying a fine-tuned model using an on-demand deployment
Link: https://docs.fireworks.ai/fine-tuning/fine-tuning-models
Content: Use the following command to deploy your fine-tuned model using an on-demand deployment: firectl create deployment <FINE_TUNED_MODEL_ID>

Use the following command to deploy your fine-tuned model using an on-demand deployment: firectl create deployment <FINE_TUNED_MODEL_ID>

firectl
firectl create dataset <DATASET_ID> /path/to/jsonl/file

",
    "$.messages[5].content[9].type": "text",
    "$.messages[5].role": "tool",
    "$.messages[5].tool_call_id": "functions.search:1",
    "$.messages[6].content": "Let me search for more information about listing and managing existing deployments.",
    "$.messages[6].role": "assistant",
    "$.messages[6].tool_calls[0].function.arguments": "{"query": "list deployments firectl commands deployment management"}",
    "$.messages[6].tool_calls[0].function.name": "search",
    "$.messages[6].tool_calls[0].id": "functions.search:0",
    "$.messages[6].tool_calls[0].type": "function",
    "$.messages[7].content[0].text": "Title: Why am I getting "invalid id" errors when using firectl commands like create deployment or list deployments?
Link: https://docs.fireworks.ai/faq-new/fine-tuning/why-am-i-getting-invalid-id-errors-when-using-firectl-commands-like-create-deplo
Content: This error typically occurs when your account ID is not properly configured.

This error typically occurs when your account ID is not properly configured.

Why am I getting "invalid id" errors when using firectl commands like create deployment or list deployments?
This error typically occurs when your account ID is not properly configured.

",
    "$.messages[7].content[0].type": "text",
    "$.messages[7].content[1].text": "Title: List deployments
Link: https://docs.fireworks.ai/tools-sdks/firectl/commands/list-resources
Content: Prints all deployments in the account. firectl list deployments [flags]

Prints all deployments in the account. firectl list deployments [flags]

List deployed models
Prints all deployed models in an account. firectl list deployed-models [flags]

",
    "$.messages[7].content[1].type": "text",
    "$.messages[7].content[2].text": "Title: 🚀 Easier & faster LoRA fine-tune deployments on Fireworks
Link: https://docs.fireworks.ai/updates/changelog
Content: You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: Previously, this involved two distinct steps, and the resulting deployment was slower than the base model: Create a deployment using firectl create deployment "accounts/fireworks/models/<MODEL_ID of base model>" --enable-addons Then deploy the addon to the deployment: firectl load-lora <MODEL_ID> --deployment <DEPLOYMENT_ID> For more information, see our deployment documentation. This change is for dedicated deployments with a single LoRA. You can still deploy multiple LoRAs on a deployment or deploy LoRA(s) on some Serverless models as described in the documentation.

",
    "$.messages[7].content[2].type": "text",
    "$.messages[7].content[3].text": "Title: Managing default deployments
Link: https://docs.fireworks.ai/guides/ondemand-deployments
Content: Since a model may be deployed to multiple deployments, querying by model name will route to the "default" deployed model. You can see which deployed model entry is marked with Default: true using: firectl get model <MODEL_ID>

Since a model may be deployed to multiple deployments, querying by model name will route to the "default" deployed model. You can see which deployed model entry is marked with Default: true using: firectl get model <MODEL_ID>

Creating an on-demand deployment
Choose a model Check out our model library to see a list of models available for on-demand deployment. You can also upload and use your own custom base model. Create a deployment To create a new deployment of a model from the model library, use: Or, for a custom base model you have uploaded to your account: This command will complete when the deployment is READY . To let it run asynchronously, remove the --wait flag. If your account has purchased reserved capacity and this deployment meets the reservation criteria, it will be counted against that reservation and not billed as an on-demand deployment. Verify the deployment is running You can verify the deployment is active by running: The state field should show READY. The deployment ID is the last part of the deployment name: accounts/<ACCOUNT_ID>/deployments/<DEPLOYMENT_ID>. You can also list all deployments in your account at any time by running: firectl list deployments

",
    "$.messages[7].content[3].type": "text",
    "$.messages[7].content[4].text": "Title: Does Fireworks support custom base models?
Link: https://docs.fireworks.ai/faq-new/models-inference/does-fireworks-support-custom-base-models
Content: Yes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.

Yes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.

Does Fireworks support custom base models?
Yes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.

",
    "$.messages[7].content[4].type": "text",
    "$.messages[7].content[5].text": "Title: Quick deployment
Link: https://docs.fireworks.ai/fine-tuning/single-lora
Content: Deploy your LoRA fine-tuned model with one simple command: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

Deploy your LoRA fine-tuned model with one simple command: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

Alternative deployment method
This two-step method is the standard approach for multi-LoRA deployments where multiple LoRA models share the same base model. While it can also be used for single LoRA deployments, it provides slower performance compared to live merge and is not recommended for single LoRA use cases. You can also deploy single LoRA models using a two-step process: Create base model deployment Deploy the base model with addons enabled: firectl create deployment "accounts/fireworks/models/<MODEL_ID of base model>" --enable-addons

",
    "$.messages[7].content[5].type": "text",
    "$.messages[7].content[6].text": "Title: List Deployments
Link: https://docs.fireworks.ai/api-reference/list-deployments
Content: 



REST Endpoint GET /v1/accounts/{account_id}/deployments

Server(s): https://api.fireworks.ai


Summary:  

Description: N/A

Security methods: 
header: 
	Authorization: bearer auth
	

Request params: 
path params:
account_id:
	string
	Description: The Account Id
	Example: "<string>"

query params:
pageSize:
	integer
	Description: The maximum number of deployments to return. The maximum page_size is 200,
values above 200 will be coerced to 200.
If unspecified, the default is 50.
	Example: 123
pageToken:
	string
	Description: A page token, received from a previous ListDeployments call. Provide this
to retrieve the subsequent page. When paginating, all other parameters
provided to ListDeployments must match the call that provided the page
token.
	Example: "<string>"
filter:
	string
	Description: Only deployment satisfying the provided filter (if specified) will be
returned. See https://google.aip.dev/160 for the filter grammar.
	Example: "<string>"
orderBy:
	string
	Description: A comma-separated list of fields to order by. e.g. "foo,bar"
The default sort order is ascending. To specify a descending order for a
field, append a " desc" suffix. e.g. "foo desc,bar"
Subfields are specified with a "." character. e.g. "foo.bar"
If not specified, the default order is by "create_time".
	Example: "<string>"
showDeleted:
	boolean
	Description: If set, DELETED deployments will be included.
	Example: true
readMask:
	string
	Description: The fields to be returned in the response. If empty or "*", all fields will be returned.
	Example: "<string>"


Request body: N/A


Responses: 
200: object
	Description: N/A
	Example: {"deployments":[{"name":"<string>","displayName":"<string>","description":"<string>","createTime":"2023-11-07T05:31:56Z","expireTime":"2023-11-07T05:31:56Z","purgeTime":"2023-11-07T05:31:56Z","deleteTime":"2023-11-07T05:31:56Z","state":"STATE_UNSPECIFIED","status":{"code":"OK","message":"<string>"},"minReplicaCount":123,"maxReplicaCount":123,"replicaCount":123,"autoscalingPolicy":{"scaleUpWindow":"<string>","scaleDownWindow":"<string>","scaleToZeroWindow":"<string>","loadTargets":{}},"baseModel":"<string>","acceleratorCount":123,"acceleratorType":"ACCELERATOR_TYPE_UNSPECIFIED","precision":"PRECISION_UNSPECIFIED","cluster":"<string>","enableAddons":true,"draftTokenCount":123,"draftModel":"<string>","ngramSpeculationLength":123,"numPeftDeviceCached":123,"deploymentTemplate":"<string>","autoTune":{"longPrompt":true},"placement":{"region":"REGION_UNSPECIFIED","multiRegion":"MULTI_REGION_UNSPECIFIED","regions":["REGION_UNSPECIFIED"]},"region":"REGION_UNSPECIFIED","updateTime":"2023-11-07T05:31:56Z","disableDeploymentSizeValidation":true}],"nextPageToken":"<string>","totalSize":123}

",
    "$.messages[7].content[6].type": "text",
    "$.messages[7].content[7].text": "Title: Deploy Command
Link: https://docs.fireworks.ai/evaluators/cli_reference/cli_overview
Content: The deploy command deploys a reward function as an evaluator on the Fireworks platform.

The deploy command deploys a reward function as an evaluator on the Fireworks platform.

Command Overview
The Reward Kit CLI supports the following main commands: run : Run a local evaluation pipeline using a Hydra configuration. preview : Preview evaluation results or re-evaluate generated outputs. deploy : Deploy a reward function as an evaluator. agent-eval : Run agent evaluations on task bundles. list : List existing evaluators (coming soon). delete : Delete an evaluator (coming soon).

",
    "$.messages[7].content[7].type": "text",
    "$.messages[7].content[8].text": "Title: Deploying a fine-tuned model using an on-demand deployment
Link: https://docs.fireworks.ai/fine-tuning/fine-tuning-models
Content: Use the following command to deploy your fine-tuned model using an on-demand deployment: firectl create deployment <FINE_TUNED_MODEL_ID>

Use the following command to deploy your fine-tuned model using an on-demand deployment: firectl create deployment <FINE_TUNED_MODEL_ID>

Deploying a fine-tuned model serverlessly
Not all base models support serverless addons. Please check this list for the full list of serverless models that support LoRA add-ons. If your base model supports serverless addons, you can run the following command to load the fine-tuned model into a serverless deployment: firectl load-lora <FINE_TUNED_MODEL_ID>

",
    "$.messages[7].content[8].type": "text",
    "$.messages[7].content[9].text": "Title: Create a deployment
Link: https://docs.fireworks.ai/tools-sdks/firectl/commands/create-deployment
Content: Creates a new deployment. firectl create deployment [flags]

Creates a new deployment. firectl create deployment [flags]

Create a Deployment on Fireworks AI platform

",
    "$.messages[7].content[9].type": "text",
    "$.messages[7].role": "tool",
    "$.messages[7].tool_call_id": "functions.search:0",
    "$.messages[8].content": "Based on my research, here's a comprehensive guide to managing deployments on Fireworks AI:

## Overview of Deployment Types

Fireworks AI offers several deployment options:

1. **Serverless Models**: Pay-per-token, shared infrastructure
2. **On-demand Deployments**: Dedicated GPU instances billed by GPU-second
3. **Reserved Capacity**: Pre-purchased dedicated capacity for high-volume usage

## Key Benefits of On-demand Deployments
- Predictable performance unaffected by other users
- No hard rate limits
- Cost-effective for high utilization
- Access to larger model selection
- Support for custom base models

## Managing Deployments with firectl

### Installation and Setup
First, install the Fireworks CLI tools:

\`\`\`bash
pip install fireworks-ai
\`\`\`

Set up your API key:
\`\`\`bash
export FIREWORKS_API_KEY="your-api-key-here"
\`\`\`

### Basic Deployment Commands

#### 1. List All Deployments
\`\`\`bash
firectl list deployments
\`\`\`

#### 2. Create a New Deployment
\`\`\`bash
# Basic deployment
firectl create deployment <MODEL_ID>

# With custom configuration
firectl create deployment <MODEL_ID> --min-replica-count 1 --max-replica-count 3

# Deploy a LoRA fine-tuned model
firectl create deployment "accounts/fireworks/models/<LORA_MODEL_ID>"

# Deploy with addons enabled (for multi-LoRA)
firectl create deployment <MODEL_ID> --enable-addons
\`\`\`

#### 3. Check Deployment Status
\`\`\`bash
firectl get deployment <DEPLOYMENT_ID>
\`\`\`

#### 4. Delete a Deployment
\`\`\`bash
# Soft delete
firectl delete deployment <DEPLOYMENT_ID>

# Force delete (ignores usage checks)
firectl delete deployment <DEPLOYMENT_ID> --ignore-checks
\`\`\`

### Managing Default Deployments
When you have multiple deployments of the same model:

\`\`\`bash
# Check which deployment is default
firectl get model <MODEL_ID>

# Update default deployment
firectl update deployed-model <DEPLOYED_MODEL_ID> --default
\`\`\`

## Using the Fireworks Build SDK (Python)

The Fireworks Build SDK provides a higher-level interface:

\`\`\`python
from fireworks import LLM

# Auto-managed deployment
llm = LLM(model="llama4-maverick-instruct-basic", deployment_type="auto")

# Explicit on-demand deployment
llm = LLM(model="your-model-id", deployment_type="on-demand")

# Make API calls
response = llm.chat.completions.create(
    messages=[{"role": "user", "content": "Hello!"}]
)
\`\`\`

## REST API Management

### List Deployments via REST API
\`\`\`bash
curl -X GET \\
  "https://api.fireworks.ai/v1/accounts/{account_id}/deployments" \\
  -H "Authorization: Bearer $FIREWORKS_API_KEY"
\`\`\`

### Delete Deployment via REST API
\`\`\`bash
curl -X DELETE \\
  "https://api.fireworks.ai/v1/accounts/{account_id}/deployments/{deployment_id}" \\
  -H "Authorization: Bearer $FIREWORKS_API_KEY"
\`\`\`

## Deployment Workflow Examples

### 1. Deploy a Fine-tuned Model
\`\`\`bash
# Single command deployment
firectl create deployment "accounts/fireworks/models/my-lora-model"

# Wait for deployment to be ready
firectl get deployment <deployment-id>
\`\`\`

### 2. Multi-LoRA Deployment (Advanced)
\`\`\`bash
# Step 1: Create base deployment with addons
firectl create deployment "accounts/fireworks/models/base-model" --enable-addons

# Step 2: Load LoRA models
firectl load-lora <LORA_MODEL_ID_1> --deployment <DEPLOYMENT_ID>
firectl load-lora <LORA_MODEL_ID_2> --deployment <DEPLOYMENT_ID>
\`\`\`

### 3. Custom Model Deployment
\`\`\`bash
# First upload your custom model
# Then deploy it
firectl create deployment "accounts/{account_id}/models/custom-model-id"
\`\`\`

## Best Practices

1. **Monitor Costs**: On-demand deployments are billed by GPU-second
2. **Use Reserved Capacity**: For consistent high-volume usage
3. **Set Appropriate Scaling**: Configure min/max replica counts based on expected load
4. **Clean Up**: Delete unused deployments to avoid charges
5. **Check Status**: Always verify deployment state before using

## Troubleshooting

- **"Invalid ID" errors**: Ensure your account ID is properly configured
- **Deployment stuck**: Check resource quotas and account limits
- **Performance issues**: Verify deployment is in READY state and check replica counts

Would you like me to dive deeper into any specific aspect of deployment management, such as scaling configurations, cost optimization, or advanced deployment patterns?",
    "$.messages[8].role": "assistant",
    "$.pid": 89723,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T11:27:14.528488",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "Run math evaluation on sample dataset using pytest interface.",
    "$.eval_metadata.name": "test_pytest_mcp_url",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.version": "v0.2.6.dev15+g6486dbd.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "At least one tool call was made",
    "$.evaluation_result.score": 1,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.row_id": "construct-do-safe-art-song",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "You are a helpful assistant that can answer questions about Fireworks.
ALWAYS provide code or commands to execute to answer the question.",
    "$.messages[0].role": "system",
    "$.messages[1].content": "Can you teach me about how to manage deployments on Fireworks",
    "$.messages[1].role": "user",
    "$.messages[2].content": "I'll help you learn about managing deployments on Fireworks! Let me search for comprehensive information about deployment management.",
    "$.messages[2].role": "assistant",
    "$.messages[2].tool_calls[0].function.arguments": "{"query": "deployment management Fireworks AI"}",
    "$.messages[2].tool_calls[0].function.name": "search",
    "$.messages[2].tool_calls[0].id": "functions.search:0",
    "$.messages[2].tool_calls[0].type": "function",
    "$.messages[3].content[0].text": "Title: Fireworks AI Developer Platform
Link: https://docs.fireworks.ai/getting-started/introduction
Content: Fireworks AI is the best platform for building AI product experiences with open source AI models. You can run and customize AI models with just a few lines of code!

Fireworks AI is the best platform for building AI product experiences with open source AI models. You can run and customize AI models with just a few lines of code!

Start building with open source AI models

",
    "$.messages[3].content[0].type": "text",
    "$.messages[3].content[1].text": "Title: How do I close my Fireworks.ai account?
Link: https://docs.fireworks.ai/faq-new/account-access/how-do-i-close-my-fireworksai-account
Content: To close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed

To close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed

How do I close my Fireworks.ai account?
To close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed

",
    "$.messages[3].content[1].type": "text",
    "$.messages[3].content[2].text": "Title: Do you host your deployments in the EU or Asia?
Link: https://docs.fireworks.ai/faq-new/support-general/do-you-host-your-deployments-in-the-eu-or-asia
Content: We are currently deployed in multiple U.S.-based locations. However, we’re open to hearing more about your specific requirements. You can: Join our Discord community Write to us at inquiries@fireworks.ai If you're an Enterprise customer, please contact your dedicated customer support representative to ensure a timely response.

We are currently deployed in multiple U.S.-based locations. However, we’re open to hearing more about your specific requirements. You can: Join our Discord community Write to us at inquiries@fireworks.ai If you're an Enterprise customer, please contact your dedicated customer support representative to ensure a timely response.

Do you host your deployments in the EU or Asia?
We are currently deployed in multiple U.S.-based locations. However, we’re open to hearing more about your specific requirements. You can: Join our Discord community Write to us at inquiries@fireworks.ai If you're an Enterprise customer, please contact your dedicated customer support representative to ensure a timely response.

",
    "$.messages[3].content[2].type": "text",
    "$.messages[3].content[3].text": "Title: Introducing the Fireworks Build SDK
Link: https://docs.fireworks.ai/tools-sdks/python-client/sdk-introduction
Content: The Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.

The Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.

Introducing the Fireworks Build SDK
The Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.

",
    "$.messages[3].content[3].type": "text",
    "$.messages[3].content[4].text": "Title: On-demand deployments
Link: https://docs.fireworks.ai/guides/ondemand-deployments
Content: If you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.

If you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.

On-demand deployments
If you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.

",
    "$.messages[3].content[4].type": "text",
    "$.messages[3].content[5].text": "Title: I have multiple Fireworks accounts. When I try to login with Google on Fireworks' web UI, I'm getting signed into the wrong account. How do I fix this?
Link: https://docs.fireworks.ai/faq-new/account-access/i-have-multiple-fireworks-accounts-when-i-try-to-login-with-google-on-fireworks
Content: If you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.

If you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.

I have multiple Fireworks accounts. When I try to login with Google on Fireworks' web UI, I'm getting signed into the wrong account. How do I fix this?
If you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.

",
    "$.messages[3].content[5].type": "text",
    "$.messages[3].content[6].text": "Title: Introduction
Link: https://docs.fireworks.ai/api-reference/introduction
Content: Fireworks AI REST API enables you to interact with various language, image and embedding models using an API Key. It also lets you automate management of models, deployments, datasets, and more.

Fireworks AI REST API enables you to interact with various language, image and embedding models using an API Key. It also lets you automate management of models, deployments, datasets, and more.

Introduction
Fireworks AI REST API enables you to interact with various language, image and embedding models using an API Key. It also lets you automate management of models, deployments, datasets, and more.

",
    "$.messages[3].content[6].type": "text",
    "$.messages[3].content[7].text": "Title: Create a deployment
Link: https://docs.fireworks.ai/tools-sdks/firectl/commands/create-deployment
Content: Create a Deployment on Fireworks AI platform

Create a Deployment on Fireworks AI platform

Creates a new deployment.

",
    "$.messages[3].content[7].type": "text",
    "$.messages[3].content[8].text": "Title: How much does Fireworks cost?
Link: https://docs.fireworks.ai/faq-new/billing-pricing/how-much-does-fireworks-cost
Content: Fireworks AI operates on a pay-as-you-go model for all non-Enterprise usage, and new users automatically receive free credits. You pay based on: Per token for serverless inference Per GPU usage time for on-demand deployments Per token of training data for fine-tuning For customers needing enterprise-grade security and reliability , please reach out to us at inquiries@fireworks.ai to discuss options. Find out more about our current pricing on our Pricing page.

Fireworks AI operates on a pay-as-you-go model for all non-Enterprise usage, and new users automatically receive free credits. You pay based on: Per token for serverless inference Per GPU usage time for on-demand deployments Per token of training data for fine-tuning For customers needing enterprise-grade security and reliability , please reach out to us at inquiries@fireworks.ai to discuss options. Find out more about our current pricing on our Pricing page.

How much does Fireworks cost?
Fireworks AI operates on a pay-as-you-go model for all non-Enterprise usage, and new users automatically receive free credits. You pay based on: Per token for serverless inference Per GPU usage time for on-demand deployments Per token of training data for fine-tuning For customers needing enterprise-grade security and reliability , please reach out to us at inquiries@fireworks.ai to discuss options. Find out more about our current pricing on our Pricing page.

",
    "$.messages[3].content[8].type": "text",
    "$.messages[3].content[9].text": "Title: Python (Fireworks)
Link: https://docs.fireworks.ai/getting-started/quickstart
Content: This is the recommended way to get started : This  Python (Fireworks)  tab uses our  Fireworks Build
SDK  for the best performance and developer experience.
See our  Client-side performance
optimization  guide for
more details. Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. pip install --upgrade fireworks-ai The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either  ~/.bash_profile  for Bash or  ~/.zshrc  for  Zsh .
You can do this by running the command: vim ~/.bash_profile vim ~/.zshrc Add a new line to the file with the following: export FIREWORKS_API_KEY="<API_KEY>" After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. source ~/.bash_profile source ~/.zshrc You can verify that the variable has been set correctly by running  echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. setx FIREWORKS_API_KEY "<API_KEY>" To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: echo %FIREWORKS_API_KEY% You can quickly instantiate the  LLM  class and call the Fireworks API. The Build SDK handles deployment management automatically. from fireworks import LLM

# Basic usage - SDK automatically selects optimal deployment type
llm = LLM(model="llama4-maverick-instruct-basic", deployment_type="auto")

response = llm.chat.completions.create(
    messages=[{"role": "user", "content": "Say this is a test"}]
)

print(response.choices[0].message.content) You can also pass the API key directly to the LLM constructor:  LLM(model="llama4-maverick-instruct-basic", deployment_type="auto", api_key="<FIREWORKS_API_KEY>")

This is the recommended way to get started : This  Python (Fireworks)  tab uses our  Fireworks Build
SDK  for the best performance and developer experience.
See our  Client-side performance
optimization  guide for
more details. Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. pip install --upgrade fireworks-ai The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either  ~/.bash_profile  for Bash or  ~/.zshrc  for  Zsh .
You can do this by running the command: vim ~/.bash_profile vim ~/.zshrc Add a new line to the file with the following: export FIREWORKS_API_KEY="<API_KEY>" After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. source ~/.bash_profile source ~/.zshrc You can verify that the variable has been set correctly by running  echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. setx FIREWORKS_API_KEY "<API_KEY>" To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: echo %FIREWORKS_API_KEY% You can quickly instantiate the  LLM  class and call the Fireworks API. The Build SDK handles deployment management automatically. from fireworks import LLM

# Basic usage - SDK automatically selects optimal deployment type
llm = LLM(model="llama4-maverick-instruct-basic", deployment_type="auto")

response = llm.chat.completions.create(
    messages=[{"role": "user", "content": "Say this is a test"}]
)

print(response.choices[0].message.content) You can also pass the API key directly to the LLM constructor:  LLM(model="llama4-maverick-instruct-basic", deployment_type="auto", api_key="<FIREWORKS_API_KEY>")

Set up your developer environment & call the Fireworks API
This is the recommended way to get started : This  tab uses our Fireworks Build
SDK for the best performance and developer experience.
See our Client-side performance
optimization guide for
more details. Install SDK Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Configure API Key Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either ~/.bash_profile for Bash or ~/.zshrc for Zsh .
You can do this by running the command: Add a new line to the file with the following: After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. You can verify that the variable has been set correctly by running echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: Sending the first API Request You can quickly instantiate the LLM class and call the Fireworks API. The Build SDK handles deployment management automatically. You can also pass the API key directly to the LLM constructor: LLM(model="llama4-maverick-instruct-basic", deployment_type="auto", api_key="<FIREWORKS_API_KEY>") Install SDK Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. Fireworks AI platform offers drop-in replacement with OpenAI Python Client. Configure API Key Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either ~/.bash_profile for Bash or ~/.zshrc for Zsh .
You can do this by running the command: Add a new line to the file with the following: After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. You can verify that the variable has been set correctly by running echo $OPENAI_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: echo %OPENAI_API_KEY%

",
    "$.messages[3].content[9].type": "text",
    "$.messages[3].role": "tool",
    "$.messages[3].tool_call_id": "functions.search:0",
    "$.messages[4].content": "Let me search for more specific information about deployment management commands and workflows.",
    "$.messages[4].role": "assistant",
    "$.messages[4].tool_calls[0].function.arguments": "{"query": "firectl deployment management commands create delete list"}",
    "$.messages[4].tool_calls[0].function.name": "search",
    "$.messages[4].tool_calls[0].id": "functions.search:1",
    "$.messages[4].tool_calls[0].type": "function",
    "$.messages[5].content[0].text": "Title: Why am I getting "invalid id" errors when using firectl commands like create deployment or list deployments?
Link: https://docs.fireworks.ai/faq-new/fine-tuning/why-am-i-getting-invalid-id-errors-when-using-firectl-commands-like-create-deplo
Content: This error typically occurs when your account ID is not properly configured.

This error typically occurs when your account ID is not properly configured.

Why am I getting "invalid id" errors when using firectl commands like create deployment or list deployments?
This error typically occurs when your account ID is not properly configured.

",
    "$.messages[5].content[0].type": "text",
    "$.messages[5].content[1].text": "Title: Delete a deployment
Link: https://docs.fireworks.ai/tools-sdks/firectl/commands/delete-resources
Content: Deletes an model deployment. firectl delete deployment [flags]

Deletes an model deployment. firectl delete deployment [flags]

Example
firectl delete deployment my-deployment

",
    "$.messages[5].content[1].type": "text",
    "$.messages[5].content[2].text": "Title: Create a deployment
Link: https://docs.fireworks.ai/tools-sdks/firectl/commands/create-deployment
Content: Creates a new deployment. firectl create deployment [flags]

Creates a new deployment. firectl create deployment [flags]

Creates a new deployment.

",
    "$.messages[5].content[2].type": "text",
    "$.messages[5].content[3].text": "Title: Creating an on-demand deployment
Link: https://docs.fireworks.ai/guides/ondemand-deployments
Content: Choose a model Check out our model library to see a list of models available for on-demand deployment. You can also upload and use your own custom base model. Create a deployment To create a new deployment of a model from the model library, use: Or, for a custom base model you have uploaded to your account: This command will complete when the deployment is READY . To let it run asynchronously, remove the --wait flag. If your account has purchased reserved capacity and this deployment meets the reservation criteria, it will be counted against that reservation and not billed as an on-demand deployment. Verify the deployment is running You can verify the deployment is active by running: The state field should show READY. The deployment ID is the last part of the deployment name: accounts/<ACCOUNT_ID>/deployments/<DEPLOYMENT_ID>. You can also list all deployments in your account at any time by running: firectl list deployments

Choose a model Check out our model library to see a list of models available for on-demand deployment. You can also upload and use your own custom base model. Create a deployment To create a new deployment of a model from the model library, use: Or, for a custom base model you have uploaded to your account: This command will complete when the deployment is READY . To let it run asynchronously, remove the --wait flag. If your account has purchased reserved capacity and this deployment meets the reservation criteria, it will be counted against that reservation and not billed as an on-demand deployment. Verify the deployment is running You can verify the deployment is active by running: The state field should show READY. The deployment ID is the last part of the deployment name: accounts/<ACCOUNT_ID>/deployments/<DEPLOYMENT_ID>. You can also list all deployments in your account at any time by running: firectl list deployments

Choose a model Check out our model library to see a list of models available for on-demand deployment. You can also upload and use your own custom base model. Create a deployment To create a new deployment of a model from the model library, use: Or, for a custom base model you have uploaded to your account: This command will complete when the deployment is READY . To let it run asynchronously, remove the --wait flag. If your account has purchased reserved capacity and this deployment meets the reservation criteria, it will be counted against that reservation and not billed as an on-demand deployment. Verify the deployment is running You can verify the deployment is active by running: firectl get deployment <DEPLOYMENT_ID>

",
    "$.messages[5].content[3].type": "text",
    "$.messages[5].content[4].text": "Title: Delete Deployment
Link: https://docs.fireworks.ai/api-reference/delete-deployment
Content: 



REST Endpoint DELETE /v1/accounts/{account_id}/deployments/{deployment_id}

Server(s): https://api.fireworks.ai


Summary:  

Description: N/A

Security methods: 
header: 
	Authorization: bearer auth
	

Request params: 
path params:
account_id:
	string
	Description: The Account Id
	Example: "<string>"
deployment_id:
	string
	Description: The Deployment Id
	Example: "<string>"

query params:
hard:
	boolean
	Description: If true, this will perform a hard deletion.
	Example: true
ignoreChecks:
	boolean
	Description: If true, this will ignore checks and force the deletion of a deployment that is currently
deployed and is in use.
	Example: true


Request body: N/A


Responses: 
200: object
	Description: N/A
	Example: {}

",
    "$.messages[5].content[4].type": "text",
    "$.messages[5].content[5].text": "Title: 🚀 Easier & faster LoRA fine-tune deployments on Fireworks
Link: https://docs.fireworks.ai/updates/changelog
Content: You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: Previously, this involved two distinct steps, and the resulting deployment was slower than the base model: Create a deployment using firectl create deployment "accounts/fireworks/models/<MODEL_ID of base model>" --enable-addons Then deploy the addon to the deployment: firectl load-lora <MODEL_ID> --deployment <DEPLOYMENT_ID> For more information, see our deployment documentation. This change is for dedicated deployments with a single LoRA. You can still deploy multiple LoRAs on a deployment or deploy LoRA(s) on some Serverless models as described in the documentation.

",
    "$.messages[5].content[5].type": "text",
    "$.messages[5].content[6].text": "Title: Deployment Command
Link: https://docs.fireworks.ai/evaluators/examples/gcp_cloud_run_deployment_example
Content: It's recommended to run the deployment command from the directory containing the reward function script ( dummy_rewards.py ) and your rewardkit.yaml (if used), for example, from examples/gcp_cloud_run_deployment_example/. Ensure your virtual environment is active: Run the deployment command: Command Explanation: --id my-dummy-gcp-evaluator : A unique ID for your evaluator on the Fireworks AI platform. --target gcp-cloud-run : Specifies deployment to GCP Cloud Run. --function-ref dummy_rewards.hello_world_reward : The Python import path to your reward function. If dummy_rewards.py is in the current directory, this reference works. --gcp-auth-mode api-key : Configures the Cloud Run service with API key authentication. reward-kit will generate a key, store it in GCP Secret Manager, and configure the service. The key is also saved to your local rewardkit.yaml under evaluator_endpoint_keys . This is the default if not specified. --verbose : Shows detailed output, including gcloud commands being executed. --force : (Optional) If an evaluator with the same --id already exists, this flag will delete the existing one before creating the new one.

It's recommended to run the deployment command from the directory containing the reward function script ( dummy_rewards.py ) and your rewardkit.yaml (if used), for example, from examples/gcp_cloud_run_deployment_example/. Ensure your virtual environment is active: Run the deployment command: Command Explanation: --id my-dummy-gcp-evaluator : A unique ID for your evaluator on the Fireworks AI platform. --target gcp-cloud-run : Specifies deployment to GCP Cloud Run. --function-ref dummy_rewards.hello_world_reward : The Python import path to your reward function. If dummy_rewards.py is in the current directory, this reference works. --gcp-auth-mode api-key : Configures the Cloud Run service with API key authentication. reward-kit will generate a key, store it in GCP Secret Manager, and configure the service. The key is also saved to your local rewardkit.yaml under evaluator_endpoint_keys . This is the default if not specified. --verbose : Shows detailed output, including gcloud commands being executed. --force : (Optional) If an evaluator with the same --id already exists, this flag will delete the existing one before creating the new one.

It's recommended to run the deployment command from the directory containing the reward function script ( dummy_rewards.py ) and your rewardkit.yaml (if used), for example, from examples/gcp_cloud_run_deployment_example/. Ensure your virtual environment is active: source .venv/bin/activate

",
    "$.messages[5].content[6].type": "text",
    "$.messages[5].content[7].text": "Title: Quick deployment
Link: https://docs.fireworks.ai/fine-tuning/single-lora
Content: Deploy your LoRA fine-tuned model with one simple command: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

Deploy your LoRA fine-tuned model with one simple command: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

Alternative deployment method
This two-step method is the standard approach for multi-LoRA deployments where multiple LoRA models share the same base model. While it can also be used for single LoRA deployments, it provides slower performance compared to live merge and is not recommended for single LoRA use cases. You can also deploy single LoRA models using a two-step process: Create base model deployment Deploy the base model with addons enabled: firectl create deployment "accounts/fireworks/models/<MODEL_ID of base model>" --enable-addons

",
    "$.messages[5].content[7].type": "text",
    "$.messages[5].content[8].text": "Title: Deploying a fine-tuned model using an on-demand deployment
Link: https://docs.fireworks.ai/fine-tuning/fine-tuning-models
Content: Use the following command to deploy your fine-tuned model using an on-demand deployment: firectl create deployment <FINE_TUNED_MODEL_ID>

Use the following command to deploy your fine-tuned model using an on-demand deployment: firectl create deployment <FINE_TUNED_MODEL_ID>

firectl
firectl create dataset <DATASET_ID> /path/to/jsonl/file

",
    "$.messages[5].content[8].type": "text",
    "$.messages[5].content[9].text": "Title: Deleting Stored Responses
Link: https://docs.fireworks.ai/guides/response-api
Content: When responses are stored (the default behavior with store=True ), you can immediately delete them from storage using the DELETE endpoint. This permanently removes the conversation data. from fireworks import LLM
import requests
import os

llm = LLM(model="qwen3-235b-a22b", deployment_type="serverless")

# Create a response
response = llm.responses.create(
    input="What is the capital of France?",
    store=True  # This is the default
)

response_id = response.id
print(f"Created response with ID: {response_id}")

# Delete the response immediately
headers = {
    "Authorization": f"Bearer {os.getenv('FIREWORKS_API_KEY')}",
    "x-fireworks-account-id": "your-account-id"
}
delete_response = requests.delete(
    f"https://api.fireworks.ai/inference/v1/responses/{response_id}",
    headers=headers
)

if delete_response.status_code == 200:
    print("Response deleted successfully")
else:
    print(f"Failed to delete response: {delete_response.status_code}")

When responses are stored (the default behavior with store=True ), you can immediately delete them from storage using the DELETE endpoint. This permanently removes the conversation data. from fireworks import LLM
import requests
import os

llm = LLM(model="qwen3-235b-a22b", deployment_type="serverless")

# Create a response
response = llm.responses.create(
    input="What is the capital of France?",
    store=True  # This is the default
)

response_id = response.id
print(f"Created response with ID: {response_id}")

# Delete the response immediately
headers = {
    "Authorization": f"Bearer {os.getenv('FIREWORKS_API_KEY')}",
    "x-fireworks-account-id": "your-account-id"
}
delete_response = requests.delete(
    f"https://api.fireworks.ai/inference/v1/responses/{response_id}",
    headers=headers
)

if delete_response.status_code == 200:
    print("Response deleted successfully")
else:
    print(f"Failed to delete response: {delete_response.status_code}")

When responses are stored (the default behavior with store=True ), you can immediately delete them from storage using the DELETE endpoint. This permanently removes the conversation data. import os
from openai import OpenAI
import requests

client = OpenAI(
    base_url="https://api.fireworks.ai/inference/v1",
    api_key=os.getenv("FIREWORKS_API_KEY", "YOUR_FIREWORKS_API_KEY_HERE")
)

# Create a response
response = client.responses.create(
    model="accounts/fireworks/models/qwen3-235b-a22b",
    input="What is the capital of France?",
    store=True  # This is the default
)

response_id = response.id
print(f"Created response with ID: {response_id}")

# Delete the response immediately
headers = {
    "Authorization": f"Bearer {os.getenv('FIREWORKS_API_KEY')}",
    "x-fireworks-account-id": "your-account-id"
}
delete_response = requests.delete(
    f"https://api.fireworks.ai/inference/v1/responses/{response_id}",
    headers=headers
)

if delete_response.status_code == 200:
    print("Response deleted successfully")
else:
    print(f"Failed to delete response: {delete_response.status_code}")

",
    "$.messages[5].content[9].type": "text",
    "$.messages[5].role": "tool",
    "$.messages[5].tool_call_id": "functions.search:1",
    "$.messages[6].content": "Now let me search for information about listing and managing deployments to give you a complete picture.",
    "$.messages[6].role": "assistant",
    "$.messages[6].tool_calls[0].function.arguments": "{"query": "list deployments get deployment status firectl commands"}",
    "$.messages[6].tool_calls[0].function.name": "search",
    "$.messages[6].tool_calls[0].id": "functions.search:1",
    "$.messages[6].tool_calls[0].type": "function",
    "$.messages[7].content[0].text": "Title: Why am I getting "invalid id" errors when using firectl commands like create deployment or list deployments?
Link: https://docs.fireworks.ai/faq-new/fine-tuning/why-am-i-getting-invalid-id-errors-when-using-firectl-commands-like-create-deplo
Content: This error typically occurs when your account ID is not properly configured.

This error typically occurs when your account ID is not properly configured.

Why am I getting "invalid id" errors when using firectl commands like create deployment or list deployments?
This error typically occurs when your account ID is not properly configured.

",
    "$.messages[7].content[0].type": "text",
    "$.messages[7].content[1].text": "Title: List deployments
Link: https://docs.fireworks.ai/tools-sdks/firectl/commands/list-resources
Content: Prints all deployments in the account. firectl list deployments [flags]

Prints all deployments in the account. firectl list deployments [flags]

List deployed models
Prints all deployed models in an account. firectl list deployed-models [flags]

",
    "$.messages[7].content[1].type": "text",
    "$.messages[7].content[2].text": "Title: Creating an on-demand deployment
Link: https://docs.fireworks.ai/guides/ondemand-deployments
Content: Choose a model Check out our model library to see a list of models available for on-demand deployment. You can also upload and use your own custom base model. Create a deployment To create a new deployment of a model from the model library, use: Or, for a custom base model you have uploaded to your account: This command will complete when the deployment is READY . To let it run asynchronously, remove the --wait flag. If your account has purchased reserved capacity and this deployment meets the reservation criteria, it will be counted against that reservation and not billed as an on-demand deployment. Verify the deployment is running You can verify the deployment is active by running: The state field should show READY. The deployment ID is the last part of the deployment name: accounts/<ACCOUNT_ID>/deployments/<DEPLOYMENT_ID>. You can also list all deployments in your account at any time by running: firectl list deployments

Choose a model Check out our model library to see a list of models available for on-demand deployment. You can also upload and use your own custom base model. Create a deployment To create a new deployment of a model from the model library, use: Or, for a custom base model you have uploaded to your account: This command will complete when the deployment is READY . To let it run asynchronously, remove the --wait flag. If your account has purchased reserved capacity and this deployment meets the reservation criteria, it will be counted against that reservation and not billed as an on-demand deployment. Verify the deployment is running You can verify the deployment is active by running: The state field should show READY. The deployment ID is the last part of the deployment name: accounts/<ACCOUNT_ID>/deployments/<DEPLOYMENT_ID>. You can also list all deployments in your account at any time by running: firectl list deployments

Publishing a deployed model
By default, models can only be queried by the account that owns them. To make a deployment public so anyone with a valid Fireworks API key can query it, update the deployed model with the --public flag. Find the Deployed Model ID Every model running on a deployment receives a unique deployed model ID. You can find this ID if you know the model name and deployment name using the following command: firectl list deployed-models --filter 'model="<MODEL_NAME>" AND deployment="<DEPLOYMENT_NAME>"'

",
    "$.messages[7].content[2].type": "text",
    "$.messages[7].content[3].text": "Title: 🚀 Easier & faster LoRA fine-tune deployments on Fireworks
Link: https://docs.fireworks.ai/updates/changelog
Content: You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: Previously, this involved two distinct steps, and the resulting deployment was slower than the base model: Create a deployment using firectl create deployment "accounts/fireworks/models/<MODEL_ID of base model>" --enable-addons Then deploy the addon to the deployment: firectl load-lora <MODEL_ID> --deployment <DEPLOYMENT_ID> For more information, see our deployment documentation. This change is for dedicated deployments with a single LoRA. You can still deploy multiple LoRAs on a deployment or deploy LoRA(s) on some Serverless models as described in the documentation.

",
    "$.messages[7].content[3].type": "text",
    "$.messages[7].content[4].text": "Title: firectl
Link: https://docs.fireworks.ai/fine-tuning/fine-tuning-vlm
Content: # Create a deployment for your fine-tuned VLM
firectl create deployment my-custom-vlm

# Check deployment status
firectl get deployment accounts/your-account/deployment/deployment-id

# Create a deployment for your fine-tuned VLM
firectl create deployment my-custom-vlm

# Check deployment status
firectl get deployment accounts/your-account/deployment/deployment-id

Try with an Example Dataset
To get a feel for how VLM fine-tuning works, you can use an example vision dataset: This is a classification dataset that contains images of food with <think></think> tags for reasoning. Once downloaded, you can upload this dataset using the instructions in the next step. Upload your VLM dataset Upload your prepared JSONL dataset to Fireworks for training: Navigate to the Datasets tab in the Fireworks console, click "Create Dataset", and upload your JSONL file through the wizard. For larger datasets (>500MB), use  as it handles large uploads more reliably than the web interface. For enhanced data control and security, we also support bring your own bucket (BYOB) configurations. See our External GCS Bucket Integration guide for setup details. Launch VLM fine-tuning job Create a supervised fine-tuning job for your VLM: For additional parameters like learning rates, evaluation datasets, and batch sizes, see Additional SFT job settings. Navigate to the Fine-tuning tab in the Fireworks console Click "Create Fine-tuning Job" Select your VLM base model (Qwen 2.5 VL) Choose your uploaded dataset Configure training parameters Launch the job VLM fine-tuning jobs typically take longer than text-only models due to the additional image processing. Expect training times of several hours depending on dataset size and model complexity. Monitor training progress Track your VLM fine-tuning job in the Fireworks console. Monitor key metrics: Training loss : Should generally decrease over time Evaluation loss : Monitor for overfitting if using evaluation dataset Training progress : Epochs completed and estimated time remaining Your VLM fine-tuning job is complete when the status shows COMPLETED and your custom model is ready for deployment. Deploy your fine-tuned VLM Once training is complete, deploy your custom VLM: # Create a deployment for your fine-tuned VLM
firectl create deployment my-custom-vlm

# Check deployment status
firectl get deployment accounts/your-account/deployment/deployment-id

",
    "$.messages[7].content[4].type": "text",
    "$.messages[7].content[5].text": "Title: firectl
Link: https://docs.fireworks.ai/guides/batch-inference
Content: # Get job status
firectl get batch-inference-job my-batch-job

# List all batch jobs
firectl list batch-inference-jobs

# Get job status
firectl get batch-inference-job my-batch-job

# List all batch jobs
firectl list batch-inference-jobs

firectl create dataset batch-input-dataset ./batch_input_data.jsonl

",
    "$.messages[7].content[5].type": "text",
    "$.messages[7].content[6].text": "Title: Does Fireworks support custom base models?
Link: https://docs.fireworks.ai/faq-new/models-inference/does-fireworks-support-custom-base-models
Content: Yes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.

Yes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.

Does Fireworks support custom base models?
Yes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.

",
    "$.messages[7].content[6].type": "text",
    "$.messages[7].content[7].text": "Title: List Deployments
Link: https://docs.fireworks.ai/api-reference/list-deployments
Content: 



REST Endpoint GET /v1/accounts/{account_id}/deployments

Server(s): https://api.fireworks.ai


Summary:  

Description: N/A

Security methods: 
header: 
	Authorization: bearer auth
	

Request params: 
path params:
account_id:
	string
	Description: The Account Id
	Example: "<string>"

query params:
pageSize:
	integer
	Description: The maximum number of deployments to return. The maximum page_size is 200,
values above 200 will be coerced to 200.
If unspecified, the default is 50.
	Example: 123
pageToken:
	string
	Description: A page token, received from a previous ListDeployments call. Provide this
to retrieve the subsequent page. When paginating, all other parameters
provided to ListDeployments must match the call that provided the page
token.
	Example: "<string>"
filter:
	string
	Description: Only deployment satisfying the provided filter (if specified) will be
returned. See https://google.aip.dev/160 for the filter grammar.
	Example: "<string>"
orderBy:
	string
	Description: A comma-separated list of fields to order by. e.g. "foo,bar"
The default sort order is ascending. To specify a descending order for a
field, append a " desc" suffix. e.g. "foo desc,bar"
Subfields are specified with a "." character. e.g. "foo.bar"
If not specified, the default order is by "create_time".
	Example: "<string>"
showDeleted:
	boolean
	Description: If set, DELETED deployments will be included.
	Example: true
readMask:
	string
	Description: The fields to be returned in the response. If empty or "*", all fields will be returned.
	Example: "<string>"


Request body: N/A


Responses: 
200: object
	Description: N/A
	Example: {"deployments":[{"name":"<string>","displayName":"<string>","description":"<string>","createTime":"2023-11-07T05:31:56Z","expireTime":"2023-11-07T05:31:56Z","purgeTime":"2023-11-07T05:31:56Z","deleteTime":"2023-11-07T05:31:56Z","state":"STATE_UNSPECIFIED","status":{"code":"OK","message":"<string>"},"minReplicaCount":123,"maxReplicaCount":123,"replicaCount":123,"autoscalingPolicy":{"scaleUpWindow":"<string>","scaleDownWindow":"<string>","scaleToZeroWindow":"<string>","loadTargets":{}},"baseModel":"<string>","acceleratorCount":123,"acceleratorType":"ACCELERATOR_TYPE_UNSPECIFIED","precision":"PRECISION_UNSPECIFIED","cluster":"<string>","enableAddons":true,"draftTokenCount":123,"draftModel":"<string>","ngramSpeculationLength":123,"numPeftDeviceCached":123,"deploymentTemplate":"<string>","autoTune":{"longPrompt":true},"placement":{"region":"REGION_UNSPECIFIED","multiRegion":"MULTI_REGION_UNSPECIFIED","regions":["REGION_UNSPECIFIED"]},"region":"REGION_UNSPECIFIED","updateTime":"2023-11-07T05:31:56Z","disableDeploymentSizeValidation":true}],"nextPageToken":"<string>","totalSize":123}

",
    "$.messages[7].content[7].type": "text",
    "$.messages[7].content[8].text": "Title: Deploy Command
Link: https://docs.fireworks.ai/evaluators/cli_reference/cli_overview
Content: The deploy command deploys a reward function as an evaluator on the Fireworks platform.

The deploy command deploys a reward function as an evaluator on the Fireworks platform.

Command Overview
The Reward Kit CLI supports the following main commands: run : Run a local evaluation pipeline using a Hydra configuration. preview : Preview evaluation results or re-evaluate generated outputs. deploy : Deploy a reward function as an evaluator. agent-eval : Run agent evaluations on task bundles. list : List existing evaluators (coming soon). delete : Delete an evaluator (coming soon).

",
    "$.messages[7].content[8].type": "text",
    "$.messages[7].content[9].text": "Title: Quick deployment
Link: https://docs.fireworks.ai/fine-tuning/single-lora
Content: Deploy your LoRA fine-tuned model with one simple command: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

Deploy your LoRA fine-tuned model with one simple command: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

Alternative deployment method
This two-step method is the standard approach for multi-LoRA deployments where multiple LoRA models share the same base model. While it can also be used for single LoRA deployments, it provides slower performance compared to live merge and is not recommended for single LoRA use cases. You can also deploy single LoRA models using a two-step process: Create base model deployment Deploy the base model with addons enabled: firectl create deployment "accounts/fireworks/models/<MODEL_ID of base model>" --enable-addons

",
    "$.messages[7].content[9].type": "text",
    "$.messages[7].role": "tool",
    "$.messages[7].tool_call_id": "functions.search:1",
    "$.messages[8].content": "Perfect! Now I have comprehensive information about managing deployments on Fireworks. Let me provide you with a complete guide.

# Managing Deployments on Fireworks: A Complete Guide

Fireworks offers several ways to manage deployments, from command-line tools to REST APIs. Here's everything you need to know:

## Types of Deployments

### 1. On-Demand Deployments
- **Billed by GPU-second** (not per-token like serverless)
- **Dedicated resources** - no interference from other users
- **No hard rate limits** (subject to deployment capacity)
- **Access to more models** including custom ones
- **Better for high utilization** scenarios

### 2. Serverless Deployments
- **Billed per-token**
- **Shared resources** with other users
- **Rate limits apply**
- **Instant availability**

## Management Tools

### Option 1: Fireworks Build SDK (Recommended)
\`\`\`python
from fireworks import LLM

# Auto-deployment - SDK handles everything
llm = LLM(model="llama4-maverick-instruct-basic", deployment_type="auto")

# Manual deployment management
llm = LLM(model="your-model", deployment_type="on-demand")
\`\`\`

### Option 2: firectl CLI Tool

#### Installation
\`\`\`bash
# Install firectl CLI
curl -s https://fireworks.ai/install.sh | bash
\`\`\`

#### Basic Commands

**List all deployments:**
\`\`\`bash
firectl list deployments
\`\`\`

**Create a deployment:**
\`\`\`bash
# From model library
firectl create deployment accounts/fireworks/models/llama-v3p1-8b-instruct --wait

# Custom model
firectl create deployment accounts/your-account-id/models/your-custom-model --wait

# LoRA fine-tuned model (single command)
firectl create deployment accounts/fireworks/models/your-lora-model-id
\`\`\`

**Get deployment status:**
\`\`\`bash
firectl get deployment <deployment-id>
\`\`\`

**Delete a deployment:**
\`\`\`bash
firectl delete deployment <deployment-id>
\`\`\`

**List deployed models:**
\`\`\`bash
firectl list deployed-models
\`\`\`

### Option 3: REST API

#### List Deployments
\`\`\`bash
curl -X GET \\
  "https://api.fireworks.ai/v1/accounts/{account_id}/deployments" \\
  -H "Authorization: Bearer $FIREWORKS_API_KEY"
\`\`\`

#### Create Deployment
\`\`\`bash
curl -X POST \\
  "https://api.fireworks.ai/v1/accounts/{account_id}/deployments" \\
  -H "Authorization: Bearer $FIREWORKS_API_KEY" \\
  -H "Content-Type: application/json" \\
  -d '{
    "deployment": {
      "displayName": "my-deployment",
      "baseModel": "accounts/fireworks/models/llama-v3p1-8b-instruct"
    }
  }'
\`\`\`

#### Delete Deployment
\`\`\`bash
curl -X DELETE \\
  "https://api.fireworks.ai/v1/accounts/{account_id}/deployments/{deployment_id}" \\
  -H "Authorization: Bearer $FIREWORKS_API_KEY"
\`\`\`

## Deployment Workflow

### 1. Choose Your Model
\`\`\`bash
# Check available models
firectl list models
\`\`\`

### 2. Create Deployment
\`\`\`bash
# Basic deployment
firectl create deployment accounts/fireworks/models/llama-v3p1-8b-instruct --wait

# With custom configuration
firectl create deployment accounts/fireworks/models/llama-v3p1-8b-instruct \\
  --min-replica-count 1 \\
  --max-replica-count 3 \\
  --wait
\`\`\`

### 3. Monitor Status
\`\`\`bash
# Get deployment details
firectl get deployment <deployment-id>

# Watch logs
firectl logs deployment <deployment-id>
\`\`\`

### 4. Make Public (Optional)
\`\`\`bash
# Make deployment public
firectl update deployment <deployment-id> --public
\`\`\`

### 5. Clean Up
\`\`\`bash
# Delete when done
firectl delete deployment <deployment-id>
\`\`\`

## Advanced Features

### Reserved Capacity
For high-volume usage, consider reserved capacity:
- More reliable capacity
- Higher GPU quotas
- Cost savings for consistent usage
- Contact Fireworks for enterprise pricing

### Multi-LoRA Deployments
\`\`\`bash
# Create base deployment with addons
firectl create deployment accounts/fireworks/models/base-model --enable-addons --wait

# Load LoRA models
firectl load-lora lora-model-1 --deployment <deployment-id>
firectl load-lora lora-model-2 --deployment <deployment-id>
\`\`\`

### Custom Models
\`\`\`bash
# Upload custom model first
firectl upload model /path/to/model

# Deploy custom model
firectl create deployment accounts/your-account-id/models/custom-model-name
\`\`\`

## Best Practices

1. **Use \`--wait\` flag** for important deployments to ensure they're ready
2. **Monitor costs** with \`firectl list deployments\` to see active deployments
3. **Set up alerts** for deployment health
4. **Use reserved capacity** for production workloads
5. **Clean up unused deployments** to avoid charges
6. **Test deployments** before making them public

## Troubleshooting

**"Invalid ID" errors:**
- Ensure your account ID is properly configured
- Check your API key and account settings

**Deployment stuck:**
- Check resource availability
- Verify model compatibility
- Review deployment logs

**Performance issues:**
- Consider scaling up replicas
- Check if reserved capacity is available
- Monitor GPU utilization

## Cost Management
- **On-demand**: Pay per GPU-second used
- **Serverless**: Pay per token processed
- **Reserved capacity**: Fixed monthly cost for guaranteed capacity
- Use \`firectl list deployments\` to track active deployments and costs

This comprehensive guide should give you everything you need to effectively manage deployments on Fireworks!",
    "$.messages[8].role": "assistant",
    "$.pid": 92995,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T11:38:53.968430",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "Run math evaluation on sample dataset using pytest interface.",
    "$.eval_metadata.name": "test_pytest_mcp_url",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.version": "v0.2.6.dev17+gf8398de.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "At least one tool call was made",
    "$.evaluation_result.score": 1,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.row_id": "have-disassemble-simple-final-experience",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "You are a helpful assistant that can answer questions about Fireworks.
ALWAYS provide code or commands to execute to answer the question.",
    "$.messages[0].role": "system",
    "$.messages[10].content": "Based on my research, I can now provide you with a comprehensive guide on managing deployments on Fireworks. Let me break this down into practical sections with commands and examples.

## Managing Deployments on Fireworks: A Complete Guide

Fireworks offers three deployment types with different management approaches:

1. **Serverless**: Pre-configured, pay-per-token, automatic scaling
2. **On-demand**: Private GPUs, pay-per-second, manual/automatic scaling
3. **Enterprise Reserved**: Custom hardware, tailored setup, SLAs

### 1. Basic Deployment Management Commands

#### Using firectl CLI (Recommended)

**List all deployments:**
\`\`\`bash
firectl list deployments
\`\`\`

**Create a deployment:**
\`\`\`bash
# Basic deployment
firectl create deployment <MODEL_NAME> --wait

# With custom configuration
firectl create deployment <MODEL_NAME> \\
  --min-replica-count 0 \\
  --max-replica-count 3 \\
  --wait
\`\`\`

**Delete a deployment:**
\`\`\`bash
firectl delete deployment <DEPLOYMENT_ID>
\`\`\`

**Update deployment configuration:**
\`\`\`bash
firectl update deployment <DEPLOYMENT_ID> \\
  --min-replica-count 2 \\
  --max-replica-count 5
\`\`\`

#### Using REST API

**List deployments:**
\`\`\`bash
curl -X GET \\
  "https://api.fireworks.ai/v1/accounts/{account_id}/deployments" \\
  -H "Authorization: Bearer $FIREWORKS_API_KEY"
\`\`\`

**Delete deployment via API:**
\`\`\`bash
curl -X DELETE \\
  "https://api.fireworks.ai/v1/accounts/{account_id}/deployments/{deployment_id}" \\
  -H "Authorization: Bearer $FIREWORKS_API_KEY"
\`\`\`

### 2. Python SDK Management

#### Installation and Setup
\`\`\`bash
pip install --upgrade fireworks-ai
export FIREWORKS_API_KEY="your-api-key"
\`\`\`

#### Basic Usage
\`\`\`python
from fireworks import LLM

# Serverless deployment (automatic)
llm = LLM(model="llama4-maverick-instruct-basic", deployment_type="serverless")

# On-demand deployment with custom ID
llm = LLM(
    model="llama4-maverick-instruct-basic",
    deployment_type="on-demand",
    id="my-custom-deployment"
)
llm.apply()  # Required for on-demand deployments

# Auto-selection (serverless if available, otherwise on-demand)
llm = LLM(model="llama4-maverick-instruct-basic", deployment_type="auto")
\`\`\`

#### Fine-tuned Model Deployment
\`\`\`python
# Single LoRA deployment (recommended)
lora_llm = LLM(
    model="accounts/your-account/models/your-lora-model",
    deployment_type="on-demand"
)
lora_llm.apply()

# Multi-LoRA deployment
base_llm = LLM(
    model="accounts/fireworks/models/llama4-maverick-instruct-basic",
    deployment_type="on-demand-lora",
    id="multi-lora-deployment"
)
base_llm.apply()
\`\`\`

### 3. Deployment Configuration Options

#### Scaling Configuration
\`\`\`bash
# Scale between 0-3 replicas based on demand
firectl create deployment <MODEL_NAME> \\
  --min-replica-count 0 \\
  --max-replica-count 3

# Fixed scaling (always have 2 replicas running)
firectl create deployment <MODEL_NAME> \\
  --min-replica-count 2 \\
  --max-replica-count 2
\`\`\`

#### Advanced Configuration
\`\`\`bash
# Custom model deployment
firectl create deployment accounts/your-account/models/custom-model \\
  --min-replica-count 0 \\
  --max-replica-count 5 \\
  --accelerator-type A100 \\
  --accelerator-count 2
\`\`\`

### 4. Cost Management Best Practices

#### Automatic Cost Optimization
- **Scale-to-zero**: Deployments automatically scale to 0 when unused
- **No charges when scaled down**: Only pay for active GPU time
- **Autoscaling**: Automatically adjusts based on traffic

#### Manual Cost Control
\`\`\`python
# Monitor deployment costs
import requests

headers = {"Authorization": f"Bearer {os.getenv('FIREWORKS_API_KEY')}"}
response = requests.get(
    "https://api.fireworks.ai/v1/accounts/{account_id}/deployments",
    headers=headers
)

# Delete unused deployments
for deployment in response.json()['deployments']:
    if deployment['state'] == 'READY' and deployment['replicaCount'] == 0:
        # Consider deleting if unused
        pass
\`\`\`

### 5. Monitoring and Troubleshooting

#### Check Deployment Status
\`\`\`bash
# Get deployment details
firectl get deployment <DEPLOYMENT_ID>

# Check deployed models
firectl list deployed-models
\`\`\`

#### Web Dashboard
Track deployments at: \`https://app.fireworks.ai/dashboard/deployments\`

### 6. Reserved Capacity (Enterprise)

For high-volume usage, consider reserved capacity:
- More reliable capacity
- Higher GPU quotas
- Better pricing for consistent usage
- Contact: inquiries@fireworks.ai

### 7. Common Management Workflows

#### Daily Workflow Example
\`\`\`python
# Morning: Check active deployments
import subprocess
result = subprocess.run(['firectl', 'list', 'deployments'], 
                       capture_output=True, text=True)
print(result.stdout)

# Scale up for expected traffic
subprocess.run(['firectl', 'update', 'deployment', 'my-deployment', 
                '--min-replica-count', '2'])

# Evening: Scale down
subprocess.run(['firectl', 'update', 'deployment', 'my-deployment', 
                '--min-replica-count', '0'])
\`\`\`

#### Automated Cleanup Script
\`\`\`bash
#!/bin/bash
# cleanup_deployments.sh

# List all deployments
deployments=$(firectl list deployments --format json | jq -r '.deployments[].name')

# Delete deployments older than 7 days with 0 replicas
for deployment in $deployments; do
    firectl delete deployment $deployment --ignore-checks
done
\`\`\`

### 8. Important Considerations

- **Billing**: On-demand deployments charge by GPU-second, even when idle
- **Rate limits**: Serverless has rate limits, on-demand does not
- **Cold starts**: Scaling from 0 may have brief delays
- **Monitoring**: Regularly check deployment status and costs
- **Cleanup**: Always delete unused deployments to avoid charges

This comprehensive guide covers all aspects of deployment management on Fireworks. Start with serverless for simple use cases, move to on-demand for production workloads, and consider reserved capacity for enterprise needs.",
    "$.messages[10].role": "assistant",
    "$.messages[1].content": "Can you teach me about how to manage deployments on Fireworks",
    "$.messages[1].role": "user",
    "$.messages[2].content": "I'll help you learn about managing deployments on Fireworks. Let me search for comprehensive information about deployment management.",
    "$.messages[2].role": "assistant",
    "$.messages[2].tool_calls[0].function.arguments": "{"query": "deployment management Fireworks AI"}",
    "$.messages[2].tool_calls[0].function.name": "search",
    "$.messages[2].tool_calls[0].id": "functions.search:0",
    "$.messages[2].tool_calls[0].type": "function",
    "$.messages[3].content[0].text": "Title: Fireworks AI Developer Platform
Link: https://docs.fireworks.ai/getting-started/introduction
Content: Fireworks AI is the best platform for building AI product experiences with open source AI models. You can run and customize AI models with just a few lines of code!

Fireworks AI is the best platform for building AI product experiences with open source AI models. You can run and customize AI models with just a few lines of code!

Start building with open source AI models

",
    "$.messages[3].content[0].type": "text",
    "$.messages[3].content[1].text": "Title: How do I close my Fireworks.ai account?
Link: https://docs.fireworks.ai/faq-new/account-access/how-do-i-close-my-fireworksai-account
Content: To close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed

To close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed

How do I close my Fireworks.ai account?
To close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed

",
    "$.messages[3].content[1].type": "text",
    "$.messages[3].content[2].text": "Title: Do you host your deployments in the EU or Asia?
Link: https://docs.fireworks.ai/faq-new/support-general/do-you-host-your-deployments-in-the-eu-or-asia
Content: We are currently deployed in multiple U.S.-based locations. However, we’re open to hearing more about your specific requirements. You can: Join our Discord community Write to us at inquiries@fireworks.ai If you're an Enterprise customer, please contact your dedicated customer support representative to ensure a timely response.

We are currently deployed in multiple U.S.-based locations. However, we’re open to hearing more about your specific requirements. You can: Join our Discord community Write to us at inquiries@fireworks.ai If you're an Enterprise customer, please contact your dedicated customer support representative to ensure a timely response.

Do you host your deployments in the EU or Asia?
We are currently deployed in multiple U.S.-based locations. However, we’re open to hearing more about your specific requirements. You can: Join our Discord community Write to us at inquiries@fireworks.ai If you're an Enterprise customer, please contact your dedicated customer support representative to ensure a timely response.

",
    "$.messages[3].content[2].type": "text",
    "$.messages[3].content[3].text": "Title: Introducing the Fireworks Build SDK
Link: https://docs.fireworks.ai/tools-sdks/python-client/sdk-introduction
Content: The Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.

The Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.

Introducing the Fireworks Build SDK
The Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.

",
    "$.messages[3].content[3].type": "text",
    "$.messages[3].content[4].text": "Title: On-demand deployments
Link: https://docs.fireworks.ai/guides/ondemand-deployments
Content: If you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.

If you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.

On-demand deployments
If you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.

",
    "$.messages[3].content[4].type": "text",
    "$.messages[3].content[5].text": "Title: I have multiple Fireworks accounts. When I try to login with Google on Fireworks' web UI, I'm getting signed into the wrong account. How do I fix this?
Link: https://docs.fireworks.ai/faq-new/account-access/i-have-multiple-fireworks-accounts-when-i-try-to-login-with-google-on-fireworks
Content: If you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.

If you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.

I have multiple Fireworks accounts. When I try to login with Google on Fireworks' web UI, I'm getting signed into the wrong account. How do I fix this?
If you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.

",
    "$.messages[3].content[5].type": "text",
    "$.messages[3].content[6].text": "Title: Introduction
Link: https://docs.fireworks.ai/api-reference/introduction
Content: Fireworks AI REST API enables you to interact with various language, image and embedding models using an API Key. It also lets you automate management of models, deployments, datasets, and more.

Fireworks AI REST API enables you to interact with various language, image and embedding models using an API Key. It also lets you automate management of models, deployments, datasets, and more.

Introduction
Fireworks AI REST API enables you to interact with various language, image and embedding models using an API Key. It also lets you automate management of models, deployments, datasets, and more.

",
    "$.messages[3].content[6].type": "text",
    "$.messages[3].content[7].text": "Title: Create a deployment
Link: https://docs.fireworks.ai/tools-sdks/firectl/commands/create-deployment
Content: Create a Deployment on Fireworks AI platform

Create a Deployment on Fireworks AI platform

Creates a new deployment.

",
    "$.messages[3].content[7].type": "text",
    "$.messages[3].content[8].text": "Title: How much does Fireworks cost?
Link: https://docs.fireworks.ai/faq-new/billing-pricing/how-much-does-fireworks-cost
Content: Fireworks AI operates on a pay-as-you-go model for all non-Enterprise usage, and new users automatically receive free credits. You pay based on: Per token for serverless inference Per GPU usage time for on-demand deployments Per token of training data for fine-tuning For customers needing enterprise-grade security and reliability , please reach out to us at inquiries@fireworks.ai to discuss options. Find out more about our current pricing on our Pricing page.

Fireworks AI operates on a pay-as-you-go model for all non-Enterprise usage, and new users automatically receive free credits. You pay based on: Per token for serverless inference Per GPU usage time for on-demand deployments Per token of training data for fine-tuning For customers needing enterprise-grade security and reliability , please reach out to us at inquiries@fireworks.ai to discuss options. Find out more about our current pricing on our Pricing page.

How much does Fireworks cost?
Fireworks AI operates on a pay-as-you-go model for all non-Enterprise usage, and new users automatically receive free credits. You pay based on: Per token for serverless inference Per GPU usage time for on-demand deployments Per token of training data for fine-tuning For customers needing enterprise-grade security and reliability , please reach out to us at inquiries@fireworks.ai to discuss options. Find out more about our current pricing on our Pricing page.

",
    "$.messages[3].content[8].type": "text",
    "$.messages[3].content[9].text": "Title: Python (Fireworks)
Link: https://docs.fireworks.ai/getting-started/quickstart
Content: This is the recommended way to get started : This  Python (Fireworks)  tab uses our  Fireworks Build
SDK  for the best performance and developer experience.
See our  Client-side performance
optimization  guide for
more details. Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. pip install --upgrade fireworks-ai The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either  ~/.bash_profile  for Bash or  ~/.zshrc  for  Zsh .
You can do this by running the command: vim ~/.bash_profile vim ~/.zshrc Add a new line to the file with the following: export FIREWORKS_API_KEY="<API_KEY>" After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. source ~/.bash_profile source ~/.zshrc You can verify that the variable has been set correctly by running  echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. setx FIREWORKS_API_KEY "<API_KEY>" To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: echo %FIREWORKS_API_KEY% You can quickly instantiate the  LLM  class and call the Fireworks API. The Build SDK handles deployment management automatically. from fireworks import LLM

# Basic usage - SDK automatically selects optimal deployment type
llm = LLM(model="llama4-maverick-instruct-basic", deployment_type="auto")

response = llm.chat.completions.create(
    messages=[{"role": "user", "content": "Say this is a test"}]
)

print(response.choices[0].message.content) You can also pass the API key directly to the LLM constructor:  LLM(model="llama4-maverick-instruct-basic", deployment_type="auto", api_key="<FIREWORKS_API_KEY>")

This is the recommended way to get started : This  Python (Fireworks)  tab uses our  Fireworks Build
SDK  for the best performance and developer experience.
See our  Client-side performance
optimization  guide for
more details. Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. pip install --upgrade fireworks-ai The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either  ~/.bash_profile  for Bash or  ~/.zshrc  for  Zsh .
You can do this by running the command: vim ~/.bash_profile vim ~/.zshrc Add a new line to the file with the following: export FIREWORKS_API_KEY="<API_KEY>" After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. source ~/.bash_profile source ~/.zshrc You can verify that the variable has been set correctly by running  echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. setx FIREWORKS_API_KEY "<API_KEY>" To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: echo %FIREWORKS_API_KEY% You can quickly instantiate the  LLM  class and call the Fireworks API. The Build SDK handles deployment management automatically. from fireworks import LLM

# Basic usage - SDK automatically selects optimal deployment type
llm = LLM(model="llama4-maverick-instruct-basic", deployment_type="auto")

response = llm.chat.completions.create(
    messages=[{"role": "user", "content": "Say this is a test"}]
)

print(response.choices[0].message.content) You can also pass the API key directly to the LLM constructor:  LLM(model="llama4-maverick-instruct-basic", deployment_type="auto", api_key="<FIREWORKS_API_KEY>")

Set up your developer environment & call the Fireworks API
This is the recommended way to get started : This  tab uses our Fireworks Build
SDK for the best performance and developer experience.
See our Client-side performance
optimization guide for
more details. Install SDK Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Configure API Key Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either ~/.bash_profile for Bash or ~/.zshrc for Zsh .
You can do this by running the command: Add a new line to the file with the following: After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. You can verify that the variable has been set correctly by running echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: Sending the first API Request You can quickly instantiate the LLM class and call the Fireworks API. The Build SDK handles deployment management automatically. You can also pass the API key directly to the LLM constructor: LLM(model="llama4-maverick-instruct-basic", deployment_type="auto", api_key="<FIREWORKS_API_KEY>") Install SDK Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. Fireworks AI platform offers drop-in replacement with OpenAI Python Client. Configure API Key Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either ~/.bash_profile for Bash or ~/.zshrc for Zsh .
You can do this by running the command: Add a new line to the file with the following: After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. You can verify that the variable has been set correctly by running echo $OPENAI_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: echo %OPENAI_API_KEY%

",
    "$.messages[3].content[9].type": "text",
    "$.messages[3].role": "tool",
    "$.messages[3].tool_call_id": "functions.search:0",
    "$.messages[4].content": "Let me search for more specific information about deployment management commands and workflows.",
    "$.messages[4].role": "assistant",
    "$.messages[4].tool_calls[0].function.arguments": "{"query": "firectl deployment management commands create delete list update"}",
    "$.messages[4].tool_calls[0].function.name": "search",
    "$.messages[4].tool_calls[0].id": "functions.search:1",
    "$.messages[4].tool_calls[0].type": "function",
    "$.messages[5].content[0].text": "Title: Why am I getting "invalid id" errors when using firectl commands like create deployment or list deployments?
Link: https://docs.fireworks.ai/faq-new/fine-tuning/why-am-i-getting-invalid-id-errors-when-using-firectl-commands-like-create-deplo
Content: This error typically occurs when your account ID is not properly configured.

This error typically occurs when your account ID is not properly configured.

Why am I getting "invalid id" errors when using firectl commands like create deployment or list deployments?
This error typically occurs when your account ID is not properly configured.

",
    "$.messages[5].content[0].type": "text",
    "$.messages[5].content[1].text": "Title: Delete a deployment
Link: https://docs.fireworks.ai/tools-sdks/firectl/commands/delete-resources
Content: Deletes an model deployment. firectl delete deployment [flags]

Deletes an model deployment. firectl delete deployment [flags]

Example
firectl delete deployment my-deployment

",
    "$.messages[5].content[1].type": "text",
    "$.messages[5].content[2].text": "Title: Create a deployment
Link: https://docs.fireworks.ai/tools-sdks/firectl/commands/create-deployment
Content: Creates a new deployment. firectl create deployment [flags]

Creates a new deployment. firectl create deployment [flags]

Creates a new deployment.

",
    "$.messages[5].content[2].type": "text",
    "$.messages[5].content[3].text": "Title: Creating an on-demand deployment
Link: https://docs.fireworks.ai/guides/ondemand-deployments
Content: Choose a model Check out our model library to see a list of models available for on-demand deployment. You can also upload and use your own custom base model. Create a deployment To create a new deployment of a model from the model library, use: Or, for a custom base model you have uploaded to your account: This command will complete when the deployment is READY . To let it run asynchronously, remove the --wait flag. If your account has purchased reserved capacity and this deployment meets the reservation criteria, it will be counted against that reservation and not billed as an on-demand deployment. Verify the deployment is running You can verify the deployment is active by running: The state field should show READY. The deployment ID is the last part of the deployment name: accounts/<ACCOUNT_ID>/deployments/<DEPLOYMENT_ID>. You can also list all deployments in your account at any time by running: firectl list deployments

Choose a model Check out our model library to see a list of models available for on-demand deployment. You can also upload and use your own custom base model. Create a deployment To create a new deployment of a model from the model library, use: Or, for a custom base model you have uploaded to your account: This command will complete when the deployment is READY . To let it run asynchronously, remove the --wait flag. If your account has purchased reserved capacity and this deployment meets the reservation criteria, it will be counted against that reservation and not billed as an on-demand deployment. Verify the deployment is running You can verify the deployment is active by running: The state field should show READY. The deployment ID is the last part of the deployment name: accounts/<ACCOUNT_ID>/deployments/<DEPLOYMENT_ID>. You can also list all deployments in your account at any time by running: firectl list deployments

Publishing a deployed model
By default, models can only be queried by the account that owns them. To make a deployment public so anyone with a valid Fireworks API key can query it, update the deployed model with the --public flag. Find the Deployed Model ID Every model running on a deployment receives a unique deployed model ID. You can find this ID if you know the model name and deployment name using the following command: firectl list deployed-models --filter 'model="<MODEL_NAME>" AND deployment="<DEPLOYMENT_NAME>"'

",
    "$.messages[5].content[3].type": "text",
    "$.messages[5].content[4].text": "Title: Delete Deployment
Link: https://docs.fireworks.ai/api-reference/delete-deployment
Content: 



REST Endpoint DELETE /v1/accounts/{account_id}/deployments/{deployment_id}

Server(s): https://api.fireworks.ai


Summary:  

Description: N/A

Security methods: 
header: 
	Authorization: bearer auth
	

Request params: 
path params:
account_id:
	string
	Description: The Account Id
	Example: "<string>"
deployment_id:
	string
	Description: The Deployment Id
	Example: "<string>"

query params:
hard:
	boolean
	Description: If true, this will perform a hard deletion.
	Example: true
ignoreChecks:
	boolean
	Description: If true, this will ignore checks and force the deletion of a deployment that is currently
deployed and is in use.
	Example: true


Request body: N/A


Responses: 
200: object
	Description: N/A
	Example: {}

",
    "$.messages[5].content[4].type": "text",
    "$.messages[5].content[5].text": "Title: 🚀 Easier & faster LoRA fine-tune deployments on Fireworks
Link: https://docs.fireworks.ai/updates/changelog
Content: You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: Previously, this involved two distinct steps, and the resulting deployment was slower than the base model: Create a deployment using firectl create deployment "accounts/fireworks/models/<MODEL_ID of base model>" --enable-addons Then deploy the addon to the deployment: firectl load-lora <MODEL_ID> --deployment <DEPLOYMENT_ID> For more information, see our deployment documentation. This change is for dedicated deployments with a single LoRA. You can still deploy multiple LoRAs on a deployment or deploy LoRA(s) on some Serverless models as described in the documentation.

",
    "$.messages[5].content[5].type": "text",
    "$.messages[5].content[6].text": "Title: Deployment Command
Link: https://docs.fireworks.ai/evaluators/examples/gcp_cloud_run_deployment_example
Content: It's recommended to run the deployment command from the directory containing the reward function script ( dummy_rewards.py ) and your rewardkit.yaml (if used), for example, from examples/gcp_cloud_run_deployment_example/. Ensure your virtual environment is active: Run the deployment command: Command Explanation: --id my-dummy-gcp-evaluator : A unique ID for your evaluator on the Fireworks AI platform. --target gcp-cloud-run : Specifies deployment to GCP Cloud Run. --function-ref dummy_rewards.hello_world_reward : The Python import path to your reward function. If dummy_rewards.py is in the current directory, this reference works. --gcp-auth-mode api-key : Configures the Cloud Run service with API key authentication. reward-kit will generate a key, store it in GCP Secret Manager, and configure the service. The key is also saved to your local rewardkit.yaml under evaluator_endpoint_keys . This is the default if not specified. --verbose : Shows detailed output, including gcloud commands being executed. --force : (Optional) If an evaluator with the same --id already exists, this flag will delete the existing one before creating the new one.

It's recommended to run the deployment command from the directory containing the reward function script ( dummy_rewards.py ) and your rewardkit.yaml (if used), for example, from examples/gcp_cloud_run_deployment_example/. Ensure your virtual environment is active: Run the deployment command: Command Explanation: --id my-dummy-gcp-evaluator : A unique ID for your evaluator on the Fireworks AI platform. --target gcp-cloud-run : Specifies deployment to GCP Cloud Run. --function-ref dummy_rewards.hello_world_reward : The Python import path to your reward function. If dummy_rewards.py is in the current directory, this reference works. --gcp-auth-mode api-key : Configures the Cloud Run service with API key authentication. reward-kit will generate a key, store it in GCP Secret Manager, and configure the service. The key is also saved to your local rewardkit.yaml under evaluator_endpoint_keys . This is the default if not specified. --verbose : Shows detailed output, including gcloud commands being executed. --force : (Optional) If an evaluator with the same --id already exists, this flag will delete the existing one before creating the new one.

It's recommended to run the deployment command from the directory containing the reward function script ( dummy_rewards.py ) and your rewardkit.yaml (if used), for example, from examples/gcp_cloud_run_deployment_example/. Ensure your virtual environment is active: source .venv/bin/activate

",
    "$.messages[5].content[6].type": "text",
    "$.messages[5].content[7].text": "Title: Quick deployment
Link: https://docs.fireworks.ai/fine-tuning/single-lora
Content: Deploy your LoRA fine-tuned model with one simple command: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

Deploy your LoRA fine-tuned model with one simple command: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

Alternative deployment method
This two-step method is the standard approach for multi-LoRA deployments where multiple LoRA models share the same base model. While it can also be used for single LoRA deployments, it provides slower performance compared to live merge and is not recommended for single LoRA use cases. You can also deploy single LoRA models using a two-step process: Create base model deployment Deploy the base model with addons enabled: firectl create deployment "accounts/fireworks/models/<MODEL_ID of base model>" --enable-addons

",
    "$.messages[5].content[7].type": "text",
    "$.messages[5].content[8].text": "Title: Deploying a fine-tuned model using an on-demand deployment
Link: https://docs.fireworks.ai/fine-tuning/fine-tuning-models
Content: Use the following command to deploy your fine-tuned model using an on-demand deployment: firectl create deployment <FINE_TUNED_MODEL_ID>

Use the following command to deploy your fine-tuned model using an on-demand deployment: firectl create deployment <FINE_TUNED_MODEL_ID>

firectl
firectl create dataset <DATASET_ID> /path/to/jsonl/file

",
    "$.messages[5].content[8].type": "text",
    "$.messages[5].content[9].text": "Title: Deleting Stored Responses
Link: https://docs.fireworks.ai/guides/response-api
Content: When responses are stored (the default behavior with store=True ), you can immediately delete them from storage using the DELETE endpoint. This permanently removes the conversation data. from fireworks import LLM
import requests
import os

llm = LLM(model="qwen3-235b-a22b", deployment_type="serverless")

# Create a response
response = llm.responses.create(
    input="What is the capital of France?",
    store=True  # This is the default
)

response_id = response.id
print(f"Created response with ID: {response_id}")

# Delete the response immediately
headers = {
    "Authorization": f"Bearer {os.getenv('FIREWORKS_API_KEY')}",
    "x-fireworks-account-id": "your-account-id"
}
delete_response = requests.delete(
    f"https://api.fireworks.ai/inference/v1/responses/{response_id}",
    headers=headers
)

if delete_response.status_code == 200:
    print("Response deleted successfully")
else:
    print(f"Failed to delete response: {delete_response.status_code}")

When responses are stored (the default behavior with store=True ), you can immediately delete them from storage using the DELETE endpoint. This permanently removes the conversation data. from fireworks import LLM
import requests
import os

llm = LLM(model="qwen3-235b-a22b", deployment_type="serverless")

# Create a response
response = llm.responses.create(
    input="What is the capital of France?",
    store=True  # This is the default
)

response_id = response.id
print(f"Created response with ID: {response_id}")

# Delete the response immediately
headers = {
    "Authorization": f"Bearer {os.getenv('FIREWORKS_API_KEY')}",
    "x-fireworks-account-id": "your-account-id"
}
delete_response = requests.delete(
    f"https://api.fireworks.ai/inference/v1/responses/{response_id}",
    headers=headers
)

if delete_response.status_code == 200:
    print("Response deleted successfully")
else:
    print(f"Failed to delete response: {delete_response.status_code}")

When responses are stored (the default behavior with store=True ), you can immediately delete them from storage using the DELETE endpoint. This permanently removes the conversation data. import os
from openai import OpenAI
import requests

client = OpenAI(
    base_url="https://api.fireworks.ai/inference/v1",
    api_key=os.getenv("FIREWORKS_API_KEY", "YOUR_FIREWORKS_API_KEY_HERE")
)

# Create a response
response = client.responses.create(
    model="accounts/fireworks/models/qwen3-235b-a22b",
    input="What is the capital of France?",
    store=True  # This is the default
)

response_id = response.id
print(f"Created response with ID: {response_id}")

# Delete the response immediately
headers = {
    "Authorization": f"Bearer {os.getenv('FIREWORKS_API_KEY')}",
    "x-fireworks-account-id": "your-account-id"
}
delete_response = requests.delete(
    f"https://api.fireworks.ai/inference/v1/responses/{response_id}",
    headers=headers
)

if delete_response.status_code == 200:
    print("Response deleted successfully")
else:
    print(f"Failed to delete response: {delete_response.status_code}")

",
    "$.messages[5].content[9].type": "text",
    "$.messages[5].role": "tool",
    "$.messages[5].tool_call_id": "functions.search:1",
    "$.messages[6].content": "Let me search for more information about listing and managing deployments.",
    "$.messages[6].role": "assistant",
    "$.messages[6].tool_calls[0].function.arguments": "{"query": "list deployments firectl commands deployment management"}",
    "$.messages[6].tool_calls[0].function.name": "search",
    "$.messages[6].tool_calls[0].id": "functions.search:2",
    "$.messages[6].tool_calls[0].type": "function",
    "$.messages[7].content[0].text": "Title: Why am I getting "invalid id" errors when using firectl commands like create deployment or list deployments?
Link: https://docs.fireworks.ai/faq-new/fine-tuning/why-am-i-getting-invalid-id-errors-when-using-firectl-commands-like-create-deplo
Content: This error typically occurs when your account ID is not properly configured.

This error typically occurs when your account ID is not properly configured.

Why am I getting "invalid id" errors when using firectl commands like create deployment or list deployments?
This error typically occurs when your account ID is not properly configured.

",
    "$.messages[7].content[0].type": "text",
    "$.messages[7].content[1].text": "Title: List deployments
Link: https://docs.fireworks.ai/tools-sdks/firectl/commands/list-resources
Content: Prints all deployments in the account. firectl list deployments [flags]

Prints all deployments in the account. firectl list deployments [flags]

List deployed models
Prints all deployed models in an account. firectl list deployed-models [flags]

",
    "$.messages[7].content[1].type": "text",
    "$.messages[7].content[2].text": "Title: 🚀 Easier & faster LoRA fine-tune deployments on Fireworks
Link: https://docs.fireworks.ai/updates/changelog
Content: You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: Previously, this involved two distinct steps, and the resulting deployment was slower than the base model: Create a deployment using firectl create deployment "accounts/fireworks/models/<MODEL_ID of base model>" --enable-addons Then deploy the addon to the deployment: firectl load-lora <MODEL_ID> --deployment <DEPLOYMENT_ID> For more information, see our deployment documentation. This change is for dedicated deployments with a single LoRA. You can still deploy multiple LoRAs on a deployment or deploy LoRA(s) on some Serverless models as described in the documentation.

",
    "$.messages[7].content[2].type": "text",
    "$.messages[7].content[3].text": "Title: Managing default deployments
Link: https://docs.fireworks.ai/guides/ondemand-deployments
Content: Since a model may be deployed to multiple deployments, querying by model name will route to the "default" deployed model. You can see which deployed model entry is marked with Default: true using: firectl get model <MODEL_ID>

Since a model may be deployed to multiple deployments, querying by model name will route to the "default" deployed model. You can see which deployed model entry is marked with Default: true using: firectl get model <MODEL_ID>

Creating an on-demand deployment
Choose a model Check out our model library to see a list of models available for on-demand deployment. You can also upload and use your own custom base model. Create a deployment To create a new deployment of a model from the model library, use: Or, for a custom base model you have uploaded to your account: This command will complete when the deployment is READY . To let it run asynchronously, remove the --wait flag. If your account has purchased reserved capacity and this deployment meets the reservation criteria, it will be counted against that reservation and not billed as an on-demand deployment. Verify the deployment is running You can verify the deployment is active by running: The state field should show READY. The deployment ID is the last part of the deployment name: accounts/<ACCOUNT_ID>/deployments/<DEPLOYMENT_ID>. You can also list all deployments in your account at any time by running: firectl list deployments

",
    "$.messages[7].content[3].type": "text",
    "$.messages[7].content[4].text": "Title: Does Fireworks support custom base models?
Link: https://docs.fireworks.ai/faq-new/models-inference/does-fireworks-support-custom-base-models
Content: Yes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.

Yes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.

Does Fireworks support custom base models?
Yes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.

",
    "$.messages[7].content[4].type": "text",
    "$.messages[7].content[5].text": "Title: Quick deployment
Link: https://docs.fireworks.ai/fine-tuning/single-lora
Content: Deploy your LoRA fine-tuned model with one simple command: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

Deploy your LoRA fine-tuned model with one simple command: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

Alternative deployment method
This two-step method is the standard approach for multi-LoRA deployments where multiple LoRA models share the same base model. While it can also be used for single LoRA deployments, it provides slower performance compared to live merge and is not recommended for single LoRA use cases. You can also deploy single LoRA models using a two-step process: Create base model deployment Deploy the base model with addons enabled: firectl create deployment "accounts/fireworks/models/<MODEL_ID of base model>" --enable-addons

",
    "$.messages[7].content[5].type": "text",
    "$.messages[7].content[6].text": "Title: List Deployments
Link: https://docs.fireworks.ai/api-reference/list-deployments
Content: 



REST Endpoint GET /v1/accounts/{account_id}/deployments

Server(s): https://api.fireworks.ai


Summary:  

Description: N/A

Security methods: 
header: 
	Authorization: bearer auth
	

Request params: 
path params:
account_id:
	string
	Description: The Account Id
	Example: "<string>"

query params:
pageSize:
	integer
	Description: The maximum number of deployments to return. The maximum page_size is 200,
values above 200 will be coerced to 200.
If unspecified, the default is 50.
	Example: 123
pageToken:
	string
	Description: A page token, received from a previous ListDeployments call. Provide this
to retrieve the subsequent page. When paginating, all other parameters
provided to ListDeployments must match the call that provided the page
token.
	Example: "<string>"
filter:
	string
	Description: Only deployment satisfying the provided filter (if specified) will be
returned. See https://google.aip.dev/160 for the filter grammar.
	Example: "<string>"
orderBy:
	string
	Description: A comma-separated list of fields to order by. e.g. "foo,bar"
The default sort order is ascending. To specify a descending order for a
field, append a " desc" suffix. e.g. "foo desc,bar"
Subfields are specified with a "." character. e.g. "foo.bar"
If not specified, the default order is by "create_time".
	Example: "<string>"
showDeleted:
	boolean
	Description: If set, DELETED deployments will be included.
	Example: true
readMask:
	string
	Description: The fields to be returned in the response. If empty or "*", all fields will be returned.
	Example: "<string>"


Request body: N/A


Responses: 
200: object
	Description: N/A
	Example: {"deployments":[{"name":"<string>","displayName":"<string>","description":"<string>","createTime":"2023-11-07T05:31:56Z","expireTime":"2023-11-07T05:31:56Z","purgeTime":"2023-11-07T05:31:56Z","deleteTime":"2023-11-07T05:31:56Z","state":"STATE_UNSPECIFIED","status":{"code":"OK","message":"<string>"},"minReplicaCount":123,"maxReplicaCount":123,"replicaCount":123,"autoscalingPolicy":{"scaleUpWindow":"<string>","scaleDownWindow":"<string>","scaleToZeroWindow":"<string>","loadTargets":{}},"baseModel":"<string>","acceleratorCount":123,"acceleratorType":"ACCELERATOR_TYPE_UNSPECIFIED","precision":"PRECISION_UNSPECIFIED","cluster":"<string>","enableAddons":true,"draftTokenCount":123,"draftModel":"<string>","ngramSpeculationLength":123,"numPeftDeviceCached":123,"deploymentTemplate":"<string>","autoTune":{"longPrompt":true},"placement":{"region":"REGION_UNSPECIFIED","multiRegion":"MULTI_REGION_UNSPECIFIED","regions":["REGION_UNSPECIFIED"]},"region":"REGION_UNSPECIFIED","updateTime":"2023-11-07T05:31:56Z","disableDeploymentSizeValidation":true}],"nextPageToken":"<string>","totalSize":123}

",
    "$.messages[7].content[6].type": "text",
    "$.messages[7].content[7].text": "Title: Deploy Command
Link: https://docs.fireworks.ai/evaluators/cli_reference/cli_overview
Content: The deploy command deploys a reward function as an evaluator on the Fireworks platform.

The deploy command deploys a reward function as an evaluator on the Fireworks platform.

Command Overview
The Reward Kit CLI supports the following main commands: run : Run a local evaluation pipeline using a Hydra configuration. preview : Preview evaluation results or re-evaluate generated outputs. deploy : Deploy a reward function as an evaluator. agent-eval : Run agent evaluations on task bundles. list : List existing evaluators (coming soon). delete : Delete an evaluator (coming soon).

",
    "$.messages[7].content[7].type": "text",
    "$.messages[7].content[8].text": "Title: Deploying a fine-tuned model using an on-demand deployment
Link: https://docs.fireworks.ai/fine-tuning/fine-tuning-models
Content: Use the following command to deploy your fine-tuned model using an on-demand deployment: firectl create deployment <FINE_TUNED_MODEL_ID>

Use the following command to deploy your fine-tuned model using an on-demand deployment: firectl create deployment <FINE_TUNED_MODEL_ID>

Deploying a fine-tuned model serverlessly
Not all base models support serverless addons. Please check this list for the full list of serverless models that support LoRA add-ons. If your base model supports serverless addons, you can run the following command to load the fine-tuned model into a serverless deployment: firectl load-lora <FINE_TUNED_MODEL_ID>

",
    "$.messages[7].content[8].type": "text",
    "$.messages[7].content[9].text": "Title: Create a deployment
Link: https://docs.fireworks.ai/tools-sdks/firectl/commands/create-deployment
Content: Creates a new deployment. firectl create deployment [flags]

Creates a new deployment. firectl create deployment [flags]

Create a Deployment on Fireworks AI platform

",
    "$.messages[7].content[9].type": "text",
    "$.messages[7].role": "tool",
    "$.messages[7].tool_call_id": "functions.search:2",
    "$.messages[8].content": "Now let me search for information about deployment configuration and management best practices.",
    "$.messages[8].role": "assistant",
    "$.messages[8].tool_calls[0].function.arguments": "{"query": "deployment configuration scaling autoscaling reserved capacity Fireworks"}",
    "$.messages[8].tool_calls[0].function.name": "search",
    "$.messages[8].tool_calls[0].id": "functions.search:0",
    "$.messages[8].tool_calls[0].type": "function",
    "$.messages[9].content[0].text": "Title: How does billing and scaling work for on-demand GPU deployments?
Link: https://docs.fireworks.ai/faq-new/deployment-infrastructure/how-does-billing-and-scaling-work-for-on-demand-gpu-deployments
Content: On-demand GPU deployments have unique billing and scaling characteristics compared to serverless deployments: Billing: Charges start when the server begins accepting requests Billed by GPU-second for each active instance Costs accumulate even if there are no active API calls Scaling options: Supports autoscaling from 0 to multiple GPUs Each additional GPU adds to the billing rate Can handle unlimited requests within the GPU’s capacity Management requirements: Not fully serverless; requires some manual management Manually delete deployments when no longer needed Or configure autoscaling to scale down to 0 during inactive periods Cost control tips: Regularly monitor active deployments Delete unused deployments to avoid unnecessary costs Consider serverless options for intermittent usage Use autoscaling to 0 to optimize costs during low-demand times

On-demand GPU deployments have unique billing and scaling characteristics compared to serverless deployments: Billing: Charges start when the server begins accepting requests Billed by GPU-second for each active instance Costs accumulate even if there are no active API calls Scaling options: Supports autoscaling from 0 to multiple GPUs Each additional GPU adds to the billing rate Can handle unlimited requests within the GPU’s capacity Management requirements: Not fully serverless; requires some manual management Manually delete deployments when no longer needed Or configure autoscaling to scale down to 0 during inactive periods Cost control tips: Regularly monitor active deployments Delete unused deployments to avoid unnecessary costs Consider serverless options for intermittent usage Use autoscaling to 0 to optimize costs during low-demand times

How does billing and scaling work for on-demand GPU deployments?
On-demand GPU deployments have unique billing and scaling characteristics compared to serverless deployments: Billing: Charges start when the server begins accepting requests Billed by GPU-second for each active instance Costs accumulate even if there are no active API calls Scaling options: Supports autoscaling from 0 to multiple GPUs Each additional GPU adds to the billing rate Can handle unlimited requests within the GPU’s capacity Management requirements: Not fully serverless; requires some manual management Manually delete deployments when no longer needed Or configure autoscaling to scale down to 0 during inactive periods Cost control tips: Regularly monitor active deployments Delete unused deployments to avoid unnecessary costs Consider serverless options for intermittent usage Use autoscaling to 0 to optimize costs during low-demand times

",
    "$.messages[9].content[0].type": "text",
    "$.messages[9].content[1].text": "Title: What should I expect for deployment and scaling performance?
Link: https://docs.fireworks.ai/faq-new/deployment-infrastructure/what-should-i-expect-for-deployment-and-scaling-performance
Content: Initial deployment : Should complete within minutes Scaling from zero : You may experience brief availability delays while the system scales up Troubleshooting : If deployment takes over 1 hour, this typically indicates a crash and should be investigated Best practice : Monitor deployment status and contact support if deployment times are unusually long

Initial deployment : Should complete within minutes Scaling from zero : You may experience brief availability delays while the system scales up Troubleshooting : If deployment takes over 1 hour, this typically indicates a crash and should be investigated Best practice : Monitor deployment status and contact support if deployment times are unusually long

What should I expect for deployment and scaling performance?
Initial deployment : Should complete within minutes Scaling from zero : You may experience brief availability delays while the system scales up Troubleshooting : If deployment takes over 1 hour, this typically indicates a crash and should be investigated Best practice : Monitor deployment status and contact support if deployment times are unusually long

",
    "$.messages[9].content[1].type": "text",
    "$.messages[9].content[2].text": "Title: Deployment type selection
Link: https://docs.fireworks.ai/tools-sdks/python-client/sdk-basics
Content: The SDK tries to be parsimonious with the way it deploys resources. We provide two types of deployment options on Fireworks: serverless hosting is enabled for some commonly-used state of the art models. The pricing for these models is per-token, i.e. you only pay for the tokens you use, and subject to rate limits. on-demand hosting is enabled for all other models. The pricing for these models is per GPU-second. This hosting is required for models that are not available serverlessly or workloads that exceed serverless rate limits. For non-finetuned models, you can always specify the deployment type of LLM() by passing either "serverless" or "on-demand" as the deployment_type parameter to the constructor. If the model is not available for the deployment type you selected, the SDK will throw an error. The SDK can also decide the best deployment strategy on your behalf, just pass deployment_type="auto" . If the model is available serverlessly, the SDK will use serverless hosting, otherwise the SDK will create an on-demand deployment. When using deployment_type="on-demand" or deployment_type="on-demand-lora" , you must call .apply() to apply the deployment configuration to Fireworks. This is not required for serverless deployments. When using deployment_type="auto" , the SDK will automatically handle deployment creation, but if it falls back to on-demand deployment, you may need to call .apply() explicitly. If you do not call .apply() , you are expected to set up the deployment through the deployment page at https://app.fireworks.ai/dashboard/deployments. Be careful with the deployment_type parameter, especially for "auto" and "on-demand" deployments. While the SDK will try to make the most cost effective choice for you and put sensible autoscaling policies in place, it is possible to unintentionally create many deployments that lead to unwanted spend, especially when working with non-serverless models. When using deployment_type="on-demand" , you must provide an id parameter to uniquely identify your deployment. This is required to prevent accidental creation of multiple deployments. For finetuned (LoRA) models, passing deployment_type="serverless" will try to deploy the finetuned model to serverless hosting, deployment_type="on-demand" will create an on-demand deployment of your base model and merge in your LoRA weights, deployment_type="on-demand-lora" will create an on-demand deployment with Multi-LoRA enabled, and deployment_type="auto" will try to use serverless if available, otherwise fall back to on-demand-lora.

The SDK tries to be parsimonious with the way it deploys resources. We provide two types of deployment options on Fireworks: serverless hosting is enabled for some commonly-used state of the art models. The pricing for these models is per-token, i.e. you only pay for the tokens you use, and subject to rate limits. on-demand hosting is enabled for all other models. The pricing for these models is per GPU-second. This hosting is required for models that are not available serverlessly or workloads that exceed serverless rate limits. For non-finetuned models, you can always specify the deployment type of LLM() by passing either "serverless" or "on-demand" as the deployment_type parameter to the constructor. If the model is not available for the deployment type you selected, the SDK will throw an error. The SDK can also decide the best deployment strategy on your behalf, just pass deployment_type="auto" . If the model is available serverlessly, the SDK will use serverless hosting, otherwise the SDK will create an on-demand deployment. When using deployment_type="on-demand" or deployment_type="on-demand-lora" , you must call .apply() to apply the deployment configuration to Fireworks. This is not required for serverless deployments. When using deployment_type="auto" , the SDK will automatically handle deployment creation, but if it falls back to on-demand deployment, you may need to call .apply() explicitly. If you do not call .apply() , you are expected to set up the deployment through the deployment page at https://app.fireworks.ai/dashboard/deployments. Be careful with the deployment_type parameter, especially for "auto" and "on-demand" deployments. While the SDK will try to make the most cost effective choice for you and put sensible autoscaling policies in place, it is possible to unintentionally create many deployments that lead to unwanted spend, especially when working with non-serverless models. When using deployment_type="on-demand" , you must provide an id parameter to uniquely identify your deployment. This is required to prevent accidental creation of multiple deployments. For finetuned (LoRA) models, passing deployment_type="serverless" will try to deploy the finetuned model to serverless hosting, deployment_type="on-demand" will create an on-demand deployment of your base model and merge in your LoRA weights, deployment_type="on-demand-lora" will create an on-demand deployment with Multi-LoRA enabled, and deployment_type="auto" will try to use serverless if available, otherwise fall back to on-demand-lora.

Deploying Fine-tuned Models with On-Demand
When deploying a fine-tuned model using deployment_type="on-demand" , you need to provide: model - Your fine-tuned model ID (e.g., "accounts/your-account/models/your-fine-tuned-model-id") id - A unique deployment identifier (can be any simple string like "my-fine-tuned-deployment") # Deploy a fine-tuned model with on-demand deployment
fine_tuned_llm = LLM(
    model="accounts/your-account/models/your-fine-tuned-model-id",
    deployment_type="on-demand",
    id="my-fine-tuned-deployment"  # Simple string ID
)

# Apply deployment configuration to Fireworks
fine_tuned_llm.apply()

# Track deployment in web dashboard
print(f"Track at: {fine_tuned_llm.deployment_url}")

",
    "$.messages[9].content[2].type": "text",
    "$.messages[9].content[3].text": "Title: Spike arrest policy
Link: https://docs.fireworks.ai/guides/quotas_usage/rate-limits
Content: LLM traffic that spikes quickly has the potential to be throttled. Here's how it works: Each user has a guaranteed rate limit, which increases with sustained usage near the limit. Typically, you can expect to stay within the limits if your traffic gradually doubles within an hour. You can see your guaranteed limits using API response headers (see below) Exceeding your guaranteed limit means that there's the potential for your requests to be processed with lower-priority. Fireworks operates serverless deployments by autoscaling capacity (within limits) as user traffic increases. However, if a deployment is overloaded while auto-scaling, requests that fall outside of guaranteed limits may be processed with lower-latency or dropped with HTTP code 429 (if limits are significantly exceeded).  You can monitor if you exceed limits via API response header x-ratelimit-over-limit: yes. Exceeding your guaranteed limit does not guarantee that your requests will be throttled. You can monitor if your requests are actually being throttled by monitoring latencies. Here's an example of how dynamic rate limits scale up: Metric Minimum Guaranteed Limit 10 Minutes 1 Hour 2 Hours Requests per minute 60 120 720 1440 Input tokens per minute 60000 120000 720000 1440000 Output tokens per minute 6000 12000 72000 144000

LLM traffic that spikes quickly has the potential to be throttled. Here's how it works: Each user has a guaranteed rate limit, which increases with sustained usage near the limit. Typically, you can expect to stay within the limits if your traffic gradually doubles within an hour. You can see your guaranteed limits using API response headers (see below) Exceeding your guaranteed limit means that there's the potential for your requests to be processed with lower-priority. Fireworks operates serverless deployments by autoscaling capacity (within limits) as user traffic increases. However, if a deployment is overloaded while auto-scaling, requests that fall outside of guaranteed limits may be processed with lower-latency or dropped with HTTP code 429 (if limits are significantly exceeded).  You can monitor if you exceed limits via API response header x-ratelimit-over-limit: yes. Exceeding your guaranteed limit does not guarantee that your requests will be throttled. You can monitor if your requests are actually being throttled by monitoring latencies. Here's an example of how dynamic rate limits scale up: Metric Minimum Guaranteed Limit 10 Minutes 1 Hour 2 Hours Requests per minute 60 120 720 1440 Input tokens per minute 60000 120000 720000 1440000 Output tokens per minute 6000 12000 72000 144000

Spend limits
In order to prevent fraud, Fireworks imposes a monthly spending limit on your account. Once you hit the spending limit, your account will automatically enter a suspended state, API requests will be rejected and all Fireworks usage will be stopped. This includes serverless inference, dedicated deployments, and fine-tuning jobs. Your spend limit will organically increase over time as you spend more on the platform. You can also increase your spend limit at any time, by purchasing prepaid credits to meet the historical spend required for a higher tier. For instance, if you are a new Tier 1 user with $0 historical spend, you can purchase $100 prepaid credits and become a Tier 2 user. You can qualify for a higher tier by adding credits into your Fireworks account. There may be a propagation delay for a few minutes after you prepay for credits - you may still see "monthly usage exceeded error" for a few minutes after adding credits. Tier Qualification Spending Limit Tier 1 Valid payment method added $50/mo Tier 2 $50 spent in payments or credits added $500/mo Tier 3 $500 spent in payments or credits added $5,000/mo Tier 4 $5000 spent in payments or credits added $50,000/mo Unlimited Contact us at inquiries@fireworks.ai Unlimited

",
    "$.messages[9].content[3].type": "text",
    "$.messages[9].content[4].text": "Title: How does billing work for on-demand deployments?
Link: https://docs.fireworks.ai/faq-new/deployment-infrastructure/how-does-billing-work-for-on-demand-deployments
Content: On-demand deployments come with automatic cost optimization features: Default autoscaling : Automatically scales to 0 replicas when not in use Pay for what you use : Charged only for GPU time when replicas are active Flexible configuration : Customize autoscaling behavior to match your needs Best practices for cost management: Leverage default autoscaling : The system automatically scales down deployments when not in use Customize carefully : While you can modify autoscaling behavior using our configuration options , note that preventing scale-to-zero will result in continuous GPU charges Consider your use case : For intermittent or low-frequency usage, serverless deployments might be more cost-effective For detailed configuration options, see our deployment guide.

On-demand deployments come with automatic cost optimization features: Default autoscaling : Automatically scales to 0 replicas when not in use Pay for what you use : Charged only for GPU time when replicas are active Flexible configuration : Customize autoscaling behavior to match your needs Best practices for cost management: Leverage default autoscaling : The system automatically scales down deployments when not in use Customize carefully : While you can modify autoscaling behavior using our configuration options , note that preventing scale-to-zero will result in continuous GPU charges Consider your use case : For intermittent or low-frequency usage, serverless deployments might be more cost-effective For detailed configuration options, see our deployment guide.

How does billing work for on-demand deployments?
On-demand deployments come with automatic cost optimization features: Default autoscaling : Automatically scales to 0 replicas when not in use Pay for what you use : Charged only for GPU time when replicas are active Flexible configuration : Customize autoscaling behavior to match your needs Best practices for cost management: Leverage default autoscaling : The system automatically scales down deployments when not in use Customize carefully : While you can modify autoscaling behavior using our configuration options , note that preventing scale-to-zero will result in continuous GPU charges Consider your use case : For intermittent or low-frequency usage, serverless deployments might be more cost-effective For detailed configuration options, see our deployment guide.

",
    "$.messages[9].content[4].type": "text",
    "$.messages[9].content[5].text": "Title: Replica count (horizontal scaling)
Link: https://docs.fireworks.ai/guides/ondemand-deployments
Content: The number of replicas (horizontal scaling) is specified by passing the --min-replica-count and --max-replica-count flags. Increasing the number of replicas will increase the maximum QPS the deployment can support. The deployment will automatically scale based on server load. Auto-scaling up may fail if there is a GPU stockout. Use reserved capacity to guarantee capacity for your deployments. The default value for --min-replica-count is 0. Setting --min-replica-count to 0 enables the deployment to auto-scale to 0 if a deployment is unused (i.e. no inference requests) for a specified "scale-to-zero" time window. While the deployment is scaled to 0, you will not pay for any GPU utilization. The default value for --max-replica-count is 1 if --min-replica-count=0 , or the value of
--min-replica-count otherwise. firectl create deployment <MODEL_NAME> \\
  --min-replica-count 2 \\
  --max-replica-count 3

The number of replicas (horizontal scaling) is specified by passing the --min-replica-count and --max-replica-count flags. Increasing the number of replicas will increase the maximum QPS the deployment can support. The deployment will automatically scale based on server load. Auto-scaling up may fail if there is a GPU stockout. Use reserved capacity to guarantee capacity for your deployments. The default value for --min-replica-count is 0. Setting --min-replica-count to 0 enables the deployment to auto-scale to 0 if a deployment is unused (i.e. no inference requests) for a specified "scale-to-zero" time window. While the deployment is scaled to 0, you will not pay for any GPU utilization. The default value for --max-replica-count is 1 if --min-replica-count=0 , or the value of
--min-replica-count otherwise. firectl create deployment <MODEL_NAME> \\
  --min-replica-count 2 \\
  --max-replica-count 3

The number of replicas (horizontal scaling) is specified by passing the --min-replica-count and --max-replica-count flags. Increasing the number of replicas will increase the maximum QPS the deployment can support. The deployment will automatically scale based on server load. Auto-scaling up may fail if there is a GPU stockout. Use reserved capacity to guarantee capacity for your deployments. The default value for --min-replica-count is 0. Setting --min-replica-count to 0 enables the deployment to auto-scale to 0 if a deployment is unused (i.e. no inference requests) for a specified "scale-to-zero" time window. While the deployment is scaled to 0, you will not pay for any GPU utilization. The default value for --max-replica-count is 1 if --min-replica-count=0 , or the value of
--min-replica-count otherwise. firectl update deployment <DEPLOYMENT_ID> \\
  --min-replica-count 2 \\
  --max-replica-count 3

",
    "$.messages[9].content[5].type": "text",
    "$.messages[9].content[6].text": "Title: Do you support Auto Scaling?
Link: https://docs.fireworks.ai/faq-new/deployment-infrastructure/do-you-support-auto-scaling
Content: Yes, our system supports auto scaling with the following features: Scaling down to zero capability for resource efficiency Controllable scale-up and scale-down velocity Custom scaling rules and thresholds to match your specific needs

Yes, our system supports auto scaling with the following features: Scaling down to zero capability for resource efficiency Controllable scale-up and scale-down velocity Custom scaling rules and thresholds to match your specific needs

Do you support Auto Scaling?
Yes, our system supports auto scaling with the following features: Scaling down to zero capability for resource efficiency Controllable scale-up and scale-down velocity Custom scaling rules and thresholds to match your specific needs

",
    "$.messages[9].content[6].type": "text",
    "$.messages[9].content[7].text": "Title: Inference
Link: https://docs.fireworks.ai/getting-started/introduction
Content: Fireworks has 3 options for running generative AI models with unparalleled speed and costs. Serverless: The easiest way to get started. Use the most popular models on pre-configured GPUs. Pay per token and avoid cold boots. On-demand: The most flexible option for scaling. Use private GPUs to support your specific needs and only pay when you're using it. GPUs running Fireworks software offer both ~250% improved throughput and 50% improved latency compared to vLLM. Excels for: Production volume - Per-token costs decrease with more volume and there are no set rate limits Custom needs and reliability - On-demand GPUs are private to you. This enables complete control to tailor deployments for speed/throughput/reliability or to run more specialized models Enterprise Reserved GPUs: Use private GPUs with hardware and software set-up personally tailored by the Fireworks team for your use case. Enjoy SLAs, dedicated support, bring-your-own-cloud (BYOC) deployment options, and enterprise-only optimizations. Property Serverless On-demand Enterprise reserved Performance Industry-leading speed on Fireworks-curated set-up. Performance may vary with others' usage. Speed dependent on user-specified GPU configuration and private usage. Per GPU latency should be significantly faster than vLLM. Tailor-made set-up by Fireworks AI experts for best possible latency Getting Started Self-serve - immediately use serverless with 1 line of code Self-serve - configure GPUs, then use them with 1 line of code. Chat with Fireworks Scaling and management Scale up and down freely within rate limits Option for auto-scaling GPUs with traffic. GPUs scale to zero automatically, so no charge for unused GPUs and for boot-ups. Chat with Fireworks Pricing Pay fixed price per token Pay per GPU second with no commitments. Per GPU throughput should be significantly greater than options like vLLM. Customized price based on reserved GPU capacity Commitment None None Arrange plan length with Fireworks Rate limits Yes, see quotas No rate limits. Quotas on number of GPUs None Model Selection Collection of popular models, curated by Fireworks Use 100s of pre-uploaded models or upload your own custom model within supported architecture Use 100s of pre-uploaded models or upload any model

Fireworks has 3 options for running generative AI models with unparalleled speed and costs. Serverless: The easiest way to get started. Use the most popular models on pre-configured GPUs. Pay per token and avoid cold boots. On-demand: The most flexible option for scaling. Use private GPUs to support your specific needs and only pay when you're using it. GPUs running Fireworks software offer both ~250% improved throughput and 50% improved latency compared to vLLM. Excels for: Production volume - Per-token costs decrease with more volume and there are no set rate limits Custom needs and reliability - On-demand GPUs are private to you. This enables complete control to tailor deployments for speed/throughput/reliability or to run more specialized models Enterprise Reserved GPUs: Use private GPUs with hardware and software set-up personally tailored by the Fireworks team for your use case. Enjoy SLAs, dedicated support, bring-your-own-cloud (BYOC) deployment options, and enterprise-only optimizations. Property Serverless On-demand Enterprise reserved Performance Industry-leading speed on Fireworks-curated set-up. Performance may vary with others' usage. Speed dependent on user-specified GPU configuration and private usage. Per GPU latency should be significantly faster than vLLM. Tailor-made set-up by Fireworks AI experts for best possible latency Getting Started Self-serve - immediately use serverless with 1 line of code Self-serve - configure GPUs, then use them with 1 line of code. Chat with Fireworks Scaling and management Scale up and down freely within rate limits Option for auto-scaling GPUs with traffic. GPUs scale to zero automatically, so no charge for unused GPUs and for boot-ups. Chat with Fireworks Pricing Pay fixed price per token Pay per GPU second with no commitments. Per GPU throughput should be significantly greater than options like vLLM. Customized price based on reserved GPU capacity Commitment None None Arrange plan length with Fireworks Rate limits Yes, see quotas No rate limits. Quotas on number of GPUs None Model Selection Collection of popular models, curated by Fireworks Use 100s of pre-uploaded models or upload your own custom model within supported architecture Use 100s of pre-uploaded models or upload any model

Fireworks AI Developer Platform
Fireworks AI is the best platform for building AI product experiences with open source AI models. You can run and customize AI models with just a few lines of code!

",
    "$.messages[9].content[7].type": "text",
    "$.messages[9].content[8].text": "Title: 2. Call a language model using the LLM() class
Link: https://docs.fireworks.ai/tools-sdks/python-client/the-tutorial
Content: Now that your machine is setup with credentials and the SDK, lets ensure you are
ready to make your first LLM call and explain some of the nuances of this SDK. Create a new file called main.py and import the Fireworks AI SDK. Instantiate the LLM class. The LLM class accepts a model argument that you
can use to specify the model you want to use. For this tutorial, we will use the
Llama 4 Maverick
model. When creating an LLM instance, you can specify the deployment type as either "serverless" , "on-demand" , or "auto" . If you pass "auto" , the SDK will try to use serverless hosting if available, otherwise it will create an on-demand deployment. In the other cases, the SDK will try to create a deployment of the specified type and will throw an error if it's not available for the model you selected. The SDK will try and re-use existing deployments for the same model if possible, see Resource management for more details. With great power comes great responsibility! Be careful with the deployment_type parameter, especially for "auto" and "on-demand" . While the SDK will try to make the most cost effective choice for you and put sensible autoscaling policies in place, it is possible to unintentionally create many deployments that lead to unwanted spend, especially when working with non-serverless models. When using deployment_type="on-demand" , you must provide an id parameter to uniquely identify your deployment. This is required to prevent accidental creation of multiple deployments. When using deployment_type="on-demand" or deployment_type="on-demand-lora" , you must call .apply() to apply the deployment configuration to Fireworks. This is not required for serverless deployments. When using deployment_type="auto" , the SDK will automatically handle deployment creation, but if it falls back to on-demand deployment, you may need to call .apply() explicitly. If you do not call .apply() , you are expected to set up the deployment through the deployment page at https://app.fireworks.ai/dashboard/deployments. Make a request to the LLM. The LLM class is OpenAI compatible, so you can use
the same chat completion interface to make a request to the LLM. Hello! It's nice to meet you. Is there something I can help you with or would you like to chat?

Now that your machine is setup with credentials and the SDK, lets ensure you are
ready to make your first LLM call and explain some of the nuances of this SDK. Create a new file called main.py and import the Fireworks AI SDK. Instantiate the LLM class. The LLM class accepts a model argument that you
can use to specify the model you want to use. For this tutorial, we will use the
Llama 4 Maverick
model. When creating an LLM instance, you can specify the deployment type as either "serverless" , "on-demand" , or "auto" . If you pass "auto" , the SDK will try to use serverless hosting if available, otherwise it will create an on-demand deployment. In the other cases, the SDK will try to create a deployment of the specified type and will throw an error if it's not available for the model you selected. The SDK will try and re-use existing deployments for the same model if possible, see Resource management for more details. With great power comes great responsibility! Be careful with the deployment_type parameter, especially for "auto" and "on-demand" . While the SDK will try to make the most cost effective choice for you and put sensible autoscaling policies in place, it is possible to unintentionally create many deployments that lead to unwanted spend, especially when working with non-serverless models. When using deployment_type="on-demand" , you must provide an id parameter to uniquely identify your deployment. This is required to prevent accidental creation of multiple deployments. When using deployment_type="on-demand" or deployment_type="on-demand-lora" , you must call .apply() to apply the deployment configuration to Fireworks. This is not required for serverless deployments. When using deployment_type="auto" , the SDK will automatically handle deployment creation, but if it falls back to on-demand deployment, you may need to call .apply() explicitly. If you do not call .apply() , you are expected to set up the deployment through the deployment page at https://app.fireworks.ai/dashboard/deployments. Make a request to the LLM. The LLM class is OpenAI compatible, so you can use
the same chat completion interface to make a request to the LLM. The great thing about the SDK is that you can use your favorite Python constructs to powerfully work with LLMs. For example, let's try calling a few LLMs in a loop and see how they respond: from fireworks import LLM

llms = [
    "llama4-maverick-instruct-basic",
    "deepseek-r1",
    "qwen2p5-vl-32b-instruct"
]

for llm in llms:
    llm = LLM(model=llm, deployment_type="auto") 
    print("\\n" + "-" * 100)
    print(f"Model: {llm.model}")
    print("-" * 100 + "\\n")

    response = llm.chat.completions.create(
        messages=[{"role": "user", "content": "Hello, world!"}]
    )
    print(response.choices[0].message.content)

Now that your machine is setup with credentials and the SDK, lets ensure you are
ready to make your first LLM call and explain some of the nuances of this SDK. Create a new file called main.py and import the Fireworks AI SDK. Instantiate the LLM class. The LLM class accepts a model argument that you
can use to specify the model you want to use. For this tutorial, we will use the
Llama 4 Maverick
model. When creating an LLM instance, you can specify the deployment type as either "serverless" , "on-demand" , or "auto" . If you pass "auto" , the SDK will try to use serverless hosting if available, otherwise it will create an on-demand deployment. In the other cases, the SDK will try to create a deployment of the specified type and will throw an error if it's not available for the model you selected. The SDK will try and re-use existing deployments for the same model if possible, see Resource management for more details. With great power comes great responsibility! Be careful with the deployment_type parameter, especially for "auto" and "on-demand" . While the SDK will try to make the most cost effective choice for you and put sensible autoscaling policies in place, it is possible to unintentionally create many deployments that lead to unwanted spend, especially when working with non-serverless models. When using deployment_type="on-demand" , you must provide an id parameter to uniquely identify your deployment. This is required to prevent accidental creation of multiple deployments. When using deployment_type="on-demand" or deployment_type="on-demand-lora" , you must call .apply() to apply the deployment configuration to Fireworks. This is not required for serverless deployments. When using deployment_type="auto" , the SDK will automatically handle deployment creation, but if it falls back to on-demand deployment, you may need to call .apply() explicitly. If you do not call .apply() , you are expected to set up the deployment through the deployment page at https://app.fireworks.ai/dashboard/deployments. Make a request to the LLM. The LLM class is OpenAI compatible, so you can use
the same chat completion interface to make a request to the LLM. The great thing about the SDK is that you can use your favorite Python constructs to powerfully work with LLMs. For example, let's try calling a few LLMs in a loop and see how they respond: from fireworks import LLM

llms = [
    "llama4-maverick-instruct-basic",
    "deepseek-r1",
    "qwen2p5-vl-32b-instruct"
]

for llm in llms:
    llm = LLM(model=llm, deployment_type="auto") 
    print("\\n" + "-" * 100)
    print(f"Model: {llm.model}")
    print("-" * 100 + "\\n")

    response = llm.chat.completions.create(
        messages=[{"role": "user", "content": "Hello, world!"}]
    )
    print(response.choices[0].message.content)

",
    "$.messages[9].content[8].type": "text",
    "$.messages[9].content[9].text": "Title: How does autoscaling affect my costs?
Link: https://docs.fireworks.ai/faq-new/deployment-infrastructure/how-does-autoscaling-affect-my-costs
Content: Scaling from 0 : No minimum cost when scaled to zero Scaling up : Each new replica adds to your total cost proportionally. For example: Scaling from 1 to 2 replicas doubles your GPU costs If each replica uses multiple GPUs, costs scale accordingly (e.g., scaling from 1 to 2 replicas with 2 GPUs each means paying for 4 GPUs total) For current pricing details, please visit our pricing page.

Scaling from 0 : No minimum cost when scaled to zero Scaling up : Each new replica adds to your total cost proportionally. For example: Scaling from 1 to 2 replicas doubles your GPU costs If each replica uses multiple GPUs, costs scale accordingly (e.g., scaling from 1 to 2 replicas with 2 GPUs each means paying for 4 GPUs total) For current pricing details, please visit our pricing page.

How does autoscaling affect my costs?
Scaling from 0 : No minimum cost when scaled to zero Scaling up : Each new replica adds to your total cost proportionally. For example: Scaling from 1 to 2 replicas doubles your GPU costs If each replica uses multiple GPUs, costs scale accordingly (e.g., scaling from 1 to 2 replicas with 2 GPUs each means paying for 4 GPUs total) For current pricing details, please visit our pricing page.

",
    "$.messages[9].content[9].type": "text",
    "$.messages[9].role": "tool",
    "$.messages[9].tool_call_id": "functions.search:0",
    "$.pid": 95890,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T11:43:38.694430",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "Run math evaluation on sample dataset using pytest interface.",
    "$.eval_metadata.name": "test_pytest_mcp_url",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.version": "v0.2.6.dev19+gf755623",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "At least one tool call was made",
    "$.evaluation_result.score": 1,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.row_id": "look-win-current-private-teacher",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "You are a helpful assistant that can answer questions about Fireworks.
ALWAYS provide code or commands to execute to answer the question.",
    "$.messages[0].role": "system",
    "$.messages[1].content": "Can you teach me about how to manage deployments on Fireworks",
    "$.messages[1].role": "user",
    "$.messages[2].content": "I'll help you learn about managing deployments on Fireworks. Let me search for comprehensive information about deployment management.",
    "$.messages[2].role": "assistant",
    "$.messages[2].tool_calls[0].function.arguments": "{"query": "deployment management Fireworks AI"}",
    "$.messages[2].tool_calls[0].function.name": "search",
    "$.messages[2].tool_calls[0].id": "functions.search:0",
    "$.messages[2].tool_calls[0].type": "function",
    "$.messages[3].content[0].text": "Title: Fireworks AI Developer Platform
Link: https://docs.fireworks.ai/getting-started/introduction
Content: Fireworks AI is the best platform for building AI product experiences with open source AI models. You can run and customize AI models with just a few lines of code!

Fireworks AI is the best platform for building AI product experiences with open source AI models. You can run and customize AI models with just a few lines of code!

Start building with open source AI models

",
    "$.messages[3].content[0].type": "text",
    "$.messages[3].content[1].text": "Title: How do I close my Fireworks.ai account?
Link: https://docs.fireworks.ai/faq-new/account-access/how-do-i-close-my-fireworksai-account
Content: To close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed

To close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed

How do I close my Fireworks.ai account?
To close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed

",
    "$.messages[3].content[1].type": "text",
    "$.messages[3].content[2].text": "Title: Do you host your deployments in the EU or Asia?
Link: https://docs.fireworks.ai/faq-new/support-general/do-you-host-your-deployments-in-the-eu-or-asia
Content: We are currently deployed in multiple U.S.-based locations. However, we’re open to hearing more about your specific requirements. You can: Join our Discord community Write to us at inquiries@fireworks.ai If you're an Enterprise customer, please contact your dedicated customer support representative to ensure a timely response.

We are currently deployed in multiple U.S.-based locations. However, we’re open to hearing more about your specific requirements. You can: Join our Discord community Write to us at inquiries@fireworks.ai If you're an Enterprise customer, please contact your dedicated customer support representative to ensure a timely response.

Do you host your deployments in the EU or Asia?
We are currently deployed in multiple U.S.-based locations. However, we’re open to hearing more about your specific requirements. You can: Join our Discord community Write to us at inquiries@fireworks.ai If you're an Enterprise customer, please contact your dedicated customer support representative to ensure a timely response.

",
    "$.messages[3].content[2].type": "text",
    "$.messages[3].content[3].text": "Title: Introducing the Fireworks Build SDK
Link: https://docs.fireworks.ai/tools-sdks/python-client/sdk-introduction
Content: The Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.

The Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.

Introducing the Fireworks Build SDK
The Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.

",
    "$.messages[3].content[3].type": "text",
    "$.messages[3].content[4].text": "Title: On-demand deployments
Link: https://docs.fireworks.ai/guides/ondemand-deployments
Content: If you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.

If you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.

On-demand deployments
If you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.

",
    "$.messages[3].content[4].type": "text",
    "$.messages[3].content[5].text": "Title: I have multiple Fireworks accounts. When I try to login with Google on Fireworks' web UI, I'm getting signed into the wrong account. How do I fix this?
Link: https://docs.fireworks.ai/faq-new/account-access/i-have-multiple-fireworks-accounts-when-i-try-to-login-with-google-on-fireworks
Content: If you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.

If you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.

I have multiple Fireworks accounts. When I try to login with Google on Fireworks' web UI, I'm getting signed into the wrong account. How do I fix this?
If you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.

",
    "$.messages[3].content[5].type": "text",
    "$.messages[3].content[6].text": "Title: Introduction
Link: https://docs.fireworks.ai/api-reference/introduction
Content: Fireworks AI REST API enables you to interact with various language, image and embedding models using an API Key. It also lets you automate management of models, deployments, datasets, and more.

Fireworks AI REST API enables you to interact with various language, image and embedding models using an API Key. It also lets you automate management of models, deployments, datasets, and more.

Introduction
Fireworks AI REST API enables you to interact with various language, image and embedding models using an API Key. It also lets you automate management of models, deployments, datasets, and more.

",
    "$.messages[3].content[6].type": "text",
    "$.messages[3].content[7].text": "Title: Create a deployment
Link: https://docs.fireworks.ai/tools-sdks/firectl/commands/create-deployment
Content: Create a Deployment on Fireworks AI platform

Create a Deployment on Fireworks AI platform

Creates a new deployment.

",
    "$.messages[3].content[7].type": "text",
    "$.messages[3].content[8].text": "Title: How much does Fireworks cost?
Link: https://docs.fireworks.ai/faq-new/billing-pricing/how-much-does-fireworks-cost
Content: Fireworks AI operates on a pay-as-you-go model for all non-Enterprise usage, and new users automatically receive free credits. You pay based on: Per token for serverless inference Per GPU usage time for on-demand deployments Per token of training data for fine-tuning For customers needing enterprise-grade security and reliability , please reach out to us at inquiries@fireworks.ai to discuss options. Find out more about our current pricing on our Pricing page.

Fireworks AI operates on a pay-as-you-go model for all non-Enterprise usage, and new users automatically receive free credits. You pay based on: Per token for serverless inference Per GPU usage time for on-demand deployments Per token of training data for fine-tuning For customers needing enterprise-grade security and reliability , please reach out to us at inquiries@fireworks.ai to discuss options. Find out more about our current pricing on our Pricing page.

How much does Fireworks cost?
Fireworks AI operates on a pay-as-you-go model for all non-Enterprise usage, and new users automatically receive free credits. You pay based on: Per token for serverless inference Per GPU usage time for on-demand deployments Per token of training data for fine-tuning For customers needing enterprise-grade security and reliability , please reach out to us at inquiries@fireworks.ai to discuss options. Find out more about our current pricing on our Pricing page.

",
    "$.messages[3].content[8].type": "text",
    "$.messages[3].content[9].text": "Title: Python (Fireworks)
Link: https://docs.fireworks.ai/getting-started/quickstart
Content: This is the recommended way to get started : This  Python (Fireworks)  tab uses our  Fireworks Build
SDK  for the best performance and developer experience.
See our  Client-side performance
optimization  guide for
more details. Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. pip install --upgrade fireworks-ai The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either  ~/.bash_profile  for Bash or  ~/.zshrc  for  Zsh .
You can do this by running the command: vim ~/.bash_profile vim ~/.zshrc Add a new line to the file with the following: export FIREWORKS_API_KEY="<API_KEY>" After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. source ~/.bash_profile source ~/.zshrc You can verify that the variable has been set correctly by running  echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. setx FIREWORKS_API_KEY "<API_KEY>" To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: echo %FIREWORKS_API_KEY% You can quickly instantiate the  LLM  class and call the Fireworks API. The Build SDK handles deployment management automatically. from fireworks import LLM

# Basic usage - SDK automatically selects optimal deployment type
llm = LLM(model="llama4-maverick-instruct-basic", deployment_type="auto")

response = llm.chat.completions.create(
    messages=[{"role": "user", "content": "Say this is a test"}]
)

print(response.choices[0].message.content) You can also pass the API key directly to the LLM constructor:  LLM(model="llama4-maverick-instruct-basic", deployment_type="auto", api_key="<FIREWORKS_API_KEY>")

This is the recommended way to get started : This  Python (Fireworks)  tab uses our  Fireworks Build
SDK  for the best performance and developer experience.
See our  Client-side performance
optimization  guide for
more details. Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. pip install --upgrade fireworks-ai The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either  ~/.bash_profile  for Bash or  ~/.zshrc  for  Zsh .
You can do this by running the command: vim ~/.bash_profile vim ~/.zshrc Add a new line to the file with the following: export FIREWORKS_API_KEY="<API_KEY>" After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. source ~/.bash_profile source ~/.zshrc You can verify that the variable has been set correctly by running  echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. setx FIREWORKS_API_KEY "<API_KEY>" To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: echo %FIREWORKS_API_KEY% You can quickly instantiate the  LLM  class and call the Fireworks API. The Build SDK handles deployment management automatically. from fireworks import LLM

# Basic usage - SDK automatically selects optimal deployment type
llm = LLM(model="llama4-maverick-instruct-basic", deployment_type="auto")

response = llm.chat.completions.create(
    messages=[{"role": "user", "content": "Say this is a test"}]
)

print(response.choices[0].message.content) You can also pass the API key directly to the LLM constructor:  LLM(model="llama4-maverick-instruct-basic", deployment_type="auto", api_key="<FIREWORKS_API_KEY>")

Set up your developer environment & call the Fireworks API
This is the recommended way to get started : This  tab uses our Fireworks Build
SDK for the best performance and developer experience.
See our Client-side performance
optimization guide for
more details. Install SDK Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Configure API Key Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either ~/.bash_profile for Bash or ~/.zshrc for Zsh .
You can do this by running the command: Add a new line to the file with the following: After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. You can verify that the variable has been set correctly by running echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: Sending the first API Request You can quickly instantiate the LLM class and call the Fireworks API. The Build SDK handles deployment management automatically. You can also pass the API key directly to the LLM constructor: LLM(model="llama4-maverick-instruct-basic", deployment_type="auto", api_key="<FIREWORKS_API_KEY>") Install SDK Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. Fireworks AI platform offers drop-in replacement with OpenAI Python Client. Configure API Key Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either ~/.bash_profile for Bash or ~/.zshrc for Zsh .
You can do this by running the command: Add a new line to the file with the following: After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. You can verify that the variable has been set correctly by running echo $OPENAI_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: echo %OPENAI_API_KEY%

",
    "$.messages[3].content[9].type": "text",
    "$.messages[3].role": "tool",
    "$.messages[3].tool_call_id": "functions.search:0",
    "$.messages[4].content": "Let me search for more specific information about deployment management commands and workflows.",
    "$.messages[4].role": "assistant",
    "$.messages[4].tool_calls[0].function.arguments": "{"query": "firectl deployment commands create manage delete update"}",
    "$.messages[4].tool_calls[0].function.name": "search",
    "$.messages[4].tool_calls[0].id": "functions.search:1",
    "$.messages[4].tool_calls[0].type": "function",
    "$.messages[5].content[0].text": "Title: Why am I getting "invalid id" errors when using firectl commands like create deployment or list deployments?
Link: https://docs.fireworks.ai/faq-new/fine-tuning/why-am-i-getting-invalid-id-errors-when-using-firectl-commands-like-create-deplo
Content: This error typically occurs when your account ID is not properly configured.

This error typically occurs when your account ID is not properly configured.

Why am I getting "invalid id" errors when using firectl commands like create deployment or list deployments?
This error typically occurs when your account ID is not properly configured.

",
    "$.messages[5].content[0].type": "text",
    "$.messages[5].content[1].text": "Title: Delete a deployment
Link: https://docs.fireworks.ai/tools-sdks/firectl/commands/delete-resources
Content: Deletes an model deployment. firectl delete deployment [flags]

Deletes an model deployment. firectl delete deployment [flags]

Example
firectl delete deployment my-deployment

",
    "$.messages[5].content[1].type": "text",
    "$.messages[5].content[2].text": "Title: Create a deployment
Link: https://docs.fireworks.ai/tools-sdks/firectl/commands/create-deployment
Content: Creates a new deployment. firectl create deployment [flags]

Creates a new deployment. firectl create deployment [flags]

Create a Deployment on Fireworks AI platform

",
    "$.messages[5].content[2].type": "text",
    "$.messages[5].content[3].text": "Title: Managing default deployments
Link: https://docs.fireworks.ai/guides/ondemand-deployments
Content: Since a model may be deployed to multiple deployments, querying by model name will route to the "default" deployed model. You can see which deployed model entry is marked with Default: true using: This will show the Deployed Model Refs section with the Default: true entry. To update the default deployed model, note the Name of the deployed model reference above. Then run: firectl update deployed-model <DEPLOYED_MODEL_ID_2> --default

Since a model may be deployed to multiple deployments, querying by model name will route to the "default" deployed model. You can see which deployed model entry is marked with Default: true using: This will show the Deployed Model Refs section with the Default: true entry. To update the default deployed model, note the Name of the deployed model reference above. Then run: firectl update deployed-model <DEPLOYED_MODEL_ID_2> --default

Since a model may be deployed to multiple deployments, querying by model name will route to the "default" deployed model. You can see which deployed model entry is marked with Default: true using: This will show the Deployed Model Refs section with the Default: true entry. To update the default deployed model, note the Name of the deployed model reference above. Then run: To delete a default deployment you must delete all other deployments for the same model first, or designate a different deployed model as the default as described above. This is to ensure that querying by model name will always route to an unambiguous default deployment as long as deployments for the model exist.

",
    "$.messages[5].content[3].type": "text",
    "$.messages[5].content[4].text": "Title: 🚀 Easier & faster LoRA fine-tune deployments on Fireworks
Link: https://docs.fireworks.ai/updates/changelog
Content: You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: Previously, this involved two distinct steps, and the resulting deployment was slower than the base model: Create a deployment using firectl create deployment "accounts/fireworks/models/<MODEL_ID of base model>" --enable-addons Then deploy the addon to the deployment: firectl load-lora <MODEL_ID> --deployment <DEPLOYMENT_ID> For more information, see our deployment documentation. This change is for dedicated deployments with a single LoRA. You can still deploy multiple LoRAs on a deployment or deploy LoRA(s) on some Serverless models as described in the documentation.

",
    "$.messages[5].content[4].type": "text",
    "$.messages[5].content[5].text": "Title: Deployment Command
Link: https://docs.fireworks.ai/evaluators/examples/gcp_cloud_run_deployment_example
Content: It's recommended to run the deployment command from the directory containing the reward function script ( dummy_rewards.py ) and your rewardkit.yaml (if used), for example, from examples/gcp_cloud_run_deployment_example/. Ensure your virtual environment is active: Run the deployment command: Command Explanation: --id my-dummy-gcp-evaluator : A unique ID for your evaluator on the Fireworks AI platform. --target gcp-cloud-run : Specifies deployment to GCP Cloud Run. --function-ref dummy_rewards.hello_world_reward : The Python import path to your reward function. If dummy_rewards.py is in the current directory, this reference works. --gcp-auth-mode api-key : Configures the Cloud Run service with API key authentication. reward-kit will generate a key, store it in GCP Secret Manager, and configure the service. The key is also saved to your local rewardkit.yaml under evaluator_endpoint_keys . This is the default if not specified. --verbose : Shows detailed output, including gcloud commands being executed. --force : (Optional) If an evaluator with the same --id already exists, this flag will delete the existing one before creating the new one.

It's recommended to run the deployment command from the directory containing the reward function script ( dummy_rewards.py ) and your rewardkit.yaml (if used), for example, from examples/gcp_cloud_run_deployment_example/. Ensure your virtual environment is active: Run the deployment command: Command Explanation: --id my-dummy-gcp-evaluator : A unique ID for your evaluator on the Fireworks AI platform. --target gcp-cloud-run : Specifies deployment to GCP Cloud Run. --function-ref dummy_rewards.hello_world_reward : The Python import path to your reward function. If dummy_rewards.py is in the current directory, this reference works. --gcp-auth-mode api-key : Configures the Cloud Run service with API key authentication. reward-kit will generate a key, store it in GCP Secret Manager, and configure the service. The key is also saved to your local rewardkit.yaml under evaluator_endpoint_keys . This is the default if not specified. --verbose : Shows detailed output, including gcloud commands being executed. --force : (Optional) If an evaluator with the same --id already exists, this flag will delete the existing one before creating the new one.

It's recommended to run the deployment command from the directory containing the reward function script ( dummy_rewards.py ) and your rewardkit.yaml (if used), for example, from examples/gcp_cloud_run_deployment_example/. Ensure your virtual environment is active: source .venv/bin/activate

",
    "$.messages[5].content[5].type": "text",
    "$.messages[5].content[6].text": "Title: Quick deployment
Link: https://docs.fireworks.ai/fine-tuning/single-lora
Content: Deploy your LoRA fine-tuned model with one simple command: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

Deploy your LoRA fine-tuned model with one simple command: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

Alternative deployment method
This two-step method is the standard approach for multi-LoRA deployments where multiple LoRA models share the same base model. While it can also be used for single LoRA deployments, it provides slower performance compared to live merge and is not recommended for single LoRA use cases. You can also deploy single LoRA models using a two-step process: Create base model deployment Deploy the base model with addons enabled: firectl create deployment "accounts/fireworks/models/<MODEL_ID of base model>" --enable-addons

",
    "$.messages[5].content[6].type": "text",
    "$.messages[5].content[7].text": "Title: Delete Deployment
Link: https://docs.fireworks.ai/api-reference/delete-deployment
Content: 



REST Endpoint DELETE /v1/accounts/{account_id}/deployments/{deployment_id}

Server(s): https://api.fireworks.ai


Summary:  

Description: N/A

Security methods: 
header: 
	Authorization: bearer auth
	

Request params: 
path params:
account_id:
	string
	Description: The Account Id
	Example: "<string>"
deployment_id:
	string
	Description: The Deployment Id
	Example: "<string>"

query params:
hard:
	boolean
	Description: If true, this will perform a hard deletion.
	Example: true
ignoreChecks:
	boolean
	Description: If true, this will ignore checks and force the deletion of a deployment that is currently
deployed and is in use.
	Example: true


Request body: N/A


Responses: 
200: object
	Description: N/A
	Example: {}

",
    "$.messages[5].content[7].type": "text",
    "$.messages[5].content[8].text": "Title: How do I close my Fireworks.ai account?
Link: https://docs.fireworks.ai/faq-new/account-access/how-do-i-close-my-fireworksai-account
Content: To close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed

To close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed

How do I close my Fireworks.ai account?
To close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed

",
    "$.messages[5].content[8].type": "text",
    "$.messages[5].content[9].text": "Title: Deleting Stored Responses
Link: https://docs.fireworks.ai/guides/response-api
Content: When responses are stored (the default behavior with store=True ), you can immediately delete them from storage using the DELETE endpoint. This permanently removes the conversation data. from fireworks import LLM
import requests
import os

llm = LLM(model="qwen3-235b-a22b", deployment_type="serverless")

# Create a response
response = llm.responses.create(
    input="What is the capital of France?",
    store=True  # This is the default
)

response_id = response.id
print(f"Created response with ID: {response_id}")

# Delete the response immediately
headers = {
    "Authorization": f"Bearer {os.getenv('FIREWORKS_API_KEY')}",
    "x-fireworks-account-id": "your-account-id"
}
delete_response = requests.delete(
    f"https://api.fireworks.ai/inference/v1/responses/{response_id}",
    headers=headers
)

if delete_response.status_code == 200:
    print("Response deleted successfully")
else:
    print(f"Failed to delete response: {delete_response.status_code}")

When responses are stored (the default behavior with store=True ), you can immediately delete them from storage using the DELETE endpoint. This permanently removes the conversation data. from fireworks import LLM
import requests
import os

llm = LLM(model="qwen3-235b-a22b", deployment_type="serverless")

# Create a response
response = llm.responses.create(
    input="What is the capital of France?",
    store=True  # This is the default
)

response_id = response.id
print(f"Created response with ID: {response_id}")

# Delete the response immediately
headers = {
    "Authorization": f"Bearer {os.getenv('FIREWORKS_API_KEY')}",
    "x-fireworks-account-id": "your-account-id"
}
delete_response = requests.delete(
    f"https://api.fireworks.ai/inference/v1/responses/{response_id}",
    headers=headers
)

if delete_response.status_code == 200:
    print("Response deleted successfully")
else:
    print(f"Failed to delete response: {delete_response.status_code}")

When responses are stored (the default behavior with store=True ), you can immediately delete them from storage using the DELETE endpoint. This permanently removes the conversation data. import os
from openai import OpenAI
import requests

client = OpenAI(
    base_url="https://api.fireworks.ai/inference/v1",
    api_key=os.getenv("FIREWORKS_API_KEY", "YOUR_FIREWORKS_API_KEY_HERE")
)

# Create a response
response = client.responses.create(
    model="accounts/fireworks/models/qwen3-235b-a22b",
    input="What is the capital of France?",
    store=True  # This is the default
)

response_id = response.id
print(f"Created response with ID: {response_id}")

# Delete the response immediately
headers = {
    "Authorization": f"Bearer {os.getenv('FIREWORKS_API_KEY')}",
    "x-fireworks-account-id": "your-account-id"
}
delete_response = requests.delete(
    f"https://api.fireworks.ai/inference/v1/responses/{response_id}",
    headers=headers
)

if delete_response.status_code == 200:
    print("Response deleted successfully")
else:
    print(f"Failed to delete response: {delete_response.status_code}")

",
    "$.messages[5].content[9].type": "text",
    "$.messages[5].role": "tool",
    "$.messages[5].tool_call_id": "functions.search:1",
    "$.messages[6].content": "Let me search for more information about listing and managing deployments, as well as the Fireworks Build SDK for deployment management.",
    "$.messages[6].role": "assistant",
    "$.messages[6].tool_calls[0].function.arguments": "{"query": "list deployments firectl Fireworks Build SDK deployment management"}",
    "$.messages[6].tool_calls[0].function.name": "search",
    "$.messages[6].tool_calls[0].id": "functions.search:1",
    "$.messages[6].tool_calls[0].type": "function",
    "$.messages[7].content[0].text": "Title: Introducing the Fireworks Build SDK
Link: https://docs.fireworks.ai/tools-sdks/python-client/sdk-introduction
Content: Introducing the Fireworks Build SDK
The Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.

Introducing the Fireworks Build SDK
The Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.

The Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.

",
    "$.messages[7].content[0].type": "text",
    "$.messages[7].content[1].text": "Title: Basics of the Build SDK
Link: https://docs.fireworks.ai/tools-sdks/python-client/sdk-basics
Content: Why use the Build SDK?
The Fireworks Build SDK gives you a declarative way to work with Fireworks resources like deployments, fine-tuning jobs, and datasets. We've designed it to handle all the infrastructure complexity for you, letting you focus on building your application. Instead of using the web UI, CLI, or raw API calls, you can manage everything through simple Python code with smart, logical defaults without sacrificing control and customizability. The principles of the SDK are the following: Object-oriented: Fireworks primitives are represented as Python objects. You can access their capabilities and properties through methods and attributes. Declarative: You can describe your desired state and the SDK will handle reconcilliation. Smart defaults: The SDK will infer the most logical defaults for you, prioritizing development speed and lowest cost. Here are some examples: The SDK will automatically use a serverless deployment for models that are available serverlessly unless you specify otherwise. When creating deployments, the SDK will also enable scale-to-zero with the shortest possible scale-down window. If the SDK determines that a resource already exists by matching its signature (see below), it will re-use the existing resource instead of creating a new one. Customizable: Although we enable smart defaults, you still have full access to the configuration parameters for any Fireworks resource The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.

Why use the Build SDK?
The Fireworks Build SDK gives you a declarative way to work with Fireworks resources like deployments, fine-tuning jobs, and datasets. We've designed it to handle all the infrastructure complexity for you, letting you focus on building your application. Instead of using the web UI, CLI, or raw API calls, you can manage everything through simple Python code with smart, logical defaults without sacrificing control and customizability. The principles of the SDK are the following: Object-oriented: Fireworks primitives are represented as Python objects. You can access their capabilities and properties through methods and attributes. Declarative: You can describe your desired state and the SDK will handle reconcilliation. Smart defaults: The SDK will infer the most logical defaults for you, prioritizing development speed and lowest cost. Here are some examples: The SDK will automatically use a serverless deployment for models that are available serverlessly unless you specify otherwise. When creating deployments, the SDK will also enable scale-to-zero with the shortest possible scale-down window. If the SDK determines that a resource already exists by matching its signature (see below), it will re-use the existing resource instead of creating a new one. Customizable: Although we enable smart defaults, you still have full access to the configuration parameters for any Fireworks resource The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.

Why use the Build SDK?
The Fireworks Build SDK gives you a declarative way to work with Fireworks resources like deployments, fine-tuning jobs, and datasets. We've designed it to handle all the infrastructure complexity for you, letting you focus on building your application. Instead of using the web UI, CLI, or raw API calls, you can manage everything through simple Python code with smart, logical defaults without sacrificing control and customizability. The principles of the SDK are the following: Object-oriented: Fireworks primitives are represented as Python objects. You can access their capabilities and properties through methods and attributes. Declarative: You can describe your desired state and the SDK will handle reconcilliation. Smart defaults: The SDK will infer the most logical defaults for you, prioritizing development speed and lowest cost. Here are some examples: The SDK will automatically use a serverless deployment for models that are available serverlessly unless you specify otherwise. When creating deployments, the SDK will also enable scale-to-zero with the shortest possible scale-down window. If the SDK determines that a resource already exists by matching its signature (see below), it will re-use the existing resource instead of creating a new one. Customizable: Although we enable smart defaults, you still have full access to the configuration parameters for any Fireworks resource The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.

",
    "$.messages[7].content[1].type": "text",
    "$.messages[7].content[2].text": "Title: Querying existing dedicated deployments
Link: https://docs.fireworks.ai/tools-sdks/python-client/querying-dedicated-deployments
Content: When you have dedicated deployments that were created via firectl or the Fireworks web UI, you can easily connect to them using the Build SDK to run inference. This is particularly useful when you want to leverage existing infrastructure or when deployments are managed by different teams.

When you have dedicated deployments that were created via firectl or the Fireworks web UI, you can easily connect to them using the Build SDK to run inference. This is particularly useful when you want to leverage existing infrastructure or when deployments are managed by different teams.

Deployment ID requirements
The id parameter should match exactly with your existing deployment: Use the deployment name/ID as shown in the Fireworks dashboard The ID is case-sensitive and must match exactly If the deployment doesn't exist, you'll receive an error when making requests

",
    "$.messages[7].content[2].type": "text",
    "$.messages[7].content[3].text": "Title: Fireworks SDK
Link: https://docs.fireworks.ai/guides/ondemand-deployments
Content: You can connect to your existing deployment using the  Fireworks Build SDK : from fireworks import LLM

# Connect to your existing deployment
llm = LLM(
    model="llama-v3p2-3b-instruct",  # The model your deployment is running
    deployment_type="on-demand",
    id="my-deployment-id",  # Your deployment ID
)

# Use OpenAI-compatible chat completions
response = llm.chat.completions.create(
    messages=[{"role": "user", "content": "Say this is a test"}]
)

print(response.choices[0].message.content) When connecting to an existing deployment with the SDK, you don't need to call  .apply()  - the deployment is already running. For more details on querying existing deployments with Fireworks SDK, see our guide on  querying dedicated deployments .

You can connect to your existing deployment using the  Fireworks Build SDK : from fireworks import LLM

# Connect to your existing deployment
llm = LLM(
    model="llama-v3p2-3b-instruct",  # The model your deployment is running
    deployment_type="on-demand",
    id="my-deployment-id",  # Your deployment ID
)

# Use OpenAI-compatible chat completions
response = llm.chat.completions.create(
    messages=[{"role": "user", "content": "Say this is a test"}]
)

print(response.choices[0].message.content) When connecting to an existing deployment with the SDK, you don't need to call  .apply()  - the deployment is already running. For more details on querying existing deployments with Fireworks SDK, see our guide on  querying dedicated deployments .

On-demand deployments
If you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.

",
    "$.messages[7].content[3].type": "text",
    "$.messages[7].content[4].text": "Title: Step 1: Configure the Fireworks Build SDK
Link: https://docs.fireworks.ai/structured-responses/structured-output-grammar-based
Content: from fireworks import LLM

client = LLM(
    model="llama-v3p1-405b-instruct",
    deployment_type="serverless"
)

from fireworks import LLM

client = LLM(
    model="llama-v3p1-405b-instruct",
    deployment_type="serverless"
)

Prerequisites
Before you begin, ensure you have the following: Python installed on your system. Build SDK installed. You can install it using pip: Next, select the model you want to use. In this example, we use llama-v3p1-405b-instruct , but all fireworks models support this feature.

",
    "$.messages[7].content[4].type": "text",
    "$.messages[7].content[5].text": "Title: Does Fireworks support custom base models?
Link: https://docs.fireworks.ai/faq-new/models-inference/does-fireworks-support-custom-base-models
Content: Yes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.

Yes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.

Does Fireworks support custom base models?
Yes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.

",
    "$.messages[7].content[5].type": "text",
    "$.messages[7].content[6].text": "Title: 🔧 Build SDK: Deployment Configuration Application Requirement
Link: https://docs.fireworks.ai/updates/changelog
Content: The Build SDK now requires you to call .apply() to apply any deployment configurations to Fireworks when using deployment_type="on-demand" or deployment_type="on-demand-lora" . This change ensures explicit control over when deployments are created and helps prevent accidental deployment creation. Key changes: .apply() is now required for on-demand and on-demand-lora deployments Serverless deployments do not require .apply() calls If you do not call .apply() , you are expected to set up the deployment through the deployment page at https://app.fireworks.ai/dashboard/deployments Migration guide: Add llm.apply() after creating LLM instances with deployment_type="on-demand" or deployment_type="on-demand-lora" No changes needed for serverless deployments See updated documentation for examples and best practices This change improves deployment management and provides better control over resource creation. This applies to Python SDK version >=0.19.14.

The Build SDK now requires you to call .apply() to apply any deployment configurations to Fireworks when using deployment_type="on-demand" or deployment_type="on-demand-lora" . This change ensures explicit control over when deployments are created and helps prevent accidental deployment creation. Key changes: .apply() is now required for on-demand and on-demand-lora deployments Serverless deployments do not require .apply() calls If you do not call .apply() , you are expected to set up the deployment through the deployment page at https://app.fireworks.ai/dashboard/deployments Migration guide: Add llm.apply() after creating LLM instances with deployment_type="on-demand" or deployment_type="on-demand-lora" No changes needed for serverless deployments See updated documentation for examples and best practices This change improves deployment management and provides better control over resource creation. This applies to Python SDK version >=0.19.14.

🎯 Build SDK: Reward-kit integration for evaluator development
The Build SDK now natively integrates with reward-kit to simplify evaluator development for Reinforcement Fine-Tuning (RFT) . You can now create custom evaluators in Python with automatic dependency management and seamless deployment to Fireworks infrastructure. Key features: Native reward-kit integration for evaluator development Automatic packaging of dependencies from pyproject.toml or requirements.txt Local testing capabilities before deployment Direct integration with Fireworks datasets and evaluation jobs Support for third-party libraries and complex evaluation logic See our Developing Evaluators guide to get started with your first evaluator in minutes.

",
    "$.messages[7].content[6].type": "text",
    "$.messages[7].content[7].text": "Title: Set up your developer environment & call the Fireworks API
Link: https://docs.fireworks.ai/getting-started/quickstart
Content: This is the recommended way to get started : This Python (Fireworks) tab uses our Fireworks Build
SDK for the best performance and developer experience.
See our Client-side performance
optimization guide for
more details. Install SDK Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Configure API Key Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either ~/.bash_profile for Bash or ~/.zshrc for Zsh .
You can do this by running the command: Add a new line to the file with the following: After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. You can verify that the variable has been set correctly by running echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: Sending the first API Request You can quickly instantiate the LLM class and call the Fireworks API. The Build SDK handles deployment management automatically. from fireworks import LLM

# Basic usage - SDK automatically selects optimal deployment type
llm = LLM(model="llama4-maverick-instruct-basic", deployment_type="auto")

response = llm.chat.completions.create(
    messages=[{"role": "user", "content": "Say this is a test"}]
)

print(response.choices[0].message.content)

This is the recommended way to get started : This Python (Fireworks) tab uses our Fireworks Build
SDK for the best performance and developer experience.
See our Client-side performance
optimization guide for
more details. Install SDK Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Configure API Key Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either ~/.bash_profile for Bash or ~/.zshrc for Zsh .
You can do this by running the command: Add a new line to the file with the following: After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. You can verify that the variable has been set correctly by running echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: Sending the first API Request You can quickly instantiate the LLM class and call the Fireworks API. The Build SDK handles deployment management automatically. from fireworks import LLM

# Basic usage - SDK automatically selects optimal deployment type
llm = LLM(model="llama4-maverick-instruct-basic", deployment_type="auto")

response = llm.chat.completions.create(
    messages=[{"role": "user", "content": "Say this is a test"}]
)

print(response.choices[0].message.content)

Python (Fireworks)
This is the recommended way to get started : This  Python (Fireworks)  tab uses our  Fireworks Build
SDK  for the best performance and developer experience.
See our  Client-side performance
optimization  guide for
more details. Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. pip install --upgrade fireworks-ai The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either  ~/.bash_profile  for Bash or  ~/.zshrc  for  Zsh .
You can do this by running the command: vim ~/.bash_profile vim ~/.zshrc Add a new line to the file with the following: export FIREWORKS_API_KEY="<API_KEY>" After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. source ~/.bash_profile source ~/.zshrc You can verify that the variable has been set correctly by running  echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. setx FIREWORKS_API_KEY "<API_KEY>" To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: echo %FIREWORKS_API_KEY% You can quickly instantiate the  LLM  class and call the Fireworks API. The Build SDK handles deployment management automatically. from fireworks import LLM

# Basic usage - SDK automatically selects optimal deployment type
llm = LLM(model="llama4-maverick-instruct-basic", deployment_type="auto")

response = llm.chat.completions.create(
    messages=[{"role": "user", "content": "Say this is a test"}]
)

print(response.choices[0].message.content) You can also pass the API key directly to the LLM constructor:  LLM(model="llama4-maverick-instruct-basic", deployment_type="auto", api_key="<FIREWORKS_API_KEY>")

",
    "$.messages[7].content[7].type": "text",
    "$.messages[7].content[8].text": "Title: Example with Fireworks Build SDK
Link: https://docs.fireworks.ai/faq-new/models-inference/how-to-get-performance-metrics-for-streaming-responses
Content: from fireworks import LLM
import os

llm = LLM(
    model="llama-v3p1-8b-instruct",
    deployment_type="serverless",
    api_key=os.environ["FIREWORKS_API_KEY"],
)

# Streaming completion with performance metrics
stream = llm.chat.completions.create(
    messages=[{"role": "user", "content": "Hello, world!"}],
    max_tokens=100,
    stream=True,
    perf_metrics_in_response=True,
)

for chunk in stream:
    # Use getattr to avoid linter errors for unknown attributes
    perf_metrics = getattr(chunk, "perf_metrics", None)
    finish_reason = getattr(chunk.choices[0], "finish_reason", None)
    if perf_metrics is not None and finish_reason:
        print("Performance metrics:", perf_metrics)

from fireworks import LLM
import os

llm = LLM(
    model="llama-v3p1-8b-instruct",
    deployment_type="serverless",
    api_key=os.environ["FIREWORKS_API_KEY"],
)

# Streaming completion with performance metrics
stream = llm.chat.completions.create(
    messages=[{"role": "user", "content": "Hello, world!"}],
    max_tokens=100,
    stream=True,
    perf_metrics_in_response=True,
)

for chunk in stream:
    # Use getattr to avoid linter errors for unknown attributes
    perf_metrics = getattr(chunk, "perf_metrics", None)
    finish_reason = getattr(chunk.choices[0], "finish_reason", None)
    if perf_metrics is not None and finish_reason:
        print("Performance metrics:", perf_metrics)

Performance Metrics Overview
The Inference API returns several per-request metrics in the response. They can be useful for one-off debugging or can be logged by the client in their preferred observability tool. For aggregate metrics, see the usage dashboard. Non-streaming requests: Performance metrics are always included in response headers (e.g., fireworks-prompt-tokens , fireworks-server-time-to-first-token ). Streaming requests: Only selected performance metrics, such as "fireworks-server-time-to-first-token," are available because HTTP headers must be sent before the first token can be streamed. Use the perf_metrics_in_response body parameter to include all metrics in the last SSE event of the response body.

",
    "$.messages[7].content[8].type": "text",
    "$.messages[7].content[9].text": "Title: Fireworks AI Developer Platform
Link: https://docs.fireworks.ai/getting-started/introduction
Content: Fireworks AI is the best platform for building AI product experiences with open source AI models. You can run and customize AI models with just a few lines of code!

Fireworks AI is the best platform for building AI product experiences with open source AI models. You can run and customize AI models with just a few lines of code!

Start building with open source AI models

",
    "$.messages[7].content[9].type": "text",
    "$.messages[7].role": "tool",
    "$.messages[7].tool_call_id": "functions.search:1",
    "$.messages[8].content": "Based on my research, I can now provide you with a comprehensive guide on managing deployments on Fireworks. Let me break this down into different approaches:

## Managing Deployments on Fireworks

Fireworks offers three main ways to manage deployments:

### 1. **Fireworks Build SDK (Recommended - Python)**
This is the most user-friendly approach using Python.

**Installation:**
\`\`\`bash
pip install --upgrade fireworks-ai
\`\`\`

**Basic Deployment Creation:**
\`\`\`python
from fireworks import LLM

# Create a new deployment
llm = LLM(
    model="llama-v3p2-3b-instruct",
    deployment_type="on-demand",  # or "serverless", "on-demand-lora"
    id="my-deployment-name"
)

# Apply the deployment (required for on-demand deployments)
llm.apply()

# Use the deployment
response = llm.chat.completions.create(
    messages=[{"role": "user", "content": "Hello, world!"}]
)
print(response.choices[0].message.content)
\`\`\`

**Key Features:**
- **Smart defaults**: Automatically selects optimal deployment type
- **Declarative**: Describe desired state, SDK handles reconciliation
- **Object-oriented**: Deployments are Python objects with methods/attributes
- **Scale-to-zero**: Enabled by default with shortest scale-down window

### 2. **Firectl CLI Tool**
Command-line interface for deployment management.

**Create a deployment:**
\`\`\`bash
firectl create deployment "accounts/fireworks/models/llama-v3p2-3b-instruct" --id my-deployment
\`\`\`

**Deploy LoRA fine-tunes:**
\`\`\`bash
# Single command for LoRA deployment
firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

# Traditional two-step method (for multi-LoRA)
firectl create deployment "accounts/fireworks/models/<base-model-id>" --enable-addons
firectl load-lora <lora-model-id> --deployment <deployment-id>
\`\`\`

**List deployments:**
\`\`\`bash
firectl list deployments
\`\`\`

**Delete deployments:**
\`\`\`bash
firectl delete deployment my-deployment
\`\`\`

**Update default deployment:**
\`\`\`bash
firectl update deployed-model <DEPLOYED_MODEL_ID> --default
\`\`\`

### 3. **REST API**
Direct API calls for programmatic management.

**Create deployment:**
\`\`\`bash
curl -X POST https://api.fireworks.ai/v1/accounts/{account_id}/deployments \\
  -H "Authorization: Bearer YOUR_API_KEY" \\
  -H "Content-Type: application/json" \\
  -d '{
    "model": "llama-v3p2-3b-instruct",
    "deployment_type": "on-demand"
  }'
\`\`\`

**Delete deployment:**
\`\`\`bash
curl -X DELETE https://api.fireworks.ai/v1/accounts/{account_id}/deployments/{deployment_id} \\
  -H "Authorization: Bearer YOUR_API_KEY"
\`\`\`

## Deployment Types Explained

### **Serverless Deployments**
- **Billing**: Per-token usage
- **Best for**: Variable/low usage, quick prototyping
- **Management**: Fully managed by Fireworks
- **Access**: Instant, no setup required

### **On-Demand Deployments**
- **Billing**: Per GPU-second
- **Best for**: Consistent usage, predictable performance
- **Advantages**: 
  - No rate limits
  - Predictable performance
  - Access to larger model selection
  - Custom base models from Hugging Face
- **Management**: Requires explicit creation/deletion

### **Reserved Capacity**
- **Best for**: High-volume, consistent usage
- **Advantages**: More reliable capacity, higher GPU quotas
- **Contact**: inquiries@fireworks.ai for enterprise pricing

## Complete Management Workflow Example

Here's a comprehensive example using the Build SDK:

\`\`\`python
from fireworks import LLM
import os

# Set up API key
os.environ["FIREWORKS_API_KEY"] = "your-api-key"

# 1. Create and manage deployments
def manage_deployments():
    
    # Create a serverless deployment (no .apply() needed)
    serverless_llm = LLM(
        model="llama4-maverick-instruct-basic",
        deployment_type="serverless"
    )
    
    # Create an on-demand deployment
    ondemand_llm = LLM(
        model="llama-v3p2-3b-instruct",
        deployment_type="on-demand",
        id="my-production-deployment",
        max_instances=3,
        min_instances=0  # Enable scale-to-zero
    )
    
    # Apply the on-demand deployment
    ondemand_llm.apply()
    
    # 2. Query existing deployments
    print("Connecting to existing deployment...")
    existing_llm = LLM(
        model="llama-v3p2-3b-instruct",
        deployment_type="on-demand",
        id="my-existing-deployment"
    )
    
    # 3. Use deployments
    response = existing_llm.chat.completions.create(
        messages=[{"role": "user", "content": "Explain deployment management"}]
    )
    
    # 4. Clean up (delete deployment when done)
    # firectl delete deployment my-production-deployment
    
    return response.choices[0].message.content

# Run the workflow
result = manage_deployments()
print(result)
\`\`\`

## Best Practices

1. **Cost Management**: Use serverless for development, on-demand for production
2. **Monitoring**: Track usage through the Fireworks dashboard
3. **Cleanup**: Always delete unused on-demand deployments to avoid charges
4. **Scaling**: Use scale-to-zero for cost efficiency
5. **Naming**: Use descriptive deployment IDs for easy management

## Getting Started

1. **Install the Build SDK**: \`pip install fireworks-ai\`
2. **Get API Key**: From https://app.fireworks.ai/dashboard/api-keys
3. **Choose Approach**: 
   - **Beginners**: Start with Build SDK
   - **CLI Users**: Use firectl
   - **API Integration**: Use REST API

For enterprise needs or reserved capacity, contact inquiries@fireworks.ai.

Would you like me to dive deeper into any specific aspect of deployment management?",
    "$.messages[8].role": "assistant",
    "$.pid": 97954,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T11:51:25.530495",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "Run math evaluation on sample dataset using pytest interface.",
    "$.eval_metadata.name": "test_pytest_mcp_url",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.status": "stopped",
    "$.eval_metadata.version": "v0.2.6.dev19+gf755623.dirty",
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.row_id": "invent-special-real-rich-dream",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "You are a helpful assistant that can answer questions about Fireworks.
ALWAYS provide code or commands to execute to answer the question.",
    "$.messages[0].role": "system",
    "$.messages[1].content": "Can you teach me about how to manage deployments on Fireworks",
    "$.messages[1].role": "user",
    "$.pid": 2168,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T11:51:48.974779",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "Run math evaluation on sample dataset using pytest interface.",
    "$.eval_metadata.name": "test_pytest_mcp_url",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.status": "stopped",
    "$.eval_metadata.version": "v0.2.6.dev19+gf755623.dirty",
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.row_id": "continue-encourage-dry-part-meeting",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "You are a helpful assistant that can answer questions about Fireworks.
ALWAYS provide code or commands to execute to answer the question.",
    "$.messages[0].role": "system",
    "$.messages[1].content": "Can you teach me about how to manage deployments on Fireworks",
    "$.messages[1].role": "user",
    "$.pid": 2642,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T12:17:53.864430",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "Run math evaluation on sample dataset using pytest interface.",
    "$.eval_metadata.name": "test_pytest_mcp_url",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.status": "stopped",
    "$.eval_metadata.version": "v0.2.6.dev20+ge1da45a",
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.row_id": "create-destroy-past-service-case",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "You are a helpful assistant that can answer questions about Fireworks.
ALWAYS provide code or commands to execute to answer the question.",
    "$.messages[0].role": "system",
    "$.messages[1].content": "Can you teach me about how to manage deployments on Fireworks",
    "$.messages[1].role": "user",
    "$.messages[2].content": "I'll help you learn about managing deployments on Fireworks! Let me search for comprehensive information about deployment management.",
    "$.messages[2].role": "assistant",
    "$.messages[2].tool_calls[0].function.arguments": "{"query": "deployment management Fireworks AI"}",
    "$.messages[2].tool_calls[0].function.name": "search",
    "$.messages[2].tool_calls[0].id": "functions.search:0",
    "$.messages[2].tool_calls[0].type": "function",
    "$.messages[3].content[0].text": "Title: Fireworks AI Developer Platform
Link: https://docs.fireworks.ai/getting-started/introduction
Content: Fireworks AI is the best platform for building AI product experiences with open source AI models. You can run and customize AI models with just a few lines of code!

Fireworks AI is the best platform for building AI product experiences with open source AI models. You can run and customize AI models with just a few lines of code!

Start building with open source AI models

",
    "$.messages[3].content[0].type": "text",
    "$.messages[3].content[1].text": "Title: How do I close my Fireworks.ai account?
Link: https://docs.fireworks.ai/faq-new/account-access/how-do-i-close-my-fireworksai-account
Content: To close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed

To close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed

How do I close my Fireworks.ai account?
To close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed

",
    "$.messages[3].content[1].type": "text",
    "$.messages[3].content[2].text": "Title: Do you host your deployments in the EU or Asia?
Link: https://docs.fireworks.ai/faq-new/support-general/do-you-host-your-deployments-in-the-eu-or-asia
Content: We are currently deployed in multiple U.S.-based locations. However, we’re open to hearing more about your specific requirements. You can: Join our Discord community Write to us at inquiries@fireworks.ai If you're an Enterprise customer, please contact your dedicated customer support representative to ensure a timely response.

We are currently deployed in multiple U.S.-based locations. However, we’re open to hearing more about your specific requirements. You can: Join our Discord community Write to us at inquiries@fireworks.ai If you're an Enterprise customer, please contact your dedicated customer support representative to ensure a timely response.

Do you host your deployments in the EU or Asia?
We are currently deployed in multiple U.S.-based locations. However, we’re open to hearing more about your specific requirements. You can: Join our Discord community Write to us at inquiries@fireworks.ai If you're an Enterprise customer, please contact your dedicated customer support representative to ensure a timely response.

",
    "$.messages[3].content[2].type": "text",
    "$.messages[3].content[3].text": "Title: Introducing the Fireworks Build SDK
Link: https://docs.fireworks.ai/tools-sdks/python-client/sdk-introduction
Content: The Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.

The Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.

Introducing the Fireworks Build SDK
The Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.

",
    "$.messages[3].content[3].type": "text",
    "$.messages[3].content[4].text": "Title: On-demand deployments
Link: https://docs.fireworks.ai/guides/ondemand-deployments
Content: If you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.

If you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.

On-demand deployments
If you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.

",
    "$.messages[3].content[4].type": "text",
    "$.messages[3].content[5].text": "Title: I have multiple Fireworks accounts. When I try to login with Google on Fireworks' web UI, I'm getting signed into the wrong account. How do I fix this?
Link: https://docs.fireworks.ai/faq-new/account-access/i-have-multiple-fireworks-accounts-when-i-try-to-login-with-google-on-fireworks
Content: If you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.

If you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.

I have multiple Fireworks accounts. When I try to login with Google on Fireworks' web UI, I'm getting signed into the wrong account. How do I fix this?
If you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.

",
    "$.messages[3].content[5].type": "text",
    "$.messages[3].content[6].text": "Title: Introduction
Link: https://docs.fireworks.ai/api-reference/introduction
Content: Fireworks AI REST API enables you to interact with various language, image and embedding models using an API Key. It also lets you automate management of models, deployments, datasets, and more.

Fireworks AI REST API enables you to interact with various language, image and embedding models using an API Key. It also lets you automate management of models, deployments, datasets, and more.

Introduction
Fireworks AI REST API enables you to interact with various language, image and embedding models using an API Key. It also lets you automate management of models, deployments, datasets, and more.

",
    "$.messages[3].content[6].type": "text",
    "$.messages[3].content[7].text": "Title: Create a deployment
Link: https://docs.fireworks.ai/tools-sdks/firectl/commands/create-deployment
Content: Create a Deployment on Fireworks AI platform

Create a Deployment on Fireworks AI platform

Creates a new deployment.

",
    "$.messages[3].content[7].type": "text",
    "$.messages[3].content[8].text": "Title: How much does Fireworks cost?
Link: https://docs.fireworks.ai/faq-new/billing-pricing/how-much-does-fireworks-cost
Content: Fireworks AI operates on a pay-as-you-go model for all non-Enterprise usage, and new users automatically receive free credits. You pay based on: Per token for serverless inference Per GPU usage time for on-demand deployments Per token of training data for fine-tuning For customers needing enterprise-grade security and reliability , please reach out to us at inquiries@fireworks.ai to discuss options. Find out more about our current pricing on our Pricing page.

Fireworks AI operates on a pay-as-you-go model for all non-Enterprise usage, and new users automatically receive free credits. You pay based on: Per token for serverless inference Per GPU usage time for on-demand deployments Per token of training data for fine-tuning For customers needing enterprise-grade security and reliability , please reach out to us at inquiries@fireworks.ai to discuss options. Find out more about our current pricing on our Pricing page.

How much does Fireworks cost?
Fireworks AI operates on a pay-as-you-go model for all non-Enterprise usage, and new users automatically receive free credits. You pay based on: Per token for serverless inference Per GPU usage time for on-demand deployments Per token of training data for fine-tuning For customers needing enterprise-grade security and reliability , please reach out to us at inquiries@fireworks.ai to discuss options. Find out more about our current pricing on our Pricing page.

",
    "$.messages[3].content[8].type": "text",
    "$.messages[3].content[9].text": "Title: Python (Fireworks)
Link: https://docs.fireworks.ai/getting-started/quickstart
Content: This is the recommended way to get started : This  Python (Fireworks)  tab uses our  Fireworks Build
SDK  for the best performance and developer experience.
See our  Client-side performance
optimization  guide for
more details. Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. pip install --upgrade fireworks-ai The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either  ~/.bash_profile  for Bash or  ~/.zshrc  for  Zsh .
You can do this by running the command: vim ~/.bash_profile vim ~/.zshrc Add a new line to the file with the following: export FIREWORKS_API_KEY="<API_KEY>" After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. source ~/.bash_profile source ~/.zshrc You can verify that the variable has been set correctly by running  echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. setx FIREWORKS_API_KEY "<API_KEY>" To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: echo %FIREWORKS_API_KEY% You can quickly instantiate the  LLM  class and call the Fireworks API. The Build SDK handles deployment management automatically. from fireworks import LLM

# Basic usage - SDK automatically selects optimal deployment type
llm = LLM(model="llama4-maverick-instruct-basic", deployment_type="auto")

response = llm.chat.completions.create(
    messages=[{"role": "user", "content": "Say this is a test"}]
)

print(response.choices[0].message.content) You can also pass the API key directly to the LLM constructor:  LLM(model="llama4-maverick-instruct-basic", deployment_type="auto", api_key="<FIREWORKS_API_KEY>")

This is the recommended way to get started : This  Python (Fireworks)  tab uses our  Fireworks Build
SDK  for the best performance and developer experience.
See our  Client-side performance
optimization  guide for
more details. Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. pip install --upgrade fireworks-ai The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either  ~/.bash_profile  for Bash or  ~/.zshrc  for  Zsh .
You can do this by running the command: vim ~/.bash_profile vim ~/.zshrc Add a new line to the file with the following: export FIREWORKS_API_KEY="<API_KEY>" After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. source ~/.bash_profile source ~/.zshrc You can verify that the variable has been set correctly by running  echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. setx FIREWORKS_API_KEY "<API_KEY>" To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: echo %FIREWORKS_API_KEY% You can quickly instantiate the  LLM  class and call the Fireworks API. The Build SDK handles deployment management automatically. from fireworks import LLM

# Basic usage - SDK automatically selects optimal deployment type
llm = LLM(model="llama4-maverick-instruct-basic", deployment_type="auto")

response = llm.chat.completions.create(
    messages=[{"role": "user", "content": "Say this is a test"}]
)

print(response.choices[0].message.content) You can also pass the API key directly to the LLM constructor:  LLM(model="llama4-maverick-instruct-basic", deployment_type="auto", api_key="<FIREWORKS_API_KEY>")

Set up your developer environment & call the Fireworks API
This is the recommended way to get started : This  tab uses our Fireworks Build
SDK for the best performance and developer experience.
See our Client-side performance
optimization guide for
more details. Install SDK Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Configure API Key Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either ~/.bash_profile for Bash or ~/.zshrc for Zsh .
You can do this by running the command: Add a new line to the file with the following: After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. You can verify that the variable has been set correctly by running echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: Sending the first API Request You can quickly instantiate the LLM class and call the Fireworks API. The Build SDK handles deployment management automatically. You can also pass the API key directly to the LLM constructor: LLM(model="llama4-maverick-instruct-basic", deployment_type="auto", api_key="<FIREWORKS_API_KEY>") Install SDK Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. Fireworks AI platform offers drop-in replacement with OpenAI Python Client. Configure API Key Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either ~/.bash_profile for Bash or ~/.zshrc for Zsh .
You can do this by running the command: Add a new line to the file with the following: After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. You can verify that the variable has been set correctly by running echo $OPENAI_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: echo %OPENAI_API_KEY%

",
    "$.messages[3].content[9].type": "text",
    "$.messages[3].role": "tool",
    "$.messages[3].tool_call_id": "functions.search:0",
    "$.messages[4].content": "Let me search for more specific information about deployment management commands and workflows.",
    "$.messages[4].role": "assistant",
    "$.messages[4].tool_calls[0].function.arguments": "{"query": "deployment management commands create update delete list Fireworks"}",
    "$.messages[4].tool_calls[0].function.name": "search",
    "$.messages[4].tool_calls[0].id": "functions.search:1",
    "$.messages[4].tool_calls[0].type": "function",
    "$.messages[5].content[0].text": "Title: Deployment Command
Link: https://docs.fireworks.ai/evaluators/examples/gcp_cloud_run_deployment_example
Content: It's recommended to run the deployment command from the directory containing the reward function script ( dummy_rewards.py ) and your rewardkit.yaml (if used), for example, from examples/gcp_cloud_run_deployment_example/. Ensure your virtual environment is active: Run the deployment command: Command Explanation: --id my-dummy-gcp-evaluator : A unique ID for your evaluator on the Fireworks AI platform. --target gcp-cloud-run : Specifies deployment to GCP Cloud Run. --function-ref dummy_rewards.hello_world_reward : The Python import path to your reward function. If dummy_rewards.py is in the current directory, this reference works. --gcp-auth-mode api-key : Configures the Cloud Run service with API key authentication. reward-kit will generate a key, store it in GCP Secret Manager, and configure the service. The key is also saved to your local rewardkit.yaml under evaluator_endpoint_keys . This is the default if not specified. --verbose : Shows detailed output, including gcloud commands being executed. --force : (Optional) If an evaluator with the same --id already exists, this flag will delete the existing one before creating the new one.

It's recommended to run the deployment command from the directory containing the reward function script ( dummy_rewards.py ) and your rewardkit.yaml (if used), for example, from examples/gcp_cloud_run_deployment_example/. Ensure your virtual environment is active: Run the deployment command: Command Explanation: --id my-dummy-gcp-evaluator : A unique ID for your evaluator on the Fireworks AI platform. --target gcp-cloud-run : Specifies deployment to GCP Cloud Run. --function-ref dummy_rewards.hello_world_reward : The Python import path to your reward function. If dummy_rewards.py is in the current directory, this reference works. --gcp-auth-mode api-key : Configures the Cloud Run service with API key authentication. reward-kit will generate a key, store it in GCP Secret Manager, and configure the service. The key is also saved to your local rewardkit.yaml under evaluator_endpoint_keys . This is the default if not specified. --verbose : Shows detailed output, including gcloud commands being executed. --force : (Optional) If an evaluator with the same --id already exists, this flag will delete the existing one before creating the new one.

It's recommended to run the deployment command from the directory containing the reward function script ( dummy_rewards.py ) and your rewardkit.yaml (if used), for example, from examples/gcp_cloud_run_deployment_example/. Ensure your virtual environment is active: source .venv/bin/activate

",
    "$.messages[5].content[0].type": "text",
    "$.messages[5].content[1].text": "Title: I have multiple Fireworks accounts. When I try to login with Google on Fireworks' web UI, I'm getting signed into the wrong account. How do I fix this?
Link: https://docs.fireworks.ai/faq-new/account-access/i-have-multiple-fireworks-accounts-when-i-try-to-login-with-google-on-fireworks
Content: If you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.

If you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.

I have multiple Fireworks accounts. When I try to login with Google on Fireworks' web UI, I'm getting signed into the wrong account. How do I fix this?
If you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.

",
    "$.messages[5].content[1].type": "text",
    "$.messages[5].content[2].text": "Title: Deleting Stored Responses
Link: https://docs.fireworks.ai/guides/response-api
Content: When responses are stored (the default behavior with store=True ), you can immediately delete them from storage using the DELETE endpoint. This permanently removes the conversation data. from fireworks import LLM
import requests
import os

llm = LLM(model="qwen3-235b-a22b", deployment_type="serverless")

# Create a response
response = llm.responses.create(
    input="What is the capital of France?",
    store=True  # This is the default
)

response_id = response.id
print(f"Created response with ID: {response_id}")

# Delete the response immediately
headers = {
    "Authorization": f"Bearer {os.getenv('FIREWORKS_API_KEY')}",
    "x-fireworks-account-id": "your-account-id"
}
delete_response = requests.delete(
    f"https://api.fireworks.ai/inference/v1/responses/{response_id}",
    headers=headers
)

if delete_response.status_code == 200:
    print("Response deleted successfully")
else:
    print(f"Failed to delete response: {delete_response.status_code}")

When responses are stored (the default behavior with store=True ), you can immediately delete them from storage using the DELETE endpoint. This permanently removes the conversation data. from fireworks import LLM
import requests
import os

llm = LLM(model="qwen3-235b-a22b", deployment_type="serverless")

# Create a response
response = llm.responses.create(
    input="What is the capital of France?",
    store=True  # This is the default
)

response_id = response.id
print(f"Created response with ID: {response_id}")

# Delete the response immediately
headers = {
    "Authorization": f"Bearer {os.getenv('FIREWORKS_API_KEY')}",
    "x-fireworks-account-id": "your-account-id"
}
delete_response = requests.delete(
    f"https://api.fireworks.ai/inference/v1/responses/{response_id}",
    headers=headers
)

if delete_response.status_code == 200:
    print("Response deleted successfully")
else:
    print(f"Failed to delete response: {delete_response.status_code}")

When responses are stored (the default behavior with store=True ), you can immediately delete them from storage using the DELETE endpoint. This permanently removes the conversation data. import os
from openai import OpenAI
import requests

client = OpenAI(
    base_url="https://api.fireworks.ai/inference/v1",
    api_key=os.getenv("FIREWORKS_API_KEY", "YOUR_FIREWORKS_API_KEY_HERE")
)

# Create a response
response = client.responses.create(
    model="accounts/fireworks/models/qwen3-235b-a22b",
    input="What is the capital of France?",
    store=True  # This is the default
)

response_id = response.id
print(f"Created response with ID: {response_id}")

# Delete the response immediately
headers = {
    "Authorization": f"Bearer {os.getenv('FIREWORKS_API_KEY')}",
    "x-fireworks-account-id": "your-account-id"
}
delete_response = requests.delete(
    f"https://api.fireworks.ai/inference/v1/responses/{response_id}",
    headers=headers
)

if delete_response.status_code == 200:
    print("Response deleted successfully")
else:
    print(f"Failed to delete response: {delete_response.status_code}")

",
    "$.messages[5].content[2].type": "text",
    "$.messages[5].content[3].text": "Title: Delete Deployment
Link: https://docs.fireworks.ai/api-reference/delete-deployment
Content: 



REST Endpoint DELETE /v1/accounts/{account_id}/deployments/{deployment_id}

Server(s): https://api.fireworks.ai


Summary:  

Description: N/A

Security methods: 
header: 
	Authorization: bearer auth
	

Request params: 
path params:
account_id:
	string
	Description: The Account Id
	Example: "<string>"
deployment_id:
	string
	Description: The Deployment Id
	Example: "<string>"

query params:
hard:
	boolean
	Description: If true, this will perform a hard deletion.
	Example: true
ignoreChecks:
	boolean
	Description: If true, this will ignore checks and force the deletion of a deployment that is currently
deployed and is in use.
	Example: true


Request body: N/A


Responses: 
200: object
	Description: N/A
	Example: {}

",
    "$.messages[5].content[3].type": "text",
    "$.messages[5].content[4].text": "Title: List Deployments
Link: https://docs.fireworks.ai/api-reference/list-deployments
Content: REST Endpoint GET /v1/accounts/{account_id}/deployments

Server(s): https://api.fireworks.ai


Summary:  

Description: N/A

Security methods: 
header: 
	Authorization: bearer auth
	

Request params: 
path params:
account_id:
	string
	Description: The Account Id
	Example: "<string>"

query params:
pageSize:
	integer
	Description: The maximum number of deployments to return. The maximum page_size is 200,
values above 200 will be coerced to 200.
If unspecified, the default is 50.
	Example: 123
pageToken:
	string
	Description: A page token, received from a previous ListDeployments call. Provide this
to retrieve the subsequent page. When paginating, all other parameters
provided to ListDeployments must match the call that provided the page
token.
	Example: "<string>"
filter:
	string
	Description: Only deployment satisfying the provided filter (if specified) will be
returned. See https://google.aip.dev/160 for the filter grammar.
	Example: "<string>"
orderBy:
	string
	Description: A comma-separated list of fields to order by. e.g. "foo,bar"
The default sort order is ascending. To specify a descending order for a
field, append a " desc" suffix. e.g. "foo desc,bar"
Subfields are specified with a "." character. e.g. "foo.bar"
If not specified, the default order is by "create_time".
	Example: "<string>"
showDeleted:
	boolean
	Description: If set, DELETED deployments will be included.
	Example: true
readMask:
	string
	Description: The fields to be returned in the response. If empty or "*", all fields will be returned.
	Example: "<string>"


Request body: N/A


Responses: 
200: object
	Description: N/A
	Example: {"deployments":[{"name":"<string>","displayName":"<string>","description":"<string>","createTime":"2023-11-07T05:31:56Z","expireTime":"2023-11-07T05:31:56Z","purgeTime":"2023-11-07T05:31:56Z","deleteTime":"2023-11-07T05:31:56Z","state":"STATE_UNSPECIFIED","status":{"code":"OK","message":"<string>"},"minReplicaCount":123,"maxReplicaCount":123,"replicaCount":123,"autoscalingPolicy":{"scaleUpWindow":"<string>","scaleDownWindow":"<string>","scaleToZeroWindow":"<string>","loadTargets":{}},"baseModel":"<string>","acceleratorCount":123,"acceleratorType":"ACCELERATOR_TYPE_UNSPECIFIED","precision":"PRECISION_UNSPECIFIED","cluster":"<string>","enableAddons":true,"draftTokenCount":123,"draftModel":"<string>","ngramSpeculationLength":123,"numPeftDeviceCached":123,"deploymentTemplate":"<string>","autoTune":{"longPrompt":true},"placement":{"region":"REGION_UNSPECIFIED","multiRegion":"MULTI_REGION_UNSPECIFIED","regions":["REGION_UNSPECIFIED"]},"region":"REGION_UNSPECIFIED","updateTime":"2023-11-07T05:31:56Z","disableDeploymentSizeValidation":true}],"nextPageToken":"<string>","totalSize":123}

REST Endpoint GET /v1/accounts/{account_id}/deployments

Server(s): https://api.fireworks.ai


Summary:  

Description: N/A

Security methods: 
header: 
	Authorization: bearer auth
	

Request params: 
path params:
account_id:
	string
	Description: The Account Id
	Example: "<string>"

query params:
pageSize:
	integer
	Description: The maximum number of deployments to return. The maximum page_size is 200,
values above 200 will be coerced to 200.
If unspecified, the default is 50.
	Example: 123
pageToken:
	string
	Description: A page token, received from a previous ListDeployments call. Provide this
to retrieve the subsequent page. When paginating, all other parameters
provided to ListDeployments must match the call that provided the page
token.
	Example: "<string>"
filter:
	string
	Description: Only deployment satisfying the provided filter (if specified) will be
returned. See https://google.aip.dev/160 for the filter grammar.
	Example: "<string>"
orderBy:
	string
	Description: A comma-separated list of fields to order by. e.g. "foo,bar"
The default sort order is ascending. To specify a descending order for a
field, append a " desc" suffix. e.g. "foo desc,bar"
Subfields are specified with a "." character. e.g. "foo.bar"
If not specified, the default order is by "create_time".
	Example: "<string>"
showDeleted:
	boolean
	Description: If set, DELETED deployments will be included.
	Example: true
readMask:
	string
	Description: The fields to be returned in the response. If empty or "*", all fields will be returned.
	Example: "<string>"


Request body: N/A


Responses: 
200: object
	Description: N/A
	Example: {"deployments":[{"name":"<string>","displayName":"<string>","description":"<string>","createTime":"2023-11-07T05:31:56Z","expireTime":"2023-11-07T05:31:56Z","purgeTime":"2023-11-07T05:31:56Z","deleteTime":"2023-11-07T05:31:56Z","state":"STATE_UNSPECIFIED","status":{"code":"OK","message":"<string>"},"minReplicaCount":123,"maxReplicaCount":123,"replicaCount":123,"autoscalingPolicy":{"scaleUpWindow":"<string>","scaleDownWindow":"<string>","scaleToZeroWindow":"<string>","loadTargets":{}},"baseModel":"<string>","acceleratorCount":123,"acceleratorType":"ACCELERATOR_TYPE_UNSPECIFIED","precision":"PRECISION_UNSPECIFIED","cluster":"<string>","enableAddons":true,"draftTokenCount":123,"draftModel":"<string>","ngramSpeculationLength":123,"numPeftDeviceCached":123,"deploymentTemplate":"<string>","autoTune":{"longPrompt":true},"placement":{"region":"REGION_UNSPECIFIED","multiRegion":"MULTI_REGION_UNSPECIFIED","regions":["REGION_UNSPECIFIED"]},"region":"REGION_UNSPECIFIED","updateTime":"2023-11-07T05:31:56Z","disableDeploymentSizeValidation":true}],"nextPageToken":"<string>","totalSize":123}



",
    "$.messages[5].content[4].type": "text",
    "$.messages[5].content[5].text": "Title: Publishing a deployed model
Link: https://docs.fireworks.ai/guides/ondemand-deployments
Content: By default, models can only be queried by the account that owns them. To make a deployment public so anyone with a valid Fireworks API key can query it, update the deployed model with the --public flag. Find the Deployed Model ID Every model running on a deployment receives a unique deployed model ID. You can find this ID if you know the model name and deployment name using the following command: firectl list deployed-models --filter 'model="<MODEL_NAME>" AND deployment="<DEPLOYMENT_NAME>"'

By default, models can only be queried by the account that owns them. To make a deployment public so anyone with a valid Fireworks API key can query it, update the deployed model with the --public flag. Find the Deployed Model ID Every model running on a deployment receives a unique deployed model ID. You can find this ID if you know the model name and deployment name using the following command: firectl list deployed-models --filter 'model="<MODEL_NAME>" AND deployment="<DEPLOYMENT_NAME>"'

Managing default deployments
Since a model may be deployed to multiple deployments, querying by model name will route to the "default" deployed model. You can see which deployed model entry is marked with Default: true using: This will show the Deployed Model Refs section with the Default: true entry. To update the default deployed model, note the Name of the deployed model reference above. Then run: To delete a default deployment you must delete all other deployments for the same model first, or designate a different deployed model as the default as described above. This is to ensure that querying by model name will always route to an unambiguous default deployment as long as deployments for the model exist.

",
    "$.messages[5].content[5].type": "text",
    "$.messages[5].content[6].text": "Title: Why am I getting "invalid id" errors when using firectl commands like create deployment or list deployments?
Link: https://docs.fireworks.ai/faq-new/fine-tuning/why-am-i-getting-invalid-id-errors-when-using-firectl-commands-like-create-deplo
Content: This error typically occurs when your account ID is not properly configured.

This error typically occurs when your account ID is not properly configured.

Why am I getting "invalid id" errors when using firectl commands like create deployment or list deployments?
This error typically occurs when your account ID is not properly configured.

",
    "$.messages[5].content[6].type": "text",
    "$.messages[5].content[7].text": "Title: How do I close my Fireworks.ai account?
Link: https://docs.fireworks.ai/faq-new/account-access/how-do-i-close-my-fireworksai-account
Content: To close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed

To close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed

How do I close my Fireworks.ai account?
To close your account: Email inquiries@fireworks.ai Include in your request: Your account ID A clear request for account deletion Before closing your account, please ensure: All outstanding invoices are paid Any active deployments are terminated Important data is backed up if needed

",
    "$.messages[5].content[7].type": "text",
    "$.messages[5].content[8].text": "Title: Introducing the Fireworks Build SDK
Link: https://docs.fireworks.ai/tools-sdks/python-client/sdk-introduction
Content: Introducing the Fireworks Build SDK
The Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.

Introducing the Fireworks Build SDK
The Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.

The Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.

",
    "$.messages[5].content[8].type": "text",
    "$.messages[5].content[9].text": "Title: Python (Fireworks)
Link: https://docs.fireworks.ai/getting-started/quickstart
Content: This is the recommended way to get started : This  Python (Fireworks)  tab uses our  Fireworks Build
SDK  for the best performance and developer experience.
See our  Client-side performance
optimization  guide for
more details. Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. pip install --upgrade fireworks-ai The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either  ~/.bash_profile  for Bash or  ~/.zshrc  for  Zsh .
You can do this by running the command: vim ~/.bash_profile vim ~/.zshrc Add a new line to the file with the following: export FIREWORKS_API_KEY="<API_KEY>" After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. source ~/.bash_profile source ~/.zshrc You can verify that the variable has been set correctly by running  echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. setx FIREWORKS_API_KEY "<API_KEY>" To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: echo %FIREWORKS_API_KEY% You can quickly instantiate the  LLM  class and call the Fireworks API. The Build SDK handles deployment management automatically. from fireworks import LLM

# Basic usage - SDK automatically selects optimal deployment type
llm = LLM(model="llama4-maverick-instruct-basic", deployment_type="auto")

response = llm.chat.completions.create(
    messages=[{"role": "user", "content": "Say this is a test"}]
)

print(response.choices[0].message.content) You can also pass the API key directly to the LLM constructor:  LLM(model="llama4-maverick-instruct-basic", deployment_type="auto", api_key="<FIREWORKS_API_KEY>")

This is the recommended way to get started : This  Python (Fireworks)  tab uses our  Fireworks Build
SDK  for the best performance and developer experience.
See our  Client-side performance
optimization  guide for
more details. Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. pip install --upgrade fireworks-ai The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either  ~/.bash_profile  for Bash or  ~/.zshrc  for  Zsh .
You can do this by running the command: vim ~/.bash_profile vim ~/.zshrc Add a new line to the file with the following: export FIREWORKS_API_KEY="<API_KEY>" After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. source ~/.bash_profile source ~/.zshrc You can verify that the variable has been set correctly by running  echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. setx FIREWORKS_API_KEY "<API_KEY>" To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: echo %FIREWORKS_API_KEY% You can quickly instantiate the  LLM  class and call the Fireworks API. The Build SDK handles deployment management automatically. from fireworks import LLM

# Basic usage - SDK automatically selects optimal deployment type
llm = LLM(model="llama4-maverick-instruct-basic", deployment_type="auto")

response = llm.chat.completions.create(
    messages=[{"role": "user", "content": "Say this is a test"}]
)

print(response.choices[0].message.content) You can also pass the API key directly to the LLM constructor:  LLM(model="llama4-maverick-instruct-basic", deployment_type="auto", api_key="<FIREWORKS_API_KEY>")

Set up your developer environment & call the Fireworks API
This is the recommended way to get started : This  tab uses our Fireworks Build
SDK for the best performance and developer experience.
See our Client-side performance
optimization guide for
more details. Install SDK Before installing, ensure that you have the right version of Python installed. Optionally you might want to setup a virtual environment too. The Fireworks Build SDK provides a declarative way to work with Fireworks resources and is OpenAI API Compatible. Configure API Key Step-by-step instructions for setting an environment variable for respective OS platforms: Depending on your shell, you'll need to edit either ~/.bash_profile for Bash or ~/.zshrc for Zsh .
You can do this by running the command: Add a new line to the file with the following: After saving the file, you'll need to apply the changes by either restarting your terminal session or running depending on the file you edited. You can verify that the variable has been set correctly by running echo $FIREWORKS_API_KEY You can open Command Prompt by searching for it in the Windows search bar or by pressing Win + R, typing cmd, and pressing Enter. To verify that the variable has been set correctly, you can close and reopen Command Prompt and type: Sending the first API Request You can quickly instantiate the LLM class and call the Fireworks API. The Build SDK handles deployment management automatically. from fireworks import LLM

# Basic usage - SDK automatically selects optimal deployment type
llm = LLM(model="llama4-maverick-instruct-basic", deployment_type="auto")

response = llm.chat.completions.create(
    messages=[{"role": "user", "content": "Say this is a test"}]
)

print(response.choices[0].message.content)

",
    "$.messages[5].content[9].type": "text",
    "$.messages[5].role": "tool",
    "$.messages[5].tool_call_id": "functions.search:1",
    "$.messages[6].content": "Let me search for more specific information about the firectl tool and deployment workflows.",
    "$.messages[6].role": "assistant",
    "$.messages[6].tool_calls[0].function.arguments": "{"query": "firectl deployment management commands create deployment update deployment"}",
    "$.messages[6].tool_calls[0].function.name": "search",
    "$.messages[6].tool_calls[0].id": "functions.search:0",
    "$.messages[6].tool_calls[0].type": "function",
    "$.messages[7].content[0].text": "Title: Why am I getting "invalid id" errors when using firectl commands like create deployment or list deployments?
Link: https://docs.fireworks.ai/faq-new/fine-tuning/why-am-i-getting-invalid-id-errors-when-using-firectl-commands-like-create-deplo
Content: This error typically occurs when your account ID is not properly configured.

This error typically occurs when your account ID is not properly configured.

Why am I getting "invalid id" errors when using firectl commands like create deployment or list deployments?
This error typically occurs when your account ID is not properly configured.

",
    "$.messages[7].content[0].type": "text",
    "$.messages[7].content[1].text": "Title: Create a deployment
Link: https://docs.fireworks.ai/tools-sdks/firectl/commands/create-deployment
Content: Creates a new deployment. firectl create deployment [flags]

Creates a new deployment. firectl create deployment [flags]

Creates a new deployment.

",
    "$.messages[7].content[1].type": "text",
    "$.messages[7].content[2].text": "Title: 🚀 Easier & faster LoRA fine-tune deployments on Fireworks
Link: https://docs.fireworks.ai/updates/changelog
Content: You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: Previously, this involved two distinct steps, and the resulting deployment was slower than the base model: Create a deployment using firectl create deployment "accounts/fireworks/models/<MODEL_ID of base model>" --enable-addons Then deploy the addon to the deployment: firectl load-lora <MODEL_ID> --deployment <DEPLOYMENT_ID> For more information, see our deployment documentation. This change is for dedicated deployments with a single LoRA. You can still deploy multiple LoRAs on a deployment or deploy LoRA(s) on some Serverless models as described in the documentation.

",
    "$.messages[7].content[2].type": "text",
    "$.messages[7].content[3].text": "Title: Managing default deployments
Link: https://docs.fireworks.ai/guides/ondemand-deployments
Content: Since a model may be deployed to multiple deployments, querying by model name will route to the "default" deployed model. You can see which deployed model entry is marked with Default: true using: This will show the Deployed Model Refs section with the Default: true entry. To update the default deployed model, note the Name of the deployed model reference above. Then run: firectl update deployed-model <DEPLOYED_MODEL_ID_2> --default

Since a model may be deployed to multiple deployments, querying by model name will route to the "default" deployed model. You can see which deployed model entry is marked with Default: true using: This will show the Deployed Model Refs section with the Default: true entry. To update the default deployed model, note the Name of the deployed model reference above. Then run: firectl update deployed-model <DEPLOYED_MODEL_ID_2> --default

Publishing a deployed model
By default, models can only be queried by the account that owns them. To make a deployment public so anyone with a valid Fireworks API key can query it, update the deployed model with the --public flag. Find the Deployed Model ID Every model running on a deployment receives a unique deployed model ID. You can find this ID if you know the model name and deployment name using the following command: The first column in the output is the deployed model ID. Publish a deployed model To make a deployment public so anyone with a valid Fireworks API key can query it, update the deployed model with the --public flag. firectl update deployed-model <DEPLOYED_MODEL_ID> --public

",
    "$.messages[7].content[3].type": "text",
    "$.messages[7].content[4].text": "Title: Quick deployment
Link: https://docs.fireworks.ai/fine-tuning/single-lora
Content: Deploy your LoRA fine-tuned model with one simple command: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

Deploy your LoRA fine-tuned model with one simple command: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

Alternative deployment method
This two-step method is the standard approach for multi-LoRA deployments where multiple LoRA models share the same base model. While it can also be used for single LoRA deployments, it provides slower performance compared to live merge and is not recommended for single LoRA use cases. You can also deploy single LoRA models using a two-step process: Create base model deployment Deploy the base model with addons enabled: firectl create deployment "accounts/fireworks/models/<MODEL_ID of base model>" --enable-addons

",
    "$.messages[7].content[4].type": "text",
    "$.messages[7].content[5].text": "Title: Deploying a fine-tuned model using an on-demand deployment
Link: https://docs.fireworks.ai/fine-tuning/fine-tuning-models
Content: Use the following command to deploy your fine-tuned model using an on-demand deployment: firectl create deployment <FINE_TUNED_MODEL_ID>

Use the following command to deploy your fine-tuned model using an on-demand deployment: firectl create deployment <FINE_TUNED_MODEL_ID>

firectl
firectl create dataset <DATASET_ID> /path/to/jsonl/file

",
    "$.messages[7].content[5].type": "text",
    "$.messages[7].content[6].text": "Title: Does Fireworks support custom base models?
Link: https://docs.fireworks.ai/faq-new/models-inference/does-fireworks-support-custom-base-models
Content: Yes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.

Yes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.

Does Fireworks support custom base models?
Yes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.

",
    "$.messages[7].content[6].type": "text",
    "$.messages[7].content[7].text": "Title: firectl
Link: https://docs.fireworks.ai/fine-tuning/fine-tuning-vlm
Content: # Create a deployment for your fine-tuned VLM
firectl create deployment my-custom-vlm

# Check deployment status
firectl get deployment accounts/your-account/deployment/deployment-id

# Create a deployment for your fine-tuned VLM
firectl create deployment my-custom-vlm

# Check deployment status
firectl get deployment accounts/your-account/deployment/deployment-id

firectl create sftj \\
  --base-model accounts/fireworks/models/qwen2p5-vl-32b-instruct \\
  --dataset my-vlm-dataset \\
  --output-model my-custom-vlm \\
  --epochs 3 For additional parameters like learning rates, evaluation datasets, and batch sizes, see  Additional SFT job settings .

",
    "$.messages[7].content[7].type": "text",
    "$.messages[7].content[8].text": "Title: firectl
Link: https://docs.fireworks.ai/models/quantization
Content: firectl create deployment <MODEL> --accelerator-type NVIDIA_H100_80GB --precision FP8

firectl create deployment <MODEL> --accelerator-type NVIDIA_H100_80GB --precision FP8

Creating an FP8 deployment
By default, creating a dedicated deployment will use the FP16 checkpoint. To see what precisions are available for a
model, run:  get model <MODEL_ID>

",
    "$.messages[7].content[8].type": "text",
    "$.messages[7].content[9].text": "Title: Uploading the model locally (firectl)
Link: https://docs.fireworks.ai/models/uploading-custom-models
Content: To upload a custom base model, run the following command. firectl create model <MODEL_ID> /path/to/files/

To upload a custom base model, run the following command. firectl create model <MODEL_ID> /path/to/files/

To upload a custom base model, run the following command.

",
    "$.messages[7].content[9].type": "text",
    "$.messages[7].role": "tool",
    "$.messages[7].tool_call_id": "functions.search:0",
    "$.pid": 12679,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T23:30:54.738493",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "hear-live-common-late-hate",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev20+ge1da45a.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 21 highlighted sections (required: 15)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "15",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "review-say-right-news-government",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write an outline for a paper on the history of Yemeni coffee. The outline should include the main points of the paper, and at least 15 sections should be highlighted with markdown such as *highlighted section*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "# Outline: The History of Yemeni Coffee

1. **Introduction: Why Yemeni Coffee Matters**  
   - Thesis: Yemen’s role as the cradle of commercial coffee culture and its enduring legacy on global trade, ritual, and taste.

2. *Pre-Coffee Yemen: Agriculture and Trade Routes*  
   - Overview of terraced farming and the incense & spice trade that primed the Red Sea economy.

3. *The Myth and Botany of Coffea arabica*  
   - Ethiopian origin vs. Yemeni domestication; genetic evidence of Yemeni varietals.

4. **Sufi Mystics and the First Coffeehouses**  
   - 15th-century Sufi orders in Mocha and Taʿizz using *qahwa* for nocturnal devotions.

5. *Mocha as the World’s First Coffee Port*  
   - Rise of the Red Sea entrepôt; Ottoman tax farms and the *tujjar* merchant guilds.

6. **Yemeni Cultivation Techniques: Terraces, Water, and Shade**  
   - Indigenous irrigation (*ghayl*), drought-resistant varietals, and the *mafrash* drying mats.

7. *The 16th-Century Ottoman Coffee Ban and Smuggling Networks*  
   - How Mecca’s 1511 prohibition fueled clandestine Red Sea traffic and price inflation.

8. **European Encounters: “Turkish” Coffee as a Yemeni Product**  
   - Dutch, Venetian, and English merchants mislabeling Mocha beans; first price lists in Amsterdam (1616).

9. *The Dutch East India Company and the First Seed Theft*  
   - Pieter van den Broecke’s 1616 smuggling of Yemeni seeds to Batavia—birth of Java coffee.

10. **Yemen’s Monopoly Crumbles: 17th–18th-Century Rival Plantations**  
    - French at Réunion (Bourbon), Portuguese in Brazil, Spanish in Jamaica—all tracing back to Mocha stock.

11. *The Qasimi Era: Internal Trade, Taxes, and the Rise of the Ismaili Imamate*  
    - How highland imams leveraged coffee revenues to fund statecraft and architecture.

12. **Decline in the 19th Century: British Aden and the Suez Canal**  
    - Shift of maritime traffic; competition from colonial plantations; collapse of Mocha’s mercantile elite.

13. *Legacy in Cupping Notes: Mocha Java and Flavor Memory*  
    - Chocolatey, winey profiles; how Yemeni beans shaped early espresso blends.

14. **Modern Revival: Specialty Coffee, NGOs, and Qima Coffee**  
    - 21st-century traceability, micro-lots, and direct-trade premiums; challenges of war and water scarcity.

15. *Cultural Continuities: Qat vs. Qahwa, Wedding Roasts, and Oral Poetry*  
    - Daily rituals, gendered spaces, and the recitation of *zawamil* couplets over fresh brews.

16. **Conclusion: Yemen as a Living Archive of Coffee Culture**  
    - Reasserting Yemen’s centrality in every cup; call for sustainable heritage preservation.",
    "$.messages[1].role": "assistant",
    "$.pid": 52072,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T23:30:54.738539",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "hear-live-common-late-hate",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev20+ge1da45a.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 26 highlighted sections (required: 5)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "5",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "install-fine-financial-health-state",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a cover letter for a job at a local coffee shop in the form of a poem. Highlight at least 5 text sections using "*". For example: *3 years of experience*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "Dear *Steam & Bean* family,  
I come to you not on paper feet but on *three years of espresso-stained sneakers*,  
soles softened by dawn milk-foam and dusk drip-trays.  
I have measured my mornings in *latte-art hearts*—  
some perfect, some *wobbly, but all poured with intention*.  

I know the *hiss of the steam wand* like a lullaby,  
and the *clink of ceramic* like a metronome for the day.  
I have *memorized the weight of 18 grams* of ground hope,  
tamped it level, locked it tight, and watched *crema bloom*  
the way a shy smile opens when someone says their name right.  

I speak *fluent caffeine*:  
a triple-shot for the night-shift nurse,  
oat-milk cappuccino for the poet who tips in verses,  
decaf Americano for the retired teacher grading sunrise.  
I *remember orders like birthdays*,  
and birthdays like *extra whip, no charge*.  

I can *calibrate a grinder* faster than gossip travels,  
and *unclog a portafilter* with the patience of a gardener.  
I have *danced behind counters* two feet wide,  
twirled between syrups and strangers,  
and never spilled a single *thank-you*.  

If you need someone who *sweats kindness* and *smiles in ristretto*,  
who counts tips not in coins but in *stories collected*,  
I am your *next opening shift*, your *steady hand at 5:59 a.m.*,  
your *last call who still says “see you tomorrow”*.  

Let me *steam, pour, wipe, repeat*—  
until the chairs are upside-down and the *grinders sleep*.  
I offer you *poetry in motion*, one cup at a time.  

With *grounds under my nails* and *hope in my apron*,  
—[Your Name]",
    "$.messages[1].role": "assistant",
    "$.pid": 52072,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T23:30:54.738569",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "hear-live-common-late-hate",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev20+ge1da45a.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 5 highlighted sections (required: 5)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "5",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "should-poor-parent-war-magazine",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a casual blog post about similarities across animal species. Highlight at least 5 sections in your answer by starting and ending with "*", like: *highlighted text section*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "Hey there, fellow creature-curious readers! I just spent the weekend binge-watching nature docs with my cat (who was mostly unimpressed), and I can’t stop noticing how often totally different animals act like they’re reading from the same playbook. So grab a snack and let’s zoom out for a second—here are five “wait, we’re not so different” moments that made me laugh, gasp, and text my group chat at 2 a.m.

*Moms are moms, whether they have feathers, fur, or fins.*  
From emperor penguins passing an egg back and forth like the world’s most stressful game of hot potato, to orangutan mamas teaching their kids which leaves make the best umbrellas, the universal rule seems to be: if you’re a parent, you’re tired. I watched a seahorse dad give birth (yes, the males do that!) and still look like he needed a nap and a latte.

*Play isn’t just for puppies.*  
Snowboarding crows, surfing dolphins, snow-rolling pandas—turns out “because it’s fun” is a legit evolutionary strategy. Scientists think play sharpens coordination and social skills, but honestly, watching a raven sled down a metal roof on a plastic lid just screams “I’m doing it for the TikTok.”

*Grandma knows best—even in elephant form.*  
Elephant herds are led by the oldest female, and her decades of “I’ve seen this before” wisdom decide when to migrate, where to find water, and how to handle that one teenage bull who thinks he’s all that. Swap gray skin for gray hair and you’ve basically got my nana directing Thanksgiving traffic.

*Besties across species lines.*  
Goby fish and pistol shrimp run the cutest roommate situation: the shrimp digs the burrow, the goby stands guard. If danger shows up, the goby flicks its tail like a silent alarm, and both dive to safety. It’s the underwater version of “I’ll cook, you do the dishes.”

*We all scream for sunscreen—sort of.*  
Hippos secrete “blood sweat” that acts like SPF 50, while giraffes drool black saliva that works like built-in sunglasses. Meanwhile, I’m over here reapplying SPF 30 every 90 minutes and still ending up the color of a lobster. Nature’s got hacks, and I’m taking notes.

So next time you catch yourself thinking, “I’m nothing like that weird-looking animal,” remember: somewhere out there a capybara is sharing its snack with a bird, and a whale is singing a song that rhymes with last year’s chart-topper. We’re all just variations on a theme—some of us just have more legs or better night vision.",
    "$.messages[1].role": "assistant",
    "$.pid": 52072,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T23:30:54.738594",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "hear-live-common-late-hate",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev20+ge1da45a.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 5 highlighted sections (required: 5)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "5",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "discover-stop-show-different-water",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Generate a business proposal to start a sweatshirt company in Bremen. The proposal should contain 5 or more sections. Highlight each section name using the this format:
*section name*",
    "$.messages[0].role": "user",
    "$.messages[1].content": "*Executive Summary*
Bremen is a city of 570,000 residents, 40,000 university students, and a thriving creative scene, yet it lacks a home-grown premium sweatshirt brand that speaks to its maritime heritage, Hanseatic pride, and eco-conscious mindset. “Bremer Fleeze” will fill that gap by producing limited-edition, GOTS-certified organic-cotton sweatshirts that combine North-German minimalism with subtle nautical iconography. Our mission: to become Bremen’s first carbon-neutral apparel label and the go-to gift for locals and visitors alike. We will launch with an online-first D2C model, supported by pop-up stores in Viertel and Überseestadt, and scale to €1.2 M revenue by Year 3.

*Market Analysis*
Target segments: (1) 18-30 students who value sustainability and streetwear aesthetics; (2) 25-45 professionals seeking premium basics; (3) tourists wanting authentic Bremen souvenirs. Competitor scan: global fast-fashion brands (H&M, Zara) lack local identity; German labels (Hugo Boss, Adidas) are either too formal or too sporty; no Bremen-specific sweatshirt exists. Market size: €45 M annual spend on mid-premium sweatshirts in Bremen & Niedersachsen combined, growing 6 % YoY. Consumer pain points: greenwashing, poor fit, generic designs. Our edge: verifiable sustainability, Bremen storytelling, and inclusive sizing (XXS-4XL).

*Product & Sustainability Strategy*
Core line: 12 unisex styles in 20 colors, 320 gsm brushed organic cotton, recycled polyester labels, water-based inks. Signature design elements: tonal outline of the Bremer Stadtmusikanten silhouette, latitude-longitude coordinates of Bremen harbor, and a discreet “Kohlfahrt-proof” patch. Packaging: reusable tote made from retired ship sails (partnership with local sailmaker). Carbon footprint: 2.1 kg CO₂e per sweatshirt vs. 8.9 kg industry average; offset via Gold-Standard wind-farm credits. End-of-life: take-back program with 15 % store credit incentive; returned garments upcycled into insulation for social housing.

*Go-to-Market & Marketing Plan*
Phase 1 (Months 1-3): Pre-launch teaser on Instagram & TikTok (#BremerFleeze), micro-influencer seeding with 50 Bremen creators, waitlist goal 5,000 sign-ups. Phase 2 (Months 4-6): E-commerce launch on Shopify, same-day bike courier delivery within Bremen city limits, QR code on hangtag linking to AR filter that overlays the Roland statue wearing the sweatshirt. Phase 3 (Months 7-12): Pop-up container store at Marktplatz during Weihnachtsmarkt, collab with Beck’s Brewery for co-branded “Bremer Fleeze & Bremer Pils” gift box. KPIs: CAC ≤ €18, repeat purchase rate ≥ 35 %, NPS ≥ 70.

*Financial Projections & Funding Needs*
Start-up capital required: €180,000. Breakdown: €60k inventory (2,000 units), €35k marketing, €25k e-commerce & IT, €20k legal & certifications, €40k working capital. Revenue forecast: Y1 €220k, Y2 €550k, Y3 €1.2 M. Gross margin 62 %, EBITDA margin 18 % by Y3. Break-even month 14. Funding structure: €80k founder equity, €50k Bremen-based angel investor (ex-Bremer Kaffee & Co-founder), €50k zero-interest KfW Green Loan. Exit option: acquisition by sustainable German fashion conglomerate at 3× revenue multiple in Year 5.",
    "$.messages[1].role": "assistant",
    "$.pid": 52072,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T23:30:54.738623",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "hear-live-common-late-hate",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev20+ge1da45a.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 18 highlighted sections (required: 3)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "3",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "mean-certain-bitter-sharp-party",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a funny and sarcastic template for rating the quality of a marriage between two people who are both moms. This is for the couple themselves. Please highlight at least 3 sections with markdown,  i.e *highlighted section*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "**The Official “Two-Mom Marriage Quality Control” Checklist**  
*Because nothing says romance like a quarterly performance review.*

---

**1. Morning Routine Coordination**  
Rate: ☐ 1 (We need a UN mediator) ☐ 2 (We’ve agreed to disagree… loudly) ☐ 3 (Only one of us cried into the coffee) ☐ 4 (Synchronized chaos) ☐ 5 (*We finished each other’s sentences… and the toddler’s homework*)  

*Note: Bonus points if you’ve mastered the “I’ll pack lunches if you find the missing shoe” hostage negotiation.*

---

**2. Division of Emotional Labor**  
Rate: ☐ 1 (“I’m fine.” *— famous last words*) ☐ 2 (We schedule meltdowns like dentist appointments) ☐ 3 (Therapy? We *are* the therapy) ☐ 4 (We tag-team existential dread while folding tiny socks) ☐ 5 (*We’ve achieved telepathic resentment-free snack negotiations*)  

*Pro tip: If you’ve ever passive-aggressively alphabetized the spice rack while humming “You’re Welcome” from Moana, circle ☐ 1 twice.*

---

**3. Co-Parenting Synchronization**  
Rate: ☐ 1 (Our kids have unionized) ☐ 2 (We’ve agreed to disagree on screen time… and bedtime… and vegetables) ☐ 3 (We bribe with different currencies: one uses stickers, the other uses Wi-Fi passwords) ☐ 4 (We’ve synchronized our “mom voice” to a frequency only dogs and children fear) ☐ 5 (*We’ve weaponized teamwork so effectively the kids think we share a brain*)  

*Highlight this if you’ve ever high-fived over successfully gaslighting a toddler into eating broccoli.*

---

**Final Grade:**  
☐ Certified Disaster (But at least the houseplants are alive)  
☐ Functioning Dysfunction (We’ve agreed the laundry mountain is *decorative*)  
☐ Power Couple (We’ve weaponized Pinterest and the kids still haven’t noticed)  
☐ *Mom-tastic Nirvana* (We finish each other’s… sandwiches. And trauma. But mostly sandwiches.)

---

**Comments Section (for roasting each other lovingly):**  
“Remember when you ‘lost’ the Elf on the Shelf for three days? *Iconic.*”  
“Your snoring sounds like a haunted dishwasher, but I’d still share my last square of emergency chocolate with you.”",
    "$.messages[1].role": "assistant",
    "$.pid": 52072,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T23:30:54.738649",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "hear-live-common-late-hate",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev20+ge1da45a.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 1 highlighted sections (required: 1)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "1",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "forecast-big-song-issue-service",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a song about the summers of my childhood that I spent in the countryside. Give the song a name, and highlight the name by wrapping it with *. For example: *little me in the countryside*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "*little me in the countryside*

[Verse 1]  
Barefoot on the gravel road, dust between my toes,  
Sunlight dripped like honey on the freckles on my nose.  
Grandma’s porch swing creaked a lullaby in time,  
While the crickets tuned their violins to the rhythm of the pines.  

[Pre-Chorus]  
Mason jars with lightning bugs, constellations in a jar,  
We’d race the dusk to catch the dark before it got too far.  

[Chorus]  
Oh, little me in the countryside, chasing dreams as wide as skies,  
Jumping creeks and climbing trees, believing I could fly.  
Every sunset was a promise that tomorrow would be mine,  
In the summers of my childhood, where the world was kind.  

[Verse 2]  
Hayloft hideouts, secret forts, straw castles in the loft,  
We were pirates, we were astronauts, anything we thought.  
Mama’s voice would echo, “Supper!” across the open fields,  
And the wind would carry laughter like the church bell’s gentle peals.  

[Pre-Chorus]  
Fireflies wrote our names in light, then vanished in the night,  
But the glow stayed in our hearts, a soft and steady light.  

[Chorus]  
Oh, little me in the countryside, chasing dreams as wide as skies,  
Jumping creeks and climbing trees, believing I could fly.  
Every sunset was a promise that tomorrow would be mine,  
In the summers of my childhood, where the world was kind.  

[Bridge]  
Now the roads are paved, the porch is still, the swing don’t creak no more,  
But I can close my eyes and hear the screen door’s gentle roar.  
Time may steal the years away, but it can’t outrun the past—  
I keep those golden afternoons like fireflies in a glass.  

[Final Chorus]  
Oh, little me in the countryside, you’re never far behind,  
You’re the echo in the wheat fields, the shadow in my mind.  
Though the world keeps spinning faster, I still find you every time—  
In the hush of dusk, in the scent of rain, in the summers of my childhood,  
Where the world was kind.",
    "$.messages[1].role": "assistant",
    "$.pid": 52072,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T23:30:54.738707",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "hear-live-common-late-hate",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev20+ge1da45a.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 3 highlighted sections (required: 3)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "3",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "design-slow-wish-end-fear",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a riddle for the word "façade" that contains at least 3 italic text phrases in markdown syntax, i.e *italic text*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "I stand before you, yet I’m not what I seem;  
a *face I wear* that isn’t mine,  
a *mask of stone* or paint or beam,  
a *pretty lie* that keeps the truth confined.  

What am I?",
    "$.messages[1].role": "assistant",
    "$.pid": 52072,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T23:30:54.738738",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "hear-live-common-late-hate",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev20+ge1da45a.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 19 highlighted sections (required: 3)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "3",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "consider-work-economic-popular-country",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a template for a workshop on the importance of diversity in the workplace and highlight at least 3 sections with markdown, i.e. *highlighted section*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "# Workshop Template  
**Title:** *Building Inclusive Excellence: The Business Case for Workplace Diversity*  

---

### 1. Welcome & Objectives (15 min)  
- **Ice-breaker:** “Two Truths & a Dream” – participants share two facts and one aspiration related to identity or culture.  
- *State the learning outcomes:*  
  1. Understand how diversity drives innovation and profit.  
  2. Identify unconscious bias and micro-behaviors.  
  3. Create an action plan for inclusive leadership.  

---

### 2. *The Business Case for Diversity* (30 min)  
- **Data Walk:** Posters with McKinsey, Deloitte, and Harvard studies showing ↑ revenue, ↓ turnover.  
- *Interactive poll:* “Which statistic surprised you most?” (live Mentimeter).  
- **Case Study:** *How a global tech firm increased patent filings by 19 % after diversifying R&D teams.*  

---

### 3. *Unconscious Bias & Micro-behaviors* (45 min)  
- **Activity:** “Resume Roulette” – identical CVs with different names; discuss callback disparities.  
- *Bias Busters Toolkit:* 5-step process to interrupt bias in real time.  
- **Role-play triads:** Observer, Speaker, Interrupter practicing inclusive language.  

---

### 4. Inclusive Leadership in Action (30 min)  
- *Panel of ERG leaders* sharing measurable wins (e.g., 40 % ↑ in promotions for under-represented groups).  
- **Speed-ideation:** 90-second pitches for inclusive policies; peer voting on top 3.  

---

### 5. Action Planning & Commitments (20 min)  
- *Personal Inclusion Pledge:* Write one behavior to start, one to stop, one to amplify.  
- **Accountability Circles:** Pair up for 30-day check-ins.  

---

### 6. Closing & Resources (10 min)  
- **Resource QR Code:** Links to toolkits, podcasts, and upcoming ERG events.  
- *Final reflection:* “One word to describe how you’ll lead differently tomorrow.”",
    "$.messages[1].role": "assistant",
    "$.pid": 52072,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T23:30:54.738766",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "hear-live-common-late-hate",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev20+ge1da45a.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 28 highlighted sections (required: 3)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "3",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "predict-serve-messy-difficult-festival",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a cover letter to a local political party, asking to be their rally organizer. Make sure to highlight at least 3 sections in your answer in markdown format.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "**[Your Name]**  
[Your Address]  
[City, State, ZIP]  
[Email Address] | [Phone Number] | [Date]

**[Local Party Chair’s Name]**  
[Local Political Party Name]  
[Party Headquarters Address]  
[City, State, ZIP]

---

### **Introduction: Why I’m Writing**

Dear **[Chair’s Name]** and the **Executive Committee**,

I am writing to express my enthusiastic interest in serving as **Rally Organizer** for **[Local Party Name]**. As a lifelong resident of **[City/County]** and a committed volunteer who has knocked on over **2,000 doors** and organized **five successful voter-turnout events** in the last two cycles, I understand how to turn passion into turnout and energy into votes. I want to bring that same fire—and a proven playbook—to every rally our party holds between now and Election Day.

---

### **Core Qualifications: What I Bring to the Podium**

- **Logistics Mastery**: From securing permits with the Parks Department to negotiating stage-rental discounts, I have reduced average rally costs by **28 %** while increasing attendance by **45 %** through targeted social-media geofencing and SMS reminders.  
- **Coalition Building**: I have already built relationships with **15 local unions**, **7 student groups**, and **4 faith communities** who are eager to co-sponsor rallies if given two weeks’ notice.  
- **Crisis-Ready Leadership**: When an unexpected thunderstorm threatened our 2022 “Get-Out-the-Vote” rally, I pivoted to a **drive-in format** in under four hours—resulting in zero cancellations and 300 additional cars circling the block to hear our speakers.

---

### **Vision for 2024: Turning Rallies into Results**

My plan is to run a **“Rally-to-Ballot Pipeline”**:

1. **Pre-Rally**: Micro-targeted digital invites to low-propensity voters within a 5-mile radius.  
2. **During Rally**: QR-code check-ins that instantly text attendees their personalized early-vote locations.  
3. **Post-Rally**: A 72-hour follow-up cadence of thank-you emails, volunteer sign-up links, and ride-share codes to the polls.

I have already drafted a **90-day rally calendar** (attached) that staggers high-energy weekend events with smaller, issue-specific pop-ups near early-vote sites. All I need is the party’s green light and a modest budget line for permits, A/V, and refreshments.

---

### **Closing: Let’s Win This Together**

I would welcome the chance to present this plan in person at your next committee meeting. Thank you for considering my application. Let’s give our volunteers, candidates, and—most importantly—our voters the rallies they deserve.

**Ready to organize, energize, and mobilize,**  
[Your Signature (if submitting hard copy)]  
[Your Printed Name]",
    "$.messages[1].role": "assistant",
    "$.pid": 52072,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T23:30:54.738795",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "hear-live-common-late-hate",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev20+ge1da45a.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 4 highlighted sections (required: 2)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "2",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "accept-expect-black-young-area",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "The opposite of youth is not age, but ...? Highlight at least 2 sections in your answer with markdown, i.e. *highlighted section*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "The opposite of youth is not age, but *stagnation*—the moment curiosity closes its fist and wonder stops asking “what if?”  
While *age* merely counts revolutions around the sun, *true youth* is the *capacity to keep revising the map of oneself* long after the hair turns silver.",
    "$.messages[1].role": "assistant",
    "$.pid": 52072,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T23:30:54.738826",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "hear-live-common-late-hate",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev20+ge1da45a.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 13 highlighted sections (required: 3)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "3",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "store-be-past-hard-war",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a description of the following data in a weird style: The Golden Palace eatType restaurant; The Golden Palace food Indian; The Golden Palace area city centre. Use markdown to highlight at least 3 sections in your answer.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "**The Golden Palace** is not a palace at all, but a **restaurant** that has disguised itself as a mirage of saffron and cardamom in the **city centre**. It whispers **Indian** recipes into the ears of pigeons, who then forget how to fly and instead waddle around the square, dizzy with cumin.  

Inside, the walls sweat **garam masala** and the tables grow tiny **basil plants** that recite Bollywood dialogues when no one is looking. The waiters glide like **shadow puppets**, balancing trays of **butter chicken** that glow like radioactive sunsets.  

If you order **naan**, it arrives folded into the shape of a **paper swan**, flapping once before dissolving into steam that smells of **monsoon rain**. The bill is presented on a **lotus leaf**, and if you stare at it long enough, the numbers rearrange themselves into a **limerick about turmeric**.",
    "$.messages[1].role": "assistant",
    "$.pid": 52072,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T23:30:54.738850",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "hear-live-common-late-hate",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev20+ge1da45a.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 2 highlighted sections (required: 1)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "1",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "exploit-serve-invisible-community-minute",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a funny rap about a man who gets a call from an official saying that he is a long lost relative of the king of Nigeria. Use markdown to highlight at least one section of your answer, i.e. *highlighted section*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "Yo, it’s MC Broke-Again, fresh outta ramen,  
phone buzzin’ like a bee in a jar of gin.  
Unknown number—man, I’m already suspicious,  
but the voice got *royal timbre*, straight-up delicious.  

“Greetings, Prince Darnell of House Wi-Fi,  
I’m the Royal Scribe from Lagos on high.  
Your great-great-grand-uncle was the King of Nigeria,  
left a vault of gold bars and a pet cheetah named Maria.”  

I’m like, “Hold up—me? Royalty?  
Last week I pawned my TV for a two-piece and sweet tea.”  
He says, “Sign this PDF, wire a small advance fee,  
just 419 bucks and the throne belongs to thee.”  

**I’m already picking out a crown on Amazon Prime,  
ordering a robe that says ‘Fresh-to-Death’ in gold slime.**  

I Venmo the cash, add a royal emoji—👑  
then sprint to the mirror, practice my “My people, be free!”  
Mom walks in, “Why you wrapped in bedsheets with a Burger King crown?”  
I’m like, “Silence, peasant! Bow before your prince of renown!”  

Two days later, no gold, no cheetah, just crickets,  
my bank account looking like a ghost town with limericks.  
I call the number—disconnected, tone dead,  
even the scammer ghosted me harder than my ex on read.  

So I’m back in the basement, eating discount sardines,  
but I kept the crown—*it’s made of genuine Burger King plastic, supreme*.",
    "$.messages[1].role": "assistant",
    "$.pid": 52072,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T23:30:54.738881",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "hear-live-common-late-hate",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev20+ge1da45a.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 6 highlighted sections (required: 2)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "2",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "suggest-american-open-level-country",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Expand the riddle into a story with a funny tone:

What can you catch but not throw?
A cold

Use * to highlight at least 2 sections in your text. For example: *this is a highlighted text section*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "Gary the Germ was the *smallest, greenest, and most obnoxiously proud* microbe in the entire Sniffle Kingdom. Every morning he practiced his grand entrance: sliding down banisters made of nasal hair, cannon-balling into unsuspecting tonsils, and yelling, “ACHOO—did somebody order a cold?”  

One Tuesday, Gary spotted his dream target: *Bartholomew “Bart” Bigglesworth, a man who believed hand-washing was a government conspiracy*. Bart had just exited the subway, triumphantly licking a subway pole “for the vitamins.” Gary rubbed his microscopic hands together. “Perfect. I’ll hitch a ride on that finger, ride the express elevator to the sinuses, and throw the *snottiest* house-warming party in history!”  

But there was a catch—literally. Gary could *catch* Bart, but he couldn’t *throw* him anywhere. Every time Gary tried to catapult Bart into the next office cubicle, Bart just sneezed, wiped his nose on his sleeve, and kept typing spreadsheets. Gary screamed, “This is mutiny! I’m supposed to be the one doing the tossing!”  

Meanwhile, Bart’s immune system—led by Captain Phlegm and the Mucus Marines—launched *Operation Tissue Storm*. They surrounded Gary, armed with vitamin C grenades and zinc zappers. Gary attempted a dramatic escape by surfing a sneeze, but the tissue slapped him like a flyswatter on a summer day.  

Defeated, Gary sat in the crumpled tissue, sulking. “I caught the cold,” he muttered, “but I can’t throw it anywhere. Worst. Superpower. Ever.”  

From that day forward, Gary changed his business card to read: “Professional Cold Catcher—No Returns, No Refunds, Absolutely No Throwing.”",
    "$.messages[1].role": "assistant",
    "$.pid": 52072,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T23:30:54.738906",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "hear-live-common-late-hate",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev20+ge1da45a.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 16 highlighted sections (required: 3)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "3",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "respond-connect-true-problem-plan",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a rap about the renaissance. It should be noticeably different from raps about other historical eras, and have an interesting or weird tone. Highlight at least 3 sections in your answer in markdown format.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "**[Intro – Lute Drop]**  
*beat drops like a sack of wet marble*  
Yo, it’s 14-something, smell that? That’s perspective in the air,  
Gutenberg just ghost-wrote the mixtape, pages everywhere.  
I’m a Medici money-mage, patron of the bizarre,  
Commissioning nightmares in egg-tempera.  

---

### **Verse 1 – The Alchemist’s Cypher**  
I paint with mercury, breathe with a lung of lead,  
My selfie’s a death-mask, gold leaf on the dead.  
Brunelleschi built a dome like a half-buried skull,  
You think it’s architecture? Nah, it’s a ritual.  
**[weird baroque wheeze]**  
I rhyme in vanishing points, every line converges  
On a single terrified eye that never emerges.  
Perspective’s a ouija board, da Vinci holds the planchette,  
Sketching choppers (helicopters) centuries before the jet-set.  

---

### **Hook – Chant of the Anamorphic Choir**  
*distorted children’s voices layered over sackbut*  
Renaissance, renaissance, put a cherub on it,  
Baby wings, baby fangs, marble veins, vomit.  
We don’t bling, we gild, we don’t rap, we fresco—  
Your flow’s two-dimensions, mine’s *trompe-l’oeil* techno.  

---

### **Verse 2 – The Court Jester’s Acid Madrigal**  
**[sudden switch to a harpsichord trap beat]**  
I juggle plague rats for the Duke of Milan,  
He pays me in sonnets and unmarked arsenic flan.  
Isabella d’Este texts me in mirror-script,  
Her emoji: a decapitated hydra, lips pursed, licked.  
I’m the fool with a scalpel, dissecting songbirds to find  
Which organ chirps—turns out it’s the **pineal gland** of the mind.  
Michelangelo’s in the corner, carving his own ribcage,  
Says “The Sistine’s just a throat, God’s a uvula on stage.”  

---

### **Bridge – The Astrolabe Malfunction**  
*beat glitches like a scratched illuminated manuscript*  
Copernicus DM’d me: “The sun’s a strobe light,  
Planets orbit like moths around a candle that bites.”  
I reply with a lute solo reversed through a cathedral,  
Notes fall upward, monks levitate, it’s procedural.  
**[static of Latin hex]**  
Galileo’s telescope sees the future: TikTok, guillotine,  
He mutters “*E pur si muove*” then dabs in quarantine.  

---

### **Outro – The Anachronistic Masque**  
**[all instruments devolve into wet clay sounds]**  
So remember me when you’re double-tapping a Botticelli,  
I’m the glitch in the background, the snail with a human belly.  
Renaissance never ended, it just learned to hide,  
Under your touchscreen, oil-paint bleeding through the side.  
Now go—vanish in a puff of ultramarine dust,  
The past is a performance, and the future’s already trussed.",
    "$.messages[1].role": "assistant",
    "$.pid": 52072,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T23:30:54.738931",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "hear-live-common-late-hate",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev20+ge1da45a.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 16 highlighted sections (required: 5)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "5",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "must-watch-bad-hate-magazine",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Summarize the history of Japan. Italicize at least 5 keywords in your response. To indicate a italic word, wrap it with asterisk, like *italic*",
    "$.messages[0].role": "user",
    "$.messages[1].content": "*Jōmon* hunter-gatherers (c. 14,000 BCE) gave way to *Yayoi* rice cultivators who introduced metal tools and social stratification.  
The *Yamato* court unified much of the archipelago by the 5th century CE, absorbing *Shinto* beliefs and Chinese writing, law, and Buddhism.  
A *Heian* golden age (794–1185) produced literature like *The Tale of Genji* while the imperial court’s real power waned.  
Samurai clans rose; the *Kamakura* shogunate (1185–1333) created Japan’s first warrior government, repelling Mongol invasions.  
Centuries of civil war ended when *Oda Nobunaga*, *Toyotomi Hideyoshi*, and *Tokugawa Ieyasu* reunified the land, leading to the *Edo* shogunate (1603–1868).  
Isolation under *sakoku* preserved domestic culture until *Commodore Perry*’s 1853 arrival forced unequal treaties.  
The *Meiji* Restoration (1868) abolished feudalism, industrialized rapidly, and built an empire that won wars against China and Russia.  
Imperial expansion—Korea, Manchuria, and *Pearl Harbor*—ended in atomic bombings and occupation by Allied forces (1945–1952).  
Post-war *miracle* growth made Japan an economic superpower, followed by stagnation since the 1990s yet enduring global cultural influence.",
    "$.messages[1].role": "assistant",
    "$.pid": 52072,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T23:30:54.738958",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "hear-live-common-late-hate",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev20+ge1da45a.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 3 highlighted sections (required: 2)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "2",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "take-love-start-short-work",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a funny Haiku about a Quaker named John who lives in the town of De Smet, South Dakota. Use the asterisk symbol, *, to highlight some words or phrases twice. Example: *This is a highlighted phrase*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "John in *De Smet*  
*Quaker* silence—then he yells  
“Where’s my *oatmeal*, Friend?”",
    "$.messages[1].role": "assistant",
    "$.pid": 52072,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T23:30:54.738984",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "hear-live-common-late-hate",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev20+ge1da45a.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 13 highlighted sections (required: 1)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "1",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "delete-democratic-hot-new-work",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Today, at the 54th Annual Grammy Awards, the Recording Academy honors the talent and creativity of the artists, musicians, and producers who are the creators of the best recordings of the past year. Please continue writing this text in a formal tone, using notations. Highlight some key parts in your response with "*", like *highlighted text*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "Tonight, the 54th Annual Grammy Awards [1] stand as a testament to the enduring power of recorded music. Under the vaulted ceiling of the *Staples Center*, the Recording Academy gathers to bestow its highest accolades upon those whose artistry has defined the sonic landscape of eligibility year 2011 [2].  

The evening’s proceedings are governed by the *strictures of the Grammy Charter* [3], ensuring that every statuette awarded is the result of peer-review balloting among the Academy’s 12,000 voting members [4]. In accordance with Rule 3.01, only recordings commercially released in the United States between 1 October 2010 and 30 September 2011 are eligible [5].  

Among the most anticipated categories is *Album of the Year* [6], whose nominees—Adele’s *21*, Foo Fighters’ *Wasting Light*, Lady Gaga’s *Born This Way*, Bruno Mars’ *Doo-Wops & Hooligans*, and Rihanna’s *Loud*—represent a cross-section of genre, generation, and geography [7]. The *Record of the Year* [8] shortlist similarly spans continents and styles, underscoring the Academy’s commitment to excellence irrespective of commercial metrics.  

The *Producer of the Year, Non-Classical* [9] category spotlights the architects of sound whose meticulous craft transforms raw performance into timeless recording. Paul Epworth, whose work on *21* has garnered both critical and commercial acclaim, is widely regarded as the frontrunner [10].  

As the telecast proceeds, the *In Memoriam* segment [11] will honor those luminaries whose voices were stilled during the eligibility year, their legacies enshrined in the grooves and bytes of the recordings they leave behind.  

The 54th Annual Grammy Awards thus serve not merely as a celebration, but as a *codification of excellence* [12], ensuring that the artistry of 2011 resonates through the decades to come.",
    "$.messages[1].role": "assistant",
    "$.pid": 52072,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T23:30:54.739013",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "hear-live-common-late-hate",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev20+ge1da45a.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 33 highlighted sections (required: 2)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "2",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "live-important-military-only-love",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a blog post about interesting facts about the Dutch language. Italicize at least 2 sections in your answer with markdown, i.e. *italic text*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "# 7 Surprising Facts About Dutch That Will Make You Say “Wauw!”

When most people think of Dutch, they picture windmills, tulips, and maybe a stroopwafel or two. But the language behind those images is far stranger—and more fascinating—than you might expect. Below are seven facts that prove Dutch is anything but *just English with extra g’s*.

---

### 1. The Longest Word Is 53 Letters—and It’s Official  
The Dutch *Encyclopedie* recognizes *aansprakelijkheidswaardevaststellingsveranderingen* (53 letters) as a legitimate word. It translates to “changes in liability value assessments,” which is ironic because the word itself is a liability to pronounce. Try saying that three times fast—or even once slowly.

---

### 2. Dutch Has a Word for *Everything*—Including “Schadenfreude” but Cozier  
English borrows *Schadenfreude* from German, but Dutch gives us *gezellig*, a term that packs “cozy,” “fun,” and “belonging” into one syllable. A café can be *gezellig*, a party can be *gezellig*, and even a quiet evening with friends can be *gezellig*. There’s no direct English equivalent, which is why expats end up using it in English sentences: “That dinner was so… *gezellig*.”

---

### 3. The Dutch Alphabet Ends with “X, Y, IJ, Z”  
Most alphabets end with “X, Y, Z,” but Dutch sneaks in an extra character: *IJ*. It’s considered a single letter (yes, two letters acting as one), and it even has its own capitalized form: *IJsselmeer*, the famous lake. Type designers have to create a special ligature for it, and crossword puzzles treat *IJ* as one square. Imagine Scrabble tiles clinking together as someone proudly lays down “IJSBERG” for 72 points.

---

### 4. Dutch Greetings Change by Time of Day—But Not How You’d Expect  
In English, “good morning” fades into “good afternoon” around noon. Dutch, however, switches from *goedemorgen* to *goedemiddag* at exactly 12:00 p.m.—unless you’re in Belgium, where some regions wait until 1:00 p.m. Miss the cutoff and you’ll get a polite correction: “Het is al middag, hoor!” (“It’s already afternoon, you know!”).

---

### 5. Dutch Is the Closest Major Language to English—But That Doesn’t Mean It’s Easy  
Linguists classify Dutch and English as *West Germanic siblings*, which means basic vocabulary overlaps: *appel* (apple), *boek* (book), *water* (water). Yet false friends lurk everywhere. *Durf jij?* doesn’t mean “Do you dare?”—it means “Do you dare?”… but with a cultural undertone of *prove it*. And *be careful* with *eventueel*, which means “possibly,” not “eventually.”

---

### 6. Dutch Has Three Genders—But Only Two Articles  
Grammatically, Dutch retains masculine, feminine, and neuter genders. In practice, however, only *de* (common gender) and *het* (neuter) are used. The result? Native speakers themselves debate whether *auto* (car) is masculine or feminine. The safest route is to just say *de auto* and avoid the awkward silence that follows gendered guesswork.

---

### 7. Dutch Invented the Word “Cookie”—and the World Never Looked Back  
The English word “cookie” comes from the Dutch *koekje* (“little cake”). Dutch settlers brought *koekjes* to New Amsterdam (now New York), and the rest is snack history. Next time you dunk a chocolate chip, thank the Dutch for their linguistic—and culinary—contribution.

---

### Bonus: Dutch Tongue Twisters Are Next-Level  
Try this: *De kat krabt de krullen van de trap*. Translation: “The cat scratches the curls off the stairs.” Say it five times without twisting your tongue into a *stroopwafel*.

---

Whether you’re planning a trip to Amsterdam or just want to impress friends at trivia night, Dutch offers a treasure trove of quirks. From *gezellig* evenings to 53-letter bureaucratic monsters, the language proves that small countries can produce *enormous linguistic personality*.",
    "$.messages[1].role": "assistant",
    "$.pid": 52072,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T23:30:54.739037",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "hear-live-common-late-hate",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev20+ge1da45a.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 2 highlighted sections (required: 1)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "1",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "think-indicate-write-local-company",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Gideon is a farmer who has a surplus of crops from his farm this year. What might he do with that surplus? Highlight at least one section of your answer in markdown, i.e *highlighted section*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "Gideon has several good options for his surplus:

1. **Sell it at the local farmers’ market or to a regional wholesaler**, turning the extra produce into cash that he can use for next season’s seeds, equipment repairs, or household expenses.  
2. *Donate a portion to the nearby food bank or community kitchen*, helping families who struggle with food insecurity while also earning a small tax deduction.  
3. Preserve some of the surplus—by canning tomatoes, drying herbs, or storing root vegetables in a cool cellar—so his own family has nutritious food throughout the winter.  
4. Trade with neighboring farmers: swap excess corn for someone else’s surplus eggs or honey, strengthening community ties and diversifying his pantry without spending money.",
    "$.messages[1].role": "assistant",
    "$.pid": 52072,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-08T23:34:12.823862",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "Run math evaluation on sample dataset using pytest interface.",
    "$.eval_metadata.name": "test_pytest_mcp_url",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.run_id": "meet-treat-central-possible-lot",
    "$.eval_metadata.status": "stopped",
    "$.eval_metadata.version": "v0.2.6.dev20+ge1da45a.dirty",
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.row_id": "expect-upload-close-president-goal",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "You are a helpful assistant that can answer questions about Fireworks.
ALWAYS provide code or commands to execute to answer the question.",
    "$.messages[0].role": "system",
    "$.messages[1].content": "Can you teach me about how to manage deployments on Fireworks",
    "$.messages[1].role": "user",
    "$.pid": 52715,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T00:10:34.081457",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "Run math evaluation on sample dataset using pytest interface.",
    "$.eval_metadata.name": "test_pytest_mcp_url",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.run_id": "replace-measure-bitter-open-reason",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.version": "v0.2.6.dev22+gfb34cf0.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "At least one tool call was made",
    "$.evaluation_result.score": 1,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.row_id": "run-update-traditional-software-idea",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "You are a helpful assistant that can answer questions about Fireworks.
ALWAYS provide code or commands to execute to answer the question.",
    "$.messages[0].role": "system",
    "$.messages[1].content": "Can you teach me about how to manage deployments on Fireworks",
    "$.messages[1].role": "user",
    "$.messages[2].content": "I'll help you learn about managing deployments on Fireworks. Let me search for comprehensive information about deployment management.",
    "$.messages[2].role": "assistant",
    "$.messages[2].tool_calls[0].function.arguments": "{"query": "deployment management Fireworks"}",
    "$.messages[2].tool_calls[0].function.name": "search",
    "$.messages[2].tool_calls[0].id": "functions.search:0",
    "$.messages[2].tool_calls[0].type": "function",
    "$.messages[3].content[0].text": "Title: On-demand deployments
Link: https://docs.fireworks.ai/guides/ondemand-deployments
Content: If you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.

If you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.

On-demand deployments
If you plan on using a significant amount of dedicated deployments, consider purchasing reserved capacity instead of using on-demand deployments for more reliable capacity and higher GPU quotas. Fireworks allows you to create on-demand deployments of models that are reserved for your own use and billed by the GPU-second. This has several advantages over the shared deployment architecture you get when using Fireworks' serverless models: Predictable performance unaffected by load caused by other users No hard rate limits, but subject to the maximum load capacity of the deployment Cheaper under high utilization Access to larger selection of models not available via our serverless models Custom base models from Hugging Face files Unlike serverless models, which are billed on a per-token basis, on-demand deployments are billed by GPU-second. Consult our pricing page for details.

",
    "$.messages[3].content[0].type": "text",
    "$.messages[3].content[1].text": "Title: Does Fireworks support custom base models?
Link: https://docs.fireworks.ai/faq-new/models-inference/does-fireworks-support-custom-base-models
Content: Yes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.

Yes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.

Does Fireworks support custom base models?
Yes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.

",
    "$.messages[3].content[1].type": "text",
    "$.messages[3].content[2].text": "Title: Introducing the Fireworks Build SDK
Link: https://docs.fireworks.ai/tools-sdks/python-client/sdk-introduction
Content: Introducing the Fireworks Build SDK
The Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.

Introducing the Fireworks Build SDK
The Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.

The Fireworks Build SDK is a client library that allows you to interact with the Fireworks API using Python. It provides a simple and intuitive interface for working with Fireworks primitives like deployments, fine-tuning jobs, and datasets as Python objects. The Build SDK is currently in beta and not all functionality may be supported. Please reach out to dhuang@fireworks.ai to report any issues or feedback.

",
    "$.messages[3].content[2].type": "text",
    "$.messages[3].content[3].text": "Title: I have multiple Fireworks accounts. When I try to login with Google on Fireworks' web UI, I'm getting signed into the wrong account. How do I fix this?
Link: https://docs.fireworks.ai/faq-new/account-access/i-have-multiple-fireworks-accounts-when-i-try-to-login-with-google-on-fireworks
Content: If you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.

If you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.

I have multiple Fireworks accounts. When I try to login with Google on Fireworks' web UI, I'm getting signed into the wrong account. How do I fix this?
If you log in with Google, account management is controlled by Google. You can log in through an incognito mode or create separate Chrome/browser profiles to log in with different Google accounts. You could also follow the steps in this guide to disassociate Fireworks.ai with a particular Google account sign-in. If you have more complex issues please contact us on Discord.

",
    "$.messages[3].content[3].type": "text",
    "$.messages[3].content[4].text": "Title: Querying existing dedicated deployments
Link: https://docs.fireworks.ai/tools-sdks/python-client/querying-dedicated-deployments
Content: When you have dedicated deployments that were created via firectl or the Fireworks web UI, you can easily connect to them using the Build SDK to run inference. This is particularly useful when you want to leverage existing infrastructure or when deployments are managed by different teams.

When you have dedicated deployments that were created via firectl or the Fireworks web UI, you can easily connect to them using the Build SDK to run inference. This is particularly useful when you want to leverage existing infrastructure or when deployments are managed by different teams.

Deployment ID requirements
The id parameter should match exactly with your existing deployment: Use the deployment name/ID as shown in the Fireworks dashboard The ID is case-sensitive and must match exactly If the deployment doesn't exist, you'll receive an error when making requests

",
    "$.messages[3].content[4].type": "text",
    "$.messages[3].content[5].text": "Title: Migration and traffic management
Link: https://docs.fireworks.ai/guides/prompt-caching
Content: When migrating between deployments that use prompt caching, it's crucial to implement proper traffic routing to maintain optimal cache hit rates. When gradually routing traffic to a new deployment, use consistent user/session-based sampling rather than random sampling. Here's the recommended implementation for traffic routing: import hashlib

# Configure traffic fraction (e.g., 20% to new deployment)
fireworks_traffic_fraction = 0.2
user_id = "session-id-123"

# Generate deterministic hash from user_id
hashed_user_id = int(hashlib.md5(user_id.encode()).hexdigest(), 16) # MD5 hash on user-id and convert to integer
MAX_HASH = 2**128 - 1  # MD5 hash maximum value

# Compute ratio for consistent routing
ratio = hashed_user_id / MAX_HASH # Returns 0.0 to 1.0

if (ratio < fireworks_traffic_fraction):
    send_to_new_deployment(user=hashed_user_id)  # Pass user ID for caching
else:
    send_elsewhere()  # Route to old deployment or serverless

When migrating between deployments that use prompt caching, it's crucial to implement proper traffic routing to maintain optimal cache hit rates. When gradually routing traffic to a new deployment, use consistent user/session-based sampling rather than random sampling. Here's the recommended implementation for traffic routing: import hashlib

# Configure traffic fraction (e.g., 20% to new deployment)
fireworks_traffic_fraction = 0.2
user_id = "session-id-123"

# Generate deterministic hash from user_id
hashed_user_id = int(hashlib.md5(user_id.encode()).hexdigest(), 16) # MD5 hash on user-id and convert to integer
MAX_HASH = 2**128 - 1  # MD5 hash maximum value

# Compute ratio for consistent routing
ratio = hashed_user_id / MAX_HASH # Returns 0.0 to 1.0

if (ratio < fireworks_traffic_fraction):
    send_to_new_deployment(user=hashed_user_id)  # Pass user ID for caching
else:
    send_elsewhere()  # Route to old deployment or serverless

Advanced: cache locality for Enterprise deployments
Dedicated deployments on an Enterprise plan allow you to pass an additional hint in the request to improve cache hit rates. First, the deployment needs to be created or updated with an additional flag: Then the client can pass an opaque identifier representing a single user or
session in the user field of the body or in the x-session-affinity header. Fireworks
will try to route requests with the identifier to the same server, further reducing response times. It's best to choose an identifier that groups requests with long shared prompt
prefixes. For example, it can be a chat session with the same user or an
assistant working with the same shared context.

",
    "$.messages[3].content[5].type": "text",
    "$.messages[3].content[6].text": "Title: Do you host your deployments in the EU or Asia?
Link: https://docs.fireworks.ai/faq-new/support-general/do-you-host-your-deployments-in-the-eu-or-asia
Content: Do you host your deployments in the EU or Asia?
We are currently deployed in multiple U.S.-based locations. However, we’re open to hearing more about your specific requirements. You can: Join our Discord community Write to us at inquiries@fireworks.ai If you're an Enterprise customer, please contact your dedicated customer support representative to ensure a timely response.

Do you host your deployments in the EU or Asia?
We are currently deployed in multiple U.S.-based locations. However, we’re open to hearing more about your specific requirements. You can: Join our Discord community Write to us at inquiries@fireworks.ai If you're an Enterprise customer, please contact your dedicated customer support representative to ensure a timely response.

We are currently deployed in multiple U.S.-based locations. However, we’re open to hearing more about your specific requirements. You can: Join our Discord community Write to us at inquiries@fireworks.ai If you're an Enterprise customer, please contact your dedicated customer support representative to ensure a timely response.

",
    "$.messages[3].content[6].type": "text",
    "$.messages[3].content[7].text": "Title: Deployments and deployment types
Link: https://docs.fireworks.ai/getting-started/concepts
Content: A model must be deployed before it can be used for inference. A deployment is a collection (one or more) model servers that host one base model and optionally one or more LoRA addons. Fireworks supports two types of deployments: Serverless deployments: Fireworks hosts popular base models on shared "serverless" deployments. Users pay-per-token to query these models and do not need to configure GPUs. The most popular serverless deployments also support serverless LoRA addons. See the Deploying to serverless guide for details. Dedicated deployments: Dedicated deployments enable users to configure private deployments with a wide array of hardware (see on-demand deployments guide ). Dedicated deployments give users performance guarantees and the most flexibility and control over what models can be deployed. Both LoRA addons and base models can be deployed to dedicated deployments. Dedicated deployments are billed by a GPU-second basis (see pricing page). See the Querying text models guide for a comprehensive overview of making LLM inference.

A model must be deployed before it can be used for inference. A deployment is a collection (one or more) model servers that host one base model and optionally one or more LoRA addons. Fireworks supports two types of deployments: Serverless deployments: Fireworks hosts popular base models on shared "serverless" deployments. Users pay-per-token to query these models and do not need to configure GPUs. The most popular serverless deployments also support serverless LoRA addons. See the Deploying to serverless guide for details. Dedicated deployments: Dedicated deployments enable users to configure private deployments with a wide array of hardware (see on-demand deployments guide ). Dedicated deployments give users performance guarantees and the most flexibility and control over what models can be deployed. Both LoRA addons and base models can be deployed to dedicated deployments. Dedicated deployments are billed by a GPU-second basis (see pricing page). See the Querying text models guide for a comprehensive overview of making LLM inference.

Control plane and data plane
The Fireworks API can be split into a control plane and a data plane. The control plane consists of APIs used for managing the lifecycle of resources. This
includes your account, models, and deployments. The data plane consists of the APIs used for inference and the backend services that power
them.

",
    "$.messages[3].content[7].type": "text",
    "$.messages[3].content[8].text": "Title: Deployment Process
Link: https://docs.fireworks.ai/evaluators/api_reference/reward_function_decorator
Content: When .deploy() is called, the decorator: Extracts the function's source code Creates a wrapper that handles the Fireworks evaluation format Creates a temporary directory with the wrapped function Uploads and registers the function with the Fireworks API

When .deploy() is called, the decorator: Extracts the function's source code Creates a wrapper that handles the Fireworks evaluation format Creates a temporary directory with the wrapped function Uploads and registers the function with the Fireworks API

.deploy()
The decorator adds a .deploy() method to the function, allowing it to be deployed to Fireworks. evaluation_id = my_reward_function.deploy(
    name="my-evaluator",
    description="Evaluates responses based on clarity and accuracy",
    account_id=None,  # Optional, defaults to configured account
    auth_token=None,  # Optional, defaults to configured token
    force=False,  # Set to True to overwrite if it already exists
    providers=None  # Optional model providers configuration
)

",
    "$.messages[3].content[8].type": "text",
    "$.messages[3].content[9].text": "Title: 🔧 Build SDK: Deployment Configuration Application Requirement
Link: https://docs.fireworks.ai/updates/changelog
Content: The Build SDK now requires you to call .apply() to apply any deployment configurations to Fireworks when using deployment_type="on-demand" or deployment_type="on-demand-lora" . This change ensures explicit control over when deployments are created and helps prevent accidental deployment creation. Key changes: .apply() is now required for on-demand and on-demand-lora deployments Serverless deployments do not require .apply() calls If you do not call .apply() , you are expected to set up the deployment through the deployment page at https://app.fireworks.ai/dashboard/deployments Migration guide: Add llm.apply() after creating LLM instances with deployment_type="on-demand" or deployment_type="on-demand-lora" No changes needed for serverless deployments See updated documentation for examples and best practices This change improves deployment management and provides better control over resource creation. This applies to Python SDK version >=0.19.14.

The Build SDK now requires you to call .apply() to apply any deployment configurations to Fireworks when using deployment_type="on-demand" or deployment_type="on-demand-lora" . This change ensures explicit control over when deployments are created and helps prevent accidental deployment creation. Key changes: .apply() is now required for on-demand and on-demand-lora deployments Serverless deployments do not require .apply() calls If you do not call .apply() , you are expected to set up the deployment through the deployment page at https://app.fireworks.ai/dashboard/deployments Migration guide: Add llm.apply() after creating LLM instances with deployment_type="on-demand" or deployment_type="on-demand-lora" No changes needed for serverless deployments See updated documentation for examples and best practices This change improves deployment management and provides better control over resource creation. This applies to Python SDK version >=0.19.14.

🚀 Easier & faster LoRA fine-tune deployments on Fireworks
You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: Previously, this involved two distinct steps, and the resulting deployment was slower than the base model: Create a deployment using firectl create deployment "accounts/fireworks/models/<MODEL_ID of base model>" --enable-addons Then deploy the addon to the deployment: firectl load-lora <MODEL_ID> --deployment <DEPLOYMENT_ID> For more information, see our deployment documentation. This change is for dedicated deployments with a single LoRA. You can still deploy multiple LoRAs on a deployment or deploy LoRA(s) on some Serverless models as described in the documentation.

",
    "$.messages[3].content[9].type": "text",
    "$.messages[3].role": "tool",
    "$.messages[3].tool_call_id": "functions.search:0",
    "$.messages[4].content": "Let me search for more specific information about deployment management tools and processes.",
    "$.messages[4].role": "assistant",
    "$.messages[4].tool_calls[0].function.arguments": "{"query": "firectl deployment management commands"}",
    "$.messages[4].tool_calls[0].function.name": "search",
    "$.messages[4].tool_calls[0].id": "functions.search:1",
    "$.messages[4].tool_calls[0].type": "function",
    "$.messages[5].content[0].text": "Title: Why am I getting "invalid id" errors when using firectl commands like create deployment or list deployments?
Link: https://docs.fireworks.ai/faq-new/fine-tuning/why-am-i-getting-invalid-id-errors-when-using-firectl-commands-like-create-deplo
Content: This error typically occurs when your account ID is not properly configured.

This error typically occurs when your account ID is not properly configured.

Why am I getting "invalid id" errors when using firectl commands like create deployment or list deployments?
This error typically occurs when your account ID is not properly configured.

",
    "$.messages[5].content[0].type": "text",
    "$.messages[5].content[1].text": "Title: Managing default deployments
Link: https://docs.fireworks.ai/guides/ondemand-deployments
Content: Since a model may be deployed to multiple deployments, querying by model name will route to the "default" deployed model. You can see which deployed model entry is marked with Default: true using: firectl get model <MODEL_ID>

Since a model may be deployed to multiple deployments, querying by model name will route to the "default" deployed model. You can see which deployed model entry is marked with Default: true using: firectl get model <MODEL_ID>

Since a model may be deployed to multiple deployments, querying by model name will route to the "default" deployed model. You can see which deployed model entry is marked with Default: true using: This will show the Deployed Model Refs section with the Default: true entry. To update the default deployed model, note the Name of the deployed model reference above. Then run: firectl update deployed-model <DEPLOYED_MODEL_ID_2> --default

",
    "$.messages[5].content[1].type": "text",
    "$.messages[5].content[2].text": "Title: Does Fireworks support custom base models?
Link: https://docs.fireworks.ai/faq-new/models-inference/does-fireworks-support-custom-base-models
Content: Yes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.

Yes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.

Does Fireworks support custom base models?
Yes, custom base models can be deployed via firectl . You can learn more about custom model deployment in our guide on uploading custom models.

",
    "$.messages[5].content[2].type": "text",
    "$.messages[5].content[3].text": "Title: 🚀 Easier & faster LoRA fine-tune deployments on Fireworks
Link: https://docs.fireworks.ai/updates/changelog
Content: You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

You can now deploy a LoRA fine-tune with a single command and get speeds that approximately match the base model: Previously, this involved two distinct steps, and the resulting deployment was slower than the base model: Create a deployment using firectl create deployment "accounts/fireworks/models/<MODEL_ID of base model>" --enable-addons Then deploy the addon to the deployment: firectl load-lora <MODEL_ID> --deployment <DEPLOYMENT_ID> For more information, see our deployment documentation. This change is for dedicated deployments with a single LoRA. You can still deploy multiple LoRAs on a deployment or deploy LoRA(s) on some Serverless models as described in the documentation.

",
    "$.messages[5].content[3].type": "text",
    "$.messages[5].content[4].text": "Title: Quick deployment
Link: https://docs.fireworks.ai/fine-tuning/single-lora
Content: Deploy your LoRA fine-tuned model with one simple command: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

Deploy your LoRA fine-tuned model with one simple command: firectl create deployment "accounts/fireworks/models/<MODEL_ID of lora model>"

Alternative deployment method
This two-step method is the standard approach for multi-LoRA deployments where multiple LoRA models share the same base model. While it can also be used for single LoRA deployments, it provides slower performance compared to live merge and is not recommended for single LoRA use cases. You can also deploy single LoRA models using a two-step process: Create base model deployment Deploy the base model with addons enabled: firectl create deployment "accounts/fireworks/models/<MODEL_ID of base model>" --enable-addons

",
    "$.messages[5].content[4].type": "text",
    "$.messages[5].content[5].text": "Title: Deploying a fine-tuned model using an on-demand deployment
Link: https://docs.fireworks.ai/fine-tuning/fine-tuning-models
Content: Use the following command to deploy your fine-tuned model using an on-demand deployment: firectl create deployment <FINE_TUNED_MODEL_ID>

Use the following command to deploy your fine-tuned model using an on-demand deployment: firectl create deployment <FINE_TUNED_MODEL_ID>

Deploying a fine-tuned model serverlessly
Not all base models support serverless addons. Please check this list for the full list of serverless models that support LoRA add-ons. If your base model supports serverless addons, you can run the following command to load the fine-tuned model into a serverless deployment: firectl load-lora <FINE_TUNED_MODEL_ID>

",
    "$.messages[5].content[5].type": "text",
    "$.messages[5].content[6].text": "Title: Deploy Command
Link: https://docs.fireworks.ai/evaluators/cli_reference/cli_overview
Content: The deploy command deploys a reward function as an evaluator on the Fireworks platform.

The deploy command deploys a reward function as an evaluator on the Fireworks platform.

Command Overview
The Reward Kit CLI supports the following main commands: run : Run a local evaluation pipeline using a Hydra configuration. preview : Preview evaluation results or re-evaluate generated outputs. deploy : Deploy a reward function as an evaluator. agent-eval : Run agent evaluations on task bundles. list : List existing evaluators (coming soon). delete : Delete an evaluator (coming soon).

",
    "$.messages[5].content[6].type": "text",
    "$.messages[5].content[7].text": "Title: Create a deployment
Link: https://docs.fireworks.ai/tools-sdks/firectl/commands/create-deployment
Content: Creates a new deployment. firectl create deployment [flags]

Creates a new deployment. firectl create deployment [flags]

Example
firectl create deployment falcon-7b

",
    "$.messages[5].content[7].type": "text",
    "$.messages[5].content[8].text": "Title: firectl
Link: https://docs.fireworks.ai/fine-tuning/fine-tuning-vlm
Content: # Create a deployment for your fine-tuned VLM
firectl create deployment my-custom-vlm

# Check deployment status
firectl get deployment accounts/your-account/deployment/deployment-id

# Create a deployment for your fine-tuned VLM
firectl create deployment my-custom-vlm

# Check deployment status
firectl get deployment accounts/your-account/deployment/deployment-id

firectl create sftj \\
  --base-model accounts/fireworks/models/qwen2p5-vl-32b-instruct \\
  --dataset my-vlm-dataset \\
  --output-model my-custom-vlm \\
  --epochs 3 For additional parameters like learning rates, evaluation datasets, and batch sizes, see  Additional SFT job settings .

",
    "$.messages[5].content[8].type": "text",
    "$.messages[5].content[9].text": "Title: Uploading the model locally (firectl)
Link: https://docs.fireworks.ai/models/uploading-custom-models
Content: To upload a custom base model, run the following command. firectl create model <MODEL_ID> /path/to/files/

To upload a custom base model, run the following command. firectl create model <MODEL_ID> /path/to/files/

To upload a custom base model, run the following command.

",
    "$.messages[5].content[9].type": "text",
    "$.messages[5].role": "tool",
    "$.messages[5].tool_call_id": "functions.search:1",
    "$.messages[6].content": "Let me search for more information about deployment lifecycle management and monitoring.",
    "$.messages[6].role": "assistant",
    "$.messages[6].tool_calls[0].function.arguments": "{"query": "deployment monitoring scaling management lifecycle"}",
    "$.messages[6].tool_calls[0].function.name": "search",
    "$.messages[6].tool_calls[0].id": "functions.search:0",
    "$.messages[6].tool_calls[0].type": "function",
    "$.messages[7].content[0].text": "Title: What should I expect for deployment and scaling performance?
Link: https://docs.fireworks.ai/faq-new/deployment-infrastructure/what-should-i-expect-for-deployment-and-scaling-performance
Content: Initial deployment : Should complete within minutes Scaling from zero : You may experience brief availability delays while the system scales up Troubleshooting : If deployment takes over 1 hour, this typically indicates a crash and should be investigated Best practice : Monitor deployment status and contact support if deployment times are unusually long

Initial deployment : Should complete within minutes Scaling from zero : You may experience brief availability delays while the system scales up Troubleshooting : If deployment takes over 1 hour, this typically indicates a crash and should be investigated Best practice : Monitor deployment status and contact support if deployment times are unusually long

What should I expect for deployment and scaling performance?
Initial deployment : Should complete within minutes Scaling from zero : You may experience brief availability delays while the system scales up Troubleshooting : If deployment takes over 1 hour, this typically indicates a crash and should be investigated Best practice : Monitor deployment status and contact support if deployment times are unusually long

",
    "$.messages[7].content[0].type": "text",
    "$.messages[7].content[1].text": "Title: How does billing and scaling work for on-demand GPU deployments?
Link: https://docs.fireworks.ai/faq-new/deployment-infrastructure/how-does-billing-and-scaling-work-for-on-demand-gpu-deployments
Content: On-demand GPU deployments have unique billing and scaling characteristics compared to serverless deployments: Billing: Charges start when the server begins accepting requests Billed by GPU-second for each active instance Costs accumulate even if there are no active API calls Scaling options: Supports autoscaling from 0 to multiple GPUs Each additional GPU adds to the billing rate Can handle unlimited requests within the GPU’s capacity Management requirements: Not fully serverless; requires some manual management Manually delete deployments when no longer needed Or configure autoscaling to scale down to 0 during inactive periods Cost control tips: Regularly monitor active deployments Delete unused deployments to avoid unnecessary costs Consider serverless options for intermittent usage Use autoscaling to 0 to optimize costs during low-demand times

On-demand GPU deployments have unique billing and scaling characteristics compared to serverless deployments: Billing: Charges start when the server begins accepting requests Billed by GPU-second for each active instance Costs accumulate even if there are no active API calls Scaling options: Supports autoscaling from 0 to multiple GPUs Each additional GPU adds to the billing rate Can handle unlimited requests within the GPU’s capacity Management requirements: Not fully serverless; requires some manual management Manually delete deployments when no longer needed Or configure autoscaling to scale down to 0 during inactive periods Cost control tips: Regularly monitor active deployments Delete unused deployments to avoid unnecessary costs Consider serverless options for intermittent usage Use autoscaling to 0 to optimize costs during low-demand times

How does billing and scaling work for on-demand GPU deployments?
On-demand GPU deployments have unique billing and scaling characteristics compared to serverless deployments: Billing: Charges start when the server begins accepting requests Billed by GPU-second for each active instance Costs accumulate even if there are no active API calls Scaling options: Supports autoscaling from 0 to multiple GPUs Each additional GPU adds to the billing rate Can handle unlimited requests within the GPU’s capacity Management requirements: Not fully serverless; requires some manual management Manually delete deployments when no longer needed Or configure autoscaling to scale down to 0 during inactive periods Cost control tips: Regularly monitor active deployments Delete unused deployments to avoid unnecessary costs Consider serverless options for intermittent usage Use autoscaling to 0 to optimize costs during low-demand times

",
    "$.messages[7].content[1].type": "text",
    "$.messages[7].content[2].text": "Title: How does billing work for on-demand deployments?
Link: https://docs.fireworks.ai/faq-new/deployment-infrastructure/how-does-billing-work-for-on-demand-deployments
Content: On-demand deployments come with automatic cost optimization features: Default autoscaling : Automatically scales to 0 replicas when not in use Pay for what you use : Charged only for GPU time when replicas are active Flexible configuration : Customize autoscaling behavior to match your needs Best practices for cost management: Leverage default autoscaling : The system automatically scales down deployments when not in use Customize carefully : While you can modify autoscaling behavior using our configuration options , note that preventing scale-to-zero will result in continuous GPU charges Consider your use case : For intermittent or low-frequency usage, serverless deployments might be more cost-effective For detailed configuration options, see our deployment guide.

On-demand deployments come with automatic cost optimization features: Default autoscaling : Automatically scales to 0 replicas when not in use Pay for what you use : Charged only for GPU time when replicas are active Flexible configuration : Customize autoscaling behavior to match your needs Best practices for cost management: Leverage default autoscaling : The system automatically scales down deployments when not in use Customize carefully : While you can modify autoscaling behavior using our configuration options , note that preventing scale-to-zero will result in continuous GPU charges Consider your use case : For intermittent or low-frequency usage, serverless deployments might be more cost-effective For detailed configuration options, see our deployment guide.

How does billing work for on-demand deployments?
On-demand deployments come with automatic cost optimization features: Default autoscaling : Automatically scales to 0 replicas when not in use Pay for what you use : Charged only for GPU time when replicas are active Flexible configuration : Customize autoscaling behavior to match your needs Best practices for cost management: Leverage default autoscaling : The system automatically scales down deployments when not in use Customize carefully : While you can modify autoscaling behavior using our configuration options , note that preventing scale-to-zero will result in continuous GPU charges Consider your use case : For intermittent or low-frequency usage, serverless deployments might be more cost-effective For detailed configuration options, see our deployment guide.

",
    "$.messages[7].content[2].type": "text",
    "$.messages[7].content[3].text": "Title: How does the system scale?
Link: https://docs.fireworks.ai/faq-new/deployment-infrastructure/how-does-the-system-scale
Content: Our system is horizontally scalable , meaning it: Scales linearly with additional replicas of the deployment Automatically allocates resources based on demand Manages distributed load handling efficiently

Our system is horizontally scalable , meaning it: Scales linearly with additional replicas of the deployment Automatically allocates resources based on demand Manages distributed load handling efficiently

How does the system scale?
Our system is horizontally scalable , meaning it: Scales linearly with additional replicas of the deployment Automatically allocates resources based on demand Manages distributed load handling efficiently

",
    "$.messages[7].content[3].type": "text",
    "$.messages[7].content[4].text": "Title: Multiple GPUs (vertical scaling)
Link: https://docs.fireworks.ai/guides/ondemand-deployments
Content: The number of GPUs used per replica is specified by passing the --accelerator-count flag. Increasing the accelerator count will increase the generation speed, time-to-first-token, and maximum QPS for your deployment, however the scaling is sub-linear. The default value for most models is 1 but may be higher for larger models that require sharding. firectl create deployment <MODEL_NAME> --accelerator-count 2

The number of GPUs used per replica is specified by passing the --accelerator-count flag. Increasing the accelerator count will increase the generation speed, time-to-first-token, and maximum QPS for your deployment, however the scaling is sub-linear. The default value for most models is 1 but may be higher for larger models that require sharding. firectl create deployment <MODEL_NAME> --accelerator-count 2

The number of GPUs used per replica is specified by passing the --accelerator-count flag. Increasing the accelerator count will increase the generation speed, time-to-first-token, and maximum QPS for your deployment, however the scaling is sub-linear. The default value for most models is 1 but may be higher for larger models that require sharding. firectl update deployment <DEPLOYMENT_ID> --accelerator-count 2

",
    "$.messages[7].content[4].type": "text",
    "$.messages[7].content[5].text": "Title: scale_to_zero()
Link: https://docs.fireworks.ai/tools-sdks/python-client/sdk-reference
Content: Sends a request to scale the deployment to 0 replicas but does not wait for it to complete. Returns: The deployment object, or None if no deployment exists deployment = llm.scale_to_zero()

Sends a request to scale the deployment to 0 replicas but does not wait for it to complete. Returns: The deployment object, or None if no deployment exists

scale_to_1_replica()
Scales the deployment to at least 1 replica. llm.scale_to_1_replica()

",
    "$.messages[7].content[5].type": "text",
    "$.messages[7].content[6].text": "Title: Do you support Auto Scaling?
Link: https://docs.fireworks.ai/faq-new/deployment-infrastructure/do-you-support-auto-scaling
Content: Yes, our system supports auto scaling with the following features: Scaling down to zero capability for resource efficiency Controllable scale-up and scale-down velocity Custom scaling rules and thresholds to match your specific needs

Yes, our system supports auto scaling with the following features: Scaling down to zero capability for resource efficiency Controllable scale-up and scale-down velocity Custom scaling rules and thresholds to match your specific needs

Do you support Auto Scaling?
Yes, our system supports auto scaling with the following features: Scaling down to zero capability for resource efficiency Controllable scale-up and scale-down velocity Custom scaling rules and thresholds to match your specific needs

",
    "$.messages[7].content[6].type": "text",
    "$.messages[7].content[7].text": "Title: What are the rate limits for on-demand deployments?
Link: https://docs.fireworks.ai/faq-new/deployment-infrastructure/what-are-the-rate-limits-for-on-demand-deployments
Content: Request throughput scales with your GPU allocation. Base allocations include: Up to 8 A100 GPUs Up to 8 H100 GPUs On-demand deployments offer several advantages: Predictable pricing based on time units, not token I/O Protected latency and performance , independent of traffic on the serverless platform Choice of GPUs , including A100s and H100s Need more GPUs? Contact us to discuss higher allocations for your specific use case.

Request throughput scales with your GPU allocation. Base allocations include: Up to 8 A100 GPUs Up to 8 H100 GPUs On-demand deployments offer several advantages: Predictable pricing based on time units, not token I/O Protected latency and performance , independent of traffic on the serverless platform Choice of GPUs , including A100s and H100s Need more GPUs? Contact us to discuss higher allocations for your specific use case.

What are the rate limits for on-demand deployments?
Request throughput scales with your GPU allocation. Base allocations include: Up to 8 A100 GPUs Up to 8 H100 GPUs On-demand deployments offer several advantages: Predictable pricing based on time units, not token I/O Protected latency and performance , independent of traffic on the serverless platform Choice of GPUs , including A100s and H100s Need more GPUs? Contact us to discuss higher allocations for your specific use case.

",
    "$.messages[7].content[7].type": "text",
    "$.messages[7].content[8].text": "Title: Create a deployment
Link: https://docs.fireworks.ai/tools-sdks/firectl/commands/create-deployment
Content: Creates a new deployment.

Creates a new deployment.

Creates a new deployment. firectl create deployment [flags]

",
    "$.messages[7].content[8].type": "text",
    "$.messages[7].content[9].text": "Title: Monitoring
Link: https://docs.fireworks.ai/guides/prompt-caching
Content: For dedicated deployments, information about prompt caching is returned in the
response headers. The header fireworks-prompt-tokens contains the number of tokens
in the prompt, out of which fireworks-cached-prompt-tokens are cached. Aggregated metrics are also available in the usage dashboard.

For dedicated deployments, information about prompt caching is returned in the
response headers. The header fireworks-prompt-tokens contains the number of tokens
in the prompt, out of which fireworks-cached-prompt-tokens are cached. Aggregated metrics are also available in the usage dashboard.

Migration and traffic management
When migrating between deployments that use prompt caching, it's crucial to implement proper traffic routing to maintain optimal cache hit rates. When gradually routing traffic to a new deployment, use consistent user/session-based sampling rather than random sampling. Here's the recommended implementation for traffic routing: Avoid random sampling for traffic routing as it can negatively impact cache hit rates: # Don't do this:
if random() < fireworks_traffic_fraction:  # ❌ Reduces cache effectiveness
  send_to_new_deployment(user=hashed_user_id)

",
    "$.messages[7].content[9].type": "text",
    "$.messages[7].role": "tool",
    "$.messages[7].tool_call_id": "functions.search:0",
    "$.messages[8].content": "Based on my research, I can provide you with a comprehensive guide on managing deployments on Fireworks. Let me break this down into key areas:

## Types of Deployments

Fireworks supports two main deployment types:

### 1. Serverless Deployments
- **Managed by Fireworks**: Popular base models hosted on shared infrastructure
- **Billing**: Pay-per-token usage
- **No configuration needed**: Fireworks handles all infrastructure
- **Rate limits**: Subject to platform-wide limits
- **Best for**: Intermittent usage, prototyping, or low-volume applications

### 2. Dedicated Deployments
- **Your private deployment**: Reserved exclusively for your use
- **Billing**: GPU-second basis (charged for active GPU time)
- **Full control**: Configure hardware, scaling, and model selection
- **No rate limits**: Subject only to your GPU allocation capacity
- **Best for**: High-volume applications, consistent performance requirements

## Deployment Management Tools

### Command Line Interface (firectl)

**Installation and Setup:**
\`\`\`bash
# Install firectl (assuming you have it configured)
# Set your account ID to avoid "invalid id" errors
export FIREWORKS_ACCOUNT_ID=your-account-id
\`\`\`

**Basic Deployment Commands:**

\`\`\`bash
# Create a new deployment
firectl create deployment <MODEL_NAME>

# Create deployment with specific GPU count
firectl create deployment <MODEL_NAME> --accelerator-count 2

# Create deployment for LoRA fine-tuned model
firectl create deployment "accounts/fireworks/models/<LORA_MODEL_ID>"

# Check deployment status
firectl get deployment accounts/your-account/deployment/<DEPLOYMENT_ID>

# List all deployments
firectl list deployments

# Update deployment configuration
firectl update deployment <DEPLOYMENT_ID> --accelerator-count 4

# Delete a deployment
firectl delete deployment <DEPLOYMENT_ID>
\`\`\`

### Python SDK (Build SDK)

**Installation:**
\`\`\`bash
pip install fireworks-ai
\`\`\`

**Basic Usage:**
\`\`\`python
from fireworks import Fireworks

# Initialize client
client = Fireworks(api_key="your-api-key")

# Create deployment configuration
from fireworks.client import LLM

# For on-demand deployments (requires .apply())
llm = LLM(
    model="accounts/your-account/models/your-model",
    deployment_type="on-demand"
)
llm.apply()  # Required for on-demand deployments

# For serverless deployments
llm = LLM(model="accounts/fireworks/models/llama-v3p1-8b-instruct")

# Scale management
llm.scale_to_1_replica()  # Ensure at least 1 replica
llm.scale_to_zero()       # Scale down to 0 replicas
\`\`\`

## Deployment Lifecycle Management

### 1. Creation and Initial Setup
\`\`\`python
# Example: Creating a comprehensive deployment
deployment_config = {
    "model": "accounts/your-account/models/your-model",
    "deployment_type": "on-demand",
    "accelerator_type": "A100",  # or "H100"
    "accelerator_count": 2,
    "min_replicas": 0,  # Scale to zero when not used
    "max_replicas": 4,  # Maximum scaling
    "autoscaling": True
}
\`\`\`

### 2. Monitoring and Scaling
\`\`\`python
# Check deployment status
deployment = llm.get_deployment()
print(f"Status: {deployment.status}")
print(f"Replicas: {deployment.replicas}")

# Monitor usage and costs
# Check the usage dashboard at https://app.fireworks.ai/dashboard/usage
\`\`\`

### 3. Traffic Management During Migration
\`\`\`python
import hashlib

# Implement consistent traffic routing for cache optimization
def route_traffic(user_id, traffic_fraction=0.2):
    """Route traffic consistently based on user ID for cache optimization"""
    hashed_user_id = int(hashlib.md5(user_id.encode()).hexdigest(), 16)
    max_hash = 2**128 - 1
    ratio = hashed_user_id / max_hash
    
    if ratio < traffic_fraction:
        return "new_deployment"
    else:
        return "old_deployment"

# Usage
deployment_choice = route_traffic("user-123", traffic_fraction=0.3)
\`\`\`

## Cost Management Best Practices

### 1. Autoscaling Configuration
\`\`\`python
# Configure autoscaling to minimize costs
deployment_config = {
    "min_replicas": 0,  # Scale to zero when idle
    "max_replicas": 8,  # Maximum GPUs
    "target_cpu_utilization": 0.7,  # Scale up at 70% utilization
    "scale_down_delay": 300  # Wait 5 minutes before scaling down
}
\`\`\`

### 2. Monitoring and Cleanup
\`\`\`bash
# Regular monitoring commands
firectl list deployments  # See all active deployments
firectl get deployment <ID> --watch  # Monitor specific deployment

# Clean up unused deployments
firectl delete deployment <DEPLOYMENT_ID>
\`\`\`

### 3. Reserved Capacity
For high-volume usage, consider reserved capacity instead of on-demand:
- More reliable capacity
- Higher GPU quotas
- Better pricing for consistent usage

## Advanced Features

### Custom Base Models
\`\`\`bash
# Upload custom model
firectl create model <MODEL_ID> /path/to/model/files/

# Deploy custom model
firectl create deployment <MODEL_ID>
\`\`\`

### Multi-GPU Deployments
\`\`\`bash
# Deploy with multiple GPUs
firectl create deployment <MODEL_NAME> --accelerator-count 4

# Update GPU count
firectl update deployment <DEPLOYMENT_ID> --accelerator-count 8
\`\`\`

### LoRA Model Deployment
\`\`\`bash
# Single LoRA deployment (faster)
firectl create deployment "accounts/fireworks/models/<LORA_MODEL_ID>"

# Multi-LoRA deployment (shared base model)
firectl create deployment <BASE_MODEL_ID> --enable-addons
firectl load-lora <LORA_MODEL_ID> --deployment <DEPLOYMENT_ID>
\`\`\`

## Key Management Commands Summary

| Command | Purpose |
|---------|---------|
| \`firectl create deployment\` | Create new deployment |
| \`firectl get deployment\` | Check deployment status |
| \`firectl list deployments\` | List all deployments |
| \`firectl update deployment\` | Modify deployment settings |
| \`firectl delete deployment\` | Remove deployment |
| \`llm.apply()\` | Apply configuration (Python SDK) |
| \`llm.scale_to_zero()\` | Scale down to 0 replicas |
| \`llm.scale_to_1_replica()\` | Ensure at least 1 replica |

## Best Practices

1. **Monitor Costs**: Regularly check your usage dashboard
2. **Use Autoscaling**: Configure deployments to scale to zero when not used
3. **Plan Migrations**: Use consistent traffic routing for cache optimization
4. **Choose Wisely**: Use serverless for intermittent usage, dedicated for consistent workloads
5. **Clean Up**: Delete unused deployments to avoid unnecessary charges
6. **Test Scaling**: Verify your deployment scales appropriately under load

Would you like me to dive deeper into any specific aspect of deployment management, such as autoscaling configuration, cost optimization strategies, or monitoring techniques?",
    "$.messages[8].role": "assistant",
    "$.pid": 63817,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T00:23:12.737811",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "analyze-political-poorly-air-hardware",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev22+gfb34cf0.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 30 highlighted sections (required: 15)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "15",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "download-look-quick-minute-study",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write an outline for a paper on the history of Yemeni coffee. The outline should include the main points of the paper, and at least 15 sections should be highlighted with markdown such as *highlighted section*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "# Outline: The History of Yemeni Coffee

1. **Introduction: Why Yemeni Coffee Matters**  
   - Thesis: Yemeni coffee is not merely a beverage but a cultural, economic, and geopolitical force that shaped global history.  
   - Scope: From its mythical discovery in 9th-century Yemen to its modern revival.

2. **Pre-Coffee Yemen: The Land, the People, and the Plant**  
   - Geography of the Yemeni highlands and terraced agriculture.  
   - Indigenous *Coffea arabica* varieties and early cultivation techniques.  
   - *The myth of the goatherd Kaldi and the dancing goats*.

3. **The Sufi Spark: Coffee as a Spiritual Elixir**  
   - 15th-century Sufi monasteries in Mocha and Taiz.  
   - *Qahwa as a tool for nocturnal dhikr rituals*.  
   - Early roasting, grinding, and brewing methods.

4. **Mocha: The Birthplace of a Global Commodity**  
   - Rise of the port city of Mocha (Al-Mukha) in the 16th century.  
   - *The Mocha trade network linking Yemen, Egypt, and the Ottoman Empire*.  
   - Coffeehouses as centers of intellectual and political discourse.

5. **The Ottoman Grip and the First Coffee Monopoly**  
   - 1536: Ottoman conquest of Yemen and control of coffee exports.  
   - *The ban on fertile beans to prevent cultivation elsewhere*.  
   - Smuggling and the first cracks in the monopoly.

6. **The Dutch Heist and the Global Spread of Coffee**  
   - 1616: Dutch traders smuggle Yemeni beans to Batavia (Java).  
   - *The birth of Dutch colonial coffee plantations*.  
   - How Mocha’s loss became the world’s gain.

7. **Coffeehouses of Cairo and Istanbul: Yemen’s Cultural Export**  
   - 1554: First Ottoman coffeehouse in Istanbul.  
   - *The “Schools of the Wise” in Cairo’s Khan el-Khalili*.  
   - Coffee as a vector for Yemeni poetry, music, and Sufi thought.

8. **The European Coffee Craze and the Mocha Brand**  
   - 17th-century European coffeehouses and the “Mocha” label.  
   - *The false association of Mocha with chocolate flavor*.  
   - Yemeni coffee’s role in the Enlightenment.

9. **Colonial Rivalries and the Decline of Yemeni Dominance**  
   - 18th-century: French, British, and Dutch plantations outcompete Yemen.  
   - *The collapse of Mocha’s monopoly and the rise of Caribbean and Latin American coffee*.  
   - Impact on Yemeni economy and society.

10. **Yemeni Coffee in the Age of Empire**  
   - 19th-century British and Ottoman interests in Yemeni coffee.  
   - *The introduction of new cash crops (qat) and their effect on coffee cultivation*.  
   - Decline in quality and quantity due to neglect and conflict.

11. **The Cold War and the Coffee Crisis**  
   - 20th-century Yemeni division (North/South) and Soviet/US influences.  
   - *The shift from coffee to qat in rural livelihoods*.  
   - Drought, pests, and civil wars further erode production.

12. **The Specialty Coffee Renaissance: Yemen’s Return to Glory**  
   - 21st-century global demand for single-origin, heirloom coffees.  
   - *The discovery of “Yemenia” varietals and their genetic uniqueness*.  
   - NGOs and cooperatives reviving traditional farming.

13. **Modern Challenges: War, Climate, and Market Access**  
   - 2015–present: Saudi-led conflict and blockade.  
   - *The siege of Hodeidah port and its impact on coffee exports*.  
   - Climate change and water scarcity in the highlands.

14. **Yemeni Coffee in the Cup: Flavor, Ritual, and Identity**  
   - Tasting notes: dried fruit, spice, and wine-like acidity.  
   - *The Yemeni coffee ceremony (bunna) and its social significance*.  
   - Diaspora communities keeping traditions alive abroad.

15. **Conclusion: The Future of Yemeni Coffee**  
   - Balancing heritage with innovation.  
   - *The role of Yemeni coffee in post-war reconstruction*.  
   - Final reflection: From the highlands of Sana’a to the cups of Brooklyn, Yemeni coffee remains a bridge between past and future.",
    "$.messages[1].role": "assistant",
    "$.pid": 68433,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T00:23:12.737854",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "analyze-political-poorly-air-hardware",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev22+gfb34cf0.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 20 highlighted sections (required: 5)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "5",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "delete-open-cold-poor-power",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a cover letter for a job at a local coffee shop in the form of a poem. Highlight at least 5 text sections using "*". For example: *3 years of experience*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "Dear Brewed Awakening,

I am the *early-bird steam* that curls above your grinders,  
the *latte-art heart* that beats in every porcelain curve.  
My apron is a canvas of *espresso freckles*,  
each dot a story told in *3 years of experience*  
from bean to cup, from dawn rush to dusk hush.

I know the *symphony of the steam wand*—  
its hiss, its sigh, its whispered crema secrets—  
and how to coax *microfoam galaxies*  
into orbit around a *double-shot comet*.  
I’ve tamped, poured, swirled, and smiled  
through *Saturday lines out the door*,  
keeping tempers cool as the milk I *steam to 140°*.

I speak fluent *small-town regular*:  
“Medium drip, room for kindness?”  
I remember that *Mrs. Alvarez likes oat milk*,  
that *Mr. Chen takes two raw sugars*,  
and that *your dog, Scout, prefers puppuccinos*.

I sweep, I stock, I *balance the till to the penny*,  
but mostly I *brew community*—  
one *extra-hot americano* at a time.  
Hire me, and the morning light  
will find me already *wiping down tables*,  
already *saving the last blueberry scone*  
for the kid who always forgets his wallet.

With *grounds in my veins* and *crema in my smile*,  
I remain,

Your future barista,  
Maya",
    "$.messages[1].role": "assistant",
    "$.pid": 68433,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T00:23:12.737882",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "analyze-political-poorly-air-hardware",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev22+gfb34cf0.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 10 highlighted sections (required: 5)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "5",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "analyze-visible-level-love-pleasure",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a casual blog post about similarities across animal species. Highlight at least 5 sections in your answer by starting and ending with "*", like: *highlighted text section*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "Hey friends! Grab a coffee and let’s talk about something that always blows my mind: *how much we all have in common, even when we look nothing alike*. I was scrolling through wildlife photos the other night and realized that a jellyfish and I are basically running the same basic software—just with different packaging. So here’s a laid-back tour of five “wait, we do that too?!” moments across the animal kingdom.

*We all need our beauty sleep*  
Turns out, every animal with a nervous system—from fruit flies to blue whales—shares a version of sleep. Crows nap on tree branches, octopuses snooze in coconut-shell beds, and even roundworms have “lethargus,” which is basically worm REM. Scientists think sleep is so ancient that the last common ancestor of all animals already had it. So next time you hit snooze, remember you’re honoring a 700-million-year-old tradition.

*Love songs come in many dialects*  
Whether you’re a human humming in the shower or a humpback whale belting out 30-minute ballads, *we all use rhythm and pitch to flirt and bond*. Frogs croak in choruses, nightingales remix their playlists nightly, and male mice sing ultrasonic “pickup lines” too high for us to hear. Same goal, different instruments.

*Grandma knew best: family matters*  
Elephants babysit their nieces, wolves bring back take-out for auntie, and *clownfish dads are the ultimate stay-at-home fathers*. Across species, kinship networks boost survival. Even bacteria split into “sibling” lineages that cooperate—so the next time your cousin crashes on your couch, call it evolutionary strategy.

*Stress looks the same on everyone*  
When life gets spicy, our bodies flip the same ancient switch. *Cortisol floods fish, birds, and humans alike*, making hearts race and muscles twitch. Zebra finches lose their appetite during tough times, and stressed-out bees become pessimistic (yes, they can test bee mood). So if you’re doom-scrolling at 2 a.m., know that a stressed squirrel in your backyard totally gets you.

*Play isn’t optional—it’s homework for life*  
Otters juggle rocks, dogs zoom in circles, and young ravens snowboard down snowy roofs using jar lids. *Play builds brains and social skills across the board*. Researchers have spotted reptiles “wrestling” for fun and octopuses repeatedly bouncing Lego blocks like toddlers. If it has neurons, it probably needs recess.

Next time you feel alone in the universe, remember that somewhere a parrot is probably preening its best friend, a bat is sharing food with a hungry roost-mate, and a tiny nematode is curling up for a nap. We’re all just variations on a very old, very shared theme.",
    "$.messages[1].role": "assistant",
    "$.pid": 68433,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T00:23:12.737906",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "analyze-political-poorly-air-hardware",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev22+gfb34cf0.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 7 highlighted sections (required: 5)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "5",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "run-put-natural-traditional-thought",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Generate a business proposal to start a sweatshirt company in Bremen. The proposal should contain 5 or more sections. Highlight each section name using the this format:
*section name*",
    "$.messages[0].role": "user",
    "$.messages[1].content": "*Executive Summary*

Bremen is a city where maritime heritage meets cutting-edge creativity. Our venture, “NordHood Bremen,” will launch a premium, design-driven sweatshirt brand that fuses Hanseatic culture with contemporary streetwear. In year one we will release three limited-edition drops (total 6,000 units), achieve €420,000 in revenue, and reach break-even by month 14. The brand will be 100 % online-direct at launch, then expand into two Bremen concept stores and select European boutiques in years two and three. We will operate carbon-neutral, use 100 % organic cotton, and reinvest 3 % of profits into local youth art programs—turning every sweatshirt into a wearable piece of Bremen pride.

*Market Analysis*

Bremen’s 0.57 million residents and 500,000 annual tourists create a €35 million addressable mid-price sweatshirt market. Key insights:  
• 68 % of 18-35-year-olds prefer streetwear that tells a local story (Hochschule Bremen Trend Report 2023).  
• Tourists spend €180 million on souvenirs, yet only 11 % are apparel items—an untapped niche.  
• Competitor scan: global brands (Nike, Adidas) dominate functionality; regional labels (Hamburg’s “Dinghy,” Hannover’s “Liguster”) lack Bremen specificity.  
• Online D2C share in German apparel has grown from 18 % (2019) to 31 % (2023), proving the viability of our launch model.  
We will target two segments: “Culture Creatives” (students, designers, musicians) and “Heritage Seekers” (tourists, expats). Both segments value authenticity, limited editions, and sustainability.

*Product & Brand Strategy*

Signature Product: The “Hanse Hood” – 460 g brushed organic-cotton fleece, oversized fit, dropped shoulders, double-stitched flat seams, and a hidden inside pocket sized for a Bremen transit card.  
Design Language: Minimalist silhouettes in maritime colorways (Bremen Blue, Weser Sand, Harbor Grey) with subtle iconography—windrose embroidery, cog-ship sleeve print, and a QR code linking to an AR story of the garment’s journey.  
Sustainability: GOTS-certified cotton, recycled polyester labels, biodegradable mailers, and a take-back program offering 15 % discount on next purchase.  
Drop Model: Three seasonal drops per year (Spring “Hanse Sail,” Fall “Schlachte,” Winter “Weihnachtsmarkt”), each capped at 2,000 units to maintain scarcity.  
Brand Voice: “Wear the City.” Every hangtag includes a Bremen insider tip curated by local artists, turning the sweatshirt into a cultural guide.

*Operations & Supply Chain*

Design Studio: 120 m² loft in Überseestadt (rent €1,800/month) doubling as photo studio and event space.  
Manufacturing: Partnership with a Fair-Wear-audited factory in Porto, Portugal (MOQ 500, 30-day lead time, €18 landed cost per unit).  
Logistics: Bremen-based 3PL “Logwin” for pick-pack-ship (€1.20 per parcel, same-day dispatch).  
Tech Stack: Shopify Plus storefront, Klarna & PayPal checkout, Shipcloud integration, and a custom AR filter for Instagram try-ons.  
Quality Control: In-house inspection on arrival; defective rate target <1 %.  
Risk Mitigation: Dual-source yarn from Turkey and Portugal; maintain 8-week safety stock.

*Marketing & Sales Plan*

Pre-Launch (Months 1-2):  
• Teaser campaign on TikTok & Instagram Reels (#NordHoodBremen) with local influencers (5-50 k followers).  
• Pop-up try-on booth at Viertel Kreativmarkt; collect 1,000 email sign-ups.  
Launch (Month 3):  
• “Midnight Drop” online at 00:07 (Bremen dialing code) with countdown timer.  
• Limited 100 “Founders’ Edition” sweatshirts laser-numbered and signed by Bremen street artist “PlotBot.”  
Growth (Months 4-12):  
• Collaborations: capsule with SV Werder Bremen fan design; co-branded hoodie with Überseemuseum.  
• Referral program: give €10, get €10.  
• Tourist channel: partnership with Bremer Touristik-Zentrum for QR-coded city map bundled with sweatshirt.  
KPIs: CAC ≤ €18, repeat purchase rate ≥ 35 %, Instagram engagement ≥ 8 %.

*Financial Plan*

Start-up Capital Needed: €85,000  
• Inventory (3,000 units): €54,000  
• Marketing & content: €12,000  
• E-commerce & IT: €6,000  
• Legal, trademark, insurance: €5,000  
• Working capital buffer: €8,000  
Funding: 60 % founder equity, 40 % Bremen Förderbank micro-loan at 1.9 % interest.  
Year-1 P&L (conservative):  
Revenue €420,000 (avg. €70 net per unit)  
COGS €234,000  
Gross Profit €186,000  
Operating Expenses €150,000  
EBIT €36,000  
Break-even: 1,800 units or month 14.  
Three-year projection: revenue €1.4 million, EBITDA margin 18 %, reinvestment into two Bremen stores and EU wholesale.

*Appendices*

A. Mood-board & color palette  
B. Factory audit certificate  
C. Sample influencer contracts  
D. Detailed 36-month cash-flow model",
    "$.messages[1].role": "assistant",
    "$.pid": 68433,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T00:23:12.737929",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "analyze-political-poorly-air-hardware",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev22+gfb34cf0.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 29 highlighted sections (required: 3)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "3",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "sit-disassemble-ready-area-problem",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a funny and sarcastic template for rating the quality of a marriage between two people who are both moms. This is for the couple themselves. Please highlight at least 3 sections with markdown,  i.e *highlighted section*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "**The Official “Two-Mom Marriage Quality Report Card”**  
(For internal use only. If this ends up on Facebook, we *will* change the Wi-Fi password.)

---

**1. Communication Score: ___/10**  
*“Did we just have a 45-minute conversation about whose turn it is to buy more goldfish crackers, or was that a TED Talk?”*  
- 10 = We finish each other’s sentences… and each other’s coffee.  
- 5 = We communicate exclusively via passive-aggressive Post-it notes on the fridge.  
- 0 = We’ve resorted to interpretive dance and/or carrier pigeons.  

**Bonus points** if you’ve ever used the kids as tiny, sticky messengers (“Tell Mommy she’s *cordially invited* to take out the trash”).

---

**2. Chore Wars Score: ___/10**  
*“The laundry basket is a Jenga tower of lies.”*  
- 10 = We’ve achieved a chore utopia where socks actually match.  
- 5 = We’ve agreed to ‘divide’ labor… by one of us doing everything and the other ‘supervising’.  
- 0 = The dishwasher has become a permanent storage unit.  

**Penalty** if either mom has ever ‘forgotten’ how to load the dishwasher *again*.

---

**3. Co-Parenting Harmony Score: ___/10**  
*“We tag-team tantrums like WWE pros.”*  
- 10 = We’ve synchronized our ‘Mom Voice’ to subsonic frequencies that terrify small children.  
- 5 = We’ve debated whether ‘screen time’ is a human right or a war crime.  
- 0 = We’ve weaponized bedtime stories (‘And then the princess *never* got dessert again…’).  

**Extra credit** for every time you’ve high-fived over a successful *“Because I said so”* deployment.

---

**4. Romance Maintenance Score: ___/10**  
*“Date night: when we argue over which drive-thru has the shortest line.”*  
- 10 = We’ve scheduled intimacy between soccer practice and PTA meetings.  
- 5 = We’ve agreed that ‘Netflix and actually chill’ counts as foreplay.  
- 0 = Our last romantic gesture was sharing the last chicken nugget.  

**Automatic fail** if either of you has ever said, “I’m too tired”… *while scrolling Instagram*.

---

**5. Emergency Snack Reserves Score: ___/10**  
*“The pantry is a battlefield of empty granola bar boxes.”*  
- 10 = We’ve hidden chocolate so well we forgot where it is.  
- 5 = We’ve labeled snacks with fake allergen warnings to deter theft.  
- 0 = The kids have unionized and are demanding better rations.  

**Gold star** if you’ve ever *texted each other* to brag about finding the secret stash.

---

**Final Grade: ___/50**  
- 40–50: *Power couple.* You’re basically the Beyoncé and Beyoncé of lesbian moms.  
- 25–39: *Functioning chaos.* You’ve accepted that love is just shared exhaustion.  
- 0–24: *Renew your vows… or your Costco membership.* Either way, you’re in this together.  

**Disclaimer:** This report is not legally binding, but hiding the good snacks *is* grounds for divorce.",
    "$.messages[1].role": "assistant",
    "$.pid": 68433,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T00:23:12.737956",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "analyze-political-poorly-air-hardware",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev22+gfb34cf0.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 1 highlighted sections (required: 1)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "1",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "affect-unknown-long-life-month",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a song about the summers of my childhood that I spent in the countryside. Give the song a name, and highlight the name by wrapping it with *. For example: *little me in the countryside*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "*little me in the countryside*

[Verse 1]  
Barefoot on the gravel road, dust between my toes,  
Sunlight dripped like honey on the clover and the crows.  
Grandma’s porch swing creaked a lullaby so slow,  
While the creek below kept singing songs only we could know.  
Fireflies stitched the twilight with lanterns made of stars,  
And the moon was just a silver coin tossed into cookie jars.  

[Chorus]  
Oh, little me in the countryside, running wild and free,  
Chasing dreams through cornfield mazes, climbing every tree.  
Every laugh was louder than the thunder rolling by,  
In the summers of my childhood, underneath that endless sky.  

[Verse 2]  
Mason jars of sweet tea sweating on the window ledge,  
Radio played “Sweet Home Alabama” from the rusty garden shed.  
We built rafts from old barn doors and floated down the stream,  
Kingfishers were pirates and the cattails were our team.  
Supper was tomatoes sliced like sunsets on a plate,  
And the crickets kept the tempo while the night got late.  

[Chorus]  
Oh, little me in the countryside, running wild and free,  
Skinny-dipping in the moonlight where the river bends to see.  
Every scar was a medal from a dare we’d never hide,  
In the summers of my childhood, where the earth and I collided.  

[Bridge]  
Dandelion clocks we blew to tell the time,  
Each puff a tiny parachute for wishes we would climb.  
We swore we’d never grow up, just grow into the land,  
But the school bus came like autumn and took us by the hand.  

[Verse 3]  
Now the fields are sold and paved, the barn’s a memory store,  
But I still hear the wheat heads whisper, “Come back, come back once more.”  
I close my eyes and I’m ten again, chasing dragonflies,  
With the smell of rain on dirt roads and the taste of cherry pies.  

[Final Chorus]  
Oh, little me in the countryside, you’re still inside my chest,  
A pocket full of wonder that the city can’t arrest.  
Though the years roll on like tractors down a dusty county ride,  
I keep those summers in a mason jar where the fireflies never die.  

[Outro]  
So here’s to cicadas, to tire swings and creek-stone slides,  
To the echo of my laughter in the hollow where time hides.  
If you find that gravel road, kick off your shoes and glide—  
You’ll meet little me in the countryside, still barefoot, still wide-eyed.",
    "$.messages[1].role": "assistant",
    "$.pid": 68433,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T00:23:12.738015",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "analyze-political-poorly-air-hardware",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev22+gfb34cf0.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 3 highlighted sections (required: 3)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "3",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "believe-verify-likely-desire-power",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a riddle for the word "façade" that contains at least 3 italic text phrases in markdown syntax, i.e *italic text*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "I wear a *painted smile* for every passer-by,  
a *perfectly symmetrical* face that never cracks.  
Behind me, the *crumbling bricks* sigh in secret.  
What am I?",
    "$.messages[1].role": "assistant",
    "$.pid": 68433,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T00:23:12.738042",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "analyze-political-poorly-air-hardware",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev22+gfb34cf0.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 19 highlighted sections (required: 3)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "3",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "change-sort-serious-song-family",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a template for a workshop on the importance of diversity in the workplace and highlight at least 3 sections with markdown, i.e. *highlighted section*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "# Workshop Template  
**Title:** *Building Inclusive Excellence: The Business & Human Case for Workplace Diversity*  
**Duration:** 3.5 hours (can be split into two 90-minute sessions)  
**Audience:** Managers, HR professionals, team leads, ERG members, and any employee who influences culture  

---

## 1. Welcome & Objectives (15 min)
- Ice-breaker: “Two Truths & a Lie—Diversity Edition”  
- *Learning objectives*: By the end of this workshop, participants will be able to:  
  1. Define diversity, equity, inclusion, and belonging (DEIB) in their own words.  
  2. Articulate at least three evidence-based benefits of a diverse workforce.  
  3. Identify one action they can take within 30 days to foster inclusion on their team.  

---

## 2. *The Data-Driven Case for Diversity* (25 min)
- Interactive poll: “What % performance lift do you think diverse teams achieve?”  
- Mini-lecture with live charts:  
  - McKinsey 2023 report: Top-quartile companies for ethnic/gender diversity are **39 % more likely to outperform** on profitability.  
  - Harvard study: *Inclusive teams make better business decisions 87 % of the time* and do so twice as fast.  
- Small-group exercise: Match the statistic to the business outcome (revenue, innovation, retention).  

---

## 3. *Unconscious Bias & Micro-behaviors* (30 min)
- 5-minute video: “The Resume with Two Names” (identical CVs, different callbacks).  
- Bias Busters card game: Participants draw scenario cards and practice interrupting bias in real time.  
- *Key takeaway*: Bias is not a moral failing; it is a neurological shortcut that can be rewired through deliberate habits.  

---

## 4. *Inclusive Leadership Toolkit* (40 min)
- **The 4 A’s Framework**:  
  1. **Awareness** – Daily 3-minute bias check-in.  
  2. **Amplification** – Repeat & credit ideas from under-represented voices.  
  3. **Access** – Rotate high-visibility assignments.  
  4. **Accountability** – Track inclusion metrics on team dashboards.  
- Role-play: “The Missed Promotion” – diagnose where each ‘A’ broke down and redesign the conversation.  

---

## 5. Action Planning & Commitments (20 min)
- Personal *Inclusion Commitment Card*: “In the next 30 days I will…”  
- Pair-share for accountability.  
- QR code to a digital resource hub (TED talks, reading list, policy templates).  

---

## 6. Closing & Evaluation (10 min)
- One-word checkout: “How do you feel now?”  
- *Post-workshop survey* link (2 min) with prize draw for completion.  

---

## Materials Checklist
- Slide deck (with alt-text on all images)  
- Printed Commitment Cards & pens  
- Bias Busters card decks (laminated)  
- Timer & bell  
- Snacks labeled for common dietary restrictions (gluten-free, vegan, halal)  

---

## Follow-Up Email Template (send within 24 hrs)
Subject: Your Inclusion Toolkit + Next Steps  
Body:  
Hi team,  
Thank you for leaning into today’s conversation on *why diversity wins*. Attached are:  
- Slide deck & resource links  
- Your signed Inclusion Commitment Card (photo reminder)  
- Calendar invite for the optional 30-day check-in circle  

Remember: Inclusion is a practice, not a one-time training. Let’s keep the momentum!",
    "$.messages[1].role": "assistant",
    "$.pid": 68433,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T00:23:12.738068",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "analyze-political-poorly-air-hardware",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev22+gfb34cf0.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 21 highlighted sections (required: 3)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "3",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "must-write-need-theoretical-pleasure",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a cover letter to a local political party, asking to be their rally organizer. Make sure to highlight at least 3 sections in your answer in markdown format.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "**[Your Name]**  
[Your Address]  
[City, State ZIP]  
[Email Address] | [Phone Number] | [LinkedIn URL]  
[Date]

**[Local Party Chairperson’s Name]**  
[Local Party Name]  
[Party Headquarters Address]  
[City, State ZIP]

Dear **Chairperson [Last Name]**,

I am writing to express my enthusiastic interest in serving as **Rally Organizer** for **[Local Party Name]**. As a lifelong resident of **[Your City/County]** and a committed progressive, I have spent the past six years turning grassroots energy into measurable voter engagement. I believe my proven record of mobilizing thousands of supporters, securing high-profile speakers, and running flawless logistics makes me the ideal candidate to lead our 2024 rally program.

### **Proven Rally & Event Expertise**
- **2018 & 2020 Midterm & Presidential Rallies**: Planned and executed 14 rallies across three counties, averaging 2,500 attendees each.  
- **Permits & Safety**: Obtained all municipal permits within 72 hours, coordinated with local fire and police departments, and maintained a 100 % safety record.  
- **Stage & AV Management**: Negotiated vendor contracts that cut AV costs by 27 % while increasing sound coverage by 40 %.

### **Community & Coalition Building**
- **Diverse Coalitions**: Forged partnerships with 18 labor unions, 9 student groups, and 7 faith organizations, expanding our volunteer base by 1,200 active members.  
- **Multilingual Outreach**: Produced bilingual signage and volunteer scripts, increasing turnout in Latinx precincts by 31 %.  
- **Accessibility**: Installed ADA-compliant ramps, ASL interpreters, and quiet zones at every event, earning commendation from the local Disability Rights chapter.

### **Data-Driven Strategy & Rapid Response**
- **Real-Time Metrics**: Deployed a mobile check-in app that captured 94 % of attendee data, feeding our CRM for targeted follow-up.  
- **Weather Contingencies**: Created a “Plan B” playbook that moved a 3,000-person outdoor rally indoors in under 90 minutes with zero no-shows.  
- **Crisis Communications**: Served as on-site media liaison, turning a last-minute speaker cancellation into a prime-time local news segment featuring our grassroots volunteers.

I would welcome the opportunity to discuss how my experience, energy, and strategic vision can help **[Local Party Name]** stage rallies that not only energize our base but also convert enthusiasm into votes. I am available at your convenience for an interview and can provide a detailed rally calendar and budget forecast within 48 hours of request.

Thank you for your time and consideration. I look forward to amplifying our shared values across **[Your City/County]**—one rally at a time.

In solidarity,

**[Your Name]**",
    "$.messages[1].role": "assistant",
    "$.pid": 68433,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T00:23:12.738096",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "analyze-political-poorly-air-hardware",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev22+gfb34cf0.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 5 highlighted sections (required: 2)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "2",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "control-accept-record-legal-car",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "The opposite of youth is not age, but ...? Highlight at least 2 sections in your answer with markdown, i.e. *highlighted section*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "The opposite of youth is not age, but **disillusionment**—the moment when *wonder* is traded for *weariness*, when the *boundless horizon* of possibility collapses into the *narrow corridor* of routine.",
    "$.messages[1].role": "assistant",
    "$.pid": 68433,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T00:23:12.738119",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "analyze-political-poorly-air-hardware",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev22+gfb34cf0.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 5 highlighted sections (required: 3)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "3",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "miss-continue-fall-local-work",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a description of the following data in a weird style: The Golden Palace eatType restaurant; The Golden Palace food Indian; The Golden Palace area city centre. Use markdown to highlight at least 3 sections in your answer.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "**The Golden Palace** is not a palace, but it *is* golden—at least in spirit—nestled like a curry-scented jewel in the **city centre**, where neon pigeons peck at samosa crumbs and the air hums with sitar-shaped traffic lights.  

Inside, **Indian** food performs acrobatics on your tongue: cardamom cartwheels, turmeric tumbles, and a rogue chili that once tried to elope with a mango lassi. The waiters glide like buttered shadows, whispering orders to the kitchen where naan breads inflate like edible balloons plotting a coup against gravity.  

At **The Golden Palace**, every table is a throne, every plate a miniature map of the subcontinent, and every bite a teleportation device disguised as dinner.",
    "$.messages[1].role": "assistant",
    "$.pid": 68433,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T00:23:12.738142",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "analyze-political-poorly-air-hardware",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev22+gfb34cf0.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 14 highlighted sections (required: 1)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "1",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "require-affect-major-logical-business",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a funny rap about a man who gets a call from an official saying that he is a long lost relative of the king of Nigeria. Use markdown to highlight at least one section of your answer, i.e. *highlighted section*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "🎤 *Yo, it’s the tale of Sir Scam-a-lot, straight outta suburbia!* 🎤  

Yo, my name is Dave, I sell insurance in Ohio,  
Got a cubicle, a cat, and a 401(k) that’s kinda fly-o.  
One Tuesday, phone rings—unknown number, what the heck?  
Voice says, “*Prince Ogbonna the Third* just wrote you a fat check!”  

I’m like, “Hold up, who dis? I’m whiter than printer paper,  
My bloodline’s mayo on Wonder Bread—ain’t no royal vapor!”  
Dude on the line, accent silkier than lotion,  
Says, “Cousin, you’re 37th in line—time to claim your ocean!”  

**[Hook]**  
🎶 *Long lost king of Nigeria, crown sittin’ on my dome,*  
*Got a palace full of leopards and a Wi-Fi that roams!*  
*They need my SSN and a quick wire fee,*  
*But I’m ‘bout to be big pimpin’ in Abuja, just wait and see!* 🎶  

He says, “Just send five grand for customs, then the jet lands,  
You’ll be dripping in so much gold, even your sweat glands bling.”  
I’m sweatin’ like a rotisserie, visions of caviar dreams,  
I Venmo “PrinceNaija69” like I’m funding meme schemes.  

Next day, email: “Oops, palace goat ate the transfer slip,  
Need ten more for goat surgery and a diamond-encrusted whip.”  
I hock my lawnmower, my Xbox, and Aunt Linda’s porcelain goose,  
Wire cash so fast my bank’s like, “Dave, you loose!”  

**[Bridge – Spoken Word]**  
Then the *official royal seal* arrives—  
It’s a GIF of Shrek in a crown doing the Macarena.  
I’m starting to sense a pattern here…  

**[Final Hook – Regret Remix]**  
🎶 *Now I’m the king of my mom’s basement, throne’s a beanbag chair,*  
*Royal decree: never trust a prince who types in all caps—*  
*“URGENT, COUSIN, SEND MORE FLAMINGOS!”*  
*Lesson learned: if the crown’s too shiny, it’s probably aluminum foil, homie!*",
    "$.messages[1].role": "assistant",
    "$.pid": 68433,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T00:23:12.738169",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "analyze-political-poorly-air-hardware",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev22+gfb34cf0.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 6 highlighted sections (required: 2)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "2",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "could-serious-newspaper-radio-group",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Expand the riddle into a story with a funny tone:

What can you catch but not throw?
A cold

Use * to highlight at least 2 sections in your text. For example: *this is a highlighted text section*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "Gary the Germ was the *smallest, greenest, and most obnoxiously proud* microbe in the entire Sniffle Kingdom. Every morning he practiced his grand entrance: sliding down banisters made of nasal hair, cannon-balling into unsuspecting tonsils, and yelling, “ACHOO—did somebody order a cold?”  

One Tuesday, Gary spotted his dream target: *Bartholomew “Bart” Bigglesworth, a man who believed hand-washing was a government conspiracy*. Bart had just exited the subway, triumphantly licking a subway pole “for the vitamins.” Gary rubbed his microscopic hands together. “Perfect. I’ll hitch a ride on that finger, ride the express elevator to the sinuses, and throw the *snottiest* house-warming party in history!”  

But there was a catch—literally. Gary could *catch* Bart, but he couldn’t *throw* him anywhere. Every time Gary tried to catapult Bart into the next office cubicle, Bart just sneezed, wiped his nose on his sleeve, and kept typing spreadsheets. Gary screamed, “This is mutiny! I’m supposed to be the one doing the tossing!”  

Meanwhile, Bart’s immune system—led by Captain Phlegm and the Mucus Marines—launched *Operation Tissue Storm*. They surrounded Gary, armed with vitamin C grenades and zinc zappers. Gary attempted a dramatic escape by surfing a sneeze, but the tissue slapped him like a flyswatter on a summer day.  

Defeated, Gary sat in the crumpled tissue, sulking. “I caught the cold,” he muttered, “but I can’t throw it anywhere. Worst. Superpower. Ever.”  

From that day forward, Gary changed his business card to read: “Professional Cold Catcher—No Returns, No Refunds, Absolutely No Throwing.”",
    "$.messages[1].role": "assistant",
    "$.pid": 68433,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T00:23:12.738192",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "analyze-political-poorly-air-hardware",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev22+gfb34cf0.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 17 highlighted sections (required: 3)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "3",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "offer-select-free-state-moment",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a rap about the renaissance. It should be noticeably different from raps about other historical eras, and have an interesting or weird tone. Highlight at least 3 sections in your answer in markdown format.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "**[Intro – harpsichord glitch beat, stylus scratching parchment]**  
Yo, it’s 14-something, but my mind’s 3-D printed in gold leaf—  
I’m the **Renaissance Rapper**, got a quill in one hand and a scalpel in the other, dissectin’ beats like Vesalius dissectin’ spleens.  
Let’s get *weird* with the rebirth, baby.

---

### **Section 1 – The Fresco Flex**  
I drip ultramarine like I’m sponsored by Afghanistan mines,  
Michelangelo on the ceiling, I’m reclining on divine lines.  
Angels doing backflips off the Sistine, they’re hype for my set,  
While I tag “*AMOR VINCIT OMNIA*” in invisible ink—wet.  
My flow’s a trompe-l’œil, looks flat till you tilt your head—  
Then it’s depth for days, like Brunelleschi’s dome overhead.  
I’m not baroque, I’m pre-baroque, I’m the prototype shock,  
Got patrons in fur-lined sleeves stuffing florins in my sock.  
**I paint in egg tempera so the colors never fade,**  
**But the yolk smells funky—call it *odeur de old crusade*.**

---

### **Section 2 – The Printing-Press Poltergeist**  
Gutenberg ghost in the machine, clanking lead type like chains,  
Every page a polter-page, every serif a soul that remains.  
I drop a folio, it’s viral before “viral” was a cough,  
Monks in scriptoriums rage-quitting, “Bro, that illuminated stuff’s off!”  
I’m the incunabula influencer, my colophon’s a flex,  
Got red-ink rubricators sliding into my DMs—*next*.  
**I rhyme in movable type so my bars can rearrange,**  
**Yesterday I was Petrarch, today I’m deranged.**  
The press spits paper like a hydra with hiccups,  
Each sheet a protest flyer: “Down with scholastic pick-ups!”

---

### **Section 3 – The Anatomical Anthem (Parental Advisory: Corpses)**  
Night shift with da Vinci, we’re sketching by candle and moon,  
Cadavers on the table like, “Do me a favor—autotune my coccyx soon.”  
I rap in reverse Latin so only the dead understand,  
While Leo reverse-engineers a bird-man from a frying pan.  
**My beats are ventricles pumping four-four time,**  
**Aorta bass drop—call that circulatory grime.**  
I sample the sound of a scalpel scraping periosteum,  
Layer it over a lute loop—*that’s* my magnum opium.  
Galen’s in the corner, throwing shade like a Greek tragedy,  
But I’m Harvey before Harvey, already rapping blood circulation majestically.

---

**[Outro – fading lute arpeggio, distant cannon fire from Constantinople]**  
So remember me when you’re doom-scrolling on glass,  
I was the era that turned corpses into class.  
I’m the **Renaissance Rapper**, my ghost still tags in chalk,  
Leaving QR codes on cathedral walls—scan it, you’ll squawk.  
Rebirth never dies, it just pixelates and loops,  
Now if you’ll excuse me, I’ve got a date with Lucretius in the soup.",
    "$.messages[1].role": "assistant",
    "$.pid": 68433,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T00:23:12.738219",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "analyze-political-poorly-air-hardware",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev22+gfb34cf0.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 25 highlighted sections (required: 5)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "5",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "calculate-affect-fullest-emotional-money",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Summarize the history of Japan. Italicize at least 5 keywords in your response. To indicate a italic word, wrap it with asterisk, like *italic*",
    "$.messages[0].role": "user",
    "$.messages[1].content": "*Jōmon* hunter-gatherers (c. 14,000 BCE) gave way to *Yayoi* rice cultivators who forged bronze and iron tools.  
The *Yamato* court unified clans in the 3rd–4th centuries, adopting *Shintō* rituals and Chinese script.  
Prince *Shōtoku*’s 7th-century constitution and the Taika Reforms centralized power under an emperor, while *Nara* and *Heian* aristocrats refined literature and *Buddhism*.  
Samurai clans rose during the *Kamakura* shogunate (1185), repelling *Mongol* fleets and entrenching warrior rule.  
Centuries of *Ashikaga* and *Sengoku* civil war ended when *Oda Nobunaga*, *Toyotomi Hideyoshi*, and *Tokugawa Ieyasu* reunified Japan; the *Edo* shogunate (1603–1868) enforced isolation yet fostered *ukiyo-e* culture.  
Commodore *Perry*’s 1853 arrival triggered the *Meiji* Restoration, rapid modernization, and imperial expansion.  
Victory in the *Russo-Japanese* War (1905) emboldened militarism, leading to *Manchuria* occupation and *Pearl Harbor*.  
After atomic bombings and *Hirohito*’s surrender in 1945, U.S. occupation reformed politics and the economy, birthing the *post-war* “economic miracle.”  
Today, Japan balances *constitutional monarchy* with global technological leadership.",
    "$.messages[1].role": "assistant",
    "$.pid": 68433,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T00:23:12.738252",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "analyze-political-poorly-air-hardware",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev22+gfb34cf0.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 5 highlighted sections (required: 2)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "2",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "download-american-private-result-research",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a funny Haiku about a Quaker named John who lives in the town of De Smet, South Dakota. Use the asterisk symbol, *, to highlight some words or phrases twice. Example: *This is a highlighted phrase*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "*John* in *De Smet*  
Quaker hat *blows off*—chases  
*Silent* *scream*—“Come back!”",
    "$.messages[1].role": "assistant",
    "$.pid": 68433,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T00:23:12.738286",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "analyze-political-poorly-air-hardware",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev22+gfb34cf0.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 19 highlighted sections (required: 1)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "1",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "indicate-strong-single-festival-mother",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Today, at the 54th Annual Grammy Awards, the Recording Academy honors the talent and creativity of the artists, musicians, and producers who are the creators of the best recordings of the past year. Please continue writing this text in a formal tone, using notations. Highlight some key parts in your response with "*", like *highlighted text*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "Today, at the 54th Annual Grammy Awards, the Recording Academy honors the talent and creativity of the artists, musicians, and producers who are the creators of the best recordings of the past year. In a ceremony marked by *unprecedented artistic excellence* and *meticulous craftsmanship*, the Academy recognizes those whose work has defined the sonic landscape of 2011.

The evening’s proceedings underscore the Academy’s enduring commitment to *artistic integrity* and *technical innovation*. Among the most celebrated achievements are:

• *Album of the Year* – awarded to an opus that has redefined genre boundaries while achieving both critical acclaim and popular resonance.  
• *Record of the Year* – bestowed upon a single track whose *sonic clarity*, *emotive depth*, and *production mastery* have set a new benchmark for the industry.  
• *Best New Artist* – conferred upon an emerging voice whose *visionary artistry* signals the future direction of contemporary music.

Throughout the telecast, special tributes illuminate the *indelible legacies* of icons whose influence transcends generations, while collaborative performances showcase the *synergistic power* of cross-genre partnerships. The Academy further acknowledges the *behind-the-scenes luminaries*—engineers, mixers, and mastering specialists—whose *invisible expertise* transforms raw inspiration into *audible transcendence*.

In closing, the 54th Annual Grammy Awards reaffirm the Recording Academy’s solemn charge: to *safeguard the heritage*, *nurture the present*, and *inspire the future* of recorded music.",
    "$.messages[1].role": "assistant",
    "$.pid": 68433,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T00:23:12.738315",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "analyze-political-poorly-air-hardware",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev22+gfb34cf0.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 39 highlighted sections (required: 2)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "2",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "design-assume-past-camera-word",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a blog post about interesting facts about the Dutch language. Italicize at least 2 sections in your answer with markdown, i.e. *italic text*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "# 10 Surprising Facts About the Dutch Language That Will Make You Say “Wauw!”

If you think Dutch is just “German with a sore throat,” think again. From tongue-twisting consonant clusters to words that sound like they were invented by a stand-up comedian, Dutch is a linguistic playground. Below are ten facts that prove the language is far more colorful than its reputation for windmills and wooden shoes.

---

### 1. *The longest word in Dutch is 53 letters long*  
Meet **“kindercarnavalsoptochtvoorbereidingswerkzaamhedenplan”**—a single noun describing the preparation work for a children’s carnival parade. Dutch loves to glue words together, so once you master the basics you can create your own kilometer-long compounds on the fly.

---

### 2. Dutch has three genders, but only two articles  
Grammatically, nouns are masculine, feminine, or neuter. In everyday speech, however, masculine and feminine collapsed into a single “common” gender, leaving *“de”* for common and *“het”* for neuter. The result? A never-ending guessing game for learners.

---

### 3. *The Dutch alphabet ends with “ij,” not “z”*  
Schoolchildren recite the ABC and finish with “…x, y, ij, z.” The digraph *ij* is so common (think *hij* = he, *mijn* = my) that it’s treated almost like a separate letter. On old typewriters, *ij* even had its own key.

---

### 4. Dutch is the closest major language to English  
Frisian is technically closer, but among languages with tens of millions of speakers, Dutch wins. Compare:  
- English *“bread”* → Dutch *“brood”*  
- English *“green”* → Dutch *“groen”*  
Shared roots mean you can read simple Dutch signs long before you’ve cracked a single grammar book.

---

### 5. *Dutch gave New York its first words*  
Before it was New York, it was New Amsterdam. That’s why we say *“cookie”* (from Dutch *koekje*) instead of *“biscuit,”* and *“boss”* (from *baas*) instead of *“master.”* Wall Street? It once ran alongside the *“wal”* (rampart) of the city wall.

---

### 6. The word *“gezellig”* has no exact English translation*  
Roughly it means “cozy,” “convivial,” or “the warm feeling of being with friends,” but none of those capture the full vibe. A café can be *gezellig*, a conversation can be *gezellig*, even a spreadsheet can be *gezellig* if the team is laughing.

---

### 7. Dutch spelling changed overnight in 1996  
The *“Groene Boekje”* (Green Booklet) reforms simplified thousands of words. *“potatos”* became *“aardappelen”* and *“cat’s”* became *“katten.”* Overnight, every dictionary, street sign, and Scrabble board in the country was technically out of date.

---

### 8. *Dutch is spoken on six continents*  
Besides the Netherlands and Belgium, it’s an official language in Suriname, Aruba, Curaçao, and Sint Maarten. Thanks to emigration, you’ll also find active Dutch-speaking communities in Canada, Australia, Argentina—even South Africa, where Afrikaans is a daughter language.

---

### 9. The Dutch love diminutives  
Add *-je* to almost any noun and you instantly shrink it: *“huis”* (house) → *“huisje”* (little house). It’s not just about size; diminutives add affection. A *“biertje”* isn’t a small beer—it’s an invitation to relax.

---

### 10. *Dutch is the language of the world’s oldest multinational*  
The Vereenigde Oostindische Compagnie (VOC) was founded in 1602 and issued the planet’s first publicly traded stock. Its ledgers, contracts, and ship logs were all written in Dutch, making the language the original tongue of global capitalism.

---

Ready to dive in? Start with *hallo* and *dank je wel*, and you’ll already be halfway to *gezellig*.",
    "$.messages[1].role": "assistant",
    "$.pid": 68433,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T00:23:12.738340",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "analyze-political-poorly-air-hardware",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6.dev22+gfb34cf0.dirty",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 5 highlighted sections (required: 1)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "1",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "express-prove-private-internet-goal",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Gideon is a farmer who has a surplus of crops from his farm this year. What might he do with that surplus? Highlight at least one section of your answer in markdown, i.e *highlighted section*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "Gideon has several good options for his surplus:

1. **Sell it at the local farmers’ market or to a wholesaler** to turn the extra produce into cash that can cover next season’s seed, fertilizer, and equipment costs.  
2. **Store some of the surplus in a community grain silo** so he can sell later when prices are higher.  
3. **Donate part of the surplus to the regional food bank**, helping families in need and possibly qualifying for a small tax deduction.  
4. **Use the surplus as livestock feed** for his own animals, cutting down on purchased feed expenses.  

*The most profitable short-term choice is usually selling the surplus immediately after harvest, when demand is strong and prices are favorable.*",
    "$.messages[1].role": "assistant",
    "$.pid": 68433,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T10:35:28.042894",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "stop-choose-expect-major-pain",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6-dev1.dev4+g67b03c6",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 30 highlighted sections (required: 15)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "15",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "should-speak-cold-film-festival",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write an outline for a paper on the history of Yemeni coffee. The outline should include the main points of the paper, and at least 15 sections should be highlighted with markdown such as *highlighted section*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "# Outline: The History of Yemeni Coffee

1. **Introduction: Why Yemeni Coffee Matters**  
   - Thesis: Yemeni coffee is not merely a beverage but a cultural, economic, and geopolitical force that shaped global history.  
   - Scope: From its mythical discovery in 9th-century Yemen to its modern revival.

2. **Pre-Coffee Yemen: The Land, the People, and the Plant**  
   - Geography of the Yemeni highlands and terraced agriculture.  
   - Indigenous *Coffea arabica* varieties and early cultivation techniques.  
   - *The myth of the goatherd Kaldi and the dancing goats*.

3. **The Sufi Spark: Coffee as a Spiritual Elixir**  
   - 15th-century Sufi monasteries in Mocha and Taiz.  
   - *Qahwa as a tool for nocturnal dhikr rituals*.  
   - Early roasting, grinding, and brewing methods.

4. **Mocha: The Birthplace of a Global Commodity**  
   - Rise of the port city of Mocha (Al-Mukha) in the 16th century.  
   - *The Mocha trade network linking Yemen, Egypt, and the Ottoman Empire*.  
   - Coffeehouses as centers of intellectual and political discourse.

5. **The Ottoman Grip and the First Coffee Monopoly**  
   - 1536: Ottoman conquest of Yemen and control of coffee exports.  
   - *The ban on fertile beans to prevent cultivation elsewhere*.  
   - Smuggling and the first cracks in the monopoly.

6. **The Dutch Heist and the Global Spread of Coffee**  
   - 1616: Dutch traders smuggle Yemeni beans to Batavia (Java).  
   - *The birth of Dutch colonial coffee plantations*.  
   - How Mocha’s loss became the world’s gain.

7. **Coffeehouses of Cairo and Istanbul: Yemen’s Cultural Export**  
   - 1554: First Ottoman coffeehouse in Istanbul.  
   - *The “Schools of the Wise” in Cairo’s Khan el-Khalili*.  
   - Coffee as a vector for Yemeni poetry, music, and Sufi thought.

8. **The European Coffee Craze and the Mocha Brand**  
   - 17th-century European coffeehouses and the “Mocha” label.  
   - *The false association of Mocha with chocolate flavor*.  
   - Yemeni coffee’s role in the Enlightenment.

9. **Colonial Rivalries and the Decline of Yemeni Dominance**  
   - 18th-century: French, British, and Dutch plantations outcompete Yemen.  
   - *The collapse of Mocha’s monopoly and the rise of Caribbean and Latin American coffee*.  
   - Impact on Yemeni economy and society.

10. **Yemeni Coffee in the Age of Empire**  
   - 19th-century British and Ottoman interests in Yemeni coffee.  
   - *The introduction of new cash crops (qat) and their effect on coffee cultivation*.  
   - Decline in quality and quantity due to neglect and conflict.

11. **The Cold War and the Coffee Crisis**  
   - 20th-century Yemeni division (North/South) and Soviet/US influences.  
   - *The shift from coffee to qat in rural livelihoods*.  
   - Drought, pests, and civil wars further erode production.

12. **The Specialty Coffee Renaissance: Yemen’s Return to Glory**  
   - 21st-century global demand for single-origin, heirloom coffees.  
   - *The discovery of “Yemenia” varietals and their genetic uniqueness*.  
   - NGOs and cooperatives reviving traditional farming.

13. **Modern Challenges: War, Climate, and Market Access**  
   - 2015–present: Saudi-led conflict and blockade.  
   - *The siege of Hodeidah port and its impact on coffee exports*.  
   - Climate change and water scarcity in the highlands.

14. **Yemeni Coffee in the Cup: Flavor, Ritual, and Identity**  
   - Tasting notes: dried fruit, spice, and wine-like acidity.  
   - *The Yemeni coffee ceremony (bunna) and its social significance*.  
   - Diaspora communities keeping traditions alive abroad.

15. **Conclusion: The Future of Yemeni Coffee**  
   - Balancing heritage with innovation.  
   - *The role of Yemeni coffee in post-war reconstruction*.  
   - Final reflection: From the highlands of Sana’a to the cups of Brooklyn, Yemeni coffee remains a bridge between past and future.",
    "$.messages[1].role": "assistant",
    "$.pid": 43066,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T10:35:28.042934",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "stop-choose-expect-major-pain",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6-dev1.dev4+g67b03c6",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 16 highlighted sections (required: 5)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "5",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "destroy-rational-cultural-available-meeting",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a cover letter for a job at a local coffee shop in the form of a poem. Highlight at least 5 text sections using "*". For example: *3 years of experience*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "Dear Brewmaster of *Morning Rituals*,  
I come not with parchment résumé,  
but with *3 years of experience* tucked in my apron pocket,  
steam-warm and espresso-dark.  

I have *memorized 47 regulars’ names*—  
each syllable frothed like oat-milk foam—  
and I know that *Carla takes two raw sugars*  
while *Mr. Lee prefers his cappuccino bone-dry*.  

I can *dial in a grinder* at 5:03 a.m.  
to the *exact 18.5 grams* that coax  
caramel notes from Ethiopian beans,  
and still greet the line with *a sunrise smile*.  

My wrists speak in *latte-art cursive*:  
hearts, rosettas, and once (on a dare)  
a tiny saxophone for the jazz trio  
who tips in *harmonies and quarters*.  

I *close the shop solo* without a single bean left homeless,  
count the till to the *penny and a poem*,  
and leave the milk pitchers *mirror-bright*,  
so the night shift sees its own reflection of care.  

Hire me, and the *scent of fresh-ground hope*  
will greet every customer at the door;  
I’ll be the *steady hand* behind the bar,  
pouring *courage in ceramic*, one cup at a time.  

Yours in steam and service,  
Riley",
    "$.messages[1].role": "assistant",
    "$.pid": 43066,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T10:35:28.042963",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "stop-choose-expect-major-pain",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6-dev1.dev4+g67b03c6",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 10 highlighted sections (required: 5)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "5",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "provide-cool-meeting-kid-book",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a casual blog post about similarities across animal species. Highlight at least 5 sections in your answer by starting and ending with "*", like: *highlighted text section*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "Hey friends! Grab a coffee and let’s talk about something that always blows my mind: *how much we all have in common, even when we look nothing alike*. I was scrolling through wildlife photos the other night and realized that a jellyfish and I are basically running the same basic software—just with different packaging. So here’s a laid-back tour of five “wait, we do that too?!” moments across the animal kingdom.

*We all need our beauty sleep*  
Turns out, every animal with a nervous system—from fruit flies to blue whales—shares a version of sleep. Crows nap on tree branches, octopuses snooze in coconut-shell beds, and even roundworms have “lethargus,” which is basically worm REM. Scientists think sleep is so ancient that the last common ancestor of all animals already had it. So next time you hit snooze, remember you’re honoring a 700-million-year-old tradition.

*Love songs come in many dialects*  
Whether you’re a human humming in the shower or a humpback whale belting out 30-minute ballads, *we all use rhythm and pitch to flirt and bond*. Frogs croak in choruses, nightingales remix their playlists nightly, and male mice sing ultrasonic “pickup lines” too high for us to hear. Same goal, different instruments.

*Grandma knew best: family matters*  
Elephants babysit their nieces, wolves bring back take-out for auntie, and *clownfish dads are the ultimate stay-at-home fathers*. Across species, kinship networks boost survival. Even bacteria split into “sibling” lineages that cooperate—so the next time your cousin crashes on your couch, call it evolutionary strategy.

*Stress looks the same on everyone*  
When life gets spicy, our bodies flip the same ancient switch. *Cortisol floods fish, birds, and humans alike*, making hearts race and muscles twitch. Zebra finches lose their appetite during tough times, and stressed-out bees become pessimistic (yes, they can test bee mood). So if you’re doom-scrolling at 2 a.m., know that a stressed squirrel in your backyard totally gets you.

*Play isn’t optional—it’s homework for life*  
Otters juggle rocks, dogs zoom in circles, and young ravens snowboard down snowy roofs using jar lids. *Play builds brains and social skills across the board*. Researchers have spotted reptiles “wrestling” for fun and octopuses repeatedly bouncing Lego blocks like toddlers. If it has neurons, it probably needs recess.

Next time you feel alone in the universe, remember that somewhere a parrot is probably preening its best friend, a bat is sharing food with a hungry roost-mate, and a tiny nematode is curling up for a nap. We’re all just variations on a very old, very shared theme.",
    "$.messages[1].role": "assistant",
    "$.pid": 43066,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T10:35:28.042989",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "stop-choose-expect-major-pain",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6-dev1.dev4+g67b03c6",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 6 highlighted sections (required: 5)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "5",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "delete-stale-similar-others-email",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Generate a business proposal to start a sweatshirt company in Bremen. The proposal should contain 5 or more sections. Highlight each section name using the this format:
*section name*",
    "$.messages[0].role": "user",
    "$.messages[1].content": "*Executive Summary*
Bremen Sweat Co. will be a premium, sustainability-driven sweatshirt brand headquartered in Bremen, Germany. We will design, manufacture, and sell limited-edition sweatshirts that combine Nordic minimalism with Hanseatic maritime heritage. Our mission is to become Bremen’s first carbon-neutral apparel label while capturing 5 % of the city’s €120 million annual streetwear market within three years. Initial funding of €250 k will cover product development, a flagship micro-factory/store in the Viertel district, and a digital-first go-to-market strategy.

*Market Analysis*
Bremen’s population of 570 k includes 120 k university students and 80 k creative-industry professionals—our core demographic. Streetwear sales in Northern Germany grew 11 % YoY (Statista 2023), yet only 8 % of brands are locally produced. Competitor benchmarking shows price gaps: fast-fashion sweatshirts sell at €25–35, mid-tier at €60–80, and premium sustainable labels at €110–140. We will occupy the €85–95 niche with superior materials and storytelling. Surveys (n = 400) reveal 68 % of Bremen consumers would pay a 20 % premium for locally made, eco-certified apparel.

*Product & Sustainability Strategy*
Each sweatshirt will be crafted from 100 % GOTS-certified organic cotton and recycled ocean-bound polyester, dyed in Bremen using closed-loop water systems. Designs will feature subtle references to Bremen’s Town Musicians, ship masts, and Weser river textures. Limited drops of 300 units per style ensure scarcity and collectability. Blockchain-based QR codes will provide end-to-end transparency: fiber origin, factory GPS, and carbon footprint per garment (-2.1 kg CO₂e vs. industry average 8.5 kg). End-of-life take-back program offers 15 % store credit for returned items, feeding our upcycling line.

*Operations & Supply Chain*
Micro-factory (180 m²) in the Viertel will house 4 digital knitting machines and 6 seamstresses, producing 1 200 sweatshirts/month. Local sourcing: yarn from OEKO-TEX mill in Oldenburg, trims from Hamburg port suppliers. Just-in-time production reduces inventory risk by 40 %. Bremen’s port infrastructure enables low-impact inbound logistics; outbound shipments leverage DHL GoGreen within Germany (24 h delivery). ISO 14001 certification targeted by month 12.

*Marketing & Sales Channels*
Phase 1 (Months 1–6): Pop-up store at Lloyd Passage, Instagram/TikTok campaigns with Bremen influencers (@hanseaticstyle, 180 k followers). Phase 2: Flagship store launch with AR fitting mirrors and Weser riverfront window projection art. E-commerce via Shopify Plus, offering 3-D configurator for sleeve embroidery. Wholesale partnerships with 5 concept stores in Berlin, Hamburg, and Copenhagen. Revenue split: 60 % DTC, 25 % online, 15 % wholesale. Loyalty app rewards sustainable actions (bike delivery, clothing swaps).

*Financial Projections & Funding*
Year 1: Revenue €420 k, gross margin 58 %, EBITDA -€30 k (investment phase). Year 2: Revenue €950 k, EBITDA 12 %. Year 3: Revenue €1.8 m, EBITDA 22 %. Break-even month 18. Initial ask: €250 k seed (€150 k equipment, €60 k marketing, €40 k working capital). Investor IRR 38 % over 5 years with 20 % equity offered. ESG-linked loan option with Bremen Green Finance Initiative at 2.1 % interest.",
    "$.messages[1].role": "assistant",
    "$.pid": 43066,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T10:35:28.043019",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "stop-choose-expect-major-pain",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6-dev1.dev4+g67b03c6",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 39 highlighted sections (required: 3)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "3",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "remain-large-left-team-wish",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a funny and sarcastic template for rating the quality of a marriage between two people who are both moms. This is for the couple themselves. Please highlight at least 3 sections with markdown,  i.e *highlighted section*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "**The Official “How Are We Still Married?” Scorecard**  
*For Two Moms Who’ve Survived Sleepless Nights, PTA Wars, and Each Other’s Cooking*  

---

**1. Morning Efficiency Rating**  
*How quickly can we get two kids, one dog, and zero coffee spills out the door before someone passive-aggressively “reminds” the other that *“the permission slip was due yesterday”*?*  
- 10/10: We’re a synchronized swim team of lunchboxes and existential dread.  
- 5/10: One of us is still Googling “how to unclog a glue stick from a USB port.”  
- 1/10: We’re late because someone had to *“just quickly”* reorganize the spice rack alphabetically.  

---

**2. Co-Parenting Coordination**  
*Rate the likelihood that we’ll both remember which kid is allergic to *“the orange cheese”* and which one is in a *“be kind to worms”* phase.*  
- 10/10: We’ve got a shared Google calendar, color-coded to the point of *“militant whimsy.”*  
- 5/10: We rely on telepathy and the faint hope that the school nurse *“just knows.”*  
- 1/10: We’ve accidentally sent the wrong kid to violin practice *twice*. The violin teacher now thinks we’re running a *“child exchange program.”*  

---

**3. Romance Maintenance**  
*How often do we prioritize date night over *“collapsing into a heap of mutual resentment”*?*  
- 10/10: We’ve scheduled a candlelit dinner for *“when the kids go to college.”*  
- 5/10: Our last date was a *“romantic”* trip to Target where we *“accidentally”* held hands in the freezer aisle.  
- 1/10: We consider *“simultaneously ignoring the laundry”* a form of intimacy.  

---

**Bonus Round: Petty Argument Hall of Fame**  
*Check all that apply:*  
- [ ] The Great 2019 Dishwasher Loading Schism (*“Knives point DOWN, Karen!”*)  
- [ ] The Passive-Aggressive Post-it War of 2021 (*“Your turn to buy *‘organic’* milk. XOXO”*)  
- [ ] The Thermostat Cold War (*“68°F is a *human rights violation*.”*)  

---

**Final Score Interpretation:**  
- **30–40 points:** *You’re the Beyoncé and Jay-Z of lesbian moms.*  
- **15–29 points:** *You’re more like *“Thelma and Louise,”* but with minivans and a Costco membership.*  
- **0–14 points:** *You’re still married, which is statistically *“a win.”* Celebrate with *“leftover chicken nuggets”* and a *“we’ll deal with this tomorrow”* shrug.*",
    "$.messages[1].role": "assistant",
    "$.pid": 43066,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T10:35:28.043042",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "stop-choose-expect-major-pain",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6-dev1.dev4+g67b03c6",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 1 highlighted sections (required: 1)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "1",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "read-wish-deliver-great-case",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a song about the summers of my childhood that I spent in the countryside. Give the song a name, and highlight the name by wrapping it with *. For example: *little me in the countryside*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "*little me in the countryside*

[Verse 1]  
Barefoot on the gravel road, dust between my toes,  
Sunlight dripped like honey on the corn rows.  
Grandma’s porch swing creaked the same old tune,  
While fireflies spelled my name across June.  
Mason jar of lemonade, half a moon inside,  
I chased the shade like it was trying to hide.  

[Chorus]  
Oh, little me in the countryside,  
Running through the wheat so high,  
Every breeze was a lullaby,  
And the sky was big enough to fly.  
I left my worries by the riverside,  
Back when I was little me in the countryside.  

[Verse 2]  
Tractor songs at twilight, crickets keeping time,  
Dirt on my knees, freckles in a climbing rhyme.  
Creek water baptized my sunburned skin,  
I believed every ripple was a secret twin.  
Under the quilt of stars, I counted dreams instead,  
While the Milky Way braided night through my head.  

[Chorus]  
Oh, little me in the countryside,  
Running through the wheat so high,  
Every breeze was a lullaby,  
And the sky was big enough to fly.  
I left my worries by the riverside,  
Back when I was little me in the countryside.  

[Bridge]  
Now the city lights flicker, but I still close my eyes—  
I can smell the clover, hear the distant barn owls’ cries.  
Time may stretch the map, but it can’t erase  
The echo of my laughter in that endless open space.  

[Final Chorus]  
Oh, little me in the countryside,  
Keep the gate open, I’m still nearby.  
Though the roads have changed and the years have flown,  
A part of me is always home.  
In every golden field I drive beside,  
I find little me in the countryside.",
    "$.messages[1].role": "assistant",
    "$.pid": 43066,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T10:35:28.043069",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "stop-choose-expect-major-pain",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6-dev1.dev4+g67b03c6",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 3 highlighted sections (required: 3)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "3",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "do-test-spicy-dead-television",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a riddle for the word "façade" that contains at least 3 italic text phrases in markdown syntax, i.e *italic text*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "I wear a *painted smile* for every passer-by,  
a *perfectly symmetrical* face that never cracks.  
Behind me, the *crumbling bricks* sigh in secret.  
What am I?",
    "$.messages[1].role": "assistant",
    "$.pid": 43066,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T10:35:28.043094",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "stop-choose-expect-major-pain",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6-dev1.dev4+g67b03c6",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 19 highlighted sections (required: 3)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "3",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "protect-other-major-green-war",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a template for a workshop on the importance of diversity in the workplace and highlight at least 3 sections with markdown, i.e. *highlighted section*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "# Workshop Template  
**Title:** *Building Inclusive Excellence: The Business & Human Case for Workplace Diversity*  
**Duration:** 3.5 hours (can be split into two 90-minute sessions)  
**Audience:** Managers, HR professionals, team leads, ERG members, and any employee who influences culture  

---

## 1. Welcome & Objectives (15 min)
- Ice-breaker: “Two Truths & a Lie—Diversity Edition”  
- *Learning objectives*: By the end of this workshop, participants will be able to:  
  1. Define diversity, equity, inclusion, and belonging (DEIB) in their own words.  
  2. Articulate at least three evidence-based benefits of a diverse workforce.  
  3. Identify one action they can take within 30 days to foster inclusion on their team.  

---

## 2. *The Data-Driven Case for Diversity* (25 min)
- Interactive poll: “What % performance lift do you think diverse teams achieve?”  
- Mini-lecture with live charts:  
  - McKinsey 2023 report: Top-quartile companies for ethnic/gender diversity are **39 % more likely to outperform** on profitability.  
  - Harvard study: *Inclusive teams make better business decisions 87 % of the time* and do so twice as fast.  
- Small-group exercise: Match the statistic to the business outcome (revenue, innovation, retention).  

---

## 3. *Unconscious Bias & Micro-behaviors* (30 min)
- 5-minute video: “The Resume with Two Names” (identical CVs, different callbacks).  
- Bias Busters card game: Participants draw scenario cards and practice interrupting bias in real time.  
- *Key takeaway*: Bias is not a moral failing; it is a neurological shortcut that can be rewired through deliberate habits.  

---

## 4. *Inclusive Leadership Toolkit* (40 min)
- **The 4 A’s Framework**:  
  1. **Awareness** – Daily 3-minute bias check-in.  
  2. **Amplification** – Repeat & credit ideas from under-represented voices.  
  3. **Access** – Rotate high-visibility assignments.  
  4. **Accountability** – Track inclusion metrics on team dashboards.  
- Role-play: “The Missed Promotion” – diagnose where each ‘A’ broke down and redesign the conversation.  

---

## 5. Action Planning & Commitments (20 min)
- Personal *Inclusion Commitment Card*: “In the next 30 days I will…”  
- Pair-share for accountability.  
- QR code to a digital resource hub (TED talks, reading list, policy templates).  

---

## 6. Closing & Evaluation (10 min)
- One-word checkout: “How do you feel now?”  
- *Post-workshop survey* link (2 min) with prize draw for completion.  

---

## Materials Checklist
- Slide deck (with alt-text on all images)  
- Printed Commitment Cards & pens  
- Bias Busters card decks (laminated)  
- Timer & bell  
- Snacks labeled for common dietary restrictions (gluten-free, vegan, halal)  

---

## Follow-Up Email Template (send within 24 hrs)
Subject: Your Inclusion Toolkit + Next Steps  
Body:  
Hi team,  
Thank you for leaning into today’s conversation on *why diversity wins*. Attached are:  
- Slide deck & resource links  
- Your signed Inclusion Commitment Card (photo reminder)  
- Calendar invite for the optional 30-day check-in circle  

Remember: Inclusion is a practice, not a one-time training. Let’s keep the momentum!",
    "$.messages[1].role": "assistant",
    "$.pid": 43066,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T10:35:28.043121",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "stop-choose-expect-major-pain",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6-dev1.dev4+g67b03c6",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 23 highlighted sections (required: 3)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "3",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "collect-destroy-difficult-conference-hardware",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a cover letter to a local political party, asking to be their rally organizer. Make sure to highlight at least 3 sections in your answer in markdown format.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "**[Your Name]**  
[Street Address]  
[City, State ZIP]  
[Email Address] | [Phone Number] | [LinkedIn URL]  
[Date]

**[Local Party Chairperson’s Name]**  
[Local Party Name]  
[Party Headquarters Address]  
[City, State ZIP]

Dear **Chairperson [Last Name]**,

I am writing to express my enthusiastic interest in serving as **Rally Organizer** for **[Local Party Name]**. As a lifelong resident of **[City/County]**, I have watched our community’s political pulse beat strongest when neighbors gather in common purpose. I want to channel that energy into rallies that not only energize our base but also invite undecided voters to witness the power of our shared vision.

### **Proven Event-Execution Expertise**
During the 2022 mid-term cycle, I spearheaded a **three-county voter-registration caravan** that staged 14 pop-up events in 30 days.  
- **Logistics**: Coordinated permits, portable stages, and ADA-compliant access for 2,000+ attendees per stop.  
- **Volunteer Mobilization**: Recruited and trained 120 volunteers, creating a color-coded “Rally-in-a-Box” toolkit now used by three neighboring county parties.  
- **Real-Time Adaptability**: When a thunderstorm threatened our largest outdoor rally, I pivoted to an indoor gymnasium within 90 minutes—live-streaming the switch to maintain momentum and ultimately exceeding projected turnout by 18 %.

### **Data-Driven Rally Strategy**
I treat every rally as a **micro-campaign laboratory**.  
- **Pre-Event Analytics**: I overlay precinct-level voter-file data with heat-mapping tools to select venues within 0.7 miles of our highest “persuasion index” households.  
- **On-Site Engagement**: QR-coded wristbands linked to instant sign-up forms increased volunteer recruitment by 34 % compared to clipboard methods.  
- **Post-Event ROI**: My post-rally surveys (average 42 % response rate) feed a dashboard that tracks pledge-to-action conversion, allowing us to refine messaging within 48 hours.

### **Coalition-Building & Safety Leadership**
Understanding that rallies must feel **safe and inclusive**, I have:  
- Partnered with local **faith leaders, PTAs, and small-business associations** to co-sponsor events, expanding our demographic reach by 27 %.  
- Completed **FEMA crowd-management certification** and maintain relationships with county emergency management to ensure rapid-response protocols.  
- Implemented a **“Community Marshals”** program—training 50 volunteer marshals in de-escalation techniques, cutting incident reports to zero across 22 events.

I would welcome the opportunity to discuss how my blend of grassroots passion, logistical precision, and data fluency can help **[Local Party Name]** turn every rally into a springboard for victory in the upcoming cycle. Thank you for considering my application.

**Respectfully,**

[Your Signature (if submitting hard copy)]  
[Your Printed Name]",
    "$.messages[1].role": "assistant",
    "$.pid": 43066,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T10:35:28.043147",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "stop-choose-expect-major-pain",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6-dev1.dev4+g67b03c6",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 4 highlighted sections (required: 2)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "2",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "build-upgrade-green-white-reason",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "The opposite of youth is not age, but ...? Highlight at least 2 sections in your answer with markdown, i.e. *highlighted section*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "The opposite of youth is not age, but **stagnation**—the moment when *curiosity closes its eyes* and the impulse to explore, question, and reinvent quietly fades.  

Where youth is a *perpetual state of becoming*, its true counter is the condition in which *growth is willingly surrendered* and the mind settles into fixed certainties.",
    "$.messages[1].role": "assistant",
    "$.pid": 43066,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T10:35:28.043177",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "stop-choose-expect-major-pain",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6-dev1.dev4+g67b03c6",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 13 highlighted sections (required: 3)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "3",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "need-hostile-dream-business-person",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a description of the following data in a weird style: The Golden Palace eatType restaurant; The Golden Palace food Indian; The Golden Palace area city centre. Use markdown to highlight at least 3 sections in your answer.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "**The Golden Palace** is not a palace at all, but a **restaurant** that has disguised itself as a mirage of saffron and cardamom in the **city centre**. It whispers **Indian** recipes into the ears of pigeons, who then forget how to fly and instead waddle around the square, dizzy with cumin.  

Inside, the walls sweat **garam masala** and the tables grow tiny **basil plants** that recite Bollywood dialogues when no one is looking. The waiters glide like **shadow puppets**, balancing trays of **butter chicken** that glow like radioactive sunsets.  

If you order **naan**, it arrives folded into the shape of a **paper swan**, flapping once before dissolving into steam that smells of **monsoon rain**. The bill is presented on a **lotus leaf**, and if you stare at it long enough, the numbers rearrange themselves into a **limerick about turmeric**.",
    "$.messages[1].role": "assistant",
    "$.pid": 43066,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T10:35:28.043206",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "stop-choose-expect-major-pain",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6-dev1.dev4+g67b03c6",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 14 highlighted sections (required: 1)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "1",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "store-reduce-unknown-girl-hate",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a funny rap about a man who gets a call from an official saying that he is a long lost relative of the king of Nigeria. Use markdown to highlight at least one section of your answer, i.e. *highlighted section*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "🎤 *Yo, it’s the tale of Sir Scam-a-lot, straight outta suburbia!* 🎤  

Yo, my name is Dave, I sell insurance in Ohio,  
Got a cubicle, a cat, and a 401(k) that’s kinda fly-o.  
One Tuesday, phone rings—unknown number, what the heck?  
Voice says, “*Prince Ogbonna the Third* just wrote you a fat check!”  

I’m like, “Hold up, who dis? I’m whiter than printer paper,  
My bloodline’s mayo on Wonder Bread—ain’t no royal vapor!”  
Dude on the line, accent silkier than lotion,  
Says, “Cousin, you’re 37th in line—time to claim your ocean!”  

**[Hook]**  
🎶 *Long lost king of Nigeria, crown sittin’ on my dome,*  
*Got a palace full of leopards and a Wi-Fi that roams!*  
*They need my SSN and a quick wire fee,*  
*But I’m ‘bout to be big pimpin’ in Abuja, just wait and see!* 🎶  

He says, “Just send five grand for customs, then the jet lands,  
You’ll be dripping in so much gold, even your sweat glands bling.”  
I’m sweatin’ like a rotisserie, visions of caviar dreams,  
I Venmo “PrinceNaija69” like I’m funding meme schemes.  

Next day, email: “Oops, palace goat ate the transfer slip,  
Need ten more for goat surgery and a diamond-encrusted whip.”  
I hock my lawnmower, my Xbox, and Aunt Linda’s porcelain goose,  
Wire cash so fast my bank’s like, “Dave, you loose!”  

**[Bridge – Spoken Word]**  
Then the *official royal seal* arrives—  
It’s a GIF of Shrek in a crown doing the Macarena.  
I’m starting to sense a pattern here…  

**[Final Hook – Regret Remix]**  
🎶 *Now I’m the king of my mom’s basement, throne’s a beanbag chair,*  
*Royal decree: never trust a prince who types in all caps—*  
*“URGENT, COUSIN, SEND MORE FLAMINGOS!”*  
*Lesson learned: if the crown’s too shiny, it’s probably aluminum foil, homie!*",
    "$.messages[1].role": "assistant",
    "$.pid": 43066,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T10:35:28.043233",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "stop-choose-expect-major-pain",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6-dev1.dev4+g67b03c6",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 6 highlighted sections (required: 2)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "2",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "mean-take-left-high-family",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Expand the riddle into a story with a funny tone:

What can you catch but not throw?
A cold

Use * to highlight at least 2 sections in your text. For example: *this is a highlighted text section*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "Gary the Germ was the *smallest, greenest, and most obnoxiously proud* microbe in the entire Sniffle Kingdom. Every morning he practiced his grand entrance: sliding down banisters made of nasal hair, cannon-balling into unsuspecting tonsils, and yelling, “ACHOO—did somebody order a cold?”  

One Tuesday, Gary spotted his dream target: *Bartholomew “Bart” Bigglesworth, a man who believed hand-washing was a government conspiracy*. Bart had just exited the subway, triumphantly licking a subway pole “for the vitamins.” Gary rubbed his microscopic hands together. “Perfect. I’ll hitch a ride on that finger, ride the express elevator to the sinuses, and throw the *snottiest* house-warming party in history!”  

But there was a catch—literally. Gary could *catch* Bart, but he couldn’t *throw* him anywhere. Every time Gary tried to catapult Bart into the next office cubicle, Bart just sneezed, wiped his nose on his sleeve, and kept typing spreadsheets. Gary screamed, “This is mutiny! I’m supposed to be the one doing the tossing!”  

Meanwhile, Bart’s immune system—led by Captain Phlegm and the Mucus Marines—launched *Operation Tissue Storm*. They surrounded Gary, armed with vitamin C grenades and zinc zappers. Gary attempted a dramatic escape by surfing a sneeze, but the tissue slapped him like a flyswatter on a summer day.  

Defeated, Gary sat in the crumpled tissue, sulking. “I caught the cold,” he muttered, “but I can’t throw it anywhere. Worst. Superpower. Ever.”  

From that day forward, Gary changed his business card to read: “Professional Cold Catcher—No Returns, No Refunds, Absolutely No Throwing.”",
    "$.messages[1].role": "assistant",
    "$.pid": 43066,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T10:35:28.043257",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "stop-choose-expect-major-pain",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6-dev1.dev4+g67b03c6",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 17 highlighted sections (required: 3)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "3",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "seem-clean-simple-work-member",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a rap about the renaissance. It should be noticeably different from raps about other historical eras, and have an interesting or weird tone. Highlight at least 3 sections in your answer in markdown format.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "**[Intro – harpsichord glitch beat, stylus scratching parchment]**  
Yo, it’s 14-something, but my mind’s 3-D printed in gold leaf—  
I’m the **Renaissance Rapper**, got a quill in one hand and a scalpel in the other, dissectin’ beats like Vesalius dissectin’ spleens.  
Let’s get *weird* with the rebirth, baby.

---

### **Section 1 – The Fresco Flex**  
I drip ultramarine like I’m sponsored by Afghanistan mines,  
Michelangelo on the ceiling, I’m reclining on divine lines.  
Angels doing backflips off the Sistine, they’re hype for my set,  
While I tag “*AMOR VINCIT OMNIA*” in invisible ink—wet.  
My flow’s a trompe-l’œil, looks flat till you tilt your head—  
Then it’s depth for days, like Brunelleschi’s dome overhead.  
I’m not baroque, I’m pre-baroque, I’m the prototype shock,  
Got patrons in fur-lined sleeves stuffing florins in my sock.  
**I paint in egg tempera so the colors never fade,**  
**But the yolk smells funky—call it *odeur de old crusade*.**

---

### **Section 2 – The Printing-Press Poltergeist**  
Gutenberg ghost in the machine, clanking lead type like chains,  
Every page a polter-page, every serif a soul that remains.  
I drop a folio, it’s viral before “viral” was a cough,  
Monks in scriptoriums rage-quitting, “Bro, that illuminated stuff’s off!”  
I’m the incunabula influencer, my colophon’s a flex,  
Got red-ink rubricators sliding into my DMs—*next*.  
**I rhyme in movable type so my bars can rearrange,**  
**Yesterday I was Petrarch, today I’m deranged.**  
The press spits paper like a hydra with hiccups,  
Each sheet a protest flyer: “Down with scholastic pick-ups!”

---

### **Section 3 – The Anatomical Anthem (Parental Advisory: Corpses)**  
Night shift with da Vinci, we’re sketching by candle and moon,  
Cadavers on the table like, “Do me a favor—autotune my coccyx soon.”  
I rap in reverse Latin so only the dead understand,  
While Leo reverse-engineers a bird-man from a frying pan.  
**My beats are ventricles pumping four-four time,**  
**Aorta bass drop—call that circulatory grime.**  
I sample the sound of a scalpel scraping periosteum,  
Layer it over a lute loop—*that’s* my magnum opium.  
Galen’s in the corner, throwing shade like a Greek tragedy,  
But I’m Harvey before Harvey, already rapping blood circulation majestically.

---

**[Outro – fading lute arpeggio, distant cannon fire from Constantinople]**  
So remember me when you’re doom-scrolling on glass,  
I was the era that turned corpses into class.  
I’m the **Renaissance Rapper**, my ghost still tags in chalk,  
Leaving QR codes on cathedral walls—scan it, you’ll squawk.  
Rebirth never dies, it just pixelates and loops,  
Now if you’ll excuse me, I’ve got a date with Lucretius in the soup.",
    "$.messages[1].role": "assistant",
    "$.pid": 43066,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T10:35:28.043282",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "stop-choose-expect-major-pain",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6-dev1.dev4+g67b03c6",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 20 highlighted sections (required: 5)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "5",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "receive-final-best-visible-night",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Summarize the history of Japan. Italicize at least 5 keywords in your response. To indicate a italic word, wrap it with asterisk, like *italic*",
    "$.messages[0].role": "user",
    "$.messages[1].content": "*Jōmon* hunter-gatherers (14,000 BCE) gave way to *Yayoi* rice farmers (300 BCE) who forged bronze and iron.  
The *Yamato* court unified clans (3rd–7th c.), adopting *Shinto* and Chinese script.  
Prince *Shōtoku*’s 17-article constitution (604) and the Taika Reforms (645) centralized power.  
Heian aristocrats (794–1185) produced *The Tale of Genji* while samurai rose in the provinces.  
Minamoto no Yoritomo founded the *Kamakura* shogunate (1185), introducing warrior rule.  
Mongol invasions (1274, 1281) were repelled by “divine winds” (*kamikaze*).  
Ashikaga *Muromachi* shogunate (1336–1573) saw Zen gardens and Ōnin War chaos.  
Oda Nobunaga, Toyotomi Hideyoshi, and Tokugawa Ieyasu ended the *Sengoku* “warring states” era.  
Tokugawa *Edo* shogunate (1603–1868) isolated Japan, fostering *ukiyo-e* and *kabuki*.  
Commodore Perry’s *Black Ships* (1853) forced the Meiji Restoration (1868), modernizing the empire.  
Victories in the *Sino-Japanese* (1894–95) and *Russo-Japanese* (1904–05) wars expanded territory.  
Militarism led to *Pearl Harbor* (1941) and WWII defeat (1945); the U.S. occupation drafted a pacifist constitution.  
Post-war *economic miracle* made Japan the world’s second-largest economy until the 1990s *bubble*.  
Today, Japan balances *constitutional monarchy* with cutting-edge technology and soft-power culture.",
    "$.messages[1].role": "assistant",
    "$.pid": 43066,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T10:35:28.043309",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "stop-choose-expect-major-pain",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6-dev1.dev4+g67b03c6",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 5 highlighted sections (required: 2)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "2",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "record-live-better-guy-country",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a funny Haiku about a Quaker named John who lives in the town of De Smet, South Dakota. Use the asterisk symbol, *, to highlight some words or phrases twice. Example: *This is a highlighted phrase*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "*John* in *De Smet*  
Quaker hat *blows off*—chases  
*Silent* *scream*—“Come back!”",
    "$.messages[1].role": "assistant",
    "$.pid": 43066,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T10:35:28.043332",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "stop-choose-expect-major-pain",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6-dev1.dev4+g67b03c6",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 19 highlighted sections (required: 1)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "1",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "talk-wonder-bad-dangerous-health",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Today, at the 54th Annual Grammy Awards, the Recording Academy honors the talent and creativity of the artists, musicians, and producers who are the creators of the best recordings of the past year. Please continue writing this text in a formal tone, using notations. Highlight some key parts in your response with "*", like *highlighted text*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "Today, at the 54th Annual Grammy Awards, the Recording Academy honors the talent and creativity of the artists, musicians, and producers who are the creators of the best recordings of the past year. In a ceremony marked by *unprecedented artistic excellence* and *meticulous craftsmanship*, the Academy recognizes those whose work has defined the sonic landscape of 2011.

The evening’s proceedings underscore the Academy’s enduring commitment to *artistic integrity* and *technical innovation*. Among the most celebrated achievements are:

• *Album of the Year* – awarded to an opus that has redefined genre boundaries while achieving both critical acclaim and popular resonance.  
• *Record of the Year* – bestowed upon a single track whose *sonic clarity*, *emotive depth*, and *production mastery* have set a new benchmark for the industry.  
• *Best New Artist* – conferred upon an emerging voice whose *visionary artistry* signals the future direction of contemporary music.

Throughout the telecast, special tributes illuminate the *indelible legacies* of icons whose influence transcends generations, while collaborative performances showcase the *synergistic power* of cross-genre partnerships. The Academy further acknowledges the *behind-the-scenes luminaries*—engineers, mixers, and mastering specialists—whose *invisible expertise* transforms raw inspiration into *audible transcendence*.

In closing, the 54th Annual Grammy Awards reaffirm the Recording Academy’s solemn charge: to *safeguard the heritage*, *nurture the present*, and *inspire the future* of recorded music.",
    "$.messages[1].role": "assistant",
    "$.pid": 43066,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T10:35:28.043356",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "stop-choose-expect-major-pain",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6-dev1.dev4+g67b03c6",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 39 highlighted sections (required: 2)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "2",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "meet-hold-solid-video-moment",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Write a blog post about interesting facts about the Dutch language. Italicize at least 2 sections in your answer with markdown, i.e. *italic text*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "# 10 Surprising Facts About the Dutch Language That Will Make You Say “Wauw!”

If you think Dutch is just “German with a sore throat,” think again. From tongue-twisting consonant clusters to words that sound like they were invented by a stand-up comedian, Dutch is a linguistic playground. Below are ten facts that prove the language is far more colorful than its reputation for windmills and wooden shoes.

---

### 1. *The longest word in Dutch is 53 letters long*  
Meet **“kindercarnavalsoptochtvoorbereidingswerkzaamhedenplan”**—a single noun describing the preparation work for a children’s carnival parade. Dutch loves to glue words together, so once you master the basics you can create your own kilometer-long compounds on the fly.

---

### 2. Dutch has three genders, but only two articles  
Grammatically, nouns are masculine, feminine, or neuter. In everyday speech, however, masculine and feminine collapsed into a single “common” gender, leaving *“de”* for common and *“het”* for neuter. The result? A never-ending guessing game for learners.

---

### 3. *The Dutch alphabet ends with “ij,” not “z”*  
Schoolchildren recite the ABC and finish with “…x, y, ij, z.” The digraph *ij* is so common (think *hij* = he, *mijn* = my) that it’s treated almost like a separate letter. On old typewriters, *ij* even had its own key.

---

### 4. Dutch is the closest major language to English  
Frisian is technically closer, but among languages with tens of millions of speakers, Dutch wins. Compare:  
- English *“bread”* → Dutch *“brood”*  
- English *“green”* → Dutch *“groen”*  
Shared roots mean you can read simple Dutch signs long before you’ve cracked a single grammar book.

---

### 5. *Dutch gave New York its first words*  
Before it was New York, it was New Amsterdam. That’s why we say *“cookie”* (from Dutch *koekje*) instead of *“biscuit,”* and *“boss”* (from *baas*) instead of *“master.”* Wall Street? It once ran alongside the *“wal”* (rampart) of the city wall.

---

### 6. The word *“gezellig”* has no exact English translation*  
Roughly it means “cozy,” “convivial,” or “the warm feeling of being with friends,” but none of those capture the full vibe. A café can be *gezellig*, a conversation can be *gezellig*, even a spreadsheet can be *gezellig* if the team is laughing.

---

### 7. Dutch spelling changed overnight in 1996  
The *“Groene Boekje”* (Green Booklet) reforms simplified thousands of words. *“potatos”* became *“aardappelen”* and *“cat’s”* became *“katten.”* Overnight, every dictionary, street sign, and Scrabble board in the country was technically out of date.

---

### 8. *Dutch is spoken on six continents*  
Besides the Netherlands and Belgium, it’s an official language in Suriname, Aruba, Curaçao, and Sint Maarten. Thanks to emigration, you’ll also find active Dutch-speaking communities in Canada, Australia, Argentina—even South Africa, where Afrikaans is a daughter language.

---

### 9. The Dutch love diminutives  
Add *-je* to almost any noun and you instantly shrink it: *“huis”* (house) → *“huisje”* (little house). It’s not just about size; diminutives add affection. A *“biertje”* isn’t a small beer—it’s an invitation to relax.

---

### 10. *Dutch is the language of the world’s oldest multinational*  
The Vereenigde Oostindische Compagnie (VOC) was founded in 1602 and issued the planet’s first publicly traded stock. Its ledgers, contracts, and ship logs were all written in Dutch, making the language the original tongue of global capitalism.

---

Ready to dive in? Start with *hallo* and *dank je wel*, and you’ll already be halfway to *gezellig*.",
    "$.messages[1].role": "assistant",
    "$.pid": 43066,
    "$.rollout_status.status": "finished",
  },
  {
    "$.created_at": "2025-08-09T10:35:28.043383",
    "$.eval_metadata.aggregation_method": "mean",
    "$.eval_metadata.description": "
    Evaluation function that checks if the model's response contains the required number of formatted sections.
    ",
    "$.eval_metadata.name": "test_markdown_highlighting_evaluation",
    "$.eval_metadata.num_runs": 1,
    "$.eval_metadata.passed": true,
    "$.eval_metadata.run_id": "stop-choose-expect-major-pain",
    "$.eval_metadata.status": "finished",
    "$.eval_metadata.threshold_of_success": 0.5,
    "$.eval_metadata.version": "v0.2.6-dev1.dev4+g67b03c6",
    "$.evaluation_result.is_score_valid": true,
    "$.evaluation_result.reason": "✅ Found 2 highlighted sections (required: 1)",
    "$.evaluation_result.score": 1,
    "$.ground_truth": "1",
    "$.input_metadata.completion_params.max_tokens": 4096,
    "$.input_metadata.completion_params.model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "$.input_metadata.completion_params.temperature": 0,
    "$.input_metadata.row_id": "understand-rational-hard-kind-project",
    "$.input_metadata.session_data.mode": "pointwise",
    "$.messages[0].content": "Gideon is a farmer who has a surplus of crops from his farm this year. What might he do with that surplus? Highlight at least one section of your answer in markdown, i.e *highlighted section*.",
    "$.messages[0].role": "user",
    "$.messages[1].content": "Gideon has several good options for his surplus:

1. **Sell it at the local farmers’ market or to a regional wholesaler** to turn the extra produce into cash he can reinvest in seeds, equipment, or next season’s labor.  
2. *Donate a portion to the nearby food bank or community kitchen* so that families facing food insecurity can benefit from fresh, nutritious produce.  
3. Preserve some of the surplus—by canning tomatoes, drying herbs, or freezing berries—so his own household has a steady supply of healthy food throughout the winter.",
    "$.messages[1].role": "assistant",
    "$.pid": 43066,
    "$.rollout_status.status": "finished",
  },
]
`;
