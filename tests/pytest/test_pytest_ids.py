import eval_protocol.pytest.evaluation_test as evaluation_test_module
from eval_protocol.models import EvaluationRow
from eval_protocol.pytest.default_no_op_rollout_process import default_no_op_rollout_processor
from eval_protocol.pytest.evaluation_test import evaluation_test as evaluation_decorator
from tests.pytest.test_markdown_highlighting import markdown_dataset_to_evaluation_row


class InMemoryLogger:
    def __init__(self):
        self._rows = []

    def log(self, row):
        self._rows.append(row)

    def read(self):
        return list(self._rows)


def test_evaluation_test_decorator_ids_single(monkeypatch):
    # Use an in-memory logger to avoid sqlite side effects
    in_memory_logger = InMemoryLogger()
    monkeypatch.setattr(evaluation_test_module, "default_logger", in_memory_logger, raising=False)

    unique_run_ids = set()
    unique_cohort_ids = set()
    unique_rollout_ids = set()
    unique_invocation_ids = set()
    unique_row_ids = set()

    @evaluation_decorator(
        input_dataset=[
            "tests/pytest/data/markdown_dataset.jsonl",
            "tests/pytest/data/markdown_dataset.jsonl",
        ],
        rollout_input_params=[{"temperature": 0.0}, {"temperature": 1.0}],
        model=["dummy/local-model"],
        dataset_adapter=markdown_dataset_to_evaluation_row,
        rollout_processor=default_no_op_rollout_processor,
        mode="pointwise",
        combine_datasets=False,
        num_runs=5,
    )
    def eval_fn(row: EvaluationRow) -> EvaluationRow:
        unique_run_ids.add(row.run_id)
        unique_cohort_ids.add(row.cohort_id)
        unique_rollout_ids.add(row.rollout_id)
        unique_invocation_ids.add(row.invocation_id)
        unique_row_ids.add(row.input_metadata.row_id)
        return row

    dataset_paths = [
        "tests/pytest/data/markdown_dataset.jsonl",
        "tests/pytest/data/markdown_dataset.jsonl",
    ]
    input_params_list = [{"temperature": 0.0}, {"temperature": 1.0}]

    # Manually invoke all parameter combinations within a single test
    for ds_path in dataset_paths:
        for params in input_params_list:
            eval_fn(model="dummy/local-model", dataset_path=[ds_path], input_params=params)

    # Assertions on IDs generated by the decorator logic
    assert len(unique_invocation_ids) == 1
    assert len(unique_run_ids) == 20  # 4 combinations * 5 runs each
    assert len(unique_cohort_ids) == 2 * 2  # 2 datasets * 2 param sets
    assert len(unique_row_ids) == 19  # from the markdown dataset
    assert len(unique_rollout_ids) == 19 * 5 * 2 * 2  # rows * runs * datasets * params
