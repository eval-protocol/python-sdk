from typing import List

import eval_protocol.dataset_logger as dataset_logger
from eval_protocol.dataset_logger.dataset_logger import DatasetLogger
from eval_protocol.models import EvaluationRow
from eval_protocol.pytest.default_no_op_rollout_process import default_no_op_rollout_processor
from tests.pytest.test_markdown_highlighting import markdown_dataset_to_evaluation_row


class InMemoryLogger(DatasetLogger):
    def __init__(self):
        self._rows: dict[str, EvaluationRow] = {}

    def log(self, row: EvaluationRow):
        print(row.run_id, row.rollout_id)
        self._rows[row.rollout_id] = row

    def read(self):
        return list(self._rows.values())


def test_evaluation_test_decorator(monkeypatch):
    from eval_protocol.pytest.evaluation_test import evaluation_test

    logger = InMemoryLogger()

    @evaluation_test(
        input_dataset=[
            "tests/pytest/data/markdown_dataset.jsonl",
        ],
        model=["dummy/local-model"],
        dataset_adapter=markdown_dataset_to_evaluation_row,
        rollout_processor=default_no_op_rollout_processor,
        mode="pointwise",
        combine_datasets=False,
        num_runs=2,
        logger=logger,
    )
    def eval_fn(row: EvaluationRow) -> EvaluationRow:
        return row

    dataset_paths = [
        "tests/pytest/data/markdown_dataset.jsonl",
    ]

    # Manually invoke all parameter combinations within a single test
    for ds_path in dataset_paths:
        eval_fn(model="dummy/local-model", dataset_path=[ds_path])

    # Assertions on IDs generated by the decorator logic
    assert len(logger.read()) == 38


def test_evaluation_test_decorator_ids_single(monkeypatch):
    in_memory_logger = InMemoryLogger()
    unique_run_ids = set()
    unique_cohort_ids = set()
    unique_rollout_ids = set()
    unique_invocation_ids = set()
    unique_row_ids = set()

    from eval_protocol.pytest.evaluation_test import evaluation_test

    @evaluation_test(
        input_dataset=[
            "tests/pytest/data/markdown_dataset.jsonl",
            "tests/pytest/data/markdown_dataset.jsonl",
        ],
        rollout_input_params=[{"temperature": 0.0}, {"temperature": 1.0}],
        model=["dummy/local-model"],
        dataset_adapter=markdown_dataset_to_evaluation_row,
        rollout_processor=default_no_op_rollout_processor,
        mode="pointwise",
        combine_datasets=False,
        num_runs=5,
        logger=InMemoryLogger(),
    )
    def eval_fn(row: EvaluationRow) -> EvaluationRow:
        unique_run_ids.add(row.run_id)
        unique_cohort_ids.add(row.cohort_id)
        unique_rollout_ids.add(row.rollout_id)
        unique_invocation_ids.add(row.invocation_id)
        unique_row_ids.add(row.input_metadata.row_id)
        return row

    dataset_paths = [
        "tests/pytest/data/markdown_dataset.jsonl",
        "tests/pytest/data/markdown_dataset.jsonl",
    ]
    input_params_list = [{"temperature": 0.0}, {"temperature": 1.0}]

    # Manually invoke all parameter combinations within a single test
    for ds_path in dataset_paths:
        for params in input_params_list:
            eval_fn(model="dummy/local-model", dataset_path=[ds_path], input_params=params)

    # Assertions on IDs generated by the decorator logic
    assert len(unique_invocation_ids) == 1
    assert len(unique_run_ids) == 20  # 4 combinations * 5 runs each
    assert len(unique_cohort_ids) == 2 * 2  # 2 datasets * 2 param sets
    assert len(unique_row_ids) == 19  # from the markdown dataset
    assert len(unique_rollout_ids) == 19 * 5 * 2 * 2  # rows * runs * datasets * params
