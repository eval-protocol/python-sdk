from typing import List

from eval_protocol.models import EvaluationRow
from eval_protocol.pytest.default_no_op_rollout_processor import NoOpRolloutProcessor
from tests.pytest.test_markdown_highlighting import markdown_dataset_to_evaluation_row


async def test_evaluation_test_decorator_ids_single():
    from eval_protocol.pytest.evaluation_test import evaluation_test

    row_ids = set()

    input_dataset = [
        "tests/pytest/data/markdown_dataset.jsonl",
        "tests/pytest/data/markdown_dataset.jsonl",
    ]
    completion_params_list = [
        {"temperature": 0.0, "model": "dummy/local-model"},
        {"temperature": 1.0, "model": "dummy/local-model"},
    ]

    @evaluation_test(
        input_dataset=input_dataset,
        completion_params=completion_params_list,
        dataset_adapter=markdown_dataset_to_evaluation_row,
        rollout_processor=NoOpRolloutProcessor(),
        mode="pointwise",
        combine_datasets=False,
        num_runs=5,
    )
    def eval_fn(row: EvaluationRow) -> EvaluationRow:
        row_ids.add(row.input_metadata.row_id)
        return row

    # Manually invoke all parameter combinations within a single test
    for ds_path in input_dataset:
        for params in completion_params_list:
            await eval_fn(dataset_path=[ds_path], completion_params=params)

    # Assertions on IDs generated by the decorator logic
    assert len(row_ids) == 19  # from the markdown dataset
