from typing import List

from eval_protocol.models import EvaluationRow, Message
from eval_protocol.pytest.default_no_op_rollout_processor import NoOpRolloutProcessor
from tests.pytest.test_markdown_highlighting import markdown_dataset_to_evaluation_row


def test_evaluation_test_decorator_ids_single():
    from eval_protocol.pytest.evaluation_test import evaluation_test

    row_ids = set()

    input_dataset = [
        "tests/pytest/data/markdown_dataset.jsonl",
        "tests/pytest/data/markdown_dataset.jsonl",
    ]
    completion_params_list = [
        {"temperature": 0.0, "model": "dummy/local-model"},
        {"temperature": 1.0, "model": "dummy/local-model"},
    ]

    @evaluation_test(
        input_dataset=input_dataset,
        completion_params=completion_params_list,
        dataset_adapter=markdown_dataset_to_evaluation_row,
        rollout_processor=NoOpRolloutProcessor(),
        mode="pointwise",
        combine_datasets=False,
        num_runs=5,
    )
    def eval_fn(row: EvaluationRow) -> EvaluationRow:
        row_ids.add(row.input_metadata.row_id)
        return row

    # Manually invoke all parameter combinations within a single test
    for ds_path in input_dataset:
        for params in completion_params_list:
            eval_fn(dataset_path=[ds_path], completion_params=params)

    # Second invocation to ensure that IDs are stable across multiple invocations
    for ds_path in input_dataset:
        for params in completion_params_list:
            eval_fn(dataset_path=[ds_path], completion_params=params)

    # Assertions on IDs generated by the decorator logic
    assert len(row_ids) == 19  # from the markdown dataset


def test_evaluation_test_generated_row_ids_without_dataset_keys():
    from eval_protocol.pytest.evaluation_test import evaluation_test

    # Adapter that does NOT set row_id; lets evaluation_test generate IDs
    def markdown_dataset_no_row_id_adapter(data: List[dict]) -> List[EvaluationRow]:
        return [
            EvaluationRow(
                messages=[Message(role="user", content=row["prompt"])],
                ground_truth=str(row["num_highlights"]),
            )
            for row in data
        ]

    row_ids = set()

    input_dataset = ["tests/pytest/data/markdown_dataset.jsonl", "tests/pytest/data/markdown_dataset.jsonl"]
    completion_params = [
        {"temperature": 0.0, "model": "dummy/local-model"},
        {"temperature": 1.0, "model": "dummy/local-model"},
    ]

    @evaluation_test(
        input_dataset=input_dataset,
        completion_params=completion_params,
        dataset_adapter=markdown_dataset_no_row_id_adapter,
        rollout_processor=NoOpRolloutProcessor(),
        mode="pointwise",
        combine_datasets=False,
        num_runs=5,
    )
    def eval_fn(row: EvaluationRow) -> EvaluationRow:
        # row_id should be auto-generated by evaluation_test/InputMetadata
        assert row.input_metadata is not None
        assert row.input_metadata.row_id is not None and isinstance(row.input_metadata.row_id, str)
        row_ids.add(row.input_metadata.row_id)
        return row

    # Single invocation (one dataset, one param set) with multiple runs
    for ds_path in input_dataset:
        for params in completion_params:
            eval_fn(dataset_path=[ds_path], completion_params=params)

    # Second invocation to ensure that IDs are stable across multiple invocations
    for ds_path in input_dataset:
        for params in completion_params:
            eval_fn(dataset_path=[ds_path], completion_params=params)

    # Even with multiple runs, generated row_ids should be stable within the invocation
    assert len(row_ids) == 19  # equals dataset size when IDs are generated once and preserved across runs
