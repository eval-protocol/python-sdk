{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Model Comparison Eval Harness: Tau2-Bench Airline\n",
        "\n",
        "This notebook compares different models on airline customer service scenarios using tau2-bench natural language evaluation.\n",
        "\n",
        "**Models being compared:**\n",
        "- Claude 4 Opus (AnthropicPolicy)\n",
        "- GPT 4.1 (OpenAIPolicy)\n",
        "- Kimi K2 (FireworksPolicy)\n",
        "\n",
        "**Evaluation Framework:** tau2-bench with natural language assertions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install eval-protocol anthropic fireworks-ai tau2-bench pytest-asyncio\n",
        "!pip install firectl  # For sharing results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… All imports successful!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import asyncio\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Any, Tuple\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import logging\n",
        "from litellm import cost_per_token\n",
        "from loguru import logger\n",
        "\n",
        "# Import eval protocol and tau2-bench\n",
        "import eval_protocol as rk\n",
        "from eval_protocol import reward_function, EvaluateResult\n",
        "from eval_protocol.models import LLMUsageStats\n",
        "\n",
        "from examples.tau2_mcp.tests.test_tau2_e2e import MCPServerManager\n",
        "\n",
        "from vendor.tau2.evaluator.evaluator_nl_assertions import NLAssertionsEvaluator\n",
        "from vendor.tau2.data_model.message import (\n",
        "    SystemMessage,\n",
        "    AssistantMessage,\n",
        "    UserMessage,\n",
        "    ToolMessage,\n",
        ")\n",
        "\n",
        "print(\"âœ… All imports successful!\")\n",
        "\n",
        "logging.basicConfig(level=logging.WARNING, force=True)\n",
        "\n",
        "logger.remove()  # Remove default handler\n",
        "logger.add(lambda _: None, level=\"ERROR\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Set Up Evaluation Benchmark\n",
        "\n",
        "First, let's load the evaluation dataset we want to benchmark our models on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Loaded airline dataset with 50 scenarios\n"
          ]
        }
      ],
      "source": [
        "with open(\"datasets/airline.json\", \"r\") as f:\n",
        "    tau2_eval_dataset = json.load(f)\n",
        "    # TODO: something here is broken\n",
        "\n",
        "print(f\"âœ… Loaded airline dataset with {len(tau2_eval_dataset)} scenarios\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Evaluation Function: Tau2-Bench\n",
        "\n",
        "Now, let's implement the actual evaluation function (also called a reward function), based on Tau2-Bench. If you haven't heard of Tau2-Bench, it's a customer support benchmark from Sierra AI. Check out more information here: https://github.com/sierra-research/tau2-bench"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@reward_function\n",
        "async def airline_eval(messages: List[Any], nl_assertions: List[str] = None, **kwargs) -> EvaluateResult:\n",
        "    \"\"\"\n",
        "    Evaluate airline conversation using tau2-bench NLAssertionsEvaluator.\n",
        "\n",
        "    Args:\n",
        "        messages: Conversation between agent and customer\n",
        "        nl_assertions: List of natural language assertions to evaluate\n",
        "        **kwargs: Additional parameters\n",
        "\n",
        "    Returns:\n",
        "        EvaluateResult with binary pass/fail and detailed assertion breakdown\n",
        "    \"\"\"\n",
        "    # Default assertions if none provided\n",
        "    if nl_assertions is None:\n",
        "        nl_assertions = [\"The agent handled the customer request appropriately according to airline policy\"]\n",
        "\n",
        "    # Convert Message objects directly to tau2-bench message objects\n",
        "    trajectory_objects = []\n",
        "    for msg in messages:\n",
        "        role = msg.role\n",
        "        content = msg.content\n",
        "\n",
        "        if role == \"system\":\n",
        "            trajectory_objects.append(SystemMessage(role=role, content=content))\n",
        "        elif role == \"assistant\":\n",
        "            trajectory_objects.append(AssistantMessage(role=role, content=content))\n",
        "        elif role == \"user\":\n",
        "            trajectory_objects.append(UserMessage(role=role, content=content))\n",
        "        elif role == \"tool\":\n",
        "            tool_id = msg.tool_call_id\n",
        "            trajectory_objects.append(ToolMessage(id=tool_id, role=role, content=content))\n",
        "\n",
        "    # Run the synchronous tau2-bench evaluation in a thread pool to avoid blocking\n",
        "    loop = asyncio.get_event_loop()\n",
        "    nl_assertions_checks = await loop.run_in_executor(\n",
        "        None, \n",
        "        NLAssertionsEvaluator.evaluate_nl_assertions,\n",
        "        trajectory_objects, \n",
        "        nl_assertions\n",
        "    )\n",
        "\n",
        "    all_expectations_met = all(result.met for result in nl_assertions_checks)\n",
        "    reward = 1.0 if all_expectations_met else 0.0\n",
        "\n",
        "    # Build reason string\n",
        "    if all_expectations_met:\n",
        "        reason = f\"All {len(nl_assertions)} natural language assertions passed\"\n",
        "    else:\n",
        "        failed_assertions = [nl_assertions[i] for i, result in enumerate(nl_assertions_checks) if not result.met]\n",
        "        reason = f\"Failed assertions: {failed_assertions}\"\n",
        "\n",
        "    return EvaluateResult(\n",
        "        score=reward,\n",
        "        reason=reason,\n",
        "        metrics={},\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Set Up Model Policies\n",
        "\n",
        "Configure the three models we want to compare: Claude 4 Opus, GPT-4.1, and Kimi K2.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… All required API keys are set\n"
          ]
        }
      ],
      "source": [
        "# Check for required API keys (set these as environment variables)\n",
        "# Example: export ANTHROPIC_API_KEY=your-key-here\n",
        "\n",
        "required_keys = [\"ANTHROPIC_API_KEY\", \"OPENAI_API_KEY\", \"FIREWORKS_API_KEY\"]\n",
        "missing_keys = [key for key in required_keys if not os.getenv(key)]\n",
        "\n",
        "if missing_keys:\n",
        "    print(f\"âš ï¸  Missing API keys: {missing_keys}\")\n",
        "    print(\"Please set these environment variables:\")\n",
        "    for key in missing_keys:\n",
        "        print(f\"  export {key}='your-key-here'\")\n",
        "else:\n",
        "    print(\"âœ… All required API keys are set\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Model policies created:\n",
            "  - Claude 4 Sonnet (Anthropic)\n",
            "  - Kimi K2 (Fireworks)\n"
          ]
        }
      ],
      "source": [
        "# Create model policies\n",
        "openai_policy = rk.OpenAIPolicy(\n",
        "    model_id=\"gpt-4.1\",\n",
        "    temperature=0.1,\n",
        "    max_tokens=4096,\n",
        ")\n",
        "\n",
        "anthropic_policy = rk.AnthropicPolicy(\n",
        "    model_id=\"claude-sonnet-4-20250514\",\n",
        "    temperature=0.1,\n",
        "    max_tokens=4096,\n",
        ")\n",
        "\n",
        "kimi_policy = rk.FireworksPolicy(\n",
        "    model_id=\"accounts/fireworks/models/kimi-k2-instruct\",\n",
        "    temperature=0.1,\n",
        "    max_tokens=4096,\n",
        ")\n",
        "\n",
        "models_to_test = {\n",
        "    # \"gpt-4.1\": {\n",
        "    #     \"policy\": openai_policy,\n",
        "    #     \"name\": \"GPT-4.1\",\n",
        "    #     \"provider\": \"OpenAI\"\n",
        "    # },\n",
        "    \"claude-sonnet-4\": {\n",
        "        \"policy\": anthropic_policy,\n",
        "        \"name\": \"Claude 4 Sonnet\",\n",
        "        \"provider\": \"Anthropic\"\n",
        "    },\n",
        "    \"kimi-k2\": {\n",
        "        \"policy\": kimi_policy,\n",
        "        \"name\": \"Kimi K2\", \n",
        "        \"provider\": \"Fireworks\"\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"âœ… Model policies created:\")\n",
        "for model_id, model_info in models_to_test.items():\n",
        "    print(f\"  - {model_info['name']} ({model_info['provider']})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "source": [
        "## 4. Run Evaluations\n",
        "\n",
        "Now we'll run the airline evaluation on both models and compare their performance.\n",
        "\n",
        "First, let's set up some code to manager our MCP server. We will run this server later on for our MCP tools to make calls to."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before we get into the main logic, we'd like to track quality and cost across the different models, so this is a bit of setup for tracking cost. For Kimi K2, we're using the official pricing from Firework's website, since litellm doesn't contain it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MANUAL_PRICING = {\n",
        "    \"accounts/fireworks/models/kimi-k2-instruct\": {\n",
        "        \"input_cost_per_1m\": 0.60,  # Estimated based on Fireworks pricing\n",
        "        \"output_cost_per_1m\": 2.50,  # Estimated - Fireworks often uses same price for input/output\n",
        "    }\n",
        "}\n",
        "\n",
        "def calculate_evaluation_cost(model_id: str, llm_usage_summary: LLMUsageStats) -> Dict[str, Any]:\n",
        "    input_tokens = llm_usage_summary.prompt_tokens or 0\n",
        "    output_tokens = llm_usage_summary.completion_tokens or 0\n",
        "    total_tokens = llm_usage_summary.total_tokens or (input_tokens + output_tokens)\n",
        " \n",
        "    if model_id in MANUAL_PRICING:\n",
        "        pricing = MANUAL_PRICING[model_id]\n",
        "        \n",
        "        input_cost = input_tokens * pricing[\"input_cost_per_1m\"] / 1000000\n",
        "        output_cost = output_tokens * pricing[\"output_cost_per_1m\"] / 1000000\n",
        "        total_cost = input_cost + output_cost\n",
        "        \n",
        "        cost_source = \"manual_pricing\"\n",
        "\n",
        "    else:\n",
        "        input_cost, output_cost = cost_per_token(\n",
        "            model=model_id,\n",
        "            prompt_tokens=input_tokens,\n",
        "            completion_tokens=output_tokens\n",
        "        )\n",
        "        total_cost = input_cost + output_cost\n",
        "        \n",
        "        cost_source = \"litellm\"\n",
        "        \n",
        "    return {\n",
        "        \"total_cost\": total_cost,\n",
        "        \"input_cost\": input_cost,\n",
        "        \"output_cost\": output_cost,\n",
        "        \"total_tokens\": total_tokens,\n",
        "        \"input_tokens\": input_tokens,\n",
        "        \"output_tokens\": output_tokens,\n",
        "        \"model_id\": model_id,\n",
        "        \"cost_source\": cost_source,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below is our core logic for running the Tau2-bench eval for a single model. We use the eval protocol framework to do rk.make() and rk.rollout(), "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def run_model_evaluation(model_id: str, model_info: Dict, dataset: List[Dict]) -> Tuple[List[Dict], List[Dict]]:\n",
        "    \"\"\"\n",
        "    Run evaluation for a single model on the airline dataset.\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (evaluation_results, evaluation_records)\n",
        "    \"\"\"\n",
        "    print(f\"\\nğŸ§ª Starting evaluation for {model_info['name']}...\")\n",
        "\n",
        "    # Use context manager for automatic cleanup even on exceptions\n",
        "    with MCPServerManager(\"../examples/tau2_mcp/server.py\", port=8000, domain=\"airline\") as server:\n",
        "        policy = model_info[\"policy\"]\n",
        "        \n",
        "        envs = rk.make(\n",
        "            \"http://localhost:8000/mcp/\",\n",
        "            dataset=dataset, \n",
        "            model_id=policy.model_id,\n",
        "        )\n",
        "        \n",
        "        print(f\"ğŸ“Š Created {len(envs.sessions)} environment sessions\")\n",
        "        \n",
        "        start_time = time.time()\n",
        "        evaluation_rows = await rk.rollout(envs, policy=policy, steps=30, max_concurrent_rollouts=8)\n",
        "        duration = time.time() - start_time\n",
        "        \n",
        "        print(f\"âœ… Completed {len(evaluation_rows)} evaluation rows in {duration:.2f}s\")\n",
        "        \n",
        "        # Create a helper function to process each evaluation row\n",
        "        async def process_evaluation_row(i: int, eval_row, dataset_item):\n",
        "            nl_assertions = dataset_item[\"assertions\"]\n",
        "            \n",
        "            # Run tau2-bench evaluation (now async and parallelizable!)\n",
        "            eval_result = await airline_eval(eval_row.messages, nl_assertions)\n",
        "            \n",
        "            # Calculate cost using existing LLMUsageStats and LiteLLM/manual pricing\n",
        "            llm_usage = eval_row.llm_usage_summary\n",
        "            print(f\"  ğŸ“Š LLM Usage for {dataset_item['id']}: {llm_usage}\")  # Debug: show actual usage\n",
        "            cost_info = calculate_evaluation_cost(policy.model_id, llm_usage)\n",
        "\n",
        "            num_assertions = len(nl_assertions)\n",
        "\n",
        "            # Create evaluation result\n",
        "            result = {\n",
        "                \"scenario_id\": dataset_item[\"id\"],\n",
        "                \"model_id\": policy.model_id,\n",
        "                \"score\": eval_result.score,\n",
        "                \"num_assertions\": num_assertions,\n",
        "                \"cost_info\": cost_info,  # Include cost information in results\n",
        "            }\n",
        "            \n",
        "            # Create comprehensive evaluation record\n",
        "            evaluation_record = {\n",
        "                \"model_id\": policy.model_id,\n",
        "                \"scenario_id\": dataset_item[\"id\"],\n",
        "                \"conversation_history\": eval_row.messages,\n",
        "                \"evaluation\": {\n",
        "                    \"score\": eval_result.score,\n",
        "                    \"num_assertions\": num_assertions,\n",
        "                    \"reason\": eval_result.reason,\n",
        "                    \"assertions\": [\n",
        "                        {\n",
        "                            \"assertion\": assertion,\n",
        "                            \"passed\": eval_result.score > 0  # All pass or all fail for this simple implementation\n",
        "                        }\n",
        "                        for assertion in nl_assertions\n",
        "                    ]\n",
        "                },\n",
        "                \"cost_info\": cost_info,  # Add cost information to evaluation record\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "            }\n",
        "            \n",
        "            print(f\"  ğŸ“‹ {result['scenario_id']}: {result['score']:.1f}, total {result['num_assertions']} assertions)\")\n",
        "            return result, evaluation_record\n",
        "            \n",
        "        # Process all evaluation rows in parallel using asyncio.gather\n",
        "        print(f\"ğŸš€ Processing {len(evaluation_rows)} evaluation row evaluations in parallel...\")\n",
        "        eval_start_time = time.time()\n",
        "        \n",
        "        tasks = [\n",
        "            process_evaluation_row(i, eval_row, dataset[i]) \n",
        "            for i, eval_row in enumerate(evaluation_rows)\n",
        "        ]\n",
        "        \n",
        "        # Run all evaluations concurrently\n",
        "        results_and_records = await asyncio.gather(*tasks)\n",
        "        \n",
        "        eval_duration = time.time() - eval_start_time\n",
        "        print(f\"âœ… Completed parallel evaluations in {eval_duration:.2f}s\")\n",
        "        \n",
        "        # Separate results and evaluation records\n",
        "        results = []\n",
        "        evaluation_records = []\n",
        "        for result, evaluation_record in results_and_records:\n",
        "            results.append(result)\n",
        "            evaluation_records.append(evaluation_record)\n",
        "        \n",
        "        await envs.close()\n",
        "        # Server cleanup happens automatically via context manager\n",
        "        \n",
        "        return results, evaluation_records"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ğŸ§ª Starting evaluation for Claude 4 Sonnet...\n",
            "âœ… Server started successfully on port 8000\n",
            "ğŸ“Š Created 50 environment sessions\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:eval_protocol.mcp.client.connection:Session c581b1937dfd10fa4e177cc027a41035: Invalid JSON from update_reservation_flights: Error executing tool update_reservation_flights: Flight HAT030 not available on date 2024-05-13. Error: Expecting value: line 1 column 1 (char 0)\n",
            "WARNING:eval_protocol.mcp.client.connection:Session 84de98d36b6446b307f60fe1e534a067: Invalid JSON from update_reservation_baggages: Error executing tool update_reservation_baggages: Gift card balance is not enough. Error: Expecting value: line 1 column 1 (char 0)\n",
            "WARNING:eval_protocol.mcp.client.connection:Session 84de98d36b6446b307f60fe1e534a067: Invalid JSON from update_reservation_baggages: Error executing tool update_reservation_baggages: Gift card balance is not enough. Error: Expecting value: line 1 column 1 (char 0)\n",
            "WARNING:eval_protocol.mcp.client.connection:Session aa8e35d6b8cfee9df34e24b405b60f94: Invalid JSON from update_reservation_flights: Error executing tool update_reservation_flights: Gift card balance is not enough. Error: Expecting value: line 1 column 1 (char 0)\n",
            "WARNING:eval_protocol.mcp.client.connection:Session 84de98d36b6446b307f60fe1e534a067: Invalid JSON from update_reservation_baggages: Error executing tool update_reservation_baggages: Gift card balance is not enough. Error: Expecting value: line 1 column 1 (char 0)\n",
            "WARNING:eval_protocol.mcp.client.connection:Session 41dba4c12d152158564c1e49f986c220: Invalid JSON from update_reservation_flights: Error executing tool update_reservation_flights: Certificate cannot be used to update reservation. Error: Expecting value: line 1 column 1 (char 0)\n",
            "WARNING:eval_protocol.mcp.client.connection:Session 105e5b441bcc0be055a231d0189ee750: Invalid JSON from book_reservation: Error executing tool book_reservation: Payment amount does not add up, total price is 290, but paid 304. Error: Expecting value: line 1 column 1 (char 0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ§¹ Closing 50 MCP sessions...\n",
            "âœ… All MCP sessions closed.\n",
            "âœ… Completed 50 trajectories in 438.92s\n",
            "ğŸš€ Processing 50 trajectory evaluations in parallel...\n",
            "  ğŸ“Š LLM Usage for airline_task_6: {'prompt_tokens': 11809, 'completion_tokens': 439, 'total_tokens': 12248}\n",
            "  ğŸ“‹ airline_task_6: 1.0, total 1 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_1: {'prompt_tokens': 48521, 'completion_tokens': 465, 'total_tokens': 48986}\n",
            "  ğŸ“‹ airline_task_1: 0.0, total 1 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_0: {'prompt_tokens': 18067, 'completion_tokens': 255, 'total_tokens': 18322}\n",
            "  ğŸ“‹ airline_task_0: 0.0, total 1 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_10: {'prompt_tokens': 70113, 'completion_tokens': 1132, 'total_tokens': 71245}\n",
            "  ğŸ“‹ airline_task_10: 1.0, total 1 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_13: {'prompt_tokens': 73350, 'completion_tokens': 1136, 'total_tokens': 74486}\n",
            "  ğŸ“‹ airline_task_13: 0.0, total 1 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_5: {'prompt_tokens': 31643, 'completion_tokens': 416, 'total_tokens': 32059}\n",
            "  ğŸ“‹ airline_task_5: 0.0, total 2 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_3: {'prompt_tokens': 18289, 'completion_tokens': 281, 'total_tokens': 18570}\n",
            "  ğŸ“‹ airline_task_3: 1.0, total 2 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_4: {'prompt_tokens': 47856, 'completion_tokens': 838, 'total_tokens': 48694}\n",
            "  ğŸ“‹ airline_task_4: 1.0, total 2 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_12: {'prompt_tokens': 32974, 'completion_tokens': 545, 'total_tokens': 33519}\n",
            "  ğŸ“‹ airline_task_12: 1.0, total 2 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_19: {'prompt_tokens': 31917, 'completion_tokens': 452, 'total_tokens': 32369}\n",
            "  ğŸ“‹ airline_task_19: 1.0, total 1 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_16: {'prompt_tokens': 34448, 'completion_tokens': 748, 'total_tokens': 35196}\n",
            "  ğŸ“‹ airline_task_16: 0.0, total 2 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_9: {'prompt_tokens': 48912, 'completion_tokens': 825, 'total_tokens': 49737}\n",
            "  ğŸ“‹ airline_task_9: 0.0, total 4 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_15: {'prompt_tokens': 56489, 'completion_tokens': 949, 'total_tokens': 57438}\n",
            "  ğŸ“‹ airline_task_15: 0.0, total 2 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_11: {'prompt_tokens': 32715, 'completion_tokens': 395, 'total_tokens': 33110}\n",
            "  ğŸ“‹ airline_task_11: 0.0, total 3 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_2: {'prompt_tokens': 64625, 'completion_tokens': 925, 'total_tokens': 65550}\n",
            "  ğŸ“‹ airline_task_2: 1.0, total 4 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_14: {'prompt_tokens': 105792, 'completion_tokens': 1297, 'total_tokens': 107089}\n",
            "  ğŸ“‹ airline_task_14: 0.0, total 5 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_7: {'prompt_tokens': 49692, 'completion_tokens': 518, 'total_tokens': 50210}\n",
            "  ğŸ“‹ airline_task_7: 0.0, total 4 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_26: {'prompt_tokens': 18258, 'completion_tokens': 498, 'total_tokens': 18756}\n",
            "  ğŸ“‹ airline_task_26: 1.0, total 1 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_8: {'prompt_tokens': 56129, 'completion_tokens': 939, 'total_tokens': 57068}\n",
            "  ğŸ“‹ airline_task_8: 1.0, total 4 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_17: {'prompt_tokens': 57622, 'completion_tokens': 710, 'total_tokens': 58332}\n",
            "  ğŸ“‹ airline_task_17: 0.0, total 3 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_28: {'prompt_tokens': 18305, 'completion_tokens': 519, 'total_tokens': 18824}\n",
            "  ğŸ“‹ airline_task_28: 1.0, total 2 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_22: {'prompt_tokens': 102233, 'completion_tokens': 1630, 'total_tokens': 103863}\n",
            "  ğŸ“‹ airline_task_22: 0.0, total 3 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_20: {'prompt_tokens': 75987, 'completion_tokens': 1169, 'total_tokens': 77156}\n",
            "  ğŸ“‹ airline_task_20: 0.0, total 2 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_31: {'prompt_tokens': 17973, 'completion_tokens': 508, 'total_tokens': 18481}\n",
            "  ğŸ“‹ airline_task_31: 1.0, total 1 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_18: {'prompt_tokens': 290410, 'completion_tokens': 3217, 'total_tokens': 293627}\n",
            "  ğŸ“‹ airline_task_18: 0.0, total 6 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_27: {'prompt_tokens': 91697, 'completion_tokens': 900, 'total_tokens': 92597}\n",
            "  ğŸ“‹ airline_task_27: 1.0, total 3 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_29: {'prompt_tokens': 45098, 'completion_tokens': 921, 'total_tokens': 46019}\n",
            "  ğŸ“‹ airline_task_29: 1.0, total 2 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_30: {'prompt_tokens': 34284, 'completion_tokens': 851, 'total_tokens': 35135}\n",
            "  ğŸ“‹ airline_task_30: 1.0, total 2 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_25: {'prompt_tokens': 34098, 'completion_tokens': 791, 'total_tokens': 34889}\n",
            "  ğŸ“‹ airline_task_25: 1.0, total 2 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_32: {'prompt_tokens': 25880, 'completion_tokens': 505, 'total_tokens': 26385}\n",
            "  ğŸ“‹ airline_task_32: 1.0, total 2 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_21: {'prompt_tokens': 91852, 'completion_tokens': 1140, 'total_tokens': 92992}\n",
            "  ğŸ“‹ airline_task_21: 0.0, total 3 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_34: {'prompt_tokens': 33951, 'completion_tokens': 1036, 'total_tokens': 34987}\n",
            "  ğŸ“‹ airline_task_34: 1.0, total 1 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_24: {'prompt_tokens': 64616, 'completion_tokens': 1547, 'total_tokens': 66163}\n",
            "  ğŸ“‹ airline_task_24: 0.0, total 3 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_36: {'prompt_tokens': 27295, 'completion_tokens': 347, 'total_tokens': 27642}\n",
            "  ğŸ“‹ airline_task_36: 1.0, total 1 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_41: {'prompt_tokens': 66964, 'completion_tokens': 577, 'total_tokens': 67541}\n",
            "  ğŸ“‹ airline_task_41: 0.0, total 2 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_42: {'prompt_tokens': 86379, 'completion_tokens': 951, 'total_tokens': 87330}\n",
            "  ğŸ“‹ airline_task_42: 1.0, total 2 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_40: {'prompt_tokens': 11763, 'completion_tokens': 229, 'total_tokens': 11992}\n",
            "  ğŸ“‹ airline_task_40: 1.0, total 1 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_33: {'prompt_tokens': 43097, 'completion_tokens': 814, 'total_tokens': 43911}\n",
            "  ğŸ“‹ airline_task_33: 1.0, total 3 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_45: {'prompt_tokens': 5584, 'completion_tokens': 62, 'total_tokens': 5646}\n",
            "  ğŸ“‹ airline_task_45: 1.0, total 2 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_46: {'prompt_tokens': 12057, 'completion_tokens': 322, 'total_tokens': 12379}\n",
            "  ğŸ“‹ airline_task_46: 1.0, total 1 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_23: {'prompt_tokens': 134261, 'completion_tokens': 2398, 'total_tokens': 136659}\n",
            "  ğŸ“‹ airline_task_23: 1.0, total 8 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_47: {'prompt_tokens': 17987, 'completion_tokens': 221, 'total_tokens': 18208}\n",
            "  ğŸ“‹ airline_task_47: 0.0, total 1 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_38: {'prompt_tokens': 40105, 'completion_tokens': 852, 'total_tokens': 40957}\n",
            "  ğŸ“‹ airline_task_38: 0.0, total 4 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_35: {'prompt_tokens': 56123, 'completion_tokens': 1211, 'total_tokens': 57334}\n",
            "  ğŸ“‹ airline_task_35: 0.0, total 3 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_49: {'prompt_tokens': 11756, 'completion_tokens': 332, 'total_tokens': 12088}\n",
            "  ğŸ“‹ airline_task_49: 1.0, total 1 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_43: {'prompt_tokens': 56223, 'completion_tokens': 554, 'total_tokens': 56777}\n",
            "  ğŸ“‹ airline_task_43: 0.0, total 2 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_48: {'prompt_tokens': 18950, 'completion_tokens': 592, 'total_tokens': 19542}\n",
            "  ğŸ“‹ airline_task_48: 1.0, total 1 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_37: {'prompt_tokens': 50314, 'completion_tokens': 870, 'total_tokens': 51184}\n",
            "  ğŸ“‹ airline_task_37: 0.0, total 3 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_39: {'prompt_tokens': 100087, 'completion_tokens': 1072, 'total_tokens': 101159}\n",
            "  ğŸ“‹ airline_task_39: 1.0, total 4 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_44: {'prompt_tokens': 146859, 'completion_tokens': 1910, 'total_tokens': 148769}\n",
            "  ğŸ“‹ airline_task_44: 0.0, total 5 assertions)\n",
            "âœ… Completed parallel evaluations in 16.19s\n",
            "ğŸ§¹ Closing 50 MCP sessions...\n",
            "âœ… All MCP sessions closed.\n",
            "ğŸ›‘ Stopping server on port 8000...\n",
            "ğŸ§¹ Cleaned up log file: /Users/derekxu/Documents/code/python-sdk/local_evals/server_output_airline_8000.log\n",
            "\n",
            "ğŸ§ª Starting evaluation for Kimi K2...\n",
            "âœ… Server started successfully on port 8000\n",
            "ğŸ“Š Created 50 environment sessions\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:eval_protocol.mcp.client.connection:Session 77f6847cd7f4eaa908955b68fc08b75e: Invalid JSON from get_reservation_details: Error executing tool get_reservation_details: Reservation L7X4P9 not found. Error: Expecting value: line 1 column 1 (char 0)\n",
            "WARNING:eval_protocol.mcp.client.connection:Control plane status endpoint timed out after 3.0s\n",
            "WARNING:eval_protocol.mcp.client.connection:Session 0de216038acb0986989909c3b22b5373: Invalid JSON from get_reservation_details: Error executing tool get_reservation_details: Reservation 45698213 not found. Error: Expecting value: line 1 column 1 (char 0)\n",
            "WARNING:eval_protocol.mcp.client.connection:Session 0de216038acb0986989909c3b22b5373: Invalid JSON from get_reservation_details: Error executing tool get_reservation_details: Reservation Q7ZB34 not found. Error: Expecting value: line 1 column 1 (char 0)\n",
            "WARNING:eval_protocol.mcp.client.connection:Control plane reward endpoint timed out after 3.0s\n",
            "WARNING:eval_protocol.mcp.client.connection:Session 20dd3f68f9165c4cc4bd81aec770c9d4: Invalid JSON from update_reservation_flights: Error executing tool update_reservation_flights: Payment method not found. Error: Expecting value: line 1 column 1 (char 0)\n",
            "WARNING:eval_protocol.mcp.client.connection:Control plane status endpoint timed out after 3.0s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ§¹ Closing 50 MCP sessions...\n",
            "âœ… All MCP sessions closed.\n",
            "âœ… Completed 50 trajectories in 373.16s\n",
            "ğŸš€ Processing 50 trajectory evaluations in parallel...\n",
            "  ğŸ“Š LLM Usage for airline_task_0: {'prompt_tokens': 10394, 'completion_tokens': 348, 'total_tokens': 10742}\n",
            "  ğŸ“‹ airline_task_0: 1.0, total 1 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_1: {'prompt_tokens': 42103, 'completion_tokens': 192, 'total_tokens': 42295}\n",
            "  ğŸ“‹ airline_task_1: 0.0, total 1 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_6: {'prompt_tokens': 4932, 'completion_tokens': 38, 'total_tokens': 4970}\n",
            "  ğŸ“‹ airline_task_6: 1.0, total 1 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_13: {'prompt_tokens': 38788, 'completion_tokens': 663, 'total_tokens': 39451}\n",
            "  ğŸ“‹ airline_task_13: 0.0, total 1 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_10: {'prompt_tokens': 43693, 'completion_tokens': 366, 'total_tokens': 44059}\n",
            "  ğŸ“‹ airline_task_10: 1.0, total 1 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_5: {'prompt_tokens': 33828, 'completion_tokens': 479, 'total_tokens': 34307}\n",
            "  ğŸ“‹ airline_task_5: 0.0, total 2 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_3: {'prompt_tokens': 15963, 'completion_tokens': 230, 'total_tokens': 16193}\n",
            "  ğŸ“‹ airline_task_3: 1.0, total 2 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_12: {'prompt_tokens': 37553, 'completion_tokens': 577, 'total_tokens': 38130}\n",
            "  ğŸ“‹ airline_task_12: 0.0, total 2 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_4: {'prompt_tokens': 55540, 'completion_tokens': 610, 'total_tokens': 56150}\n",
            "  ğŸ“‹ airline_task_4: 0.0, total 2 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_19: {'prompt_tokens': 16020, 'completion_tokens': 118, 'total_tokens': 16138}\n",
            "  ğŸ“‹ airline_task_19: 1.0, total 1 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_2: {'prompt_tokens': 37009, 'completion_tokens': 243, 'total_tokens': 37252}\n",
            "  ğŸ“‹ airline_task_2: 1.0, total 4 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_11: {'prompt_tokens': 4766, 'completion_tokens': 146, 'total_tokens': 4912}\n",
            "  ğŸ“‹ airline_task_11: 0.0, total 3 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_15: {'prompt_tokens': 57885, 'completion_tokens': 533, 'total_tokens': 58418}\n",
            "  ğŸ“‹ airline_task_15: 0.0, total 2 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_16: {'prompt_tokens': 30439, 'completion_tokens': 429, 'total_tokens': 30868}\n",
            "  ğŸ“‹ airline_task_16: 0.0, total 2 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_9: {'prompt_tokens': 35496, 'completion_tokens': 376, 'total_tokens': 35872}\n",
            "  ğŸ“‹ airline_task_9: 1.0, total 4 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_17: {'prompt_tokens': 49670, 'completion_tokens': 610, 'total_tokens': 50280}\n",
            "  ğŸ“‹ airline_task_17: 0.0, total 3 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_8: {'prompt_tokens': 49402, 'completion_tokens': 510, 'total_tokens': 49912}\n",
            "  ğŸ“‹ airline_task_8: 0.0, total 4 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_7: {'prompt_tokens': 51445, 'completion_tokens': 275, 'total_tokens': 51720}\n",
            "  ğŸ“‹ airline_task_7: 0.0, total 4 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_26: {'prompt_tokens': 15790, 'completion_tokens': 426, 'total_tokens': 16216}\n",
            "  ğŸ“‹ airline_task_26: 1.0, total 1 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_14: {'prompt_tokens': 59997, 'completion_tokens': 1031, 'total_tokens': 61028}\n",
            "  ğŸ“‹ airline_task_14: 0.0, total 5 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_21: {'prompt_tokens': 96182, 'completion_tokens': 603, 'total_tokens': 96785}\n",
            "  ğŸ“‹ airline_task_21: 0.0, total 3 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_22: {'prompt_tokens': 28531, 'completion_tokens': 143, 'total_tokens': 28674}\n",
            "  ğŸ“‹ airline_task_22: 0.0, total 3 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_25: {'prompt_tokens': 10238, 'completion_tokens': 138, 'total_tokens': 10376}\n",
            "  ğŸ“‹ airline_task_25: 0.0, total 2 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_18: {'prompt_tokens': 85908, 'completion_tokens': 689, 'total_tokens': 86597}\n",
            "  ğŸ“‹ airline_task_18: 1.0, total 6 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_20: {'prompt_tokens': 42172, 'completion_tokens': 548, 'total_tokens': 42720}\n",
            "  ğŸ“‹ airline_task_20: 0.0, total 2 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_27: {'prompt_tokens': 80408, 'completion_tokens': 488, 'total_tokens': 80896}\n",
            "  ğŸ“‹ airline_task_27: 1.0, total 3 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_30: {'prompt_tokens': 30393, 'completion_tokens': 396, 'total_tokens': 30789}\n",
            "  ğŸ“‹ airline_task_30: 1.0, total 2 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_24: {'prompt_tokens': 63523, 'completion_tokens': 731, 'total_tokens': 64254}\n",
            "  ğŸ“‹ airline_task_24: 0.0, total 3 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_34: {'prompt_tokens': 23049, 'completion_tokens': 485, 'total_tokens': 23534}\n",
            "  ğŸ“‹ airline_task_34: 1.0, total 1 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_28: {'prompt_tokens': 10066, 'completion_tokens': 360, 'total_tokens': 10426}\n",
            "  ğŸ“‹ airline_task_28: 1.0, total 2 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_31: {'prompt_tokens': 16464, 'completion_tokens': 226, 'total_tokens': 16690}\n",
            "  ğŸ“‹ airline_task_31: 1.0, total 1 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_36: {'prompt_tokens': 10772, 'completion_tokens': 202, 'total_tokens': 10974}\n",
            "  ğŸ“‹ airline_task_36: 1.0, total 1 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_40: {'prompt_tokens': 10333, 'completion_tokens': 114, 'total_tokens': 10447}\n",
            "  ğŸ“‹ airline_task_40: 1.0, total 1 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_33: {'prompt_tokens': 30350, 'completion_tokens': 393, 'total_tokens': 30743}\n",
            "  ğŸ“‹ airline_task_33: 1.0, total 3 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_29: {'prompt_tokens': 38672, 'completion_tokens': 501, 'total_tokens': 39173}\n",
            "  ğŸ“‹ airline_task_29: 0.0, total 2 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_41: {'prompt_tokens': 49179, 'completion_tokens': 227, 'total_tokens': 49406}\n",
            "  ğŸ“‹ airline_task_41: 1.0, total 2 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_32: {'prompt_tokens': 22825, 'completion_tokens': 297, 'total_tokens': 23122}\n",
            "  ğŸ“‹ airline_task_32: 0.0, total 2 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_42: {'prompt_tokens': 84720, 'completion_tokens': 491, 'total_tokens': 85211}\n",
            "  ğŸ“‹ airline_task_42: 1.0, total 2 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_37: {'prompt_tokens': 61432, 'completion_tokens': 572, 'total_tokens': 62004}\n",
            "  ğŸ“‹ airline_task_37: 0.0, total 3 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_46: {'prompt_tokens': 5083, 'completion_tokens': 83, 'total_tokens': 5166}\n",
            "  ğŸ“‹ airline_task_46: 1.0, total 1 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_47: {'prompt_tokens': 10303, 'completion_tokens': 74, 'total_tokens': 10377}\n",
            "  ğŸ“‹ airline_task_47: 0.0, total 1 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_45: {'prompt_tokens': 10672, 'completion_tokens': 107, 'total_tokens': 10779}\n",
            "  ğŸ“‹ airline_task_45: 0.0, total 2 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_38: {'prompt_tokens': 35391, 'completion_tokens': 495, 'total_tokens': 35886}\n",
            "  ğŸ“‹ airline_task_38: 0.0, total 4 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_39: {'prompt_tokens': 77165, 'completion_tokens': 331, 'total_tokens': 77496}\n",
            "  ğŸ“‹ airline_task_39: 1.0, total 4 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_48: {'prompt_tokens': 10259, 'completion_tokens': 330, 'total_tokens': 10589}\n",
            "  ğŸ“‹ airline_task_48: 1.0, total 1 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_49: {'prompt_tokens': 10091, 'completion_tokens': 257, 'total_tokens': 10348}\n",
            "  ğŸ“‹ airline_task_49: 1.0, total 1 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_35: {'prompt_tokens': 32788, 'completion_tokens': 406, 'total_tokens': 33194}\n",
            "  ğŸ“‹ airline_task_35: 0.0, total 3 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_43: {'prompt_tokens': 43329, 'completion_tokens': 275, 'total_tokens': 43604}\n",
            "  ğŸ“‹ airline_task_43: 0.0, total 2 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_23: {'prompt_tokens': 50198, 'completion_tokens': 921, 'total_tokens': 51119}\n",
            "  ğŸ“‹ airline_task_23: 0.0, total 8 assertions)\n",
            "  ğŸ“Š LLM Usage for airline_task_44: {'prompt_tokens': 41578, 'completion_tokens': 345, 'total_tokens': 41923}\n",
            "  ğŸ“‹ airline_task_44: 0.0, total 5 assertions)\n",
            "âœ… Completed parallel evaluations in 17.52s\n",
            "ğŸ§¹ Closing 50 MCP sessions...\n",
            "âœ… All MCP sessions closed.\n",
            "ğŸ›‘ Stopping server on port 8000...\n",
            "ğŸ§¹ Cleaned up log file: /Users/derekxu/Documents/code/python-sdk/local_evals/server_output_airline_8000.log\n",
            "\n",
            "âœ… Completed evaluations for 2 models\n",
            "ğŸ“Š Total results: 100\n",
            "ğŸ“Š Total trajectories: 100\n"
          ]
        }
      ],
      "source": [
        "all_results = []\n",
        "all_evaluation_records = []\n",
        "\n",
        "for model_id, model_info in models_to_test.items():\n",
        "    model_results, evaluation_records = await run_model_evaluation(model_id, model_info, tau2_eval_dataset)\n",
        "    all_results.extend(model_results)\n",
        "    all_evaluation_records.extend(evaluation_records)\n",
        "\n",
        "print(f\"\\nâœ… Completed evaluations for {len(models_to_test)} models\")\n",
        "print(f\"ğŸ“Š Total results: {len(all_results)}\")\n",
        "print(f\"ğŸ“Š Total evaluation records: {len(all_evaluation_records)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Analyze Results\n",
        "\n",
        "Let's analyze and visualize the comparison between Claude 4 Opus, GPT-4.1, and Kimi K2.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ğŸ“ˆ Summary Statistics:\n",
            "   Claude 4 Sonnet: 54.00% success rate (27.0/50) - Cost: $8.79 (via litellm)\n",
            "   Kimi K2: 46.00% success rate (23.0/50) - Cost: $1.14 (via manual_pricing)\n",
            "\n",
            "ğŸ’° Total evaluation cost: $9.93\n",
            "ğŸ“Š Cost calculation uses actual API usage data from LLMUsageStats\n"
          ]
        }
      ],
      "source": [
        "model_id_to_config = {}\n",
        "for config_key, model_info in models_to_test.items():\n",
        "    actual_model_id = model_info[\"policy\"].model_id\n",
        "    model_id_to_config[actual_model_id] = model_info\n",
        "\n",
        "print(f\"\\nğŸ“ˆ Summary Statistics:\")\n",
        "total_cost = 0.0\n",
        "for actual_model_id, model_info in model_id_to_config.items():\n",
        "    model_results_subset = [r for r in all_results if r['model_id'] == actual_model_id]\n",
        "    avg_score = sum(r['score'] for r in model_results_subset) / len(model_results_subset) if model_results_subset else 0\n",
        "    \n",
        "    # Calculate total cost for this model\n",
        "    model_total_cost = sum(r['cost_info']['total_cost'] for r in model_results_subset if 'cost_info' in r)\n",
        "    total_cost += model_total_cost\n",
        "    \n",
        "    # Show cost source info\n",
        "    cost_sources = [r['cost_info'].get('cost_source', 'unknown') for r in model_results_subset if 'cost_info' in r]\n",
        "    cost_source_summary = f\" (via {cost_sources[0]})\" if cost_sources else \"\"\n",
        "    \n",
        "    print(f\"   {model_info['name']}: {avg_score:.2%} success rate ({sum(r['score'] for r in model_results_subset)}/{len(model_results_subset)}) - Cost: ${model_total_cost:.2f}{cost_source_summary}\")\n",
        "\n",
        "print(f\"\\nğŸ’° Total evaluation cost: ${total_cost:.2f}\")\n",
        "print(f\"ğŸ“Š Cost calculation uses actual API usage data from LLMUsageStats\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“„ Saved JSONL file: trajectory_outputs/all_trajectories.jsonl\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "PosixPath('trajectory_outputs/all_trajectories.jsonl')"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def save_results_jsonl(evaluation_records: List[Dict], output_file: str = \"evaluation_outputs/all_evaluations.jsonl\"):\n",
        "    \"\"\"Save all evaluation records in JSONL format (one JSON object per line).\"\"\"\n",
        "    output_path = Path(output_file)\n",
        "    output_path.parent.mkdir(exist_ok=True)\n",
        "    \n",
        "    with open(output_path, 'w') as f:\n",
        "        for record in evaluation_records:\n",
        "            json.dump(record, f, default=str)\n",
        "            f.write('\\n')\n",
        "    \n",
        "    print(f\"ğŸ“„ Saved JSONL file: {output_path}\")\n",
        "    return output_path\n",
        "\n",
        "save_results_jsonl(all_evaluation_records)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ğŸ“ Saved trajectory files to: trajectory_outputs\n",
            "   - 100 individual trajectory files\n",
            "   - 1 evaluation summary file\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "PosixPath('trajectory_outputs')"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def save_evaluation_files(evaluation_records: List[Dict], output_dir: str = \"evaluation_outputs\"):\n",
        "    \"\"\"Save evaluation records to individual files and create summary.\"\"\"\n",
        "    output_path = Path(output_dir)\n",
        "    output_path.mkdir(exist_ok=True)\n",
        "    \n",
        "    # Save individual evaluation files\n",
        "    for record in evaluation_records:\n",
        "        # Sanitize model_id for filename (replace slashes with underscores)\n",
        "        safe_model_id = record['model_id'].replace('/', '_').replace('\\\\', '_')\n",
        "        filename = f\"{safe_model_id}_{record['scenario_id']}_evaluation.json\"\n",
        "        filepath = output_path / filename\n",
        "        \n",
        "        with open(filepath, 'w') as f:\n",
        "            json.dump(record, f, indent=2, default=str)\n",
        "    \n",
        "    # Create summary file\n",
        "    summary = {\n",
        "        \"evaluation_summary\": {\n",
        "            \"total_evaluations\": len(evaluation_records),\n",
        "            \"models_evaluated\": list(set(r['model_id'] for r in evaluation_records)),\n",
        "            \"scenarios_evaluated\": list(set(r['scenario_id'] for r in evaluation_records)),\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "        },\n",
        "        \"model_performance\": {},\n",
        "        \"scenario_difficulty\": {}\n",
        "    }\n",
        "    \n",
        "    # Calculate model performance\n",
        "    for model_id in summary[\"evaluation_summary\"][\"models_evaluated\"]:\n",
        "        model_records = [r for r in evaluation_records if r['model_id'] == model_id]\n",
        "        total_score = sum(r['evaluation']['score'] for r in model_records)\n",
        "        avg_score = total_score / len(model_records) if model_records else 0\n",
        "        \n",
        "        # Calculate cost metrics\n",
        "        total_cost = sum(r.get('cost_info', {}).get('total_cost', 0) for r in model_records)\n",
        "        total_tokens = sum(r.get('cost_info', {}).get('total_tokens', 0) for r in model_records)\n",
        "        avg_cost_per_scenario = total_cost / len(model_records) if model_records else 0\n",
        "        \n",
        "        summary[\"model_performance\"][model_id] = {\n",
        "            \"total_scenarios\": len(model_records),\n",
        "            \"total_score\": total_score,\n",
        "            \"average_score\": avg_score,\n",
        "            \"pass_rate\": avg_score,  # Since scores are 0 or 1\n",
        "            \"total_cost\": total_cost,\n",
        "            \"average_cost_per_scenario\": avg_cost_per_scenario,\n",
        "            \"total_tokens\": total_tokens,\n",
        "            \"cost_per_success\": total_cost / total_score if total_score > 0 else 0\n",
        "        }\n",
        "    \n",
        "    # Calculate scenario difficulty\n",
        "    for scenario_id in summary[\"evaluation_summary\"][\"scenarios_evaluated\"]:\n",
        "        scenario_records = [r for r in evaluation_records if r['scenario_id'] == scenario_id]\n",
        "        total_score = sum(r['evaluation']['score'] for r in scenario_records)\n",
        "        avg_score = total_score / len(scenario_records) if scenario_records else 0\n",
        "        \n",
        "        summary[\"scenario_difficulty\"][scenario_id] = {\n",
        "            \"models_tested\": len(scenario_records),\n",
        "            \"total_score\": total_score,\n",
        "            \"average_score\": avg_score,\n",
        "            \"difficulty\": \"easy\" if avg_score > 0.8 else \"medium\" if avg_score > 0.5 else \"hard\"\n",
        "        }\n",
        "    \n",
        "    # Save summary\n",
        "    summary_path = output_path / \"evaluation_summary.json\"\n",
        "    with open(summary_path, 'w') as f:\n",
        "        json.dump(summary, f, indent=2, default=str)\n",
        "    \n",
        "    print(f\"\\nğŸ“ Saved evaluation files to: {output_path}\")\n",
        "    print(f\"   - {len(evaluation_records)} individual evaluation files\")\n",
        "    print(f\"   - 1 evaluation summary file\")\n",
        "    \n",
        "    return output_path\n",
        "\n",
        "save_evaluation_files(all_evaluation_records)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "source": [
        "## 7. Share Results with Firectl\n",
        "\n",
        "Finally, let's create a dataset with our evaluation results to share using `firectl create dataset`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "source": [
        "## Summary\n",
        "\n",
        "This notebook provides a complete eval harness for comparing models using tau2-bench airline evaluation with proper dataset structure:\n",
        "\n",
        "1. **Dataset Structure**: Following tau2-bench pattern with separate JSON datasets and markdown system prompts\n",
        "2. **Models**: Configured Claude 4 Sonnet (AnthropicPolicy) and Kimi K2 (FireworksPolicy)\n",
        "3. **Evaluation**: Used tau2-bench NLAssertionsEvaluator for objective scoring with EvaluationRow format\n",
        "4. **Analysis**: Compared performance across multiple dimensions\n",
        "5. **Sharing**: Prepared results for sharing via `firectl create dataset`\n",
        "\n",
        "### Key Features:\n",
        "- **Clean Dataset Structure**: Separate JSON data and markdown prompts like the tau2 examples\n",
        "- **Natural Language Evaluation**: Uses human-readable assertions instead of code-based metrics\n",
        "- **Multi-Model Comparison**: Easy to add more models for comparison\n",
        "- **Comprehensive Analysis**: Performance, accuracy, and efficiency metrics with cost tracking\n",
        "- **EvaluationRow Support**: Updated to work with the new EvaluationRow format from eval_protocol\n",
        "- **Reproducible**: Results can be shared and reproduced via firectl\n",
        "\n",
        "### Next Steps:\n",
        "1. Set your API keys as environment variables:\n",
        "   ```bash\n",
        "   export ANTHROPIC_API_KEY=\"your-anthropic-key-here\"\n",
        "   export OPENAI_API_KEY=\"your-openai-key-here\"\n",
        "   export FIREWORKS_API_KEY=\"your-fireworks-key-here\"\n",
        "   ```\n",
        "2. Start the tau2 MCP server: `cd examples/tau2_mcp && python server.py --port 8000`\n",
        "3. Run the evaluation cells\n",
        "4. Share results with the community using the provided firectl command\n",
        "\n",
        "### Expected Results:\n",
        "Based on the tau2-bench framework, we expect different models to show varying performance on natural language assertion evaluation, demonstrating their ability to adhere to airline policy compliance and customer service protocols.\n",
        "\n",
        "This structure uses the updated EvaluationRow format and provides comprehensive cost analysis across different model providers."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
