# Hydra Configuration for running evaluation on XLAM Function Calling dataset
# using reward_kit.cli run

defaults:
  - _self_
  - dataset: xlam_fc_eval_prompts # Selects the dataset config from the search path
  - override hydra/job_logging: default
  - override hydra/hydra_logging: default

hydra:
  run:
    dir: ./outputs/tool_calling_example/xlam_fc_eval/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweep: # For multi-run/sweeping, not used in single run
    dir: ./multirun/tool_calling_example/xlam_fc_eval/${now:%Y-%m-%d}/${now:%H-%M-%S}
    subdir: ${hydra.job.num}
  searchpath:
    # Add project_root/conf to search for dataset configs like xlam_fc_eval_prompts.yaml
    # This allows `dataset: xlam_fc_eval_prompts` to find `project_root/conf/dataset/xlam_fc_eval_prompts.yaml`.
    - file://${oc.env:PWD}/conf

# Dataset configuration is now inherited from the selected dataset in the defaults list.
# We can override specific dataset parameters here if needed, for example:
# dataset:
#   derived_max_samples: 10 # Override max_samples for this specific run

# Model Generation Configuration
# These are illustrative. User should configure their desired model.
generation:
  enabled: true # Added to enable the generation stage
  _target_: reward_kit.generation.generate_responses # Placeholder, actual target might vary
  model_name: "accounts/fireworks/models/qwen3-235b-a22b" # Example model, user should change
  # model_name: "gpt-4-turbo-preview" # Or an OpenAI model
  # For OpenAI models, ensure FIREWORKS_API_KEY (if using Fireworks proxy) or OPENAI_API_KEY is set.
  # For local/HF models, ensure the model is accessible.
  batch_size: 4
  max_new_tokens: 512
  temperature: 0.1
  top_p: 0.95
  cache:
    enabled: true # Add cache configuration
  api_params:
    rate_limit_qps: 1.0 # Example value, adjust as needed
    max_retries: 3      # Example value
    max_concurrent_requests: 5 # Default used in pipeline if not found
  # Depending on the model, other parameters like 'do_sample', 'repetition_penalty' might be needed.
  # For function calling, the model needs to support it and be prompted correctly.
  # The 'messages' and 'tools' from the dataset will be used for prompting.

# Reward Function Configuration
reward:
  function_path: "reward_kit.rewards.function_calling.exact_tool_match_reward"
  # params: {} # If the reward function takes additional parameters, define them here.
  # No specific parameters needed for exact_tool_match_reward beyond messages and ground_truth

# Evaluation Parameters
evaluation_params:
  limit_samples: 10

# Output Configuration
output:
  results_file: "eval_results.jsonl" # Relative to hydra.run.dir
  preview_pairs_file: "preview_samples.jsonl" # Relative to hydra.run.dir

logging_params:
  batch_log_interval: 10 # Default from pipeline code, can be adjusted

# Other settings
seed: 42
verbose: true
