# MCP Agent Filesystem RL Example

This example demonstrates how to evaluate LLM responses for filesystem tasks using the reward-kit framework. It showcases the clean architecture where reward functions focus purely on evaluation while the framework handles model orchestration.

## Overview

- **Task**: Evaluate LLM responses for filesystem operations (move, copy, create files)
- **Model**: LLM generates text responses describing filesystem operations
- **Evaluation**: Reward function analyzes response quality and task completion indicators
- **Framework**: Standard reward-kit CLI with Hydra configuration

## Files Structure

```
examples/mcp_agent_filesystem_rl/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ main.py                      # Main reward function (follows math_example pattern)
â”œâ”€â”€ environment_manager.py       # Environment setup and cleanup functions (for future MCP integration)
â”œâ”€â”€ config.yaml                  # Hydra configuration for evaluation
â”œâ”€â”€ dataset.jsonl               # Dataset with tasks and ground truth
â”œâ”€â”€ templates/                   # Template directory structure (for future MCP integration)
â”‚   â””â”€â”€ workspace/
â”‚       â”œâ”€â”€ source_dir/
â”‚       â”‚   â”œâ”€â”€ file_to_move.txt
â”‚       â”‚   â””â”€â”€ sample.txt
â”‚       â”œâ”€â”€ target_dir/
â”‚       â””â”€â”€ backup_dir/
â””â”€â”€ outputs/                     # Generated by reward-kit (evaluation results)
```

## Quick Start

### 1. Prerequisites

```bash
# Ensure you're in the reward-kit repository root
cd /path/to/reward-kit

# Activate virtual environment
source .venv/bin/activate

# Set up Fireworks API key
export FIREWORKS_API_KEY="your_api_key_here"
```

### 2. Run the Example

```bash
# Navigate to the example directory
cd examples/mcp_agent_filesystem_rl

# Run evaluation with reward-kit CLI
python -m reward_kit.cli run --config-path . --config-name config
```

### 3. View Results

```bash
# Check the latest output directory
ls -la outputs/

# View detailed results
cat outputs/YYYY-MM-DD/HH-MM-SS/mcp_filesystem_rl_results.jsonl

# View input/output pairs for analysis
cat outputs/YYYY-MM-DD/HH-MM-SS/preview_input_output_pairs.jsonl
```

## What This Example Does

### Dataset Tasks

The example evaluates the model on three filesystem tasks:

1. **File Move**: Move 'file_to_move.txt' from source_dir to target_dir
2. **File Copy**: Copy all .txt files from source_dir to backup_dir
3. **File Create**: Create 'report.txt' with specified content

### Evaluation Process

1. **Model Generation**: LLM generates responses for each filesystem task
2. **Response Analysis**: Reward function analyzes completion indicators in the text
3. **Task Evaluation**: Compares response quality against ground truth expectations
4. **Scoring**: Provides detailed metrics for task completion quality

### Sample Output

```
[INFO] Sample ID: fs_move_1, Score: 0.30, Reason: Filesystem match: 0.00, Task completion: 0.30
[INFO] Sample ID: fs_copy_1, Score: 0.70, Reason: Filesystem match: 0.00, Task completion: 0.70
[INFO] Sample ID: fs_create_1, Score: 1.00, Reason: Filesystem match: 0.00, Task completion: 1.00
```

## Implementation Details

### Main Reward Function (`main.py`)

Follows the standard reward-kit pattern:

```python
@reward_function
def evaluate(
    messages: List[Message],
    ground_truth: str,
    final_filesystem_state: Optional[Dict[str, Any]] = None,
    task_description: Optional[str] = None
) -> EvaluateResult:
    # Analyzes model response quality
    # Returns detailed evaluation metrics
```

### Configuration (`config.yaml`)

Standard Hydra configuration that:
- Loads dataset from JSONL file
- Configures Fireworks model client
- Sets up evaluation parameters
- References reward function

### Environment Management (`environment_manager.py`)

Functions ready for future MCP integration:
- `setup_environment()`: Initialize isolated environments
- `capture_filesystem_state()`: Extract filesystem state after execution
- `cleanup_environment()`: Clean up resources

## Current vs. Future MCP Integration

### Current Implementation
- âœ… **Model Generation**: LLM generates text responses about filesystem tasks
- âœ… **Text Analysis**: Reward function evaluates response quality
- âœ… **Standard Integration**: Works with reward-kit CLI and Hydra
- âœ… **Metrics**: Task completion scoring based on language indicators

### Future MCP Integration
- ðŸ”„ **Actual Execution**: LLM will interact with real filesystem tools via MCP
- ðŸ”„ **State Verification**: Compare actual filesystem changes vs. expected outcomes
- ðŸ”„ **Isolated Environments**: Each rollout in separate Docker containers
- ðŸ”„ **Tool Usage**: Evaluate proper use of MCP filesystem tools

## Extending This Example

### 1. Add More Tasks

Add entries to `dataset.jsonl`:

```json
{"id": "fs_task_4", "user_query": "List all files in the workspace", "ground_truth_for_eval": "list workspace contents"}
```

### 2. Customize Evaluation

Modify the reward function in `main.py` to:
- Weight different aspects of responses
- Check for specific technical accuracy
- Evaluate step-by-step reasoning

### 3. Try Different Models

Update `config.yaml`:

```yaml
generation:
  model_name: "accounts/fireworks/models/llama-v3p3-70b-instruct"  # or other models
```

### 4. Adjust Evaluation Parameters

```yaml
evaluation_params:
  limit_samples: 10  # Run more tasks
```

## Configuration Reference

### Key Config Sections

```yaml
# Dataset configuration
dataset:
  source_type: "jsonl"
  path_or_name: "dataset.jsonl"

# Model configuration
generation:
  model_name: "accounts/fireworks/models/llama-v3p3-70b-instruct"
  temperature: 0.1
  max_tokens: 2048

# Reward function
reward:
  function_path: "examples.mcp_agent_filesystem_rl.main.evaluate"

# Evaluation settings
evaluation_params:
  limit_samples: 3
```

## Troubleshooting

### Common Issues

1. **Model Not Found**: Update `model_name` in config.yaml to a valid Fireworks model
2. **API Key Issues**: Ensure `FIREWORKS_API_KEY` environment variable is set
3. **Import Errors**: Ensure you're running from the reward-kit repository root
4. **Dataset Format**: Verify dataset.jsonl has `user_query` and `ground_truth_for_eval` columns

### Debug Commands

```bash
# Test dataset loading
python -c "from reward_kit.datasets.loader import load_and_process_dataset; print(load_and_process_dataset(source_type='jsonl', path_or_name='dataset.jsonl'))"

# Test reward function import
python -c "from examples.mcp_agent_filesystem_rl.main import evaluate; print('Import successful')"

# Run with debug logging
PYTHONPATH=. python -m reward_kit.cli run --config-path examples/mcp_agent_filesystem_rl --config-name config
```

## Architecture Benefits

This example demonstrates reward-kit's clean architecture:

- **Separation of Concerns**: Framework handles model orchestration, reward functions focus on evaluation
- **Standard Patterns**: Follows established math_example patterns for consistency
- **Extensibility**: Ready for MCP integration when that framework component is available
- **Flexibility**: Easy to modify tasks, models, and evaluation criteria

The current implementation provides a solid foundation for filesystem task evaluation that will seamlessly extend to actual MCP agent execution in the future.
